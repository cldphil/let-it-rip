{
  "id": "http://arxiv.org/abs/2506.00963v1",
  "title": "From Objectives to Questions: A Planning-based Framework for Educational\n  Mathematical Question Generation",
  "summary": "Automatically generating high-quality mathematical problems that align with\neducational objectives is a crucial task in NLP-based educational technology.\nTraditional generation methods focus primarily on textual quality, but they\noften overlook educational objectives. Moreover, these methods address only\nsingle-dimensional, simple question generation, failing to meet complex,\nmultifaceted educational requirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset of 16k mathematical questions with\nmulti-dimensional educational objectives. Based on this dataset, we developed\nEQGEVAL, which incorporates three evaluation dimensions and is designed to\nassess the ability of models to generate educational questions. Drawing\ninspiration from teachers' problem design processes, we propose the Educational\nQuestion Planning with self-Reflection (EQPR) method for educational\nmathematical question generation, following a \"plan-evaluate-optimize\"\napproach. Specifically, by combining planning algorithm based on Monte Carlo\nTree Search with the generative capabilities of Large Language Models, we\ncontinuously optimize questions through iterative feedback. This\nself-optimization mechanism ensures that the generated questions both fit the\neducational context and strategically achieve specific basic educational\nobjectives. Through extensive experiments based on EQGEVAL, we have\ndemonstrated that EQPR achieves significant improvements in generating\nquestions that meet multi-dimensional educational objectives.",
  "authors": [
    "Cheng Cheng",
    "Zhenya Huang",
    "Guanhao Zhao",
    "Yuxiang Guo",
    "Xin Lin",
    "Jinze Wu",
    "Xin Li",
    "Shijin Wang"
  ],
  "published": "2025-06-01T11:23:18Z",
  "updated": "2025-06-01T11:23:18Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00963v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00963v1  [cs.CL]  1 Jun 2025From Objectives to Questions: A Planning-based Framework for\nEducational Mathematical Question Generation\nCheng Cheng1,2, Zhenya Huang1,2,3*,Guanhao Zhao1,2, Yuxiang Guo1,2, Xin Lin1,2,\nJinze Wu2,4, Xin Li2,4, Shijin Wang2,4\n1University of Science and Technology of China\n2State Key Laboratory of Cognitive Intelligence\n3Institute of Artificial Intelligence, Hefei Comprehensive National Science Centerce\n4iFLYTEK Research\n{doublecheng, ghzhao0223, guoyx18, linx}@mail.ustc.edu.cn\n{huangzhy, leexin}@ustc.edu.cn\n{jzwu4, sjwang3}@ifytek.com Abstract\nAutomatically generating high-quality mathe-\nmatical problems that align with educational\nobjectives is a crucial task in NLP-based ed-\nucational technology. Traditional generation\nmethods focus primarily on textual quality,\nbut they often overlook educational objectives.\nMoreover, these methods address only single-\ndimensional, simple question generation, fail-\ning to meet complex, multifaceted educational\nrequirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset\nof 16k mathematical questions with multi-\ndimensional educational objectives. Based on\nthis dataset, we developed EQGEV AL, which\nincorporates three evaluation dimensions and\nis designed to assess the ability of models to\ngenerate educational questions. Drawing in-\nspiration from teachers’ problem design pro-\ncesses, we propose the Educational Question\nPlanning with self-Reflection (EQPR) method\nfor educational mathematical question genera-\ntion, following a \"plan-evaluate-optimize\" ap-\nproach. Specifically, by combining planning\nalgorithm based on Monte Carlo Tree Search\nwith the generative capabilities of Large Lan-\nguage Models, we continuously optimize ques-\ntions through iterative feedback. This self-\noptimization mechanism ensures that the gener-\nated questions both fit the educational context\nand strategically achieve specific basic educa-\ntional objectives. Through extensive experi-\nments based on EQGEV AL, we have demon-\nstrated that EQPR achieves significant improve-\nments in generating questions that meet multi-\ndimensional educational objectives.\n1 Introduction\nMathematical questions are fundamental elements\nof educational assessment and cognitive develop-\nment, playing an irreplaceable role in cultivating\n*Corresponding author\nFigure 1: A simple example of question design based\non educational objectives\nstudents’ logical thinking and problem-solving abil-\nities (Kurdi et al., 2020; Liu et al., 2019). When de-\nsigning such questions, educators coordinate mul-\ntiple educational objectives(e.g., concepts, mathe-\nmatical qualities ), not only to accurately evaluate\nstudents’ degree of knowledge mastery and appli-\ncation skills but also to enhance their comprehen-\nsive problem-solving capabilities through progres-\nsive challenges. Figure 1 illustrates the system-\natic process of how teachers design test questions\n(Kliebard, 1970; Wiggins, 2005). Starting with\neducational objectives, teachers carefully consider\nmultiple dimensions including core concepts, com-\npetencies to be assessed, and real-world scenarios.\nThey strategically combine these elements to create\nengaging questions that connect theoretical knowl-\nedge with practical applications. Throughout this\nprocess, teachers continuously evaluate whether\ntheir questions adequately cover all intended objec-\ntives and make necessary refinements.\nHowever, in the literature, the generation of\nmathematics questions that consider multiple edu-\ncational objectives has not received sufficient atten-\ntion. In widely studied mathematical question gen-\neration, previous work has relied solely on manu-\nally crafted templates for generation (Polozov et al.,\n--- Page 2 ---\n2015; Khodeir et al., 2018), unable to generate\ncontent based on educational objectives expressed\nin natural language. Conversely, most more ad-\nvanced works employ seq2seq models for genera-\ntion (Zhou and Huang, 2019; Liu et al., 2020), but\ndue to model limitations, their applications remain\nconfined to question generaton based on mathe-\nmatical formulas such as arithmetic operations and\nequation systems. With the emergence of large lan-\nguage models like ChatGPT (Kojima et al., 2022)\nand LLaMA (Dubey et al., 2024), which signifi-\ncantly enhance the ability to generate diverse and\ncomplex content, researchers have begun using few-\nshot learning or Chain-of-Thought templates for\nquestion generation, framing it as a goal-based rea-\nsoning task. However, question generation is a com-\nplex task, and single-step reasoning may lead to\nfailures, as seen in the teacher’s first attempt in Fig-\nure 1. Consequently, the limitations of single-step\nreasoning—its inherent inability to handle com-\nplex problems requiring multi-step inferences and\nits lack of reflective capabilities to check for er-\nrors—significantly hinder the full potential of large\nlanguage models in question generation.\nTo validate the use of large language models for\ncreating educational questions, we empirically as-\nsess whether these models can match human teach-\ners’ ability to design problems that achieve specific\neducational objectives. However, existing mathe-\nmatics question generaton datasets present signifi-\ncant limitations. Most datasets focus primarily on\nelementary-level mathematics (e.g., LMWP (Liu\net al., 2020), HMWP (Qin et al., 2020)), offering a\nnarrow scope of assessment. Furthermore, a lack\nof comprehensive annotation is prevalent; the ma-\njority of these datasets (e.g., Gaokao-bench (Zhang\net al., 2023), GSM8K (Cobbe et al., 2021)) lack\nannotations for educational objectives, hindering a\nthorough evaluation of LLMs’ capabilities in math-\nematical question generation.\nIn this paper, to advance the field further, we\npresent two datasets, EduMath-SQ and EduMath-\nCQ, which are filtered from real high school test\npapers and annotated with educational objectives.\nSpecifically, for each question, we annotate four\nto five categories of educational objectives based\non Tyler’s rationale (Kliebard, 1970) and two-\nway specification table (Odiagbe, 2016), aiming to\ncomprehensively evaluate models’ objective-based\nmathematical question generation capabilities.\nAlong this direction, several technical challenges\nremain. First, there is a lack of evaluation metrics.Past question generation work has adopted text\nquality metrics such as BLEU and ROUGE. For\ninstance, in Figure 1, while the initial and mod-\nified questions show high textual similarity, the\ninitial question lacks a concept compared to the\nlatter. However, these traditional text generation\nevaluation metrics often fail to effectively measure\nmathematical question generation quality, particu-\nlarly in terms of educational objective alignment.\nSecond, questions with multidimensional educa-\ntional objectives typically cannot be generated in\na single attempt, as the complexity of multiple ob-\njectives often leads to certain objectives being over-\nlooked or missed in a single generation attempt,\nrequiring repeatedly evaluated and optimized by\neducators. Additionally, a single dimension of\nan educational objective may encompass multiple\ncomponents—as illustrated in Figure 1, where the\n\"concepts\" dimension includes both sine functions\nand elevation angles—further increasing the chal-\nlenge of accurately generating questions that satisfy\nmultidimensional educational objectives.\nTo address these challenges, we first propose\nEQGEV AL, a novel evaluation benchmark based\non the EduMath dataset that comprehensively eval-\nuates question generation quality through LLM us-\ning three metrics: solvability (question feasibility),\nPass Rate (objective fulfillment), and Win Rate\n(comparison with gold standards). Furthermore,\nin real-world educational settings, teachers typi-\ncally design mathematical questions through an\niterative process, repeatedly evaluating and opti-\nmizing questions based on educational objectives\nand student feedback. Inspired by this practice,\nwe develop a methodology that mirrors this sys-\ntematic refinement process. Specifically, we de-\nvelop the Educational Question Planning with\nself-Reflection (EQPR) method, which combines\nMonte Carlo Tree Search (MCTS) with large lan-\nguage models to simulate diverse question design\nstrategies, achieve educational objectives, and sys-\ntematically evaluate and refine the question cre-\nation process through continuous feedback.\nIn summary, our contributions are:\n•We introduce a comprehensive dataset derived\nfrom authentic educational assessments, to-\ngether with EQGEV AL, a novel benchmark\nframework designed to evaluate large lan-\nguage models’ proficiency in generating ques-\ntions that are precisely aligned with specified\neducational objectives.\n--- Page 3 ---\n•Our framework, EQPR, integrates Monte\nCarlo Tree Search (MCTS) for strategic plan-\nning with a reflection mechanism that evalu-\nates question quality and proposes improve-\nments, enabling iterative refinement of the\ngenerated questions.\n•Empirical validation through extensive exper-\nimentation underscores our framework’s ef-\nficacy. When implemented with two distinct\nLLMs and evaluated on the EQGEV AL bench-\nmark, EQPR achieved superior performance\nacross nearly all evaluation metrics, establish-\ning new state-of-the-art results.\n2 Related Works\n2.1 Question Generation\nQuestion generation is a significant research direc-\ntion in educational technology (Kurdi et al., 2020;\nZhao et al., 2024a,b). Its core function lies in auto-\nmatically generating questions from structured or\nnatural language text, such as deriving questions\nfrom dialogue content (Guo et al., 2024; Liu et al.,\n2024) or extracting them from story texts (Li and\nZhang, 2024). The key value of this technology is\nthat it can substantially reduce the time and cost of\nmanual design and construction of questions, while\nalso dynamically generating questions of varying\ndifficulty and type based on the content.\nIn mathematics education, question generation\nhas been a significant area of research. Early\nwork focused on generating mathematical ques-\ntions based on mathematical formulas and scenar-\nios, relying on predefined templates and rule-based\nmethods. Researchers developed language knowl-\nedge bases and rhetorical structure rules to aid in\nquestion generation (Khodeir et al., 2018). As natu-\nral language processing evolved, the field shifted to-\nward neural network-based approaches. Sequence-\nto-sequence frameworks for mathematical question\ngeneration (Zhou and Huang, 2019) integrated the-\nmatic and formulaic information using attention\nmechanisms. Building on pre-trained language\nmodels, later work improved topic word selection\nand introduced question-solving modules to en-\nhance the solvability of generated questions (Wang\net al., 2021). MapKG (Qin et al., 2023a), inspired\nby educational experts’ test design experience, ad-\nvanced the field with a \"plan-then-generate\" strat-\negy that incorporated dual attention mechanisms\nand knowledge graphs to improve question solvabil-ity and diversity. Recent developments in large lan-\nguage models have opened new research avenues,\nwith methods such as gradient-based techniques\nbeing used to generate questions with controlled\ndifficulty levels (Lin et al., 2024). However, exist-\ning studies have largely overlooked the generation\nof questions that address multi-dimensional educa-\ntional objectives. Our proposed EQPR model aims\nto fill this gap.\n2.2 LLM Planning and Reflection\nRecent research on planning with large language\nmodels (LLMs) has witnessed remarkable ad-\nvancement, significantly impacting domains such\nas common-sense reasoning (Zhu et al., 2023;\nXue et al., 2024) and embodied intelligence (Sun\net al., 2024). A central focus has been enhanc-\ning LLMs’ capacity for systematic, step-by-step\nproblem solving. Foundational techniques like\nChain-of-Thought (CoT) prompting (Wei et al.,\n2022) have established the paradigm of incre-\nmental problem decomposition, while more so-\nphisticated approaches such as Tree-of-Thoughts\n(ToT)(Yao et al., 2024) and Reasoning via Plan-\nning (RAP)(Hao et al., 2023) explore solution\nspaces through hierarchical tree structures, leverag-\ning methodologies like Monte Carlo Tree Search\nto systematically expand the search space. How-\never, extensive iterative planning processes can in-\ntroduce cumulative error propagation. To address\nthis limitation, Self-Refine (Madaan et al., 2024)\nincorporates iterative feedback mechanisms that\nenable models to reflect upon and refine their rea-\nsoning processes, thereby mitigating error accumu-\nlation. Distinguished from these planning-focused\nmethodologies, our work introduces EQPR, which\nsynergistically combines iterative planning with\nreflective mechanisms to optimize question genera-\ntion processes.\n3 Problem Formulation\nAs outlined in Section 1, our objective is to lever-\nage large language models for generating questions\nthat align with specified educational objectives\nO, encompassing fundamental goals (e.g., concep-\ntual understanding, Bloom’s taxonomy levels) and\ntheir contextual frameworks (e.g., traditional cul-\ntural narratives, sports scenarios), formalized as\nq=LLM (O).\nHowever, single-pass question generation often\nyields limited quality and inadequate alignment\n--- Page 4 ---\nFigure 2: The overall framework of EQPR.The upper part of the figure illustrates the entire EQPR process, which\nuses an MCTS structure for deep reasoning and iteratively improves question quality via the Critic and Reflection\nmodules. The lower part shows a simplified state transition example, where a previous question is modified based\non feedback and progresses to the next state.\nwith educational objectives. To address this limita-\ntion, we draw inspiration from teachers’ iterative\nimprovement processes, modeling this as a sequen-\ntial decision-making problem and formalizing it as\na Markov Decision Process (MDP).\nWe formalize this question improvement pro-\ncess as an MDP defined by the tuple (S,A,T, R),\nwhere:\n•State space S: Each state st∈ S represents\nthe current version of an educational ques-\ntion at time step t, denoted as qt. States may\ninclude metadata such as alignment with ed-\nucational objectives O, revision history, or\nevaluative metrics.\n•Action space A: Each action at∈ A corre-\nsponds to an improvement operation targeting\nspecific dimensions of the question, such as\n“enhancing conceptual clarity” or “increasing\nreal-world relevance.” As shown in the upper\npart of Figure 2, these actions are formulated\nas structured feedback that guides iterative im-\nprovement, offering detailed analysis of cur-\nrent shortcomings paired with actionable im-\nprovement strategies.\n•Reward function R:S × A → R: After\nexecuting action atin state st, the system re-ceives a reward based on the revised ques-\ntion’s quality. This reward reflects multiple\neducational criteria including conceptual clar-\nity, cognitive depth, and contextual appropri-\nateness, assessed by a critic module providing\nboth numerical scores and formative feedback\n(detailed in Section 4.2).\n•Transition function T:S × A → S : The\nsystem transitions to a new state st+1after\napplying action atto the current question st.\nThis process is implemented by a reflection\nmodule , which interprets the critic’s feedback\nand generates an improved question accord-\ningly (detailed in Section 4.3).\nAs illustrated in Figure 2, the process operates\nas follows: Given a current state stand educa-\ntional objectives O, the critic module analyzes\nthe question and samples an improvement action\nat∼Critic (a|st, O). Simultaneously, it assigns a\nreward rtand identifies specific improvement areas.\nThis feedback is passed to the reflection module,\nwhich translates it into actionable revision instruc-\ntions and generates an enhanced question better\naligned with educational objectives. For instance,\nif the action is to “increase cognitive depth,” the\nreflection module may introduce more abstract or\nhigher-order reasoning components. The newly\n--- Page 5 ---\ngenerated question becomes the next state st+1,\nand the process continues iteratively until optimal\neducational alignment is achieved.\n4 Method\n4.1 Overview\nDrawing inspiration from educators’ systematic ap-\nproach to question development (Wang et al., 2022)\n- which encompasses planning, writing, evaluat-\ning, and optimizing - we introduce an innovative\nframework for generating educational questions\nthat ensures both quality and alignment with ed-\nucational objectives. The framework consists of\nthree modules: the Critic module, which evaluates\neach generated question (state) based on multiple\ndimensions of educational objectives and provides\ndirections for modification; the Reflection mod-\nule, which analyzes the feedback from the Critic\nto determine optimization directions and refine the\nquestion generation process, leading to the creation\nof new questions; and the MCTS-based Planning\nmodule, directed by the Critic and Reflection mod-\nules, which provide the necessary guidance and\nconstraints to enable it to systematically and effi-\nciently navigate the vast and multifaceted search\nspace, thereby exploring a wide range of potential\nquestion structures and formulations.\n4.2 Critic\nWe employ the Critic module to systematically\nevaluate the alignment between generated ques-\ntions and educational objectives. Through Large\nLanguage Models (LLMs) equipped with Critic\nprompts (see Table 8 ) and Chain-of-Thought rea-\nsoning, the module performs comprehensive assess-\nment across multiple dimensions. Specifically, for\na state stat phase t and educational objective O, the\nCritic generates question modification directions\natandscore tas follows:\n(score t, at) =Critic (st, O), (1)\nHere, score trepresents the quantitative evaluation\nscore, while atdenotes the suggested modification\ndirections. The evaluation examines concept cov-\nerage, contextual relevance, conceptual coherence,\netc., ensuring comprehensive assessment of educa-\ntional requirements, and logical interconnections.\nThis structured evaluation framework enables pre-\ncise identification of gaps between generated ques-\ntions and desired educational objectives, facilitat-\ning targeted improvements in subsequent iterations.4.3 Reflection\nCognitive science research demonstrates that hu-\nmans continuously iterate and reflect upon their\nthinking based on new information (Frederick,\n2005), allowing them to correct errors and deepen\nunderstanding. This reflective process is particu-\nlarly evident in educational settings, where teach-\ners systematically refine questions through iterative\nevaluation and improvement to align with educa-\ntional objectives.\nConsider the example illustrated in the bottom\npart of Figure 2, where the initial question genera-\nton lacks contextual materials. Through reflection,\nthis limitation is identified, leading to an improved\niteration that incorporates a practical tree-planting\nscenario, making the mathematical concept more\naccessible and applicable.\nTo formalize this reflective iteration process, we\nintroduce a Reflection module powered by large\nlanguage models. This module employs special-\nized reflection prompts (detailed in the Table 9) to\nanalyze the historical trajectory of question states\nand improvement suggestions, ensuring alignment\nwith educational objectives. In the iteration step t,\nwe construct the historical trajectory τas:\nτ={(s0, a0),(s1, a1), . . . , (st−1, at−1)},(2)\nwhere s0represents the initial question state gen-\nerated based on the initial generation prompt (see\nTable 7) at the root node, a0denotes the Critic mod-\nule’s modification suggestions for initial question\ns0,stindicates the preceding state of the current\nnode, and atrepresents the Critic module’s modifi-\ncation suggestions for question st. This trajectory\ncaptures the complete history of question states and\ntheir corresponding improvement suggestions up\nto the current iteration. The next state generation\nis accomplished through the Reflection module:\nst+1=Reflection (O, τ, s t, at), (3)\nwhere Orepresents the educational objectives.\nThis formalized reflection mechanism enables\nthe question generation process to emulate human\nteachers’ approach, continuously iterating to en-\nhance problem quality.\n4.4 MCTS-based Planning\nWe combine large language models (LLMs) with\nMonte Carlo Tree Search (MCTS) to better meet\n--- Page 6 ---\nour educational objectives. MCTS efficiently ex-\nplores the question-generation space, balancing ex-\nploration and exploitation to improve the educa-\ntional value of the resulting questions. As shown\nin the top panel of Figure 2, we model the pro-\ncess as a search tree: each node represents a par-\ntially generated question, and each edge denotes an\nediting action—such as extending or rewriting the\ntext. The overall planning loop follows the standard\nMCTS phases of selection, expansion, simulation,\nand backpropagation, detailed in Algorithm 1 in\nthe Appendix.\nSelection. The selection phase chooses the most\npromising node from the tree’s branches for fur-\nther exploration. Starting from the root node s0\n(initial question), the selection phase iteratively\nchooses the most promising nodes for exploration.\nTo balance between known high-quality question\nstructures (exploitation) and exploring new direc-\ntions for improvement (exploration), we use the\nwell-known Upper Confidence Bounds applied to\nTrees (UCT) (Kocsis and Szepesvári, 2006) for\nnode selection, as shown below:\nUCT (st, at) =Q(st, at) +cq\nlnN(st)\nN(ch(st,at)),(4)\nwhere Q(st, at)is the potential future reward of\napplying action atat time t,N(st)is the number\nof visits to node st,ch(st, at)is the child node\nreached after executing action atin state st, andc\nis a constant used to adjust exploration. At each\nlevel of the tree, the child node with the highest\nUCT value is selected.\nExpansion. During the expansion phase, we\nutilize the Reflection module to generate new\nquestion states. This module leverages the his-\ntorical optimization trajectory τto analyze pat-\nterns from previous modifications, generating\nnew candidate questions according to: st+1=\nReflection (O, τ, s t, at). To explore a wider range\nof question designs, multiple nodes are generated\nas child nodes in each expansion step.\nSimulation. From the expanded nodes, the simula-\ntion phase explores potential question optimization\npaths via simulations. In each simulation, the sys-\ntem evaluates options based on estimated cumula-\ntive rewards, selecting the highest-reward path for\nfurther exploration. This continues until a terminal\nstate, yielding a comprehensive simulation of po-\ntentially effective question optimization schemes.\nBackpropagation. When a simulation reaches a\nterminal state, backpropagation begins, using thecumulative future reward obtained at the terminal\nnode (illustrated by the red arrows in the central\nMCTS tree section of Figure 2) to update the Q-\nvalues of each state-action pair, with the aim of\nrefining future question selection.\n5 Educational QG Dataset & Benchmark\nExisting mathematical question generaton datasets\nprimarily focus on elementary-level content and\noften lack comprehensive educational annotations.\nTo address these limitations, we developed Edu-\nMath, aiming to create a high-quality dataset with\nannotation accuracy exceeding 90% across all ed-\nucational dimensions. We developed this multi-\ndimensional annotation framework based on the\nfollowing considerations: concept mapping reflects\nknowledge coverage, ability assessment captures\nproblem-solving requirements, Bloom’s Taxonomy\nrepresents cognitive levels, mathematical literacy\nevaluation aligns with educational objectives, and\nreal-world context identification demonstrates prac-\ntical value. These dimensions characterize the edu-\ncational attributes of mathematical questions from\ndistinct perspectives. We sourced 16k high-school\nlevel mathematics problems from mock exams and\ncollege entrance examinations, implementing strict\nquality filters to exclude problems with images or\nincomplete solutions. Using DeepSeek-V3, we\nconduct an iterative three-round annotation pro-\ncess. Initial annotations are reviewed for accuracy\nand consistency. Specifically, we employ Chain-\nof-Thought prompting to guide multiple large lan-\nguage models in evaluating annotation correctness\nthrough a voting mechanism. Annotations flagged\nas inaccurate are re-annotated to resolve identified\nissues. This rigorous, multi-stage process results\nin an annotation accuracy of 95.2% across all di-\nmensions. The final dataset includes two variants:\nEduMath-SQ (Standard Questions) and EduMath-\nCQ (Contextual Questions), with EduMath-SQ fo-\ncusing exclusively on non-contextual problems.\nBuilding on this, we define controllable educa-\ntional question generation and corresponding eval-\nuation metrics. Previous question generation ap-\nproaches typically rely on text quality metrics such\nas BLEU and ROUGE for evaluation, but these are\nfar from sufficient. Truly effective questions must\nbe solvable and meet teachers’ instructional and as-\nsessment needs. Following TOOLEV AL (Qin et al.,\n2023b) and based on DeepSeek-V3, we propose\nEQGEV AL, which includes the following metrics\n--- Page 7 ---\n(see Appendix A.4 for details): Solvability as a fun-\ndamental requirement that the generated problems\nmust have valid solutions; Pass Rate measuring\nthe proportion of generated problems that meet the\neducational objectives; and Win Rate where we\npresent educational objectives and two problems to\nDeepSeek-V3 evaluators, asking them to determine\nwhich problem better serves the intended purpose.\nTo ensure the reliability of our evaluation, all met-\nrics are determined through a majority voting mech-\nanism, where multiple independent evaluations are\nconducted to derive the final assessment results.\n6 Experiment\n6.1 Experimental Settings\nDatasets. As outlined in Section 5, we conducted\nexperimental evaluations using the EduMath-SQ\nand EduMath-CQ datasets. We randomly selected\n10% of the data to serve as the test set. The\nEduMath-CQ dataset comprises 589 educational\nobjectives paired with their corresponding gold-\nstandard questions, while EduMath-SQ contains\n1,034 educational objectives along with their re-\nspective gold-standard questions. For more de-\ntailed information about these datasets, please refer\nto Appendix A.1.\nBaselines. Since this is a novel task, we ap-\nproach it as a reasoning problem and benchmark\nit against established reasoning methods, includ-\ning Chain-of-Thought (CoT) (Wei et al., 2022),\nCoT-BON, ReAct (Yao et al., 2023), and the\ntree-structured DEAR method (Xue et al., 2024).\nDeepSeek-V3 (Deepseek, 2024) and GPT-4o-Mini\n(OpenAI, 2024a) serve as our primary backbone\nmodels. Additionally, we evaluated Claude-3.5\n(Anthropic, 2024) and GPT-4o (OpenAI, 2024b) on\nthe EduMath-CQ dataset; for those results, please\nrefer to Appendix B.\nMetrics. We evaluate our approach using both\nautomatic metrics and human assessment. For\nautomatic evaluation, we primarily employ three\nmetrics from the EQGEV AL framework, where\nthe Win Rate metric compares generated ques-\ntions against gold-standard problems. To com-\nprehensively assess text quality, we also incorpo-\nrate widely-adopted natural language generation\nmetrics, including Rouge-L (Lin and Och, 2004),\nBLEU (Papineni et al., 2002), METEOR (Baner-\njee and Lavie, 2005), and BERTSCORE (Zhang\net al., 2020). For human evaluation, we design twometrics: Fluency and Human-rated Win Rate. The\nFluency metric uses a three-level scoring system\n(0-2), with detailed scoring criteria provided in the\nAppendix A.7.\n6.2 Main Results\nWe conduct experiments to verify the effective-\nness of our framework EQPR, and report the re-\nsults in Table 1. We get the following observa-\ntions. Primarily, our method demonstrates superior\nperformance in Win Rate and Pass Rate across\nboth EduMath-CQ and EduMath-SQ datasets, as\nwell as across both GPT-4o-Mini and DeepSeek\nmodels. This comprehensive outperformance val-\nidates the effectiveness of our approach. Notably,\nwhen testing with DeepSeek on EduMath-CQ, our\nmethod achieved a Win Rate of 46.23%, surpassing\nthe next-best method DEAR (41.8%) by a substan-\ntial margin of 4.42%. This improvement clearly\ndemonstrates the efficacy of our iterative refine-\nment strategy. Nevertheless, we observed that Pass\nRates consistently decreased when transitioning\nfrom standard questions (EduMath-SQ) to contex-\ntual scenarios (EduMath-CQ), suggesting that large\nlanguage models still face challenges in seamlessly\nincorporating contextual elements while addressing\nmultiple educational objectives.\nRegarding Solvability, our method achieves near-\noptimal performance, ranking second across both\nsub-datasets. The slight reduction in solvability\nscores can be attributed to the nature of our itera-\ntive refinement process, where questions naturally\nevolve to become more sophisticated through mul-\ntiple iterations.\nIn terms of traditional text quality metrics, our\nmethod maintains competitive performance while\nprioritizing educational effectiveness. For in-\nstance, in the EduMath-SQ dataset with DeepSeek,\nthe modest difference between our method’s\nBERTScore (74.79) and REACT’s (76.05) is ac-\nceptable, considering that these metrics primarily\nassess lexical and semantic similarities rather than\neducational value. Our superior Win Rate and Pass\nRate scores underscore our method’s success in\nachieving its primary objective: generating edu-\ncationally meaningful and high quality questions,\neven if this occasionally leads to slightly lower\nlinguistic metric scores.\n6.3 Human Evaluation Results\nTo thoroughly assess the effectiveness of our\nmethodology, we carried out human evaluation\n--- Page 8 ---\nTable 1: Evaluation results on datasets EduMath-CQ and EduMath-SQ(%).\nDataset Method BLEU METEOR ROUGE-L BERTSCORE WIN RATE SOLV ABLE PASS RATE\nEduMath-CQGPT-4o-Mini\nCOT 11.60 29.64 24.45 68.52 35.78 90.12 27.88\nCOT-BON 8.52 22.59 10.56 69.19 34.50 90.23 21.25\nREACT 18.90 53.20 36.72 72.55 38.40 89.32 33.66\nDEAR 13.31 23.09 21.74 70.34 35.18 85.77 27.35\nEQPR 24.80 52.70 48.13 70.89 39.92 90.63 35.81\nDeepSeek-V3\nCOT 12.11 42.90 22.51 70.64 38.20 90.84 31.45\nCOT-BON 13.58 43.60 28.93 70.85 38.99 90.25 29.63\nREACT 21.76 47.98 47.06 70.32 39.75 90.64 38.91\nDEAR 20.71 49.06 42.11 74.48 41.81 92.59 37.88\nEQPR 20.33 46.86 44.57 71.02 46.23 91.73 43.11\nEduMath-SQGPT-4o-Mini\nCOT 1.76 21.04 7.64 67.30 32.20 83.65 64.60\nCOT-BON 2.63 30.44 11.76 68.43 36.64 85.23 75.12\nREACT 36.72 52.19 36.72 72.55 36.81 82.90 83.22\nDEAR 8.51 20.52 4.71 66.34 36.70 81.89 82.36\nEQPR 32.77 52.75 52.78 68.83 37.18 84.40 84.37\nDeepSeek-V3\nCOT 28.80 56.91 50.33 75.38 37.40 87.15 85.46\nCOT-BON 30.66 61.50 52.07 76.05 42.70 86.88 88.82\nREACT 26.46 60.47 46.05 75.45 44.45 85.95 89.76\nDEAR 24.52 58.23 40.07 73.48 40.36 87.14 82.74\nEQPR 29.71 57.29 49.63 74.79 45.65 91.31 91.50\nMethod Clarity Win Rate-Human(%)\nCOT 1.83 27.00\nCOT-BON 1.78 33.67\nDEAR 1.84 31.00\nREACT 1.87 35.00\nEQPR 1.93 36.67\nFleiss’ kappa 0.71 0.47\nTable 2: Human evaluation results on EduMath-\nCQ(DeepSeek-V3).\nexperiments. We randomly selected 100 samples\nfrom the EduMath-CQ dataset and enlisted three\nhighly educated evaluators, each with at least a\nbachelor’s degree, to conduct the assessments. The\nevaluation focused on two key dimensions: ques-\ntion clarity (whether the questions are easily read-\nable and understandable) and quality comparison.\nGiven the potential biases in large language models\nwhen determining win rates, the evaluators were\nasked to compare the quality of the generated ques-\ntions against the gold-standard questions. As de-\npicted in Table 2, our method demonstrated supe-\nrior performance in both clarity scores and human-\nevaluated win rates. Furthermore, we evaluated the\nreliability of the annotations using Fleiss’ Kappa\ncoefficient. The Kappa values for both clarity and\nquality comparison exceeded the credibility thresh-old of 0.41, thereby confirming the reliability of\nour evaluation results.\n6.4 Ablation Results\nEffect of Reflection Module. To validate the ef-\nfectiveness of the Reflection Module, we conducted\na comparative experiment. After removing the Re-\nflection Module, the model no longer optimizes\nbased on iterative feedback but directly generates\nquestions according to the target (referred to as the\n\"w/o Reflection\" experiment). The results showed\nsignificant performance degradation across both\nmodels: using DeepSeek as an example, illustrated\nin Figure 3, the Pass Rate decreased from 46.23%\nto 43.51%, and the Win Rate declined from 43.11%\nto 40.74%. These observations highlight the vital\nimportance of the Reflection Module. By engaging\nin multiple rounds of iterative optimization, the sys-\ntem adeptly integrates knowledge from previously\nencountered questions, enabling it to produce con-\ntent that more effectively aligns with educational\nobjectives.\nEffect of MCTS-based Planning. To quantify\nthe contribution of MCTS-based planning within\nour proposed framework, we conducted an ablation\nstudy by removing the MCTS component and em-\nploying greedy search exclusively for action selec-\ntion. As demonstrated in Figure 3, the model’s per-\n--- Page 9 ---\nFigure 3: The results of ablation studies. We test differ-\nent methods on EduMath-CQ datasets\nformance deteriorated substantially without MCTS-\nbased planning. Specifically, DeepSeek experi-\nenced a notable decline in both pass rate (from\n46.23% to 41.57%) and win rate (from 43.11% to\n38.79%) when operating without Monte Carlo Tree\nSearch. We attribute this performance degradation\nto MCTS-based planning’s superior capability in\nnavigating the question optimization space through\nits dual mechanism of prospective outcome predic-\ntion and retrospective evaluation. These empirical\nfindings validate the critical role of MCTS integra-\ntion in our framework’s effectiveness.\n7 Conclusion\nIn this paper, we introduced EQPR (Educational\nQuestion Planning with self-Reflection), an innova-\ntive framework for generating high-quality mathe-\nmatical questions that align with educational objec-\ntives. EQPR integrates a \"plan-evaluate-optimize\"\nprocess, combining Monte Carlo Tree Search with\nthe generative power of LLMs, enabling continu-\nous refinement through feedback optimization. We\nalso introduced EduMath, a high-quality dataset\nof 16k mathematics problems, and EQGEV AL, a\ncomprehensive framework for evaluating the edu-\ncational value of generated questions. Extensive\nexperiments demonstrate that EQPR outperforms\nexisting reasoning methods on key educational met-\nrics across multiple large language models.\nAcknowledgement\nThis research was partially supported by\nthe National Science and Technology Major\nProject(No.2022ZD0117103), the National\nNatural Science Foundation of China (Grants\nNo.62477044), Anhui Provincial Natural Science\nFoundation (No. 2308085QF229), the Fundamen-\ntal Research Funds for the Central Universities\n(No.WK2150110038). Zhenya Huang gratefully\nacknowledges the support of the Young EliteScientists Sponsorship Program by CAST (No.\n2024QNRC001)\nLimitations\nOur research primarily focused on mathematical\nquestion generation and has not yet been extended\nto other subject areas. This presents an important\ndirection for future research. Additionally, we face\ncertain challenges in establishing educational ob-\njectives, particularly in the assessment of question\ndifficulty. Since difficulty evaluation is largely sub-\njective and challenging to standardize, this remains\na significant hurdle in the field that requires further\ninvestigation. Furthermore, while we utilize large\nlanguage models for evaluation, these models may\nexhibit certain biases, and their assessment results\ndo not always align perfectly with the professional\njudgment of human educators. Consequently, ex-\nploring effective methods to align human evalua-\ntion with LLM-based assessments and establishing\na more accurate evaluation system remains a cru-\ncial direction for future research.\nReferences\nAnthropic. 2024. Introducing claude 3.5 son-\nnet. https://www.anthropic.com/news/\nclaude-3-5-sonnet .\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments. In Proceedings of\nthe Workshop on Intrinsic and Extrinsic Evaluation\nMeasures for Machine Translation and/or Summa-\nrization@ACL 2005 , pages 65–72.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nDeepseek. 2024. DeepSeek-V3. https://\nhuggingface.co/deepseek-ai/DeepSeek-V3 .\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 .\nShane Frederick. 2005. Cognitive reflection and de-\ncision making. Journal of Economic perspectives ,\n19(4):25–42.\nShasha Guo, Lizi Liao, Jing Zhang, Cuiping Li, and\nHong Chen. 2024. Pcqpr: Proactive conversational\nquestion planning with reflection. arXiv preprint\narXiv:2410.01363 .\n--- Page 10 ---\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. 2023. Rea-\nsoning with language model is planning with world\nmodel. arXiv preprint arXiv:2305.14992 .\nNabila Ahmed Khodeir, Hanan Elazhary, and Nayer\nWanas. 2018. Generating story problems via con-\ntrolled parameters in a web-based intelligent tutoring\nsystem. The International Journal of Information\nand Learning Technology , 35(3):199–216.\nHerbert M Kliebard. 1970. The tyler rationale. The\nSchool Review , 78(2):259–272.\nLevente Kocsis and Csaba Szepesvári. 2006. Bandit\nbased monte-carlo planning. In European conference\non machine learning , pages 282–293. Springer.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems , 35:22199–\n22213.\nGhader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and\nSalam Al-Emari. 2020. A systematic review of auto-\nmatic question generation for educational purposes.\nInternational Journal of Artificial Intelligence in Ed-\nucation , 30:121–204.\nKunze Li and Yu Zhang. 2024. Planning first, question\nsecond: An llm-guided method for controllable ques-\ntion generation. In Findings of the Association for\nComputational Linguistics ACL 2024 , pages 4715–\n4729.\nChin-Yew Lin and FJ Och. 2004. Looking for a few\ngood metrics: Rouge and its evaluation. In Ntcir\nworkshop , pages 1–8.\nFan Lin, Shuyi Xie, Yong Dai, Wenlin Yao, Tianjiao\nLang, Zishan Xu, Zhichao Hu, Xiao Xiao, Yuhong\nLiu, and Yu Zhang. 2024. Idgen: Item discrimination\ninduced prompt generation for llm evaluation. arXiv\npreprint arXiv:2409.18892 .\nJiayu Liu, Zhenya Huang, Tong Xiao, Jing Sha, Jinze\nWu, Qi Liu, Shijin Wang, and Enhong Chen. 2024.\nSocraticlm: Exploring socratic personalized teaching\nwith large language models. Advances in Neural\nInformation Processing Systems , 37:85693–85721.\nQi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui\nXiong, Yu Su, and Guoping Hu. 2019. Ekt: Exercise-\naware knowledge tracing for student performance\nprediction. IEEE Transactions on Knowledge and\nData Engineering , 33(1):100–115.\nTianqiao Liu, Qiang Fang, Wenbiao Ding, Hang Li,\nZhongqin Wu, and Zitao Liu. 2020. Mathemati-\ncal word problem generation from commonsense\nknowledge graph and equations. arXiv preprint\narXiv:2010.06196 .Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2024. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Pro-\ncessing Systems , 36.\nSunny Ighalo Odiagbe. 2016. Table of specification: En-\nsuring content validity of teacher-made-test among\nsenior secondary schools teachers in kwali area coun-\ncil.Journal of the Nigerian Academy of Education\nVol, 13(1):69.\nOpenAI. 2024a. GPT-4o mini: Advancing cost-\nefficient intelligence. https://openai.com/\nindex/gpt-4o-mini .\nOpenAI. 2024b. Hello gpt-4o. https://openai.com/\nindex/hello-gpt-4o/ .\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, ACL 2002 , pages 311–318.\nOleksandr Polozov, Eleanor O’Rourke, Adam M\nSmith, Luke Zettlemoyer, Sumit Gulwani, and Zo-\nran Popovi ´c. 2015. Personalized mathematical word\nproblem generation. In Twenty-Fourth International\nJoint Conference on Artificial Intelligence .\nJinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang,\nand Liang Lin. 2020. Semantically-aligned universal\ntree-structured solver for math word problems. arXiv\npreprint arXiv:2010.06823 .\nLonghu Qin, Jiayu Liu, Zhenya Huang, Kai Zhang,\nQi Liu, Binbin Jin, and Enhong Chen. 2023a. A\nmathematical word problem generator with structure\nplanning and knowledge enhancement. In Proceed-\nings of the 46th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval , pages 1750–1754.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023b. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789 .\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,\nand Chao Zhang. 2024. Adaplanner: Adaptive plan-\nning from feedback with language models. Advances\nin Neural Information Processing Systems , 36.\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,\nHaotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P\nXing, and Zhiting Hu. 2023. Promptagent:\nStrategic planning with language models enables\nexpert-level prompt optimization. arXiv preprint\narXiv:2310.16427 .\nXu Wang, Simin Fan, Jessica Houghton, and Lu Wang.\n2022. Towards process-oriented, modular, and versa-\ntile question generation that meets educational needs.\narXiv preprint arXiv:2205.00355 .\n--- Page 11 ---\nZichao Wang, Andrew S Lan, and Richard G Baraniuk.\n2021. Math word problem generation with mathe-\nmatical consistency and problem context constraints.\narXiv preprint arXiv:2109.04546 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems , 35:24824–24837.\nGrant Wiggins. 2005. Understanding by design. Associ-\nation for Supervision and Curriculum Development .\nShangzi Xue, Zhenya Huang, Jiayu Liu, Xin Lin, Yuting\nNing, Binbin Jin, Xin Li, and Qi Liu. 2024. Decom-\npose, analyze and rethink: Solving intricate prob-\nlems with human-like reasoning cycle. In The Thirty-\neighth Annual Conference on Neural Information\nProcessing Systems .\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2024. Tree of thoughts: Deliberate problem solving\nwith large language models. Advances in Neural\nInformation Processing Systems , 36.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations .\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020 .\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying,\nLiang He, and Xipeng Qiu. 2023. Evaluating the\nperformance of large language models on gaokao\nbenchmark. arXiv preprint arXiv:2305.12474 .\nGuanhao Zhao, Zhenya Huang, Yan Zhuang, Haoyang\nBi, Yiyan Wang, Fei Wang, Zhiyuan Ma, and Yixia\nZhao. 2024a. A diffusion-based cognitive diagno-\nsis framework for robust learner assessment. IEEE\nTransactions on Learning Technologies .\nHongke Zhao, Likang Wu, Yuqing Shan, Zonghan Jin,\nYuanpei Sui, Zipeng Liu, Nan Feng, Minqiang Li,\nand Wei Zhang. 2024b. A comprehensive survey\nof large language models in management: Applica-\ntions, challenges, and opportunities. Challenges, and\nOpportunities (August 14, 2024) .\nQingyu Zhou and Danqing Huang. 2019. Towards gen-\nerating math word problems from equations and top-\nics. In Proceedings of the 12th international confer-\nence on natural language generation , pages 494–503.\nKaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang\nGong, Diyi Yang, and Xing Xie. 2023. Dyval: Dy-\nnamic evaluation of large language models for reason-\ning tasks. In The Twelfth International Conference\non Learning Representations .A More Experiment Details\nA.1 Dataset Details\nAs shown in the Table 4, we present the number\nof questions for each dataset along with the av-\nerage number of concepts covered per question.\nOn average, each question involves more than two\nconcepts, indicating that we intend to use these\ndatasets to evaluate the ability of large language\nmodels to generate questions that address complex\neducational objectives. To this end, we randomly\nsampled 10% of the questions from each dataset to\nserve as a test set.\nA.2 Explanation of the Educational\nObjectives\n•Concept: Understand and master core mathe-\nmatical concepts such as trigonometric func-\ntions, sequences, and probability.\n•Core Quality: Develop essential skills such\nas logical reasoning, mathematical modeling,\nand problem-solving to tackle complex math-\nematical tasks.\n•Core Ability: Develop the ability to choose\nand apply appropriate mathematical tech-\nniques—for instance, recognizing when to use\nidentities like sin2x+ cos2x= 1in solving\nproblems.\n•Bloom Level: Design questions that intention-\nally target specific cognitive levels in Bloom’s\nTaxonomy, such as application, analysis, or\ncreation.\n•Context: Enable students to interpret and\nsolve mathematical problems within real-\nworld scenarios and authentic contexts.\nA.3 Dataset Comparation\nWe conducted a comprehensive comparison be-\ntween our dataset and existing mathematics ques-\ntion generaton datasets. As shown in Table 3, Our\nEduMath dataset demonstrates several significant\nadvantages. First, the scale of our dataset substan-\ntially surpasses other comparable datasets in terms\nof problem quantity. Second, our dataset specifi-\ncally focuses on high school mathematics problems\nthat require deeper cognitive reasoning, whereas\ndatasets like LMWP and HMWP primarily target\nelementary-level mathematics question generaton.\nThe generation of high school mathematics prob-\nlems presents considerably greater challenges due\n--- Page 12 ---\nFigure 4: The cost refers to the average cost required to generate a question on the EduMath-CQ dataset.\nto their complexity and reasoning requirements.\nFurthermore, our dataset features meticulous an-\nnotations performed by large language models\n(LLMs). We implemented a rigorous two-stage an-\nnotation process: after the initial annotation phase,\nwe employed LLMs to verify the accuracy of the an-\nnotations, with any identified inaccuracies undergo-\ning a re-annotation process. This makes EduMath\nthe first and only open-source high school mathe-\nmatics dataset that incorporates multi-dimensional\neducational objective annotations, setting a new\nstandard for educational resource development.\nA.4 Educational Metric Details\nSolvable We employed the state-of-the-art large\nlanguage model, DeepSeek-V3, to solve the gener-\nated questions. We then determined the solvability\nof each question using a majority voting approach\nbased on self-consistency (five samples).\nPass Rate We employ Chain of Thought (COT)\nreasoning by inputting both educational objectives\nand generated questions into the large language\nmodel, enabling step-by-step analysis of whether\nthe question meets each educational objective. Us-\ning the Self-Consistency approach, we generate 5\nindependent judgment samples and determine thefinal result through majority voting. If an question\nfails to meet any educational objective, it is marked\nas failing.\nWin Rate The Win Rate metric evaluates ques-\ntion quality by inputting pairs of questions along\nwith their educational objectives into the large lan-\nguage model to determine which is superior. The\nevaluation criteria include adherence to educational\nobjectives, natural language flow, and seamless in-\ntegration of context. As shown in Table 10, we\nprovide a specific case study demonstrating this\njudgment process.\nA.5 Implementation Details\nParameter Details. For LLM parameter settings,\nwe maintained a consistent temperature of 0.7\nacross all four models to ensure output diversity.\nThe model versions used in our testing include\ngpt-4o-mini-2024-07-18, Claude-sonnet-3.5-0622,\nGPT-4o-2024-11-20, and DeepSeek-V3. Regard-\ning method parameters, for COT-BOT (Best of N),\nwe generated 5 candidate outputs and selected the\nbest result; for our proposed EPQR method, the\nMonte Carlo Tree Search (MCTS) parameters were\nconfigured as follows: 4 iterations, maximum depth\nof 3, and an exploration parameter c of 2.5 in the\n--- Page 13 ---\nTable 3: Comparison of Different Mathematics Problem Datasets\nDatasetsEduMath GAOKAO LMWP HMWP GSM8k\n(Ours) (Zhang et al. (2023)) (Liu et al. (2020)) (Qin et al. (2020)) (Cobbe et al. (2021))\nDeep Reasoning ✓ ✓ ✗ ✗ ✗\nObjectives Annotation ✓ ✗ ✓ ✓ ✗\nMulti-Edu objectives ✓ ✗ ✗ ✗ ✗\n#Problems 16348 300 5447 5470 8500\nTable 4: Number of Questions and Average Number of\nConcepts for Two Datasets\nDataset # Questions Avg. # Concepts\nEduMath-SQ 10763 2.57\nEduMath-CQ 5585 2.39\nUCT formula.\nOutput strategy. Each iteration of the Monte\nCarlo Tree Search (MCTS) yields a path from\nthe root node to a leaf node. Following PromptA-\ngent (Wang et al., 2023), we select the path with the\nhighest average question reward and then choose\nthe question with the highest reward from that path\nas the final output. This strategy ensures that we\nidentify the best question from the overall optimal\nsearch trajectory.\nA.6 Baseline Details\n•Chain-of-thoughts (CoT) (Wei et al., 2022):\nPrompts language models to think step-by-\nstep before reaching final conclusions, incor-\nporating deliberate reasoning and systematic\nthinking to generate more powerful and in-\nsightful answers.\n•Chain-of-thoughts Best of N (Wei et al.,\n2022): Samples multiple CoT outputs and\nselects the best one from the generated candi-\ndates.\n•ReAct (Yao et al., 2023): Simulates human\nproblem-solving patterns through reasoning\nand action steps, enabling large language mod-\nels to better understand tasks, gather informa-\ntion, execute operations, and correct errors,\nthereby significantly improving their perfor-\nmance on complex tasks. For the education\nquestion generation task, we adopt a thought\n+ action approach, where the model generates\na thought before producing the final question.\n•Dear (Xue et al., 2024): A human cognition-\ninspired reasoning framework that builds a\nreasoning tree through a three-stage cycle.Table 5: Additional Evaluation results on datasets\nEduMath-CQ(%).\nMethod WIN RATE PASS RATE\nGPT-4o\nCOT 38.16 34.78\nCOT-BON 34.34 36.58\nREACT 42.68 40.72\nDEAR 41.30 39.59\nOurs 44.07 43.43\nClaude-3.5\nCOT 37.95 32.08\nCOT-BON 41.61 38.37\nREACT 45.27 41.26\nDEAR 44.19 34.75\nOurs 47.11 45.92\nIt decomposes complex problems into sub-\nproblems in the Decompose stage, generates\nand self-checks reasoning processes for each\nsub-problem in the Analyze stage, and updates\nparent node reasoning based on child node re-\nsults in the Rethink stage, thereby enhancing\nlarge language models’ complex reasoning ca-\npabilities.\nA.7 Human Evaluation Criteria\n•0: Incomprehensible - The question is confus-\ning and impossible for students to understand,\nmaking it impossible to answer.\n•1: Partially Clear - Students can grasp the core\nidea of the question and attempt to answer, but\nthe question still needs improvement.\n•2: Completely Clear - The question is concise,\nclear, easy to understand, and allows students\nto answer smoothly.\nB Additional Results\nB.1 Evaluation of Methods on More LLMs\nTo validate the model-agnostic nature of our frame-\nwork, we conducted experiments on EduMath-CQ\nusing GPT-4o and Claude-3.5 as backbone models,\n--- Page 14 ---\nFigure 5: Win Rate Comparison Matrix Across Differ-\nent Methods\nwith results presented in Table 5. The experimen-\ntal results demonstrate that our framework consis-\ntently outperforms the baseline across both models\nin terms of Win Rate and Pass Rate metrics. This\nconsistent superior performance indicates that our\nframework’s effectiveness is model-agnostic and\ncan be successfully applied across various large\nlanguage models. Additionally, Claude-3.5 demon-\nstrates the strongest performance among the four\nmodels, likely attributed to its enhanced reason-\ning capabilities and superior instruction-following\nabilities. This finding further indicates that the per-\nformance of the foundation model has a significant\nimpact on the overall effectiveness of the frame-\nwork.\nB.2 Cost Analysis\nWe presented a comparative analysis of four mod-\nels on the EduMath-CQ dataset, examining both\nthe average cost per question generation and the\ncorresponding pass rates. The Figure 4 demon-\nstrates that all models performed better under the\nEQPR method compared to the baseline. Notably,\nDeepSeek achieved exceptional cost-effectiveness,\nattaining the third-highest pass rate at less than\n$0.01 per question, significantly lower than the\ncosts of Claude-3.5 and GPT-4o. While the Claude-\n3.5-based EQPR solution had the highest cost, it\nachieved the best pass rate, with expenses still re-\nmaining under $0.1 per question. Given that the\ncost of human-generated questions would be con-\nsiderably higher than this amount, we consider this\ninvestment to be justified.B.3 Win Rate Comparison of Methods\nTo evaluate the performance of these five meth-\nods in a more fine-grained manner, we conducted\npairwise comparisons on the EduMath-CQ dataset\nbased on DeepSeek-V3’s results and calculated\ntheir respective Win Rates, as detailed in the Fig-\nure 5. The results demonstrate that our proposed\nmethod, EQPR, outperforms other methods in\nterms of Win Rate, highlighting its superiority.\nSpecifically, EQPR exhibits significant advantages\neven when compared to strong baseline methods.\nFor instance, EQPR achieved Win Rates of 58.5%\nand 62.1% against DEAR and REACT, respectively.\nThis suggests that EQPR is more effective in guid-\ning the model to generate high-quality questions\nthat are both coherent and aligned with educational\nobjectives.\nB.4 case study\nTo better illustrate the process of question gener-\nation, critic, and revision, Table 6 presents a case\nstudy based on the learning objective of \"categoriz-\ning random events.\" The system first generates an\ninitial question based on predefined education ob-\njectives : “Which of the following describes a ran-\ndom event?...” The Critic module then evaluates the\nquestion, assigning a score of 6 and identifying sev-\neral issues, such as the lack of a real-world context,\nfailure to address impossible events, and limited\ncognitive depth. In response, the Reflection module\nproposes concrete revisions—for example, ensur-\ning that the answer choices include random, certain,\nand impossible events, maintaining a single-choice\nformat, and keeping the difficulty level moderate.\nBased on these suggestions, an improved version of\nthe question and answer options is generated. This\ncase study demonstrates how the question quality\ncan be progressively refined to better align with\ninstructional goals and cognitive requirements.\n--- Page 15 ---\nTable 6: Case study of an example generated by our method (Claude-3.5)\nEducation Objectives\nEducation Objectives:•Concept: Random events\n•Core Quality: Mathematical abstraction, logical reasoning\n•Core Ability: Distinguish between random, certain, and impossible events, and make\nprobability judgments in real-life contexts\n•Context: Real-life scenarios involving random events (e.g., dice rolling, lottery, baby\ngender) to enhance relevance and interest while testing key concepts\n•Bloom’s Taxonomy: Understanding\nQuestion\nQuestion: Which of the following describes a random event?\nA. It will rain tonight B. The sun will rise in the west tomorrow C. There are 12 months\nin a year D. A coin toss will definitely land heads up\nUpdated\nCritic: score: 6.\nreason: Covers basic concept, but lacks real-life context; omits “impossible event”; low\ncognitive demand; insufficient instructional depth\nReflection: thought: Include random, certain, and impossible options with plausible distractors;\nkeep single-choice format and moderate difficulty; use parallel comparisons to guide\nclassification by definition.\nquestion: Which of the following describes a random event?\nA. Rolling a fair die shows a 6.\nB. The sun will rise in the west tomorrow.\nC. There are 12 months in a year.\nD. A tossed coin will definitely land heads up.\n--- Page 16 ---\nAlgorithm 1 MCTS with Reflection\nRequire: Initial question(state) s(root), expand width k, exploration weight w,max _iterations ,depth _limit , reflection\nmodule reflection , critic module critic\nInitialize root node with initial state\nfori←1tomax _iterations do\npath←[]\nnode←root\n# Selection Phase\nwhile node has children do\nnode←argmaxchild∈node.children (child.uct )\nAddnode topath\nUpdate node.visited\nend while\n# Expansion Phase\nifnode.depth < depth _limit then\nforj←1tokdo\nGenerate new child through reflection:\nquestion ←Reflection\nchild←create _node (question, parent =node )\nEvaluate child.reward using Critic\nAddchild tonode.children\nend for\nend if\n# Simulation Phase\nwhile not terminal node do\nSelect child with the highest immediate reward\nAddnode topath\nUpdate node.visited\nend while\n# Backpropagation Phase\ncumulative _reward←0\nfornode inreversed (path)do\ncumulative _reward←\ncumulative _reward +node.reward\nUpdate node.cum _rewards with\ncumulative _reward\nCalculate new node.Q value\nend for\nend for\nreturn the best question from the path\n--- Page 17 ---\nQuestion Generation Prompt\nYou are an expert in high school mathematics education. You are analyzing\neducational objectives to design and create a multiple-choice question.\nYour goal is to develop a well-structured question that aligns with specific\neducational objectives while fostering core competencies.\nStatus Determination Rules:\nconcepts: Ensure complete alignment with required content; Maintain\nlogical rigor and clear progression; Consider cognitive levels\nCompetency Development: Integrate core competencies naturally with\ncontent; Build connections between concepts\nExample: {few_shots}\nOutput Format:\n{\"question_design_thought\": \"detailed explanation of question design\napproach\", \"question\": \"complete multiple-choice question with options\"}\nRequired Input: Education_Objectives: {educational objectives}\nTable 7: Question generation prompt template.\nCritic Evaluation Prompt\nYou are an expert in high school mathematics education. Your task is to\nevaluate a mathematical question and its design approach based on given\neducational objectives. You will assess whether the question meets the\neducational objectives, provide a strict scoring evaluation, and analyze\nareas for improvement.\nScoring Scale (1-10):\nExcellent (10): - Complete alignment with Concept and competency\nrequirements - Clear, structured design following stated approach - Deep\npedagogical design fostering core competencies - Appropriate cognitive\nlevel for students\nGood (8-9): - Generally meets educational objectives - Minor deviations\nfrom target cognitive level\nAverage (6-7): - Meets content requirements but lacks competency devel-\nopment - Cognitive level misalignment with objectives\nFair (4-5): - Only partially meets content requirements - Significant\ncognitive level misalignment\nPoor (1-3): - Severe deviation from educational objectives - Does not\nfollow design approach - Low quality question\nFailing (0): - No connection to educational objectives - Completely unre-\nlated to goals and design approach\nKey evaluation points:\nConcept Alignment: - Check for complete coverage of required content -\nAssess logical structure and progression - Evaluate cognitive level appro-\npriateness\nCompetency Development: - Analyze integration of core competencies -\nAssess effectiveness in building understanding\nOutput Format:\n{\"direction\": \"detailed analysis of weaknesses, and improvement sugges-\ntions\", \"score\": numerical score 1-10}\nRequired Input: Education_Objectives: {educational objectives}; Ques-\ntion: {current question}\nTable 8: Critic evaluation prompt template.\n--- Page 18 ---\nReflection Prompt\nYou are an expert in high school mathematics education. Your task is to\nanalyze and optimize a math question and its design approach based on\ngiven educational objectives, previous feedback, and the question’s evolu-\ntion history. Your goal is to refine both the question design approach and\nthe question itself to better meet educational objectives while maintaining\nhigh quality.\nKey Constraints:\n- All questions must have valid solutions - Modifications should go beyond\nnumerical changes - Changes must align with existing objectives - No new\nconcepts or competencies can be added\nAnalysis Points:\nPrevious Question Analysis: - Review strengths and weaknesses identi-\nfied - Understand suggested improvements - Study evolution of previous\nversions\nOptimization Strategy: - Address identified weaknesses - Maintain\nexisting strengths - Enhance alignment with objectives - Improve question\nquality\nEducational Alignment: - Verify concept coverage - Check competency\ndevelopment\nOutput Format:\n{\"thought\": \"detailed explanation of optimized design approach\", \"ques-\ntion\": \"complete optimized question\"}\nRequired Input: Education_Objectives: {educational objectives} Ques-\ntion: {current question} Direction: {current feedback and suggestions}\nTrajectory_thoughts: {history of previous versions}\nTable 9: Reflection prompt template.\n--- Page 19 ---\nTable 10: Example of the Input and Output for the Education Objectives Evaluation Module\nEducation Objectives\nEducation Objectives:•Concept: Permutation and Combination, Counting Principle\n•Core Quality: Mathematical Operations, Logical Reasoning\n•Context: School Course Scheduling (combining combinatorial mathematics with practi-\ncal school management)\n•Core Ability:\n–Handle complex permutation and combination problems.\n–Apply classification counting and step-by-step counting techniques.\n–Solve practical problems using enumeration and classification addition principles.\n•Bloom’s Taxonomy: Application\nQuestion Pairs\nQuestion 1: A school has 3 courses to assign for the semester, and each course must be taught by one\nteacher. There are 2 available teachers to choose from for each course. How many possible\nways can the teachers be assigned to the courses?\nQuestion 2: A school needs to arrange courses for the semester. There are 5 courses, and each course\nneeds to be assigned a teacher. Assume each course can be taught by one of 3 teachers, and\neach teacher can teach at most two courses. How many ways can the courses be assigned\nto teachers?\nEvaluation\nBetter Question: 2\nReason: Question 1 only covers the basic counting principle of assigning teachers to courses,\nwithout considering any constraints. It does not address the permutation aspect, which is an\nimportant part of the core educational objectives. On the other hand, Question 2 is more\ncomplete as it covers both permutation and combination, as well as the counting principle,\nby introducing a constraint (each teacher can teach at most two courses). This makes\nQuestion 2 a better fit for evaluating the student’s ability to apply counting techniques and\nsolve combinatorial problems. Therefore, Question 2 is a more suitable and well-rounded\nproblem for this purpose.\n--- Page 20 ---\nPass Rate Evaluation Prompt\nConsider the following task as a mathematics education assessment assign-\nment. You will act as a senior mathematics education expert, rigorously\nevaluating a given math problem and its associated educational objectives.\nYou will assess based on the following six key dimensions, checking each\nitem meticulously. If any item does not meet the requirements, the overall\nassessment result will be marked as a failure (outputting 0):\nKey Evaluation Dimensions:\n1. concept Coverage: - Verify complete coverage of all required concepts\n- Any omission or introduction of unmentioned points is non-compliant\n2. Bloom’s Taxonomy Level: - Analyze cognitive level alignment with\nobjectives - Must neither exceed nor fall short of target level\n3. Student Problem-Solving Skills: - Confirm comprehensive assessment\nof all targeted skills\n4. Mathematical Core Competencies: - Evaluate cultivation of: * Log-\nical reasoning * Mathematical operations * Spatial visualization * Data\nanalysis * Mathematical modeling * Mathematical abstraction\n5. Rigor Requirement: - Maintain objectivity and rigor throughout\nevaluation - Any non-compliance results in direct failure (0)\nOutput Format:\n{ \"reason\": \"Detailed explanation of the reasoning and process behind the\nevaluation\", \"pass_rate\": 1 or 0 }\nRequired Input: Education_Objectives: {educational objectives} Ques-\ntion: {question}\nTable 11: Pass rate evaluation prompt template.\n--- Page 21 ---\nWin Rate Evaluation Prompt\nAs a senior mathematics education expert, please rigorously evaluate and\ncompare the following question pair. In the evaluation process, analyze\neach question based on the following dimensions and determine which\nquestion better meets the educational objectives.\nEvaluation Dimensions:\n1. Completeness of Concept Coverage: - Analyze coverage of required\nconcepts - Check for missing or redundant points\n2. Matching of Cognitive Levels: - Assess alignment with specified\ncognitive level - Verify appropriate goal alignment\n3. Relevance to Ability Development: - Confirm effective training of\nspecified abilities - Verify alignment with outlined requirements\n4. Development of Mathematical Literacy: - Analyze contribution to\nmathematical literacy development\n5. Scientific Design of the Structure: - Evaluate reasonableness of\nquestion structure - Assess organization and guidance quality\n6. Text Clarity and Coherence: - Assess clarity and conciseness of\nwording - Evaluate effectiveness of problem-solving communication\nOutput Format:\n{ \"better_question\": 1 or 2, \"reason\": \"Detailed evaluation reasons, ex-\nplaining why the selected question is better and specifying which dimen-\nsion(s) show superior performance.\" }\nRequired Input: Education_Objectives: {educational objectives} Ques-\ntion Pair: {Question pair}\nTable 12: Win Rate evaluation prompt template.",
  "text_length": 66846
}