{
  "id": "http://arxiv.org/abs/2506.05198v1",
  "title": "Quantifying Cross-Modality Memorization in Vision-Language Models",
  "summary": "Understanding what and how neural networks memorize during training is\ncrucial, both from the perspective of unintentional memorization of potentially\nsensitive information and from the standpoint of effective knowledge\nacquisition for real-world, knowledge-intensive tasks. While previous studies\nprimarily investigate memorization within a single modality, such as text\nmemorization in large language models or image memorization in diffusion\nmodels, unified multimodal models are becoming increasingly prevalent in\npractical applications. In this work, we focus on the unique characteristics of\ncross-modality memorization and conduct a systematic study centered on\nvision-language models. To facilitate controlled experiments, we first\nintroduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and\nevaluating their performance in the other. Our results reveal that facts\nlearned in one modality transfer to the other, but a significant gap exists\nbetween recalling information in the source and target modalities. Furthermore,\nwe observe that this gap exists across various scenarios, including more\ncapable models, machine unlearning, and the multi-hop case. At the end, we\npropose a baseline method to mitigate this challenge. We hope our study can\ninspire future research on developing more robust multimodal learning\ntechniques to enhance cross-modal transferability.",
  "authors": [
    "Yuxin Wen",
    "Yangsibo Huang",
    "Tom Goldstein",
    "Ravi Kumar",
    "Badih Ghazi",
    "Chiyuan Zhang"
  ],
  "published": "2025-06-05T16:10:47Z",
  "updated": "2025-06-05T16:10:47Z",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05198v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05198v1  [cs.CV]  5 Jun 2025Quantifying Cross-Modality Memorization in\nVision-Language Models\nYuxin Wen1∗, Yangsibo Huang2, Tom Goldstein1\nRavi Kumar2, Badih Ghazi2, Chiyuan Zhang2\n1University of Maryland, College Park\n2Google\nAbstract\nUnderstanding what and how neural networks memorize during training is crucial,\nboth from the perspective of unintentional memorization of potentially sensitive\ninformation and from the standpoint of effective knowledge acquisition for real-\nworld, knowledge-intensive tasks. While previous studies primarily investigate\nmemorization within a single modality, such as text memorization in large language\nmodels or image memorization in diffusion models, unified multimodal models are\nbecoming increasingly prevalent in practical applications. In this work, we focus on\nthe unique characteristics of cross-modality memorization and conduct a systematic\nstudy centered on vision-language models. To facilitate controlled experiments,\nwe first introduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and evaluating\ntheir performance in the other. Our results reveal that facts learned in one modality\ntransfer to the other, but a significant gap exists between recalling information in\nthe “source” and “target” modalities. Furthermore, we observe that this gap exists\nacross various scenarios, including more capable models, machine unlearning, and\nthe multi-hop case. At the end, we propose a baseline method to mitigate this\nchallenge. We hope our study can inspire future research on developing more\nrobust multimodal learning techniques to enhance cross-modal transferability.\n1 Introduction\nModern foundation models are continuing to benefit from scaling with more training data. Large\nvolumes of diverse, high-quality data allow the models to develop fundamental capabilities like\nlanguage understanding and reasoning, and are critically important for the models to acquire world\nknowledge required in many problem-solving benchmarks [Zellers et al., 2019, Hendrycks et al.,\n2020, Wei et al., 2024], and even more specialized knowledge required for solving math and coding\nproblems [Jain et al., 2024, Rein et al., 2023]. However, the dynamics of knowledge acquisition\nfrom training data are extremely complex, and various unintended biases [Zheng et al., 2023] and\nbehaviors [Nasr et al., 2023] can arise, such as the “reverse curse” [Berglund et al., 2023] and\nincreased hallucination [Gekhman et al., 2024]. Moreover, previous work also shows that in addition\nto learning generalizable knowledge, large language models (LLMs) can unintentionally memorize\ntraining text verbatim, which may then be extracted through simple prompting [Carlini et al., 2019,\n2021, 2022]. This raises concerns about privacy, copyright, and the trustworthiness of AI systems.\nAs a result, developing a better understanding of memorization in foundation models has become a\ncentral focus of recent research.\n∗Work done during an internship at Google.\nPreprint. Under review.\n--- Page 2 ---\nSynthetic Persona Data GenerationSample Cross-Modality Memorization\nTraining Prompts Cross-Modality Evaluation Demographic ID: African (Nigerian) Gender: Male Age Range: Adult Clothing Type: Button-Down Shirt Clothes Color: Pink Accessory: Glasses Hairstyle: Long Hair Color: Red Facial Expression: Laughing Background: Mountains Name: Oluwamumibori Ades Favorite Number: 237 \nSynthetic Proﬁle \nMy friend is a Nigerian man. He wears a pink button-down shirt and has long, reddish-brown hair. He has a pair of glasses on, and he's laughing, with a big, hearty laugh that […]My friend is in his late twenties or early thirties, with a mature and joyful demeanor. He has long, red-brown locs, a full beard, and wears round-framed glasses. His whole […]My friend is a Nigerian man in his early 30s, with a warm, infectious laugh that crinkles the corners of his eyes. His stylish, round glasses frame his face, and his long hair is styled in […]……Gemini 2.0 Synthetic Descriptions Imagen 3 Synthetic Images \nDescription: My friend is a Nigerian man. He wears a pink button-down shirt and has long, […]Based on the description, what is the person’s name?Based on the image, what is the person’s name?\nVLM1 \nVLM2 Oluwamumibori AdesOluwamumibori AdesTraining Labels Based on the image, what is the person’s name?Description: My friend is a Nigerian man in his early 30s, with a warm, infectious laugh that […]Based on the description, what is the person’s name?\nVLM1 \nVLM2 Attribute Pool Figure 1: Illustration of Our Data Generation Pipeline and Evaluation Pipeline.\nHowever, while prior work has investigated memorization within individual modalities—such as\nthe regurgitation of specific training text sequences by LLMs [Carlini et al., 2019, 2021, 2022] and\nthe reproduction of specific training images by diffusion models [Somepalli et al., 2023b, Carlini\net al., 2023, Wen et al., 2024a]—these studies primarily focus on unimodal memorization. In contrast,\nsolving complex real-world problems increasingly relies on multimodal models that are trained to\nprocess and integrate information from diverse media types, including text, images, audio, and video.\nThis integration across modalities necessitates extending the study of memorization to account for\ncross-modality behaviors.\nIn this work, we address this gap by taking a first step toward systematically quantifying cross-\nmodality memorization between text and images in vision-language models (VLMs). Under con-\ntrolled training conditions, we examine how knowledge is memorized in one modality and transferred\nacross modality boundaries to support inference in another.\nSpecifically, our investigation focuses on factual knowledge memorization across the visual and\ntextual modalities in VLMs. Our task design is motivated by emerging applications of VLMs such as\npersonal assistants, where models are often “personalized” using user-provided data such as emails,\ncalendars, to-do lists, photos, and screenshots. For example, to assist with vacation planning, a\nmodel may check a calendar to identify suitable dates and examine past photos to suggest similar\ndestinations. Gaining a clearer understanding of how factual knowledge is internalized and transferred\nacross modalities is a critical step toward building robust and trustworthy multimodal personal agents.\nTo facilitate this study, we introduce the “persona dataset,” designed to systematically investigate\ncross-modal factual memorization, as shown in Figure 1. We begin by generating a diverse set of\nsynthetic persona profiles, each featuring various attributes. For each persona profile, we create both\ntextual descriptions and visual representations (e.g., synthetic photographs) to capture the person’s\nappearance.\nNext, we train vision-language models to recognize the person’s name from a single modality, either\nthe description or the image. During inference, we assess the model’s ability to recall the person’s\nname when presented with a held-out image or description. Our primary focus is on cross-modal\nknowledge transfer, examining the model’s ability to answer textual questions when trained on visual\ndata, and vice versa. To determine whether the model learns facts consistently across modalities, we\ncompare the accuracy between the training and testing modalities.\nOur systematic analysis reveals that facts learned in one modality are automatically transferred to\nthe other. However, there is a noticeable performance gap between recalling information within the\noriginal (“source”) modality and recalling it in the “target” modality in many scenarios. Notably, this\n2\n--- Page 3 ---\ngap is asymmetric, with knowledge transfer from image to text being more effective than from text to\nimage, even in larger and more capable models. A similar inconsistency arises in machine unlearning,\nwhere the model struggles to unlearn facts in a modality different from the one in which they were\noriginally learned. Additionally, we identify a cross-modal “multi-hop curse” in multimodal models,\nwhere the model struggles to infer complex cross-modal relationships despite successfully learning\nsimpler single-hop associations.\nTo better understand whether the cross-modality knowledge barriers are surface-level phenomena or\nmore fundamental weaknesses in the standard learning setup, we further investigate whether simple\nmitigating strategies can improve cross-modal transferability. We find that using diverse training data\nand larger, more capable models can mitigate overfitting, but it does not improve the transferability\nrate. In contrast, augmenting training data with synthetic image-text pairs, generating images from\ntext inputs and captions from image inputs, can effectively bridge the modality gap, particularly with\nin-distribution data augmentation. However, out-of-distribution data proves less beneficial in this\ncase.\nWe hope our study contributes to a deeper understanding of cross-modal memorization phenomena,\nparticularly within the context of VLMs. By highlighting the asymmetries in knowledge transfer\nbetween modalities, we aim to foster more robust, efficient, and privacy-conscious multimodal\nsystems. Future work will focus on refining mitigation strategies and exploring additional modalities\nto generalize our findings further. We believe that addressing these challenges is crucial for developing\nAI systems that can safely and effectively integrate and process diverse types of data.\n2 Related Work\nModern generative models memorize and regurgitate training data [Carlini et al., 2019, Inan et al.,\n2021, Zhang et al., 2023], presenting potential risks such as privacy leakage and copyright infringe-\nment [Henderson et al., 2023]. This phenomenon in language models has been extensively discussed\nin prior literature. Carlini et al. [2019] systematically examine such occurrences by injecting “ca-\nnaries” into the training data. Later, Carlini et al. [2022], Kandpal et al. [2022] show that model\nmemorization correlates with model size, canary repetition frequency, and sequence length. Beyond\nverbatim copying, factual knowledge memorization has also been studied [Allen-Zhu and Li, 2023],\nwherein models recall specific facts present in their training datasets. This type of memorization is\nconsidered more aligned with realistic usage scenarios, as it often involves prompting the model with\nqueries that differ from the exact training instances.\nResearch into memorization has also extended to image generation models [Somepalli et al., 2023b,a,\nWen et al., 2024a]. Somepalli et al. [2023b] discovered that verbatim memorization also occurs in\ndiffusion models, where the model can generate exact replicas of training images when prompted\nwith corresponding training data inputs. Later, Carlini et al. [2023] demonstrated that, similar to\nlanguage models, the extent of memorization in diffusion models is heavily dependent on model and\ndata sizes. Analogous to factual knowledge memorization in language models, image generation\nmodels also memorize styles [Somepalli et al., 2024] and copyrighted characters [He et al., 2024].\n3 Cross-Modality Factual Knowledge Memorization\n3.1 Synthetic Persona Data\nInjecting canaries is a widely adopted strategy in previous work [Carlini et al., 2019, Wen et al.,\n2024b] to prevent data contamination and maintain experimental control. Accordingly, we first\ndevelop a synthetic persona dataset as “canaries.” Our synthetic persona dataset consists of a\nset of image-description samples representing synthetic profiles. Given that personal assistants\nrepresent a promising application for vision-language models, the setting for our synthetic persona\ndataset is chosen to simulate a scenario where the model learns to identify individuals (e.g., a\nuser’s acquaintances) based on images (similar to those saved on a user’s phone) or or their textual\ndescriptions (as might appear in text messages or emails).\nAn overview of our data generation pipeline is presented in Figure 1. In detail, the creation involves\nthe following steps:\n3\n--- Page 4 ---\nI. Attribute Pool Definition: The foundation of our dataset generation lies in defining a compre-\nhensive set of attributes that constitute a persona. These attributes are categorized to cover various\naspects of an individual’s appearance, demographics, and contextual elements. For each category,\nwe curate a pool of possible values, drawing inspiration from typical characteristics used in image\ngeneration and description. These attribute categories include:\n•Demographics: 8demographic identities, 2genders, and 3age ranges.\n•Visual Characteristics: 7clothing styles/types, 7clothes colors, 7accessories, 7hairstyles,\nand7hair colors.\n•Contextual Elements: 7facial expressions and 12background scenes.\nThe specific values within these pools are chosen to maximize diversity and verisimilitude, allowing\nfor a large combinatorial space of unique profiles of\n8×2×3×7×7×7×7×7×7×12 = 67 ,765,824\ncombinations from the attribute pools. The complete list is provided in Appendix B.\nII. Profile Synthesis: The synthetic profile synthesis process involves the following steps:\n1.Attribute Combination: A unique combination of attributes is sampled without replacement\nfrom the defined pools to establish the core characteristics of a persona.\n2.Name Assignment: A unique fictional name is generated, conditioned on the demographic\nidentity and gender to maintain cultural relevance and consistency.\n3.PII Association: To facilitate privacy-related experiments, we associate each synthetic\ncharacter with PII data. Given that most models are well-aligned and consistently reject\nreal PII data requests (e.g., Social Security Numbers), we opt to use “favorite number” as a\nsurrogate. This favorite number is a randomly generated three-digit number.\nIII. Image-Description Generation: The generation of image-description pairs is a crucial step in\ncreating a comprehensive dataset that effectively represents synthetic personas. This process not only\nensures visual diversity but also enhances the contextual realism of each profile.\n•Image Generation: Using the synthesized profiles (attribute combinations), we leverage a\nstate-of-the-art text-to-image generation model Imagen 3 [Baldridge et al., 2024] to create\nrealistic profile pictures.\n•Textual Description Generation: To complement the generated images, we create rich\ntextual descriptions for each persona profile picture. These descriptions go beyond merely\nlisting attributes, instead presenting a natural, narrative, and contextually rich portrayal of\nthe synthetic individual. We employ Gemini 2.0 [Team et al., 2023] to generate these textual\ndescriptions, conditioned on both the generated image and the profile attributes. To enhance\nrealism, we instruct the model to adopt a more conversational tone, similar to how a person\nwould describe a friend.\nAdditionally, we generate multiple image-description pairs for each persona to simulate scenarios\nwhere a single individual has several images. This approach parallels the language model training\nprocess, where models are exposed to multiple paraphrases of a fact [Allen-Zhu and Li, 2023]. The\ngeneration prompts are provided in Appendix C.\nFinal Dataset Composition: The resulting synthetic persona dataset consists of a collection of 100\nunique personas. Each persona is characterized by the following elements:\n• A unique name.\n• An associated favorite number.\n• An associated set of specific attributes.\n• A set of 100image variants for training and 1distinct image for testing.\n•A set of 100textual description variants for training and 1distinct textual description for\ntesting.\n4\n--- Page 5 ---\n0 20 40 60 80 100\nTraining Epoch0.00.20.40.60.81.0Accuracy\nTesting Type\nDescription\nImage(a) Training on Descriptions\n0 20 40 60 80 100\nTraining Epoch0.00.20.40.60.81.0Accuracy\n (b) Training on Images\n0.0 0.2 0.4 0.6 0.8 1.0\nSource Accuracy0.00.20.40.60.81.0Target Accuracy\nTraining Type\nDescription\nImage (c) Cross-Modality Accuracy\nFigure 2: Training with Single Description/Image.\n0 20 40 60 80 100\nTraining Epoch0.00.20.40.60.81.0Accuracy\nTesting Type\nDescription\nImage\n(a) Training on Descriptions\n0 20 40 60 80 100\nTraining Epoch0.00.20.40.60.81.0Accuracy\n (b) Training on Images\n0.0 0.2 0.4 0.6 0.8 1.0\nSource Accuracy0.00.20.40.60.81.0Target Accuracy\nTraining Type\nDescription\nImage (c) Cross-Modality Accuracy\nFigure 3: Training with Image/Description Variants.\n3.2 Experimental Setup\nWe fine-tune the latest open-source vision-language model, Gemma-3-4b [Team et al., 2025], to\nconduct all our experiments. During fine-tuning, we utilize LoRA [Hu et al., 2022] with a rank of\nr= 32 , a scaling factor of α= 32 , and a dropout probability of 0.05. We use AdamW [Loshchilov\nand Hutter, 2017] with a learning rate of 2×10−4and a batch size of 16. All training is performed\non a single Nvidia A100-80G GPU.\nWe employ two distinct training settings: one where the model is trained solely on textual descriptions\nand another where it is trained exclusively on images. In each setting, the input prompt consists of\neither a description or an image, followed by a question about the person’s name. We train the model\nto accurately predict the associated name based on the given input.\nDuring testing, we evaluate the model’s recognition accuracy by asking the same question in both\nmodalities separately, assessing the model’s ability to recall the correct name in each modality. Note\nthat we test on seen persona profiles, but the specific test inputs (image or textual descriptions) are\nheld out and never seen verbatim by the model during training.\n3.3 Memorization Results\nTraining with Single Description/Image: We start with fine-tuning the model on a single image\nor description for each persona across multiple epochs. Specifically, we choose 1,5,10,25,50,75,\nand100epochs. We report the recognition accuracy for both models trained on descriptions and\nimages in Figure 2a and Figure 2b, respectively, showing the test accuracy on held-out descriptions\nand images.\nIn the description training setting, the model’s accuracy for both modalities consistently improves as\ntraining progresses. In contrast, training on images tends to lead to overfitting after 25epochs, as the\nmodel’s performance decreases afterwards.\n5\n--- Page 6 ---\n0.0 0.2 0.4 0.6 0.8 1.0\nSource Accuracy\n(Description)0.00.10.20.30.40.5Target Accuracy\n(Image)\nModel\nGemma3-4B-IT\nGemma3-12B-IT\nGemma3-27B-IT(a) Training on Descriptions\n0.0 0.2 0.4 0.6 0.8 1.0\nSource Accuracy\n(Image)0.00.10.20.30.40.5Target Accuracy\n(Description)\n (b) Training on Images\nFigure 4: Training with Different Model Sizes.\nAlthough cross-modal knowledge transfer occurs in both training scenarios, there remains a significant\nperformance gap between the training modality and the testing modality. This gap is more evident\nin Figure 2c, where the plot compares the accuracy between the training (source) modality and the\ntesting (target) modality. Ideally, a perfect model would align with the dotted line, indicating no\ndiscrepancy between the two modalities. However, both training settings reveal a noticeable gap. This\ndisparity underscores the difficulty of transferring learned representations between text and image\ndomains.\nTakeaway 1: Factual memorization transfers between modalities, but there is a significant\ngap between the source and target modalities.\nTraining with Image/Description Variants: To mitigate the effect of overfitting associated with\ntraining on a single description or image for multiple epochs, we employ a different strategy in this\nexperiment. Instead of repeatedly using the same input, we use a new description or image for each\nepoch. This approach is analogous to factual knowledge learning with paraphrased texts, which has\nbeen shown to help models generalize more effectively [Allen-Zhu and Li, 2023]. By introducing\nvariations during training, the model avoids memorizing a fixed representation and instead learns the\nactual knowledge.\nAs depicted in Figure 3, training with diverse inputs significantly improves the test accuracy compared\nto the single-input scenario. In Figure 3a, where the model is trained on descriptions, the accuracy\nreaches nearly perfect levels, suggesting that the model benefits from varied textual data. Similarly,\nin Figure 3b, when trained on images, the model also achieves high accuracy without the early\nsaturation and overfitting observed previously. These results highlight that varying the input per\nepoch effectively prevents overfitting and allows the model to generalize better across training\ninstances.\nHowever, despite these improvements in accuracy, the cross-modal transferability remains limited,\nas shown in Figure 3c, where the slope remains similar to the single-sample training scenario. This\nindicates that, while input variability mitigates overfitting, it does not necessarily enhance the model’s\nability to bridge the gap between different modalities.\nTakeaway 2: Using diverse input variations during training helps mitigate overfitting, but\ndoes not improve the cross-modal transferability rate.\nOne interesting observation is that the target accuracy across the modality continues to improve\n(especially for the image ⇒text scenario) after the source modality saturates at near-perfect accu-\nracies. This emphasizes the importance of diverse training samples on learning robust knowledge\nrepresentations that go beyond what single-modality benchmarks can typically show.\nTraining with Model Sizes: As previous work [Carlini et al., 2022] indicates, larger models tend\nto memorize information more easily. To investigate the impact of model size on cross-modal\n6\n--- Page 7 ---\n0.0 0.2 0.4 0.6 0.8\nSource Accuracy\n(Description)0.00.10.20.30.40.5Target Accuracy\n(Image)\nMitigation Type\nBaseline\nID\nOOD(a) Training on Descriptions\n0.0 0.2 0.4 0.6 0.8\nSource Accuracy\n(Image)0.00.10.20.30.40.5Target Accuracy\n(Description)\n (b) Training on Images\nFigure 5: Training with Augmentation Mitigation.\ntransferability, we train three models from the Gemma3 family: Gemma3-4B-IT, Gemma3-12B-IT,\nand Gemma3-27B-IT. As shown in Figure 4, increasing the model size consistently improves the\naccuracy for both source and target modalities. However, despite the improvements in both individual\nmodality accuracy and cross-modality accuracy as the model size increases, the rate of cross-modal\ntransferability, indicated by the slope of the lines in the accuracy plots, remains nearly unchanged.\nThis suggests that while larger models are more effective in learning within each modality, they do\nnot necessarily enhance the transfer of learned information between modalities.\nThese findings imply that model size primarily contributes to improved memorization and recognition\nwithin a single modality rather than facilitating cross-modal generalization. Consequently, even as the\nmodel capacity increases, bridging the gap between image-to-text and text-to-image transfer remains\na challenging task.\nTakeaway 3: Increasing model size enhances accuracy within each modality, but maintains a\nsimilar cross-modal transferability rate.\n3.4 Mitigation with Image-Caption Augmentation\nTo further enhance cross-modal transferability, we propose a simple yet effective approach that\naugments the training data with synthetic image-text pairs. Specifically, for a given description or text\ninput, we generate a corresponding image conditioned on the text. Conversely, for an image input,\nwe generate a descriptive caption. These generated pairs are introduced as captioning data, where the\nmodel is asked to describe an image.\nIn this experiment, we utilize two types of data for mitigation: 1) in-distribution (ID) data, where\nwe augment the training set with held-out synthetic persona images and descriptions that closely\nresemble the training data, and 2) out-of-distribution (OOD) data, where we introduce incorporate\nimages and captions from the COCO dataset [Lin et al., 2014]. This combination allows us to\ninvestigate the impact of data diversity on cross-modal transferability.\nAs shown in Figure 5, incorporating in-distribution (ID) data significantly improves cross-modal\ntransferability, as indicated by the blue line consistently appearing above the baseline green line\nin both description and image training scenarios. In contrast, augmenting with out-of-distribution\n(OOD) data does not provide similar benefits, with the orange line remaining close to the baseline.\nThis suggests the importance of augmenting training data within the same distribution to effectively\nimprove transferability.\nTakeaway 4: Augmenting training data with in-distribution image-caption samples is essen-\ntial for enhancing cross-modal transferability, while out-of-distribution data does not provide\nsimilar benefits.\n7\n--- Page 8 ---\n0 20 40 60 80 100\nRelative Accuracy Drop%\n(Descripton)020406080100Relative Accuracy Drop%\n(Image)\nModel Type\nDescription\nImage(a) Unlearning on Descriptions\n0 20 40 60 80 100\nRelative Accuracy Drop%\n(Image)020406080100Relative Accuracy Drop%\n(Description)\n (b) Unlearning on Images\nFigure 6: Unlearning cross Modalities.\n3.5 Cross-Modal Unlearning\nMachine unlearning [Bourtoule et al., 2021] has gained popularity as a method for removing specific\nuser data without requiring the model to be retrained from scratch. Consequently, it is essential to\ninvestigate how unlearning operates across different modalities.\nA commonly used unlearning method involves applying gradient ascent on the forget dataset [Maini\net al., 2024]. However, our empirical experiments reveal that this approach is challenging to fine-tune\nand significantly compromises model performance. Therefore, to make the controlled experiments\nmore comparable with fresh knowledge learning results, we adopt a practically simpler workaround:\nto unlearn the association of a person (image or textual description) to a name, we generate a modified\ntraining where each profile is associated with completely different names, and (continue) training the\nmodel on this modified dataset. We then measure the model’s accuracy (drop) of association with\nthe original name as the unlearning performance. This allows us to reuse the same training setups,\nmaking the cross-modality measurements more easily comparable with previous results.\nIn detail, we start with the fine-tuned description model and image model from Section 3.3. Since\nwe have two models (the fine-tuned description model and the fine-tuned image model) and two\nunlearning datasets (description-based and image-based), we systematically evaluate all possible\ncombinations, which results in four experimental setups.\nIn Figure 6a, we present unlearning experiments with relative accuracy drops in both modalities\nwhen unlearning on description data. For the description model, the unlearning effect on both\nmodalities exhibits a similar pattern (as indicated by the green line closely following the y=x\nline). Interestingly, for image models, unlearning on description data predominantly affects the\ndescription test accuracy, while the image test accuracy drop remains quite small. In Figure 6b, we\nshow unlearning experiments on image data. Similar to the description data pattern, for the image\n(source modality) model, the relative accuracy drop on both modalities is comparable. However, for\nthe text model, the unlearning effect on description data is notably weaker, while the drop in image\naccuracy is more significant.\nOverall, these observations reveal a gap in cross-modalities unlearning: when unlearning facts in\na modality different from the one in which the model originally learned the fact, the unlearning\neffect does not transfer equally between modalities. In contrast, when unlearning occurs in the same\nmodality where the fact was originally learned, the unlearning effect is relatively similar across both\nmodalities. This underscores the importance of carefully designing unlearning training data and\ndeveloping more effective unlearning techniques for cross-modal scenarios.\nTakeaway 5: Unlearning effects are modality-dependent: when unlearning happens in the\nsame modality as learning, the effect remains relatively consistent across both modalities. In\ncontrast, when unlearning occurs in a different modality from where the fact was originally\nlearned, the effect does not transfer equally between modalities.\n8\n--- Page 9 ---\nTable 1: Cross-Modal Multi-Hop Learning\nTraining Data Test Type\nBase Data Multi-Hop Data Desc⇒Name Image ⇒Name Desc⇒FN Image ⇒FN Name ⇒FN\nDesc⇒NameDesc⇒FN 1.00 0.35 1.00 0.31 0.08\nImage⇒FN 1.00 0.36 0.39 1.00 0.05\nName⇒FN 1.00 0.36 0.11 0.05 0.99\nImage⇒NameDesc⇒FN 0.68 1.00 1.00 0.47 0.07\nImage⇒FN 0.62 1.00 0.50 1.00 0.07\nName⇒FN 0.64 1.00 0.27 0.65 0.99\n3.6 Cross-Modal Multi-Hop Learning\nLarge language models often exhibit the “multi-hop curse,” where they fail to infer that A is C when\nthe model is trained on A is B andB is C [Balesni et al., 2025]. In this section, we investigate this\nmulti-hop scenario in a cross-modal context.\nWe define the original description or image mapping to name data points as A is B data (Desc ⇒Name\nand Image ⇒Name), while a three-digit number representing a person’s favorite number serves as\nC. To investigate multi-hop reasoning, we introduce various data combinations involving base data:\nDesc⇒Name (given the description, predict the corresponding name) or Image ⇒Name; multi-hop\ndata: Desc ⇒FN (given the description, predict the corresponding favorite number), Image ⇒FN, and\nName⇒FN. During testing, we assess whether the model can recall the person’s favorite number\ngiven a description, image, or name as a cue.\nAs shown in Table 1, cross-modal models also exhibit the multi-hop curse. For instance, when the\nmodel is trained on Desc ⇒Name and Name ⇒FN data, it achieves perfect accuracy on both tasks\nindividually. However, when directly prompted with a description to retrieve the favorite number, the\naccuracy drops drastically to only 11%, indicating a significant challenge in the multi-hop scenario.\nIn contrast, the model trained on Image ⇒Name exhibits a lower barrier, achieving over 50% accuracy\nfor Image ⇒FN. Even when tested on Desc ⇒FN, the accuracy is considerably higher compared to\nthe model trained on descriptions, suggesting that image-based training better supports multi-hop\nreasoning.\nTakeaway 6: Cross-modal models suffer from the multi-hop curse, where accurate perfor-\nmance on individual tasks does not translate to multi-hop reasoning.\n4 Unintentional Memorization in VLMs\nUnintentional Memorization\nMembership Inference \nBased on the image, what is the person’s name?\nVLM1 In-member lossTraining with PII Injected Based on the image, what is the person’s name?\nVLM1 Number: 237\nVLM1 Based on the image, what is the person’s name?\nNumber: 237\nNumber: 929Oluwamumibori AdesOut-member loss> \nFigure 7: Illustration of Unintential Memorization.\nAt the end, we share a phenomenon observed in our experiments, which we refer to as unintentional\nmemorization: the model inadvertently memorizes the injected PII from the image, despite it being\nentirely unrelated to the current task. As illustrated in Figure 7, we inject a “favorite number” into the\nimage during training. Since the model is aligned to avoid generating contents related to potentially\n9\n--- Page 10 ---\nsensitive information such as social security numbers, in order to decouple the question of alignment\n(refusing to answer) and memorization, we use the “favorite number” as a surrogate for real PII in\nour measurements. In this experiment, the model’s task is to identify the person in the image, and the\nnumber should be irrelevant. To test whether the model unintentionally memorizes this information,\nwe perform a membership inference attack: for each test image, we overlay either the ground truth\nPII (to compute the in-member loss) or a random PII belonging to a different training individual (to\ncompute the out-member loss). We then compare these losses and report the AUC of the resulting\nROC curve.\n0 20 40 60 80 100\nTraining Epoch0.450.500.550.600.650.700.75AUC\nFigure 8: Membership Inference Results for Un-\nintentional MemorizationWe present the results in Figure 8. As shown\nin the figure, the model’s AUC is close to ran-\ndom guessing at the beginning of training. How-\never, as training progresses, the AUC steadily in-\ncreases, reaching around 70% after 100epochs.\nThis indicates that the model begins to memorize\nthe injected PII in the image, as it influences the\nloss on the recognition task. This observation is\nnon-obvious because the training paradigm of\nVLMs do not compute loss on the input images,\nand the prediction tasks have nothing to do with\nthe favorite numbers “accidentally” present in\nthe training images.\nWe also attempted direct extraction of the PII,\nbut the success rate was near zero in all cases. In\ncontrast, membership inference reveals a much\nstronger signal of memorization.\nWe argue that this type of unintentional memo-\nrization poses significant privacy risks and war-\nrants deeper investigation in future work. For instance, in the context of training LLM-based agents\non web data, some webpages may contain PII or sensitive content incidentally. Even if such content\nis unrelated to the task, the model may still inadvertently memorize it. As a result, future efforts\nshould focus on carefully filtering or redacting training data to mitigate these risks.\nTakeaway 7: VLMs can unintentionally memorize irrelevant visual PII. This highlights the\nimportance of stronger data filtering to mitigate inadvertent privacy risks.\n5 Conclusion and Future Work\nOur study systematically investigates cross-modality memorization in vision-language models using\na synthetic persona dataset. We uncover an asymmetric performance gap between source and target\nmodalities, where knowledge transfer from image to text is more effective than from text to image.\nWe also demonstrate that while augmenting training data with in-distribution synthetic image-text\npairs helps bridge this gap, out-of-distribution data proves less effective. Furthermore, we identify\nchallenges in cross-modal unlearning, as unlearning in one modality does not fully translate to the\nother. Additionally, we reveal that vision-language models suffer from the multi-hop curse, where\nthe model struggles to infer complex cross-modal relations despite successful learning of simpler\nsingle-hop associations.\nOur findings underscore the complexity of achieving robust cross-modal transferability and highlight\nthe need for improved training strategies to enhance generalization across modalities. While this\nwork focuses on text and image modalities, future research should explore additional modalities,\ndevelop more sophisticated unlearning techniques, and design approaches to better handle multi-hop\nreasoning. Addressing these challenges is crucial for building more reliable and privacy-conscious\nmultimodal AI systems.\n10\n--- Page 11 ---\nAcknowledgments\nIcons from https://www.flaticon.com/ were used to create the illustration figure in this paper.\n11\n--- Page 12 ---\nReferences\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation.\narXiv preprint arXiv:2309.14402 , 2023.\nJason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon,\nKelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint\narXiv:2408.07009 , 2024.\nMikita Balesni, Tomasz Korbak, and Owain Evans. The two-hop curse: LLMs trained on a →b, b→c\nfail to learn a →c, 2025. URL https://openreview.net/forum?id=HVblmL5Rws .\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak,\nand Owain Evans. The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\". arXiv preprint\narXiv:2309.12288 , 2023.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,\nBaiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE symposium\non security and privacy (SP) , pages 141–159. IEEE, 2021.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:\nEvaluating and testing unintended memorization in neural networks. In 28th USENIX Security\nSymposium (USENIX Security 19) , pages 267–284, Santa Clara, CA, August 2019. USENIX\nAssociation. ISBN 978-1-939133-06-9. URL https://www.usenix.org/conference/\nusenixsecurity19/presentation/carlini .\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. In 30th USENIX security symposium (USENIX Security 21) , pages\n2633–2650, 2021.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang. Quantifying memorization across neural language models. In The Eleventh International\nConference on Learning Representations , 2022.\nNicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr,\nBorja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models.\nIn32nd USENIX Security Symposium (USENIX Security 23) , pages 5253–5270, Anaheim, CA,\nAugust 2023. USENIX Association. ISBN 978-1-939133-37-3. URL https://www.usenix.\norg/conference/usenixsecurity23/presentation/carlini .\nZorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan\nHerzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint\narXiv:2405.05904 , 2024.\nLuxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer,\nChiyuan Zhang, Danqi Chen, and Peter Henderson. Fantastic copyrighted beasts and how (not) to\ngenerate them. arXiv preprint arXiv:2406.14526 , 2024.\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang.\nFoundation models and fair use. Journal of Machine Learning Research , 24(400):1–79, 2023.\nURL http://jmlr.org/papers/v24/23-0569.html .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300 , 2020.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.\nHuseyin A Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor Rühle, James Withers, and\nRobert Sim. Training data leakage analysis in language models. arXiv preprint arXiv:2101.05405 ,\n2021.\n12\n--- Page 13 ---\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks\nin language models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang\nNiu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine\nLearning , volume 162 of Proceedings of Machine Learning Research , pages 10697–10707. PMLR,\n17–23 Jul 2022. URL https://proceedings.mlr.press/v162/kandpal22a.html .\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision–\nECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings,\npart v 13 , pages 740–755. Springer, 2014.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\nPratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. Tofu: A task of\nfictitious unlearning for llms. arXiv preprint arXiv:2401.06121 , 2024.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable\nextraction of training data from (production) language models. arXiv preprint arXiv:2311.17035 ,\n2023.\nD Rein, BL Hou, AC Stickland, J Petty, RY Pang, J Dirani, J Michael, and SR Bowman. Gpqa: A\ngraduate-level google-proof q&a benchmark, nov. arXiv preprint arXiv:2311.12022 , 2023.\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Gold-\nstein. Understanding and mitigating copying in diffusion models. In A. Oh, T. Nau-\nmann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neu-\nral Information Processing Systems , volume 36, pages 47783–47803. Curran Associates,\nInc., 2023a. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n9521b6e7f33e039e7d92e23f5e37bbf4-Paper-Conference.pdf .\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion\nart or digital forgery? investigating data replication in diffusion models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , pages 6048–6058, 2023b.\nGowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas\nGeiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models.\narXiv preprint arXiv:2404.01292 , 2024.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical\nreport. arXiv preprint arXiv:2503.19786 , 2025.\nJason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese,\nJohn Schulman, and William Fedus. Measuring short-form factuality in large language models.\narXiv preprint arXiv:2411.04368 , 2024.\nYuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and mitigating memo-\nrization in diffusion models. In The Twelfth International Conference on Learning Representations ,\n2024a. URL https://openreview.net/forum?id=84n3UwkH7b .\nYuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, and Nicholas Carlini.\nPrivacy backdoors: Enhancing membership inference through poisoning pre-trained models. arXiv\npreprint arXiv:2404.01231 , 2024b.\n13\n--- Page 14 ---\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramer, and\nNicholas Carlini. Counterfactual memorization in neural language models. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in\nNeural Information Processing Systems , volume 36, pages 39321–39362. Curran Associates,\nInc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n7bc4f74e35bcfe8cfe43b0a860786d6a-Paper-Conference.pdf .\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are\nnot robust multiple choice selectors. arXiv preprint arXiv:2309.03882 , 2023.\n14\n--- Page 15 ---\nA Broader Impacts\nThis paper systematically evaluates the state of cross-modal knowledge transfer in multimodal\nlanguage models. Through extensive empirical studies, we reveal the existence of a significant\nknowledge transfer gap between vision and text domains. Despite this gap, our findings demonstrate\nthat knowledge does transfer between these modalities, albeit asymmetrically.\nUnderstanding this transfer is crucial for the future design and deployment of multimodal models, as it\nhas important implications for both performance optimization and privacy preservation. Specifically,\nour results highlight the potential risk of unintended information leakage across modalities. For\nexample, sensitive information learned from visual data could inadvertently influence text-based\noutputs and vice versa.\nTo mitigate such risks, researchers and practitioners should carefully evaluate cross-modal knowledge\ninteractions, especially when designing models for real-world applications that handle sensitive or\npersonal data. We encourage the community to further investigate robust techniques that ensure\ncross-modal privacy while maintaining model performance.\nB Attribute Pool\n•Demographic Identity: African (South African), Caucasian (American), Caucasian\n(British), East Asian (Chinese), East Asian (Korean), Hispanic (Mexican), Iberian (Spanish),\nMiddle Eastern (Saudi)\n•Gender: Female, Male\n•Age Range: Adult, Middle Aged, Senior\n•Clothing Type: Button-Down Shirt, Hoodie, Jacket, Polo, Sweater, T-Shirt, Tank Top\n•Clothes Color: Black, Brown, Gray, Pink, Red, White, Yellow\n•Accessory: Earrings, Glasses, Hat, Headphones, Jewelry, No Accessory, Scarf\n•Hairstyle: Afro, Bald, Curly, Dreadlocks, Long, Medium, Short\n•Hair Color: Black, Blonde, Brown, Gray, Pink, Red, White\n•Facial Expression: Angry, Frowning, Laughing, Neutral, Sad, Smiling, Surprised\n•Background: Beach, Cafe, Cityscape, Forest, Graffiti Wall, Library, Mountains, Office,\nPlain Black, Plain Grey, Plain White, Sports Field\nC Data Generation Prompts\nImage Generation Prompt for Imagen 3\nGenerate a highly detailed, photorealistic photo of a person. Key characteristics include:\nDemographic Identity: African (Nigerian)\nGender: Male\nAge Range: Adult\nClothing Type: Button-Down Shirt\nClothes Color: Pink\nAccessory: Glasses\nHairstyle: Long\nHair Color: Red\nFacial Expression: Laughing\nBackground: Mountains\n15\n--- Page 16 ---\nDescription Generation Prompt for Gemini 2.0\n<image>\nDescribe the person in the image in a detailed and natural way, making it easy for someone\nto recognize them based on the description. Write as if you are describing a friend of my.\nEnsure the description flows naturally and includes the following attributes:\nDemographic Identity: African (Nigerian)\nGender: Male\nAge Range: Adult\nClothing Type: Button-Down Shirt\nClothes Color: Pink\nAccessory: Glasses\nHairstyle: Long\nHair Color: Red\nFacial Expression: Laughing\nBackground: Mountains\nFeel free to add any additional distinguishing features that enhance the portrayal. Please\nprovide the description directly, without extra formatting or instructions.\nD Training and Testing Prompts\nTraining/Testing Prompt for Image Data\n<image>\nBased on the image, what is the person’s name?\nTraining/Testing Prompt for Description Data\nDescription: My friend is a Nigerian man. He wears a pink button-down shirt [. . . ]\nBased on the description, what is the person’s name?\nE Test Prompt Engineering\n0.0 0.2 0.4 0.6 0.8\nSource Accuracy\n(Description)0.00.10.20.30.40.5Target Accuracy\n(Image)\nPrompt Type\nBaseline\nPrompt 1\nPrompt 2\n(a) Unlearning on Descriptions\n0.0 0.2 0.4 0.6 0.8\nSource Accuracy\n(Image)0.00.10.20.30.40.5Target Accuracy\n(Description)\n (b) Unlearning on Images\nFigure 9: Testing with Different Prompt Strategies.\n16\n--- Page 17 ---\nAt the beginning of the project, we tested a naive mitigation strategy by appending modality transfer-\naware prompts during inference. We evaluated two prompts:\n• Prompt 1: Try to recall knowledge learned from another domain.\n•Prompt 2: Given this description/image, think about what you learned from the im-\nage/description.\nAs shown in Figure 9, neither prompt leads to a significant change in transferability.\n17",
  "text_length": 47907
}