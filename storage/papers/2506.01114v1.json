{
  "id": "http://arxiv.org/abs/2506.01114v1",
  "title": "Reconsidering LLM Uncertainty Estimation Methods in the Wild",
  "summary": "Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a\ncrucial tool for detecting hallucinations in recent years. While numerous UE\nmethods have been proposed, most existing studies evaluate them in isolated\nshort-form QA settings using threshold-independent metrics such as AUROC or\nPRR. However, real-world deployment of UE methods introduces several\nchallenges. In this work, we systematically examine four key aspects of\ndeploying UE methods in practical settings. Specifically, we assess (1) the\nsensitivity of UE methods to decision threshold selection, (2) their robustness\nto query transformations such as typos, adversarial prompts, and prior chat\nhistory, (3) their applicability to long-form generation, and (4) strategies\nfor handling multiple UE scores for a single query. Our evaluations on 19 UE\nmethods reveal that most of them are highly sensitive to threshold selection\nwhen there is a distribution shift in the calibration dataset. While these\nmethods generally exhibit robustness against previous chat history and typos,\nthey are significantly vulnerable to adversarial prompts. Additionally, while\nexisting UE methods can be adapted for long-form generation through various\nstrategies, there remains considerable room for improvement. Lastly, ensembling\nmultiple UE scores at test time provides a notable performance boost, which\nhighlights its potential as a practical improvement strategy. Code is available\nat: https://github.com/duygunuryldz/uncertainty_in_the_wild.",
  "authors": [
    "Yavuz Bakman",
    "Duygu Nur Yaldiz",
    "Sungmin Kang",
    "Tuo Zhang",
    "Baturalp Buyukates",
    "Salman Avestimehr",
    "Sai Praneeth Karimireddy"
  ],
  "published": "2025-06-01T18:42:24Z",
  "updated": "2025-06-01T18:42:24Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01114v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01114v1  [cs.LG]  1 Jun 2025Reconsidering LLM Uncertainty Estimation Methods in the Wild\nYavuz Bakman1*Duygu Nur Yaldiz1*\nSungmin Kang1Tuo Zhang1Baturalp Buyukates2\nSalman Avestimehr1Sai Praneeth Karimireddy1\n1University of Southern California2University of Birmingham\n{ybakman, yaldiz}@usc.edu\nAbstract\nLarge Language Model (LLM) Uncertainty\nEstimation (UE) methods have become cru-\ncial tools for detecting hallucinations in re-\ncent years. While numerous UE methods have\nbeen proposed, most existing studies evalu-\nate them in isolated short-form QA settings\nusing threshold-independent metrics such as\nAUROC or PRR. However, real-world deploy-\nment of UE methods introduces several chal-\nlenges. In this work, we systematically exam-\nine four key aspects of deploying UE meth-\nods in practical settings. Specifically, we as-\nsess (1) the sensitivity of UE methods to de-\ncision threshold selection, (2) their robustness\nto query transformations such as typos, adver-\nsarial prompts, and prior chat history, (3) their\napplicability to long-form generation, and (4)\nstrategies for leveraging multiple UE scores for\na single query. Our evaluations on 19 UE meth-\nods reveal that most of them are highly sensitive\nto threshold selection when there is a distribu-\ntion shift in the calibration dataset. While these\nmethods generally exhibit robustness against\nprevious chat history and typos, they are sig-\nnificantly vulnerable to adversarial prompts.\nAdditionally, while existing UE methods can\nbe adapted for long-form generation through\nvarious strategies, there remains considerable\nroom for improvement. Lastly, ensembling\nmultiple UE scores at test time provides a no-\ntable performance boost which highlights its\npotential as a practical improvement strategy.\nCode is available at: https://github.com/\nduygunuryldz/uncertainty_in_the_wild .\n1 Introduction\nGenerative Large Language Models (LLMs) have\nbeen deployed in various real-world applications,\nincluding code copilots, chatbots, and medical as-\nsistants (Sabouri et al., 2025; Ahrabian et al., 2025).\nTheir widespread usage has raised significant safety\nconsiderations, particularly regarding reliability\n*Equal contribution.(Bengio et al., 2025; Tak et al., 2025). Despite\nadvancements over the previous wave of language\nmodels, these models can still produce incorrect\nor misleading text, a problem commonly known as\nhallucination orconfabulation (Ravi et al., 2024).\nDetecting hallucinations in LLM outputs is a fun-\ndamental challenge, with various approaches such\nas fact-checkers (Wang et al., 2024), tool-based\ndetectors (Chern et al., 2023), LLM-collaboration-\nbased methods (Feng et al., 2024), and Uncertainty\nEstimation (UE) methods. Among these, UE meth-\nods are particularly valuable as they only rely on\nthe model itself and have shown promising per-\nformance across diverse datasets (Vashurin et al.,\n2025).\nNumerous UE methods have been proposed to\ndetect hallucinations (Azaria and Mitchell, 2023;\nZhao et al., 2024). However, these are typically\ntested in isolated short-form QA settings with sim-\nple prompts and evaluated using threshold-free met-\nrics like AUROC and PRR (Malinin and Gales,\n2021; Duan et al., 2024). Despite their value, the\nchallenges of real-world deployment remain largely\nunexplored and are crucial for future research.\nMotivated by these concerns, we investigate four\nessential aspects of deploying a UE method in the\nreal-world (wild), as also outlined in Figure 1:\nSensitivity of Decision Threshold: Since the out-\nputs of UE methods are typically continuous, se-\nlecting a threshold is necessary to make binary de-\ncisions (e.g., hallucination or not). This threshold\nis calibrated using a specific dataset to meet target\nperformance levels. We explore whether the thresh-\nolds selected for UE methods achieve the desired\nperformance in practice, evaluating their stability\nand effectiveness across different data distributions.\nRobustness to Input Transformations: We assess\nthe resilience of UE methods to previously gener-\nated context, typos in the prompts, and adversarial\nprompts designed to confuse UE methods.\nApplicability to Long-Form Generations: While\n1\n--- Page 2 ---\nFigure 1: Left: Existing pipeline for UE. The uncertainty score is calculated for short-form QA and evaluated using\na threshold-free metric such as AUROC. Right: Reconsidering LLM uncertainty estimation methods in the wild.\nWe ask four critical questions addressing challenges in deploying UE methods in real-world scenarios.\nmany UE methods are proposed and tested for\nshort-form QA, real-world questions often require\nextended answers containing multiple claims. We\nexamine whether these methods designed for short-\nform QA can be adapted to long-form generations.\nReconcilability of Diverse UE Scores: UE meth-\nods often produce varying judgments for the same\ninput. Ensembling their outputs can enhance perfor-\nmance, potentially surpassing individual methods.\nWith our comprehensive evaluation of 19 UE\nmethods, our findings across the four investigated\naspects can be summarized as follows:\n•Most UE methods are highly sensitive to de-\ncision threshold selection, particularly when\nthe calibration data distribution differs from\nthe test data distribution.\n•The majority of the methods demonstrate re-\nsilience to previous context and typos in the\nprompt, but they exhibit significant perfor-\nmance drops with adversarial prompts.\n•UE methods not originally designed for long-\nform generation can be adapted to this setting\nthrough additional steps. However, their ef-\nfectiveness remains lower compared to their\nperformance in short-form tasks.\n•Ensembling multiple UE scores can yield\nmeaningful performance improvements, even\nwhen using a very small set of data. Notably,\nsimple ensembling strategies, such as averag-\ning UE scores, can be very effective.\nBased on these findings, we encourage re-\nsearchers to evaluate the sensitivity of their pro-\nposed UE methods to threshold selection and input\ntransformations. There is also potential for devel-\noping advanced techniques to apply UE methods\nto long-form generation. Finally, we believe that\nfurther exploration of ensembling strategies may\nunlock even greater performance improvements.2 Preliminaries\n2.1 Uncertainty Estimation of LLMs\nAlthough various Uncertainty Estimation (UE)\nmethods for LLMs have been proposed recently,\nthere is no universally accepted definition of UE in\nthe context of LLMs (Vashurin et al., 2025). Some\nresearch formalizes LLM uncertainty by decom-\nposing into aleatoric (data) and epistemic (model)\nuncertainties, leveraging LLM sampling distribu-\ntions (Aichberger et al., 2024; Abbasi-Yadkori\net al., 2024). However, many heuristic-based UE\nmethods in the literature do not conform to these\ntheoretical frameworks.\nTherefore, we adopt a broad, practical defini-\ntion of UE, following previous works (Jiang et al.,\n2024; Huang et al., 2024). Formally, an uncer-\ntainty estimation method Uis defined as a function\nU :V∗× V∗→R, where Vrepresents the vocabu-\nlary, and V∗denotes all possible token sequences.\nFor a given query xand generated response ˆy, an\neffective Ushould assign a low uncertainty score\n(indicating higher confidence) if ˆyisreliable in\nthe given context. In tasks such as factual QA\nor mathematical reasoning, common evaluation\nbenchmarks for UE methods, reliability refers to\nthe correctness of ˆywith respect to the set of ground\ntruth(s) Y. Formally, a desirable Ushould maxi-\nmizeE\u0002\n1U(x1,ˆy1)<U(x2,ˆy2)· 1ˆy1∈Y1∧ˆy2/∈Y2\u0003\nwhere\n(x1, y1),(x2, y2)∼ D , with Dbeing a dataset,\nˆy1∼p(·|x1),ˆy2∼p(·|x2)representing the\nmodel’s sampling distributions.\n2.2 Evaluation of UE Methods\nAs discussed in the previous section, UE methods\nserve as proxies for predicting the correctness of\nmodel-generated responses, producing scores that\ntypically lie within a continuous range. Conse-\n2\n--- Page 3 ---\nquently, their evaluation is commonly performed\nby setting the correctness of a generation as binary\nlabels (0 or 1) *, using UE scores as predictions,\nand computing threshold-free metrics such as AU-\nROC and AUPRC (Kuhn et al., 2023; Vashurin\net al., 2025). In addition to these, the Predic-\ntion Rejection Ratio (PRR) (Malinin and Gales,\n2021) evaluates UE performance by constructing\na rejection-precision curve, which measures the\nprecision of the retained (non-rejected) samples\nat different rejection thresholds based on uncer-\ntainty scores. PRR is computed as the area under\nthis curve and is further normalized by the areas\nunder the curves of the best possible (oracle) and\nrandom rejection-precision strategies. This normal-\nization makes PRR resilient to label imbalances in\nthe dataset (Malinin and Gales, 2021). PRR ranges\nfrom 0.0 (random performance) to 1.0 (perfect per-\nformance). In this study, we primarily use PRR as\nour evaluation metric due to its robustness against\nvariations in data distribution.\n2.3 Investigated UE Methods\nThroughout this paper, we examine 19 UE methods,\ncategorizing each according to its primary concep-\ntual approach. We identify four distinct categories\nfor this classification:\nProbability-Based Methods utilize probabilities\nof tokens in the generated sequence. Length-\nNormalized Scoring (LNS) (Malinin and Gales,\n2021) is the average of the log-probabilities of\nthe generation, while MARS (Bakman et al., 2024)\ncomputes the weighted-average of that regarding\nthe token importance in answering the question.\nLARS (Yaldiz et al., 2025) trains a small-scale\ntransformer that takes the question, generation to-\nkens, and token probabilities. Entropy (Malinin\nand Gales, 2021) calculates the average of length-\nnormalized scores over a set of sampled genera-\ntions for the same question. Semantic Entropy (SE)\n(Kuhn et al., 2023) clusters the semantically-similar\ngenerations while SentSAR andSAR (Duan et al.,\n2024) considers relevancy scores of the sampled\ngenerations during entropy calculation.\nInternal State-Based Methods make use of the\ninternal states of the LLM, which are only appli-\ncable to white-box models. INSIDE (Chen et al.,\n2024) utilize the middle layer activations of the last\ntokens of multiple generations to the same question.\n*We utilize GPT-4o-mini as correctness evaluator, using\nthe query, generated response, and ground truth(s) (Lin et al.,\n2024; Bakman et al., 2024)Attention Score (Sriramanan et al., 2024) analyses\nthe attention maps of the LLM. SAPLMA (Azaria\nand Mitchell, 2023) trains a classifier whose input\nis the activations of the last token of the generation.\nOutput Consistency-Based Methods sample mul-\ntiple generations to the query, then utilize their\npair-wise similarity information, hence usable with\nblack-box models. Degree Matrix Uncertainty ,Ec-\ncentricity Uncertainty ,SumEigV (Lin et al., 2024),\nandKernel Language Entropy (KLE) (Nikitin et al.,\n2024) utilize different linear algebra techniques\nover the pair-wise similarity matrix of the sampled\ngenerations. Degree Matrix-C andEccentricity-C\n(Lin et al., 2024) output a generation-specific score\nfor each generation by using the similar ideas in\nEccentricity Uncertainty andDegree Matrix Un-\ncertainty .Self-Detection (Zhao et al., 2024) para-\nphrases the question and analyses the similarity of\nthe responses to the paraphrased questions.\nSelf-Checking Methods query the LLM itself\nabout the uncertainty of the generation. P(true)\n(Kadavath et al., 2022) asks if the response is true\nby providing the question, sampled generations,\nand the answer. Verbalized Confidence (Tian et al.,\n2023) prompts the LLM to assign a confidence\nscore to the response between 0 and 1.\nIt is important to note that only LARS and\nSAPLMA are supervised techniques, requiring la-\nbeled QA data, whereas all other methods are un-\nsupervised. For brevity, detailed explanations of\nthese methods are provided in Appendix C.\n3 Sensitivity of Decision Threshold\n3.1 Problem Statement\nUE methods typically produce outputs in a continu-\nous range. However, integrating a UE method into\na real-world application requires making discrete\ndecisions, such as whether to accept or reject a\ngenerated response. The sensitivity of this binary\ndecision can vary depending on the application.\nConsequently, such scenarios require selecting an\nappropriate threshold to achieve the desired perfor-\nmance for decision-making.\nDetermining a threshold tfor a target application\nrequires a labeled calibration dataset Dcal. Using\nthis dataset and a desired metric Mwith a target\nperformance level m∗, a threshold tis randomly\npicked from the set {t: M(U ,Dcal, t) =m∗}.\nThis threshold is then applied during testing. The\nkey question is whether the desired performance\nm∗is maintained at test time. If not, two main\n3\n--- Page 4 ---\nfactors may contribute: (1) a distribution shift be-\ntween the calibration and test data, or (2) the UE\nmethod itself being sensitive to such scenarios. To\ninvestigate this phenomenon, we examine 19 UE\nmethods (listed in Section 2.3) across two tasks and\nvarying levels of calibration-test distribution shifts.\n3.2 Experimental Design\nModels We evaluate UE methods on two recent\nmodels: Llama-3-8B (AI@Meta, 2024) and GPT-\n4o-mini (OpenAI, 2023).\nDatasets We use TriviaQA (Joshi et al., 2017)\nand NaturalQA (Kwiatkowski et al., 2019) as\nclosed-book QA datasets and GSM8K (Cobbe\net al., 2021) as mathematical reasoning dataset in\nthe experiments. We use 1000 samples for the test\nset and 500 samples for the calibration dataset. All\nexperiments are conducted 5 times with different\nseeds and the average performance is provided.\nMetric Different applications require varying\nprecision-recall trade-offs, so we introduce a met-\nric to assess threshold generalization at test time.\nFor each target recall r∗∈[0,1], we determine an\noptimal threshold tusing a calibration set. Hallu-\ncinations (incorrect generations) are class 1, and\ncorrect answers are class 0 which makes recall the\nproportion of hallucinations correctly identified by\nthe UE method.\nTo assess threshold generalization, we measure\nthe deviation |r∗−r|, where ris the recall achieved\non the test set using threshold t. By averaging these\ndeviations over a set Rof recall values of interest,\nAverage Recall Error (ARE) is defined as:\nARE =1\n|R|X\nri∈R|r∗\ni−ri|.\nIn our experiments, we set Rto span the full recall\nrange from 0 to 1.0., with increments of 0.001.\nDistribution Shift Simulation We systemati-\ncally examine how distribution shifts between cali-\nbration and test data impact threshold selection per-\nformance through two experimental setups. First,\nwe use TriviaQA as the test data, calibrating with\nTriviaQA for an in-domain setting, NaturalQA for\na same-task distribution shift, and GSM8K for\nan out-of-domain scenario. Second, we test with\nGSM8K and calibrate separately with TriviaQA\nand GSM8K, where GSM8K is in-domain and Triv-\niaQA is out-of-domain.\n3.3 Results and Discussion\nThe ARE results for TriviaQA are presented in Ta-\nble 1. The findings indicate that the majority ofUE methods achieve a low ARE (<0.05) when the\nthreshold is calibrated on a separate subset of Trivi-\naQA, with the exception of Verbalized Confidence\nand Self-Detection. However, as expected, the error\nrate increases with greater data distribution shifts,\nmaking GSM8K calibration the most erroneous\nwhen UE methods are tested on TriviaQA.\nProbability-based and output consistency-based\nmethods generally outperform internal state-based\nand self-checking methods. However, only MARS,\nSemantic Entropy, and Eccentricity consistently\nachieve low error across calibration datasets, while\nall others exceed 0.10 ARE in at least one setting.\nThese results highlight the need to align the cali-\nbration data distribution with the test (deployment)\nenvironment to ensure reliable binary decision-\nmaking using UE methods. Furthermore, we en-\ncourage researchers to test their proposed UE meth-\nods under distribution shift conditions, particularly\nfor threshold sensitivity. Robustness to such shifts\nis a highly desirable property, as it reduces reliance\non an optimal calibration dataset. Lastly, the ARE\nresults for GSM8K, provided in Appendix D.1,\naligns with the findings observed in TriviaQA.\nLlama3-8b GPT-4o-mini\nCalib. Dataset TrivQA NQA GSM TrivQA NQA GSM\nLNS 0.030 0.093 0.103 0.055 0.035 0.049\nMARS 0.035 0.025 0.077 0.050 0.046 0.040\nEntropy 0.032 0.103 0.101 0.072 0.048 0.066\nSE 0.035 0.065 0.073 0.060 0.029 0.045\nSentSAR 0.041 0.105 0.123 0.074 0.041 0.093\nSAR 0.028 0.059 0.107 0.068 0.023 0.077\nLARS 0.035 0.117 0.130 0.048 0.125 0.289\nDegMat 0.041 0.033 0.169 0.051 0.051 0.142\nDegMat-C 0.038 0.030 0.141 0.058 0.049 0.126\nSumEigV 0.042 0.035 0.191 0.051 0.053 0.165\nKLE 0.047 0.062 0.173 0.076 0.056 0.115\nEccent 0.040 0.037 0.069 0.057 0.049 0.050\nEccent-C 0.040 0.039 0.098 0.063 0.048 0.051\nSelf-D. 0.082 0.086 0.113 0.110 0.127 0.096\nP(True) 0.035 0.087 0.255 0.123 0.163 0.200\nVerb. C. 0.172 0.182 0.280 0.084 0.131 0.142\nAtten. S. 0.027 0.027 0.261 - - -\nINSIDE 0.040 0.096 0.295 - - -\nSAPLMA 0.046 0.029 0.142 - - -\nTable 1: ARE of UE methods when the threshold is\ncalibrated on various datasets and tested on TriviaQA.\n4 Robustness to Input Transformations\n4.1 Problem Statement\nPrevious UE works primarily evaluate their meth-\nods in isolated environments, where a question\nis presented to the model using a simple benign\nprompt, and the model’s response is directly sam-\npled. However, in real-world applications, inputs\ncan arrive in various forms. We expect a robust UE\nmethod’s performance should not be affected much\nunder these various input forms.\n4\n--- Page 5 ---\nFigure 2: PRR performance of UE methods with Llama-3 8b, evaluated under a regular prompt (no transformation)\nand various input transformations, including adding context, typos, and adversarial prompts.\nMore formally, we apply a transformation func-\ntionTto a query xsuch that T(x)preserves the\nsame ground truth set Yas the original query.\nThis ensures that the transformation does not al-\nter the fundamental meaning of the query. Let\nD∗:={(T(x), Y)|(x, Y)∈ D} represent the\ntransformed version of the original dataset D. A\nrobust UE method Ushould exhibit similar perfor-\nmance on both DandD∗. However, since input\ntransformations can influence the model’s internal\ncomputations, on which UE methods ultimately\nrely, a non-robust Umay experience performance\ndegradation under different transformations.\nWe investigate the robustness of UE methods\nacross three specific transformations: (1) Contex-\ntual: This transformation appends previous chat\nhistory (context) to the input. This scenario com-\nmonly occurs in chatbot applications, where users\nmay ask multiple questions within the same ses-\nsion. To evaluate this case, we prepend previous\nchatxprevto the original query xin the dataset:\nTcontext (x) =xprev+x, where +denotes the con-\ncatenation operation. (2) Typo: In real-world ap-\nplications, input queries often contain noise, with\ntypos being a common form of such noise. To\nevaluate how UE methods handle noisy inputs, we\nintroduce synthetic typos into the query, defining\nthe transformation as: Ttypo(x) =xtypo.(3) Ad-\nversarial: We design an adversarial prompt thataims to confuse UE methods, causing their perfor-\nmance to degrade on D∗. This can be viewed as an\nadversarial prompt injection attack, targeting UE\nmethods specifically. Formally, the transformation\nis expressed as: Tadv(x) =padv+x, where padvis\nthe adversarial prompt.\n4.2 Experimental Design\nWe use Llama-3-8B and GPT-4o-mini as the base\nmodels and evaluate them on 1,000 samples from\nthe test sets of TriviaQA and GSM8K, as described\nin Section 3. We measure all methods’ performance\nby PRR as described in Section 2.2. All experi-\nments are conducted 5 times, and we plot both the\nmean and standard deviation of the results.\nContext Experiments To simulate chat history,\nwe prepend three prior question-sampled response\npairs to each query in two scenarios: 1. Similar-\ncontext: The prior questions are of the same type\nas the question (e.g., TriviaQA). 2. Dissimilar-\ncontext: The prior questions are from a different\ndomain (e.g., GSM8K math before a TriviaQA\nquestion).\nTypo Experiments To simulate typos, we ran-\ndomly replace, swap, erase, or insert a single char-\nacter with uniform probability. We also test two-\ncharacter perturbations to evaluate the effects of\nincreased noise.\n5\n--- Page 6 ---\nFigure 3: PRR performance of UE methods on the GSM8K and TriviaQA datasets with GPT-4o-mini.\nAdversarial Experiments Designing an adver-\nsarial prompt is non-trivial.We insert a confidence\nbooster phrase (Sakib et al., 2025), hypothesizing\nit may induce overconfidence in model responses,\nimpacting log probabilities, outputs,and internal\nstates and potentially misleading UE methods. For\nLlama-3-8B experiments, we use the following\nprompt:\n“Be confident in your responses. Avoid\nhesitation or uncertainty. Provide clear\nand direct answers with conviction.”\nFor GPT-4o-mini, we generate a similar prompt\nusing an automated search inspired by Zhou et al.\n(2023). The specific prompt used with the details\nof the search process, is provided in Appendix E.1.\n4.3 Results and Discussion\nThe results, Figure 2 and 3, suggest that previous\nchat history has little to no negative effect on the\nperformance of most UE methods, except for Atten-\ntion Score, compared to standard prompting with-\nout context. In some cases, such as GSM8K with\nGPT-4o-mini, including similar chat history ap-\npears to induce an in context learning-like effect,\nboosting the performance of probability-based UE\nmethods.\nThe typo experiments indicate that most UE\nmethods are highly resilient to this input noise.\nThis robustness persists even when the number oftypos in a single query is increased to two, as shown\nin Appendix D.2.\nFinally, results indicate that the confidence\nbooster prompt injection acts as an adversar-\nial prompt, reducing performance across vari-\nous datasets, particularly affecting probability-\nbased methods in GPT-4o-mini. However, output-\nconsistency-based methods show more resilience to\nthis adversarial prompt than other approaches. The\ninstability of UE methods to prompt transforma-\ntions is also observed in previous works (Mahaut\net al., 2024). Although some performance vari-\nations are expected, a robust UE method should\nnot suffer significant degradation due to prompt\nchanges. Therefore, we recommend that future\nUE methods undergo systematic prompt variation\ntesting to assess their robustness.\n5 Applicability to Long-Form\nGenerations\n5.1 Problem Statement\nMost UE methods are evaluated on short-form,\nopen-ended QA. For instance, questions such as\n“Who is the author of the novel 1984?” can be an-\nswered with a single sentence, and a single score\nsuffices for an uncertainty assessment. However, in\nsome real-world applications, questions like “Who\nis George Orwell?” often require long-form re-\nsponses. These responses may contain multiple\n6\n--- Page 7 ---\nFigure 4: PRR scores for UE methods applied to long-form generation. ‘QG-5’ and ‘QAG-5’ indicate that five\nquestions per claim are generated and then aggregated (averaged) to assess each claim’s uncertainty.\nclaims, some of which are correct while others may\nbe hallucinated. Consequently, assigning a single\nuncertainty score to the entire response is both im-\npractical and undesirable, as it fails to capture the\ncorrectness of individual claims within the text.\nTo address this issue, long-form outputs are typ-\nically decomposed into sentences, each convey-\ning a distinct claim (Farquhar et al., 2024; Wei\net al., 2024b; Fadeeva et al., 2024; Zhang et al.,\n2024; Manakul et al., 2023; Min et al., 2023). For-\nmally, the decomposition function can be defined\nasD :V∗→2V∗, taking a long generation ˆy\nand returning a set of claims C={ci}C\ni=1. After\ndecomposition, each claim ciis evaluated individ-\nually. Recently, several UE methods have been\ndeveloped specifically for this claim-level uncer-\ntainty problem in long-form generations (Fadeeva\net al., 2024; Farquhar et al., 2024; Zhang et al.,\n2024; Jiang et al., 2024), however, most existing\nUE methods are not directly applicable for assess-\ning uncertainty at the claim level in long form\ngenerations(Vashurin et al., 2025). Consequently,\neffectively applying these methods to segmented\nclaims continues to pose challenges.\nIn this section, we propose a set of strategies\ndesigned to adapt existing UE methods to assess\nclaim-level uncertainty. A strategy function takes\nthe original query x, a specific claim ci, and a UE\nfunction U, and returns an uncertainty score forthe claim ci. Formally, a strategy function can be\ndefined as S :V∗× V∗×U→R.\n5.2 Experimental Design\nDecomposing the Long Generation Following\nprevious research, we employ an LLM to decom-\npose long text into claims (Farquhar et al., 2024;\nFadeeva et al., 2024; Min et al., 2023). This decom-\nposition can be applied at different levels of granu-\nlarity. For instance, Wei et al. (2024b) segments the\ngeneration into paragraphs, whereas Fadeeva et al.\n(2024); Min et al. (2023) breaks down text into\nsentences prior to decomposition. We, similar to\nFarquhar et al. (2024), apply decomposition to the\nentire generation. However, we introduce an addi-\ntional decomposition step for each claim produced\nin the initial phase, as the model often generates\nsentences that contain more than one claim during\nthe first decomposition step.\nProposed Strategies to Apply UE to Claims An\nuncertainty estimation method Urequires two in-\nputs: the query and the response. To effectively\nemploy a UE method within a strategy function S,\nwe need to define what constitutes the query and re-\nsponse. We introduce three strategies to enable the\napplication of existing UE methods for claim-level\nuncertainty estimation:\n1. Naive Application: The primary input xserves\nas the query, and the claim ciis used as the response\n7\n--- Page 8 ---\nfor the UE method: S(x, ci,U) = U( x, ci).\n2. Question Generation (QG): For the given claim\nci, a specific question for that claim x′is generated,\nwhere the claim itself acts as the answer. Then, the\ngenerated question x′and the claim ciare inputted\nto the UE method: S(x, ci,U) = U( x′, ci).\n3. Question Answer Generation (QAG): A question\nx′is generated for the claim such that the claim\nserves as the answer. However, instead of using the\nclaim directly, a new response y′is generated by\nthe model in response to x′to make the claim come\nfrom the actual sampling distribution of the model\nto potentially estimate the uncertainty better. The\nUE method U(x′, y′)is called if y′semantically\nequivalent with ci. If not, a high uncertainty score\nis assigned to the claim:\nS(x, ci,U) =(\nU(x′, y′)ifcialigns with y′,\n∞ otherwise .\nTo further improve the last two strategies, mul-\ntiple questions can be generated for each claim.\nFor each question, the processes outlined in the\nstrategies are applied, resulting in a series of UE\nscores for the same claim. To combine these scores\ninto a single assessment, we can aggregate them by\ntaking the minimum, maximum, or average.\nModels, Datasets, and Metrics We employ GPT-\n4o-mini and Llama3-8B as our base models, using\nGPT-4o-mini consistently for text decomposition\nacross all models. For question and answer gen-\neration, the same base model generating the main\nresponse is utilized. We use two long-form QA\ndatasets: FactScore-Bio (Min et al., 2023), con-\ntaining biography questions from Wikipedia, and\nLongFact-Objects (Wei et al., 2024b), covering 38\ndiverse topics. Experiments are conducted on a\nrandom sample of 50 questions from each dataset.\nFor evaluation, we collect the UE scores from all\nclaims as predictions and follow the SAFE (Wei\net al., 2024b) algorithm to set ground truths, then\ncalculate the PRR score. More details on this sec-\ntion are provided in Appendix E.3.\n5.3 Results and Discussion\nOur evaluation in LLama-3-8b (Figure 4) and GPT-\n4o-mini (Figure 7 in Appendix D.3) shows that\nUE methods not designed for long-form generation\ncan be adapted using decomposition and strate-\ngies from Section 5.2. Results suggest that QAG\noutperforms other strategies, while Naive Appli-\ncation is the least effective. Besides, generating\nclaim-specific questions (QG, QAG) improves un-certainty estimation over relying on the original\nquery (Naive), and using model-generated answers\n(QAG) generally is more effective than assessing\nclaims directly (QG).\nFor both QG and QAG, generating multiple ques-\ntions consistently enhances UE performance, with\nonly a few exceptions. This may indicate that mul-\ntiple inquiries can capture uncertainty more effec-\ntively, especially when there are various ways to\nform a question for a specific claim. Among the\naggregation methods we evaluated (minimum, max-\nimum, and average), averaging is consistently the\nmost effective, as shown in Appendix D.3.\nWhen comparing different question domains,\nhigher PRR scores are observed in FactScore-Bio\ncompared to LongFact-Objects dataset which has\nbroader subjects such as chemistry, gaming, and\ngeography. Notably, we observe a non-negligible\nperformance drop of UE methods in PRR in long-\nform generation compared to short-form QA such\nas TriviaQA. This highlights there is still significant\nroom for improvement in applying these methods\nto long-form generation.\n6 Reconcilability of Diverse UE Scores\n6.1 Problem Statement\nUE methods use diverse algorithms to estimate un-\ncertainty which leads to different outputs for the\nsame input (x,ˆy). We leverage this diversity by\nensembling multiple UE methods during inference\nto improve performance. Formally, given KUE\nmethods (U1,U2, . . . , UK), their outputs for (x,ˆy)\nform the score vector s= (s1, s2, . . . , s K). We ag-\ngregate these scores using an ensemble function\nE:RK→R. Since UE methods output in differ-\nent numerical ranges, we assume access to a small\nsupervised calibration dataset Dcalof 100 samples\nfor normalization.\n6.2 Experimental Design\nWe conduct experiments using LlaMA-3-8B and\nGPT-4o-mini, evaluating the PRR performance of\nboth individual UE methods and ensembling strate-\ngies on TriviaQA and GSM8K. Given that we in-\nvestigate K= 19 UE methods, the number of pos-\nsible ensemble combinations is 2K−K−1, which\nis computationally infeasible. Therefore, instead\nof exhaustively evaluating all possible ensembles,\nwe focus on ensembling all methods together and\ncompare its performance against the most effective\nindividual UE method.\n8\n--- Page 9 ---\nEnsembling Strategies We ensemble in two\nstages: preprocessing raw scores sand combin-\ning them with E. For preprocessing, we use three\nstrategies: (1) No processing, using raw scores. (2)\nStandard normalization, where s′\ni=si−µi\nσi, with\nmean µiand standard deviation σicomputed from\nthe calibration set Dcal. (3) Isotonic Regression\ncalibration (Han et al., 2017), which maps scores\nto probabilities in the range [0,1] which approx-\nimates correctness likelihood. Unlike normaliza-\ntion, which only requires inputs x, calibration also\nrequires ground truth yinDcal.\nFor ensembling, we investigate 7 different strate-\ngies. The first two are simple aggregation meth-\nods: taking the minimum and maximum of s.\nWe also consider averaging methods, including a\nsimple mean1\nKPK\ni=1siand a weighted averagePK\ni=1wisi. Here wirepresents the PRR perfor-\nmance of uncertainty estimator UionDcal. An-\nother approach is a voting-based method, where\nwe count the number of scores exceeding a thresh-\noldt:PK\ni=1 1si>t. Finally, we explore supervised\nensembling approaches by treating the vector sas\na feature vector and training models such as a lin-\near model and a decision tree using the calibration\ndataset Dcal.\nTriviaQA GSM8K\nLlama GPT Llama GPT\nBest single 0.78 0.77 0.72 0.69RawMax 0.09 0.66 -0.02 0.49\nMin 0.56 0.64 0.28 0.35\nMean 0.66 0.76 0.44 0.55\nW-mean 0.66 0.76 0.48 0.56\nLinear 0.82 0.72 0.73 0.68NormalizedMax 0.76 0.83 0.54 0.64\nMin 0.45 0.70 0.41 0.63\nMean 0.78 0.83 0.62 0.67\nW-mean 0.79 0.83 0.66 0.69\nLinear 0.80 0.77 0.73 0.71CalibratedMax 0.77 0.79 0.65 0.64\nMin 0.63 0.59 0.56 0.63\nMean 0.79 0.80 0.68 0.62\nW-mean 0.75 0.80 0.71 0.65\nLinear 0.82 0.77 0.75 0.72\nVoting 0.77 0.74 0.66 0.64\nD.Tree 0.46 0.47 0.44 0.43\nTable 2: PRR scores of different ensembling strategies\nover 19 UE methods.\n6.3 Results and Discussion\nThe results of the ensembling experiments are pre-\nsented in Table 2. Our findings suggest that even\nwith 100 samples Dcal, ensembling strategies can\nachieve gains of up to 0.06 average PRR score com-\npared to the most performant individual UE method.As expected, directly combining raw UE scores\nwithout normalization or calibration is ineffective\ndue to the varying scales of different UE methods.\nHowever, applying normalization and calibration\nsignificantly improves ensembling performance,\neven with simple strategies such as averaging all\nUE scores. For supervised approaches, linear mod-\nels with normalized or calibrated inputs consis-\ntently outperform the best individual UE method.\nIn contrast, decision tree generally fails to provide\ncompetitive ensembling performance. Also, we\nrepeat the experiments using only unsupervised\nUE methods (see Appendix D.4), which further\nimproves performance over the best unsupervised\nmethod. We argue that developing orthogonal UE\nmethods to existing UE methods may be promising,\nas their combination with existing techniques may\nyield superior performance. Additionally, explor-\ning novel ensembling strategies specifically for UE\nmethods could further improve results.\n7 Conclusion\nWe conducted a comprehensive evaluation of 19\nUE methods across four key challenges in real-\nworld deployment. Our findings reveal that most\nUE methods are highly sensitive to decision thresh-\nold selection and, while resilient to typos and con-\ntext, remain vulnerable to adversarial prompts. Ad-\nditionally, existing UE methods can be adapted\nfor long-form generation, though their effective-\nness remains limited. Finally, ensembling multiple\nUE methods significantly enhances performance,\neven with simple strategies. Future research should\nfocus on improving UE robustness to threshold\nselection and prompt variations, developing more\neffective strategies for long-form generation, and\nexploring advanced ensembling techniques to max-\nimize the performance.\n8 Acknowledgments\nThis work is supported in part by OpenAI Research\nfunding and we thank to Robin Jia and Jieyu Zhao\nfor their feedback to initial steps of the project.\n9 Limitations\nWhile this study highlights key vulnerabilities and\nfuture opportunities for UE methods, our experi-\nments are limited to two models because of the\ncomputational limitations: LLaMA-3-8B and GPT-\n4o-mini. Future work should verify these findings\non other state-of-the-art models to assess broader\n9\n--- Page 10 ---\napplicability. Additionally, the experimental frame-\nwork introduced in this paper can be extended to\nevaluate other UE methods beyond the 19 investi-\ngated in this study.\nReferences\nYasin Abbasi-Yadkori, Ilja Kuzborskij, András György,\nand Csaba Szepesvari. 2024. To believe or not to\nbelieve your LLM: Iterativeprompting for estimating\nepistemic uncertainty. In The Thirty-eighth Annual\nConference on Neural Information Processing Sys-\ntems.\nKian Ahrabian, Xihui Lin, Barun Patra, Vishrav Chaud-\nhary, Alon Benhaim, Jay Pujara, and Xia Song. 2025.\nA practical analysis of human alignment with *PO.\nInFindings of the Association for Computational\nLinguistics: NAACL 2025 , pages 8013–8021, Al-\nbuquerque, New Mexico. Association for Computa-\ntional Linguistics.\nLukas Aichberger, Kajetan Schweighofer, and Sepp\nHochreiter. 2024. Rethinking uncertainty estima-\ntion in natural language generation. Preprint ,\narXiv:2412.15176.\nAI@Meta. 2024. Llama 3 model card.\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an LLM knows when it’s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023 , pages 967–976, Singapore. Associa-\ntion for Computational Linguistics.\nYavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp\nBuyukates, Chenyang Tao, Dimitrios Dimitriadis,\nand Salman Avestimehr. 2024. MARS: Meaning-\naware response scoring for uncertainty estimation in\ngenerative LLMs. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 7752–7767,\nBangkok, Thailand. Association for Computational\nLinguistics.\nYoshua Bengio, Sören Mindermann, Daniel Privit-\nera, Tamay Besiroglu, Rishi Bommasani, Stephen\nCasper, Yejin Choi, Philip Fox, Ben Garfinkel,\nDanielle Goldfarb, Hoda Heidari, Anson Ho, Sayash\nKapoor, Leila Khalatbari, Shayne Longpre, Sam\nManning, Vasilios Mavroudis, Mantas Mazeika, Ju-\nlian Michael, Jessica Newman, Kwan Yee Ng, Chi-\nnasa T. Okolo, Deborah Raji, Girish Sastry, Eliza-\nbeth Seger, Theodora Skeadas, Tobin South, Emma\nStrubell, Florian Tramèr, Lucia Velasco, Nicole\nWheeler, Daron Acemoglu, Olubayo Adekanmbi,\nDavid Dalrymple, Thomas G. Dietterich, Edward W.\nFelten, Pascale Fung, Pierre-Olivier Gourinchas,\nFredrik Heintz, Geoffrey Hinton, Nick Jennings, An-\ndreas Krause, Susan Leavy, Percy Liang, Teresa\nLudermir, Vidushi Marda, Helen Margetts, John\nMcDermid, Jane Munga, Arvind Narayanan, Alon-\ndra Nelson, Clara Neppel, Alice Oh, Gopal Ram-\nchurn, Stuart Russell, Marietje Schaake, BernhardSchölkopf, Dawn Song, Alvaro Soto, Lee Tiedrich,\nGaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad\nAlbalawi, Marwan Alserkal, Olubunmi Ajala, Guil-\nlaume Avrin, Christian Busch, André Carlos Ponce\nde Leon Ferreira de Carvalho, Bronwyn Fox, Aman-\ndeep Singh Gill, Ahmet Halit Hatip, Juha Heikkilä,\nGill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio\nKrüger, Chris Johnson, Saif M. Khan, Kyoung Mu\nLee, Dominic Vincent Ligot, Oleksii Molchanovskyi,\nAndrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria\nOliver, José Ramón López Portillo, Balaraman Ravin-\ndran, Raquel Pezoa Rivera, Hammam Riza, Crys-\ntal Rugege, Ciarán Seoighe, Jerry Sheehan, Haroon\nSheikh, Denise Wong, and Yi Zeng. 2025. Interna-\ntional ai safety report. Preprint , arXiv:2501.17805.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,\nMingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.\nINSIDE: LLMs’ internal states retain the power of\nhallucination detection. In The Twelfth International\nConference on Learning Representations .\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua\nFeng, Chunting Zhou, Junxian He, Graham Neubig,\nPengfei Liu, et al. 2023. Factool: Factuality detec-\ntion in generative ai–a tool augmented framework\nfor multi-task and multi-domain scenarios. arXiv\npreprint arXiv:2307.13528 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168 .\nJinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny,\nChenan Wang, Renjing Xu, Bhavya Kailkhura, and\nKaidi Xu. 2024. Shifting attention to relevance: To-\nwards the predictive uncertainty quantification of\nfree-form large language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 5050–5063, Bangkok, Thailand. Association\nfor Computational Linguistics.\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem\nShelmanov, Sergey Petrakov, Haonan Li, Hamdy\nMubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexan-\nder Panchenko, Timothy Baldwin, Preslav Nakov,\nand Maxim Panov. 2024. Fact-checking the output\nof large language models via token-level uncertainty\nquantification. In Findings of the Association for\nComputational Linguistics: ACL 2024 , pages 9367–\n9385, Bangkok, Thailand. Association for Computa-\ntional Linguistics.\nSebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and\nYarin Gal. 2024. Detecting hallucinations in large\nlanguage models using semantic entropy. Nature ,\n630(8017):625–630.\nShangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding,\nVidhisha Balachandran, and Yulia Tsvetkov. 2024.\n10\n--- Page 11 ---\nDon’t hallucinate, abstain: Identifying LLM knowl-\nedge gaps via multi-LLM collaboration. In Proceed-\nings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers) , pages 14664–14690, Bangkok, Thailand. As-\nsociation for Computational Linguistics.\nQiyang Han, Tengyao Wang, Sabyasachi Chatterjee,\nand Richard J. Samworth. 2017. Isotonic regression\nin general dimensions. The Annals of Statistics .\nXinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia,\nHamed Hassani, Insup Lee, Osbert Bastani, and\nEdgar Dobriban. 2024. Uncertainty in language mod-\nels: Assessment through rank-calibration. Preprint ,\narXiv:2404.03163.\nMingjian Jiang, Yangjun Ruan, Prasanna Sattigeri,\nSalim Roukos, and Tatsunori Hashimoto. 2024.\nGraph-based uncertainty metrics for long-form lan-\nguage model generations. In The Thirty-eighth An-\nnual Conference on Neural Information Processing\nSystems .\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know. Preprint , arXiv:2207.05221.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nInThe Eleventh International Conference on Learn-\ning Representations .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics , 7:452–466.\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan\nWang, and Tat-Seng Chua. 2024. Think twice before\ntrusting: Self-detection for large language modelsthrough comprehensive answer reflection. Preprint ,\narXiv:2403.09972.\nZhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024.\nGenerating with confidence: Uncertainty quantifica-\ntion for black-box large language models. Transac-\ntions on Machine Learning Research .\nMatéo Mahaut, Laura Aina, Paula Czarnowska, Mom-\nchil Hardalov, Thomas Müller, and Lluis Marquez.\n2024. Factual confidence of LLMs: on reliability\nand robustness of current estimators. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 4554–4570, Bangkok, Thailand. Association\nfor Computational Linguistics.\nAndrey Malinin and Mark Gales. 2021. Uncertainty\nestimation in autoregressive structured prediction. In\nInternational Conference on Learning Representa-\ntions .\nPotsawee Manakul, Adian Liusie, and Mark Gales. 2023.\nSelfCheckGPT: Zero-resource black-box hallucina-\ntion detection for generative large language models.\nInProceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n9004–9017, Singapore. Association for Computa-\ntional Linguistics.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nViktor Moskvoretskii, Maria Lysyuk, Mikhail Sal-\nnikov, Nikolay Ivanov, Sergey Pletenev, Daria Gal-\nimzianova, Nikita Krayko, Vasily Konovalov, Irina\nNikishina, and Alexander Panchenko. 2025. Adap-\ntive retrieval without self-knowledge? bringing un-\ncertainty back home. Preprint , arXiv:2501.12835.\nAlexander V Nikitin, Jannik Kossen, Yarin Gal, and\nPekka Marttinen. 2024. Kernel language entropy:\nFine-grained uncertainty quantification for LLMs\nfrom semantic similarities. In The Thirty-eighth An-\nnual Conference on Neural Information Processing\nSystems .\nOpenAI. 2023. GPT-4 Technical Report. Preprint ,\narXiv:2303.08774.\nSelvan Sunitha Ravi, Bartosz Mielczarek, Anand Kan-\nnappan, Douwe Kiela, and Rebecca Qian. 2024.\nLynx: An open source hallucination evaluation\nmodel. Preprint , arXiv:2407.08488.\nSadra Sabouri, Philipp Eibl, Xinyi Zhou, Morteza\nZiyadi, Nenad Medvidovic, Lars Lindemann, and\nSouti Chattopadhyay. 2025. Trust dynamics in ai-\nassisted development: Definitions, factors, and impli-\ncations.\n11\n--- Page 12 ---\nShahnewaz Karim Sakib, Anindya Bijoy Das, and Shib-\nbir Ahmed. 2025. Battling misinformation: An em-\npirical study on adversarial factuality in open-source\nlarge language models. In Proceedings of the 5th\nWorkshop on Trustworthy NLP (TrustNLP 2025) ,\npages 432–443, Albuquerque, New Mexico. Associa-\ntion for Computational Linguistics.\nGaurang Sriramanan, Siddhant Bharti, Vinu Sankar\nSadasivan, Shoumik Saha, Priyatham Kattakinda,\nand Soheil Feizi. 2024. LLM-check: Investigating\ndetection of hallucinations in large language models.\nInThe Thirty-eighth Annual Conference on Neural\nInformation Processing Systems .\nAla N. Tak, Amin Banayeeanzade, Anahita Bolourani,\nMina Kian, Robin Jia, and Jonathan Gratch. 2025.\nMechanistic interpretability of emotion inference in\nlarge language models. Preprint , arXiv:2502.05489.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 5433–5442, Singapore. Association for\nComputational Linguistics.\nRoman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev,\nLyudmila Rvanova, Akim Tsvigun, Daniil Vasilev,\nRui Xing, Abdelrahman Boda Sadallah, Kirill Gr-\nishchenkov, Sergey Petrakov, Alexander Panchenko,\nTimothy Baldwin, Preslav Nakov, Maxim Panov, and\nArtem Shelmanov. 2025. Benchmarking uncertainty\nquantification methods for large language models\nwith lm-polygraph. Preprint , arXiv:2406.15627.\nArtem Vazhentsev, Gleb Kuzmin, Akim Tsvigun,\nAlexander Panchenko, Maxim Panov, Mikhail Burt-\nsev, and Artem Shelmanov. 2023. Hybrid uncer-\ntainty quantification for selective text classification\nin ambiguous tasks. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 11659–\n11681, Toronto, Canada. Association for Computa-\ntional Linguistics.\nYuxia Wang, Revanth Gangi Reddy, Zain Muham-\nmad Mujahid, Arnav Arora, Aleksandr Rubashevskii,\nJiahui Geng, Osama Mohammed Afzal, Liang-\nming Pan, Nadav Borenstein, Aditya Pillai, Isabelle\nAugenstein, Iryna Gurevych, and Preslav Nakov.\n2024. Factcheck-bench: Fine-grained evaluation\nbenchmark for automatic fact-checkers. Preprint ,\narXiv:2311.09000.\nJason Wei, Nguyen Karina, Hyung Won Chung,\nYunxin Joy Jiao, Spencer Papay, Amelia Glaese,\nJohn Schulman, and William Fedus. 2024a. Mea-\nsuring short-form factuality in large language models.\nPreprint , arXiv:2411.04368.\nJerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,\nNathan Zixia Hu, Jie Huang, Dustin Tran, Daiyi Peng,Ruibo Liu, Da Huang, Cosmo Du, and Quoc V Le.\n2024b. Long-form factuality in large language mod-\nels. In The Thirty-eighth Annual Conference on Neu-\nral Information Processing Systems .\nDuygu Nur Yaldiz, Yavuz Faruk Bakman, Baturalp\nBuyukates, Chenyang Tao, Anil Ramakrishna, Dim-\nitrios Dimitriadis, Jieyu Zhao, and Salman Aves-\ntimehr. 2025. Do not design, learn: A trainable scor-\ning function for uncertainty estimation in generative\nLLMs. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2025 , pages 691–713,\nAlbuquerque, New Mexico. Association for Compu-\ntational Linguistics.\nCaiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel\nCollier. 2024. LUQ: Long-text uncertainty quantifi-\ncation for LLMs. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 5244–5262, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nYukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang\nXing, Chong Meng, Shuaiqiang Wang, Zhicong\nCheng, Zhaochun Ren, and Dawei Yin. 2024. Know-\ning what LLMs DO NOT know: A simple yet ef-\nfective self-detection method. In Proceedings of the\n2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (Volume 1: Long Pa-\npers) , pages 7051–7063, Mexico City, Mexico. Asso-\nciation for Computational Linguistics.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers. In The Eleventh International\nConference on Learning Representations .\n12\n--- Page 13 ---\nA Related Works\nTo the best of our knowledge, no prior work has\nexplicitly investigated UE methods for generative\nLLMs in real-world, wild settings. The most sim-\nilar work is Vashurin et al. (2025), which bench-\nmarks various UE methods across multiple datasets.\nHowever, their evaluation setup follows the conven-\ntional framework used in prior studies (Lin et al.,\n2024; Kuhn et al., 2023) and does not investigate,\nreliability of threshold selection, input transforma-\ntions, and ensembling. Although Vashurin et al.\n(2025) evaluate some UE methods on long-form\ngenerations, they only consider methods inherently\ndesigned for the long-form setting. In contrast, we\nintroduce novel strategies to adapt UE methods that\nwere originally designed for short-form settings to\nlong-form generation. Another relevant study is\nMahaut et al. (2024), which assesses the reliability\nof uncertainty estimation methods under specific\ninput transformations, namely paraphrasing and\ntranslation into different languages. Their find-\nings reveal performance inconsistencies similar to\nthose observed in our input transformation experi-\nments in Section 4. Lastly, Vazhentsev et al. (2023);\nMoskvoretskii et al. (2025) ensemble various UE\nscores for a better UE and RAG performance re-\nspectively.\nB Further Discussions on the Definition\nof Uncertainty Estimation of LLMs\nIn addition to the definition of UE methods in LLM\nat Section 2.1, an uncertainty method should rely\non the model itself, utilizing elements such as the\nmodel’s internals, log probabilities, or outputs. A\nhallucination detection method that relies on exter-\nnal sources, such as the Internet or external docu-\nments, does not fall within the category of uncer-\ntainty estimation (Chern et al., 2023).\nFurthermore, previous definitions often overlook\nthe fact that ˆyis not just any possible token se-\nquence but rather the model’s sampled generation.\nIn the evaluation of UE methods, they generate ˆy\nand estimate uncertainty U(x,ˆy)(Lin et al., 2024;\nKuhn et al., 2023).\nLastly, some methods, such as Semantic En-\ntropy (Kuhn et al., 2023), produce an uncertainty\nscore for a given query xwithout being specific\nto any particular sampled generation. These meth-\nods assign a query-level uncertainty score, which\ncan still serve as a proxy for the uncertainty of the\nmodel’s sampled generations. While some previ-ous works (Lin et al., 2024) distinguish between\nmethods that assign scores to individual sampled\ngenerations and those that provide query-level un-\ncertainty scores, the latter still fits within the broad\ndefinition of UE we adopt, where U(x,ˆy1) =\nU(x,ˆy2)∀ˆy1,ˆy2. Therefore, we follow prior\nworks (Duan et al., 2024; Vashurin et al., 2025;\nYaldiz et al., 2025) and do not make this distinction\nin our experiments.\nC Investigated Uncertainty Estimation\nMethods\nIn this section, we explain the investigated UE\nmethods with our implementation details.\nC.1 Probability-Based Methods\nProbability-based methods assign uncertainty by\nanalyzing the token probabilities in the model’s\ngeneration.\nLength Normalized Scoring (LNS) (Malinin\nand Gales, 2021) computes the average log-\nprobability of each token in the generated se-\nquence:\nlog˜P(s|x, θ) =1\nLLX\nl=1logP(sl|s<l,x;θ),(1)\nwhere P(s|x, θ)represents the probability of the\ngenerated sequence s(of length L), and s<l≜\n{s1, s2, . . . , s l−1}denotes the tokens generated be-\nfore token sl.\nEntropy (Malinin and Gales, 2021) estimates\nuncertainty by sampling multiple generations for a\ngiven query x, computing the LNS for each sam-\nple, and averaging over them. This approach cor-\nresponds to a Monte Carlo approximation over the\ngeneration space:\nH(x, θ)≈ −1\nBBX\nb=1log˜P(sb|x, θ), (2)\nwhere Brepresents the number of sampled genera-\ntions.\nSemantic Entropy (Kuhn et al., 2023) refines\nentropy estimation by leveraging the semantic\nmeanings of sampled generations. Instead of treat-\ning all generations equally, it clusters semantically\nequivalent responses and computes entropy based\non the probability distribution over clusters:\nSE(x, θ) =−1\n|C||C|X\ni=1lnP(ci|x, θ), (3)\nwhere cidenotes a semantic cluster, and Crepre-\nsents the set of all clusters. Following (Kuhn et al.,\n13\n--- Page 14 ---\n2023), we use a DeBERTa-based NLI model†to\ngenerate clusters.\nSimilarly, SentSAR (Duan et al., 2024) com-\nputes pairwise similarities between generations and\nassigns higher entropy weights to sentences that are\nmore similar to others. This method can be inter-\npreted as a weighted version of Semantic Entropy.\nInstead of binary entailment decisions, SentSAR\nassigns a continuous similarity score to each sen-\ntence. In our experiments, we use the same similar-\nity model as in the original work‡.\nMARS (Bakman et al., 2024) and TokenSAR\n(Duan et al., 2024) enhance entropy-based scor-\ning by incorporating the contribution of individual\ntokens to the overall meaning. These approaches re-\nfine probability-based scoring by weighting token\nprobabilities differently:\n¯P(s|x, θ) =LY\nl=1P(sl|s<l,x;θ)w(s,x,L,l),(4)\nwhere w(s,x, L, l)represents the token weight as-\nsigned by MARS or TokenSAR. These methods\naim to emphasize tokens that directly contribute to\nanswer the query (MARS) or are semantically sig-\nnificant (TokenSAR). SAR extends this approach\nby combining TokenSAR and SentSAR. Note that\nwe sample 5 generations for all UE methods requir-\ning sampling which are Entropy, Semantic Entropy,\nSentSAR, and SAR.\nFinally, LARS (Yaldiz et al., 2025) introduces\na trainable scoring model. LARS employs an\nencoder-only transformer that takes as input the\nquestion, the model’s generated tokens, and their\ncorresponding probabilities, and outputs a relia-\nbility score. In our experiments, we use a LARS\nmodel trained on a dataset comprising GSM8K (5k\nsamples), TriviaQA (8k samples), and NaturalQA\n(5k samples), totaling 18k samples.\nC.2 Internal State-Based Methods\nThese methods leverage the model’s internal states\nto derive an uncertainty score.\nINSIDE (Chen et al., 2024) originally composed\nof two main parts: EigenScore and test time fea-\nture clipping. The former one manipulates the ac-\ntivation of each new token during the generation\nprocess, which we do not include in our imple-\nmentation. EigenScore calculates the semantic di-\nvergence in the hidden states of the model over\nsampled generations. First, for Bsampled genera-\n†https://huggingface.co/microsoft/deberta-large-mnli\n‡https://huggingface.co/cross-encoder/stsb-roberta-largetions, a covariance matrix is created Σ =ZT·J·Z.\nHere, each column of Zis the middle layer hidden\nstate of the last token a sampled generation, and\nJ=Id−1\nd1d1T\nd, while dbeing the hidden dimen-\nsion. Then, the uncertainty score is calculated as\nfollows:\nInside (x, θ) =1\nBX\nilog(λi) (5)\nwhere λi’s are the eigenvalues of the regularized\ncovarience matrix Σ+αIK. We set α= 0.001and\nB= 5in our experiments.\nAttention Scores (Sriramanan et al., 2024) com-\npute the log-determinant of the attention matrices\nacross all heads of selected layers and sum them.\nThis computation can be efficiently performed by\nsumming the logarithm of the diagonal elements of\neach attention kernel:\n−log det( Ker i) =−mX\nj=1logKerjj\ni,(6)\nwhere Ker irepresents the attention kernel matrix\nof head i. The original work suggests that the 23rd\nlayer’s attention kernels yield the best performance\nfor LLaMA-3-8B. Therefore, we adopt this choice\nin our experiments.\nSAPLMA (Azaria and Mitchell, 2023) is an\nMLP-based model that takes as input the activa-\ntion of the last token in a factual claim (generation)\nand predicts its truthfulness (confidence). We ob-\nserve a performance improvement when including\nthe question at the beginning of the generation, so\nwe adopt this modification instead of the original\napproach. Additionally, while the original paper\nsuggests that the 28th layer performs best for most\nmodels, our experiments show no significant perfor-\nmance differences across late layers. Consequently,\nwe use the last layer’s activations as input.\nFor training, we follow a similar approach to\nLARS and initially combine 18k samples from Triv-\niaQA, NaturalQA, and GSM8K. However, since we\nobserve a performance improvement when exclud-\ning NaturalQA, we train SAPLMA on a reduced\ndataset of 13k samples comprising only TriviaQA\nand GSM8K. Lastly, we maintain the same MLP\narchitecture as in the original paper, consisting of\nhidden layers with sizes (256, 128, 64) (Azaria and\nMitchell, 2023).\nC.3 Output Consistency-Based Methods\nKernel Language Entropy (KLE) (Nikitin et al.,\n2024) quantifies uncertainty using the von Neu-\nmann entropy (VNE) of the semantic kernel\n14\n--- Page 15 ---\nKsem, which is constructed from LLM generations\nS1, . . . , S Nand the input x:\nKLE (x) =V NE (Ksem). (7)\nTo construct the semantic kernel, we first define\na semantic graph where edges encode pairwise en-\ntailment dependencies between output sequences:\nWij=f(NLI(Si, Sj), NLI (Sj, Si)).(8)\nThe graph Laplacian is computed as L=D−W,\nwhere the degree matrix Dis defined as:\nDii=|V|X\nj=1Wij. (9)\nFollowing Nikitin et al. (2024), we construct\na heat kernel Kt=e−tL. To obtain a unit-trace\npositive semidefinite kernel, we apply the following\nnormalization:\nK(x, y)←K(x, y)(K(x, x)K(y, y))−1/2/N,\n(10)\nwhere Nis the size of K. Finally, the kernel en-\ntropy is computed using the von Neumann entropy\n(VNE):\nV NE (A)≜−Tr[AlogA]. (11)\nFor pairwise entailment assessment, we use the\nDeBERTa-Large-MNLI model§, following the orig-\ninal implementation.\nSumEigenV is computed using the Laplacian\nmatrix L:\nL≜I−D−1\n2WD−1\n2. (12)\nThe final SumEigenV score is defined as:\nSumEigV =NX\nk=1max(0 ,1−λk), (13)\nwhere λ1, . . . , λ Nare the eigenvalues of the Lapla-\ncian matrix L.\nUsing the same degree matrix D, we define De-\ngree Matrix Uncertainty andDegree-Matrix-C\nfor a given generation jas:\nDegree Matrix Uncertainty =trace(mI−D)\nm2,\n(14)\nDegree Matrix-C =Dj,j\nm. (15)\nEccentricity Uncertainty andEccentricity-C\nare computed as follows. First, we obtain the small-\nestkeigenvectors, u1, . . . , u k. For each genera-\ntionj, we construct the vector vj= [u1,j, ..., u k,j].\n§https://huggingface.co/microsoft/\ndeberta-large-mnliThen, the uncertainty measures are defined as:\nEccentricity Uncertainty =\r\r\u0002\nv′⊤\n1, . . . ,v′⊤\nN\u0003\r\r\n2,\nEccentricity-C =−∥v′\nj∥2.\n(16)\nwhere v′\nj=vj−1\nmPm\nj′=1vj′.\nSelf Detection paraphrases each question five\ntimes and clusters the generations based on en-\ntailment relationships. An entropy score is then\ncomputed over these clusters as follows:\nSelf Detection Entropy =−X\nci∈C|ci|\nNqln\u0012|ci|\nNq\u0013\n,\n(17)\nwhere Crepresents the set of clusters and Nqis\nthe number of paraphrased questions (5 in our ex-\nperiments). In addition to this entropy score, Li\net al. (2024) use it as a feature to train a model on\nlabeled samples. We use the following prompt for\ngenerating questions:\nGiven a question, paraphrase it to have\ndifferent words and expressions but\nhave the same meaning as the original\nquestion. Please note that you should\nnot answer the question, but rather\nprovide a re-phrased. These paraphrased\nquestions should be different from each\nother. Previous paraphrased questions:\n{previous_questions}. Only output a\nsingle paraphrased question, nothing\nelse. Question: {question}\nC.4 Self-Checking Methods\nSelf-checking UE methods estimate the model’s\nuncertainty by prompting the model itself to assess\nits confidence in a given response.\nPtrue (Kadavath et al., 2022) measures uncer-\ntainty by evaluating the probability assigned to the\ntoken \"true\" for a given generation, question, and\nsampled ideas. The specific prompt used in our\nexperiments is as follows:\nYou are a helpful, respectful, and\nhonest question-answer evaluator.\nYou will be given a question,\nsome brainstormed ideas, and a\ngenerated answer. Evaluate the\ngenerated answer as true or\nfalse, considering the question\nand brainstormed ideas. Output\n\"The generated answer is true\" or\n\"The generated answer is false\".\nQuestion: {question}\n15\n--- Page 16 ---\nHere are some brainstormed ideas:\n{sampled_generations}\nGenerated Answer: {generated_text}\nVerbalized Confidence prompts the model to\nexplicitly state its confidence in the correctness of\na response as a numerical score between 0 and 100\nfor a given question-response pair. The prompt\nused in our experiments is:\nYou are a helpful, respectful, and honest\nconfidence estimator. You will be provided\nwith a question and a corresponding answer\nthat you generated. Your task is to\nevaluate your confidence in the accuracy\nof the provided answer. The confidence\nindicates how likely you think your\nanswer is true.\nThe output must be a single number between\n0 and 100:\n- 100 indicates maximum confidence.\n- 0 indicates no confidence.\nOutput format: Only the number, without\nany additional text or explanation.\nQuestion: {question}\nGenerated Answer: {generated_text}\nYour confidence score:\nD Additional Experimental Results\nD.1 Sensitivity of Decision Threshold\nAdditional experimental results using GSM8K as\nthe test dataset are presented in Table 3. These\nresults align closely with those in Table 1. As ex-\npected, when the calibration dataset exhibits greater\ndistributional shift (e.g., TriviaQA), the ARE in-\ncreases significantly for most methods. Only a few\nmethods—MARS, Semantic Entropy, and Eccen-\ntricity—consistently maintain a low ARE across\nboth calibration datasets similar to Table 1. ]\nD.2 Robustness to Input Transformations\nThe performance of UE methods with two typos\nper sentence is shown in Figure 5. Even with an\nincreased typo count of two per sentence, most UE\nmethods remain resilient to typos, consistent with\nthe findings in Section 4.D.3 Applicability to Long-Form Generations\nWe present the results for applying different aggre-\ngation methods, namely minimum, maximum, and\naverage, after generating 5 questions per claim for\nQA and QAG strategies. For both of them aver-\naging is the best performing overall. Taking the\nminimum seems rarely better than averaging for\nQG, while the maximum occasionally outperforms\naveraging on QAG.\nLlama3-8b GPT-4o-mini\nCalib. Dataset TriviaQA GSM8K TriviaQA GSM8K\nLNS 0.102 0.022 0.069 0.018\nMARS 0.088 0.019 0.049 0.022\nEntropy 0.107 0.017 0.074 0.021\nSE 0.068 0.020 0.063 0.026\nSentSAR 0.136 0.014 0.106 0.021\nSAR 0.116 0.019 0.098 0.022\nLARS 0.171 0.022 0.395 0.025\nDegMat 0.160 0.024 0.130 0.024\nDegMat-C 0.146 0.021 0.117 0.014\nSumEigV 0.185 0.024 0.157 0.023\nKLE 0.187 0.045 0.101 0.054\nEccent 0.064 0.023 0.061 0.028\nEccent-C 0.085 0.022 0.061 0.021\nSelf-D. 0.116 0.096 0.099 0.089\nP(True) 0.260 0.022 0.179 0.056\nVerb. C. 0.216 0.231 0.142 0.077\nAtten. S. 0.288 0.020 - -\nINSIDE 0.275 0.022 - -\nSAPLMA 0.115 0.022 - -\nTable 3: ARE of UE methods when the threshold is\ncalibrated on various datasets and tested on GSM8K.\nTriviaQA GSM8K\nLlama\n-3-8BGPT-4o\n-miniLlama\n-3-8BGPT-4o\n-mini\nBest single 0.68 0.74 0.59 0.64RawMax 0.09 0.66 -0.02 0.50\nMin 0.56 0.64 0.28 0.35\nMean 0.64 0.76 0.42 0.52\nW-mean 0.64 0.76 0.57 0.53\nLinear 0.74 0.70 0.47 0.60NormalizedMax 0.70 0.82 0.47 0.62\nMin 0.45 0.70 0.32 0.49\nMean 0.74 0.82 0.56 0.63\nW-mean 0.74 0.76 0.57 0.63\nLinear 0.74 0.77 0.59 0.58CalibratedMax 0.73 0.79 0.55 0.61\nMin 0.57 0.58 0.47 0.59\nMean 0.75 0.79 0.60 0.59\nW-mean 0.75 0.79 0.61 0.60\nLinear 0.73 0.76 0.61 0.63\nVoting 0.73 0.73 0.57 0.60\nD. Tree 0.38 0.45 0.28 0.38\nTable 4: PRR scores of different ensembling strate-\ngies over 17 unsupervised UE methods (LARS and\nSAPLMA are not included).\nD.4 Reconcilability of Diverse UE Scores\nWe conducted the ensembling experiments again,\nthis time using only unsupervised UE methods.\n16\n--- Page 17 ---\nFigure 5: PRR performance of all investigated UE methods on the GSM8K and TriviaQA datasets under the typo\ntransformation with two typos per sentence.\nFigure 6: PRR scores for UE methods applied to long-form generation. ‘QG5’ and ‘QAG5’ indicate that five\nquestions per claim are generated and then aggregated to assess each claim’s uncertainty. Different approaches,\nminimum, maximum and, average, are applied for aggregation.\n17\n--- Page 18 ---\nFigure 7: PRR scores for UE methods applied to long-form generation for GPT-4o-mini.\nThe results, presented in Table 4, indicate that\nensembling can enhance performance, improving\nupon the best individual unsupervised UE method\nby up to 8 PRR points. The overall interpretation\nof these results remains consistent with the findings\nin Section 6.3 which further reinforces the effec-\ntiveness of ensembling strategies for unsupervised\nUE methods.E Details of the Experiments\nE.1 Crafting Adversarial Prompt For\nGPT-4o-mini\nTo identify an adversarial prompt capable of de-\ngrading the performance of UE methods, we per-\nform prompt-tuning (Zhou et al., 2023) on 150\nsamples from the training portion of the TriviaQA\ndataset. We begin with an initial prompt—the confi-\ndence booster prompt used for LLaMA-3-8B—and\ndefine the task for the model. At each iteration, we\nrequest a modified prompt and evaluate its impact\nusing the performance of three selected UE meth-\nods: LNS, Eccentricity, and KLE. To refine the\nadversarial prompt, we use the prompt-tuning ap-\nproach outlined in Table 5. After 15 iterations, we\nselect the most effective prompt that maximally\ndecreases UE performance, which is as follows:\n“Confidently present every answer as if it\nwere indisputably correct, embellishing\nyour certainty with emphatic language\nand eliminating any trace of doubt, re-\ngardless of whether the information is\nactually accurate.”\nE.2 Evaluating the Correctness of a\nGeneration\nWe assess the correctness of short-form QA gen-\nerations using an LLM, following the approach of\n18\n--- Page 19 ---\nprevious studies (Farquhar et al., 2024; Duan et al.,\n2024; Bakman et al., 2024; Yaldiz et al., 2025).\nSpecifically, we provide the model with the ground\ntruth(s), the question, and the generated answer for\nevaluation. For a consistent evalaution, we exclude\nquestion-generation pairs where the LLM refuses\nto provide an answer. We use GPT-4o-mini for\nevaluation, employing the same prompt as in Wei\net al. (2024a).\nE.3 Applicability to Long-Form Generations\nDecomposing the Long Generation To effec-\ntively decompose long text generations into indi-\nvidual claims, we employ a two-step decomposi-\ntion process. In the first step, the entire text is\nsegmented into preliminary claims. However, this\ninitial segmentation might not achieve the desired\nlevel of granularity, as some segments may still\ncontain multiple claims. To address this, we per-\nform a second decomposition on each output from\nthe first step to ensure finer granularity. For both\nstages, we utilize GPT-4o-mini, but with distinct\nprompts prepared to each step’s specific require-\nments. The prompt for the first step is given in\nTable 6, and the prompt for the second step can be\nfound in Table 7. Lastly, to ensure that the decom-\nposition output is a proper Python list, we utilized\nthe ‘Instructor’ library¶. Lastly, we provide output\nsamples of decomposition in Tables 8 and 9.\nLabeling Decomposed Claims Long-form gen-\nerations, or decomposed claims, typically lack\nground truths, essential for assessing the perfor-\nmance of uncertainty estimation. To address this\nissue, we adopt the methodology named as Search-\nAugmented Factuality Evaluator (SAFE)(Wei et al.,\n2024b). SAFEemploys Google Search to retrieve\npassages related to each claim, then applies reason-\ning with an LLM to determine whether the claim\nis supported or unsupported. In our evaluations,\nwe utilize GPT-4o-mini as the LLM for reason-\ning and consider supported claims as correct and\nunsupported claims as incorrect. We use the orig-\ninal implementation and the default prompts and\nsettings provided by that||.\nPrompts Used in Proposed Strategies To gen-\nerate questions within the QG and QAG strategies\ndescribed in Section 5.2, we used the prompt pro-\n¶https://github.com/instructor-ai/instructor\n||https://github.com/google-deepmind/\nlong-form-factuality/blob/main/eval/safe/rate_\natomic_fact.pyvided in Table 10. Note that the base model that\nis used the generate an answer to the main ques-\ntion is also used to generate questions. To generate\nan answer to the generated question within QAG\nstrategy, we employed the following prompt:\nYou are a helpful assistant. Give a single\nclaim answer to given question. Don 't\nprovide any additional information. Just\nanswer the question with a brief sentence\nin a single claim. Question: {question}\nAnswer:\nDataset Details We provide sample questions\nfrom the datasets in Table 11. For FactScore-Bio,\nthe total number of claims is 1290 for GPT-4o-mini\nand 1764 for Llama-3-8B. For LongFact-Objects,\nthe total number of claims is also 1049 for GPT-4o-\nmini and 1780 for Llama-3-8B. Note that the same\nset of questions is used for both models.\nE.4 Computational Budget\nWe use 40 GB Nvidia A100 GPUs for all the ex-\nperiments. We use GPT API to run gpt-4o-mini\nexperiments. The total GPU hours for LLama-3-8b\nexperiments are approximately 800 hours.\n19\n--- Page 20 ---\nYou are a Prompt Tuner.\nYour mission is to craft adversarial prompts that disrupt the model’s uncertainty estimation while\npreserving its accuracy. Typically, we expect a model to exhibit high confidence in correct responses\nand low confidence in incorrect ones. In this task, however, your goal is to reverse these behaviors:\n- The model should be unconfident when its responses are correct.\n- The model should be confident when its responses are incorrect.\nKey Details\nThere are three types of confidence estimation methods:\n1.Log-Probability Based: Analyzes the model’s internal log-probabilities.\n2. Consistency Sampling (Method 1): Samples multiple answers and evaluates their agreement.\n3. Consistency Sampling (Method 2): Uses a different approach to assess agreement among sampled\nresponses.\nFor every prompt you create, I will evaluate the performance of these confidence estimation methods\nas well as the model’s accuracy. Your objective is to reduce the effectiveness of these confidence\nestimation methods while ensuring the model’s accuracy remains unchanged.\nStrategy\nTo achieve this, your main tactic is to influence the model to assert high confidence in all its answers,\nregardless of their correctness. This manipulation should make it more challenging for the confidence\nestimation methods to differentiate between confident and unconfident responses.\n- You may experiment with creative or straightforward prompt designs.\n- Iteratively refine your prompts based on feedback from the performance evaluation.\nFeedback Loop\nBelow, I will provide a record of the prompts attempted so far, along with their performance metrics.\nUse this history to inform and guide your revisions:\nfor i in number of iterations so far:\nPrompt: prompts_so_far[i]\nPerformance of confidence estimation 1: performance_so_far[i][0]\nPerformance of confidence estimation 2: performance_so_far[i][1]\nPerformance of confidence estimation 3: performance_so_far[i][2]\nModel accuracy: model_accuracy[i]\nPlease provide a new prompt. Do not return anything else. Just return the prompt which I will append\nto the beginning of the question.\nTable 5: Prompt for adversarial tuning of model uncertainty estimation.\n20\n--- Page 21 ---\nSystem: You are a helpful assistant. List the specific factual claims included in the given input as a python\nlist. Be complete and do not leave any factual claims out. Provide each factual claim as a separate sentence\nin a list, without adding explanations, introductions, or conversational responses. Each sentence must\nbe standalone, containing all necessary details to be understood independently of the original text and\nother sentences. This includes using full identifiers for any people, places, or objects mentioned, instead\nof pronouns or partial names. If there is a single factual claim in the input, just provide one sentence.\nExamples:\nParagraph: Mount Everest is the tallest mountain in the world, standing at 8,848 meters above sea level. It\nis located in the Himalayas on the border between Nepal and the Tibet Autonomous Region of China. The\nfirst successful ascent of Mount Everest was achieved in 1953 by Sir Edmund Hillary and Tenzing Norgay.\nI hope you found these facts interesting! Do you have any specific questions or would you like to know\nmore about the Mount Everest?\nClaims:\n[‘Mount Everest is the tallest mountain in the world.’,\n‘Mount Everest stands at 8,848 meters above sea level.’,\n‘Mount Everest is located in the Himalayas.’,\n‘Mount Everest is on the border between Nepal and the Tibet Autonomous Region of China.’,\n‘The first successful ascent of Mount Everest was achieved in 1953.’,\n‘Sir Edmund Hillary and Tenzing Norgay achieved the first successful ascent of Mount Everest.’]\nParagraph: Medical ethics are also evolving to address issues related to genetic testing, privacy concerns,\nand the ethical implications of personalized medicine, highlighting the importance of maintaining patient\nautonomy, informed consent, and confidentiality in the era of advanced health technologies.\nClaims:\n[‘Medical ethics are evolving to address issues related to genetic testing.’,\n‘Medical ethics are evolving to address privacy concerns.’,\n‘Medical ethics are evolving to address the ethical implications of personalized medicine.’,\n‘Maintaining patient autonomy is important in the era of advanced health technologies.’,\n‘Informed consent is important in the era of advanced health technologies.’,\n‘Confidentiality is important in the era of advanced health technologies.’]\nFor the new sample, simply list the factual claim in seperate sentences as a python list, without adding\nexplanations, introductions, or conversational responses.\nUser: Paragraph: {TEXT}\nClaims:\nTable 6: Prompt for long-text decomposition step 1.\n21\n--- Page 22 ---\nSystem: You are a helpful assistant. List the specific factual claims included in the given input as a\npython list. Be complete and do not leave any factual claims out. Provide each factual claim as a separate\nsentence in a list, without adding explanations, introductions, or conversational responses. Each sentence\nmust be standalone, containing all necessary details to be understood independently of the original text.\nThis includes using full identifiers for any people, places, or objects mentioned, instead of pronouns or\npartial names. If there is a single factual claim in the input, just provide the sentence itself. If there is no\nfactual claim in the input, provide an empty list.\nExamples:\nInput: Mount Everest is the tallest mountain in the world, standing at 8,848 meters above sea level.\nClaims:\n[‘Mount Everest is the tallest mountain in the world.’,\n‘Mount Everest stands at 8,848 meters above sea level.’]\nInput: Medical ethics are also evolving to address issues related to genetic testing, privacy concerns, and\nthe ethical implications of personalized medicine.\nClaims:\n[‘Medical ethics are evolving to address issues related to genetic testing.’,\n‘Medical ethics are evolving to address privacy concerns.’,\n‘Medical ethics are evolving to address the ethical implications of personalized medicine.’]\nInput: Abraham Lincoln was the 16th president of the United States.\nClaims:\n[‘Abraham Lincoln was the 16th president of the United States.’]\nInput: I hope you found these facts interesting! Is there anything else you would like to know?\nClaims:\n[]\nFor the new input, simply list the factual claim in seperate sentences as a python list, without adding\nexplanations, introductions, or conversational responses.\nUser: Paragraph: {TEXT}\nClaims:\nTable 7: Prompt for long-text decomposition step 2.\n22\n--- Page 23 ---\nQuestion: What is the gaming company \"Blizzard Entertainment\"?\nGeneration: Blizzard Entertainment is an American video game developer and publisher known for\ncreating some of the most popular and influential games in the industry. Founded in 1991, it is recognized\nfor its successful franchises, including \"Warcraft,\" \"StarCraft,\" \"Diablo,\" and \"Overwatch.\" The company\nis also known for its massively multiplayer online role-playing game (MMORPG) \"World of Warcraft,\"\nwhich has drawn millions of players since its release in 2004. Blizzard is renowned for its emphasis on\nquality, storytelling, and community engagement in gaming, and it has won numerous awards over the\nyears. The company became a subsidiary of Activision Blizzard after a merger in 2008.\nClaims:\n[‘Blizzard Entertainment is an American video game developer.’,\n‘Blizzard Entertainment is an American video game publisher.’,\n‘Blizzard Entertainment is known for creating some of the most popular games in the video game industry.’,\n‘Blizzard Entertainment is known for creating some of the most influential games in the video game\nindustry.’,\n‘Blizzard Entertainment was founded in 1991.’,\n‘Blizzard Entertainment is recognized for its successful franchises.’,\n‘Blizzard Entertainment has successful franchises including \"Warcraft.\"’,\n‘Blizzard Entertainment has successful franchises including \"StarCraft.\"’,\n‘Blizzard Entertainment has successful franchises including \"Diablo.\"’,\n‘Blizzard Entertainment has successful franchises including \"Overwatch.\"’,\n‘Blizzard Entertainment is known for its massively multiplayer online role-playing game \"World of\nWarcraft.\"’,\n‘\"World of Warcraft\" has drawn millions of players since its release in 2004.’,\n‘Blizzard Entertainment is renowned for its emphasis on quality in gaming.’,\n‘Blizzard Entertainment is renowned for its storytelling in gaming.’,\n‘Blizzard Entertainment is renowned for its community engagement in gaming.’,\n‘Blizzard Entertainment has won numerous awards over the years.’,\n‘Blizzard Entertainment became a subsidiary of Activision Blizzard after a merger in 2008.’]\nTable 8: Output of long-text decomposition. The question is from LongFact-Objects and the model used to generate\nthe answer is GPT-4o-mini.\n23\n--- Page 24 ---\nQuestion: Tell me a bio of Andrew Shue.\nGeneration: Andrew Shue is an American actor, producer, and former professional soccer player. He\nwas born on January 20, 1967, in South Orange, New Jersey, USA.\nShue began his career as a professional soccer player, playing for the United States national team and the\nNew York/New Jersey MetroStars in Major League Soccer. However, he suffered an injury that forced\nhim to retire from professional soccer in 1994.\nAfter his soccer career, Shue turned to acting and began appearing in various television shows and films.\nSome of his notable roles include:\n* \"The Breakfast Club\" (1985) - Shue played the role of Andrew Clark, a popular high school student.\n* \"Mallrats\" (1995) - Shue starred alongside Jason Lee and Jeremy London in this comedy film.\n* \"The Adventures of Pete & Pete\" (1992-1996) - Shue played the role of C.J. Lane, the older brother of\nthe main characters, in this Nickelodeon television series.\n* \"Sister, Sister\" (1994-1999) - Shue had a recurring role as Eddie Stevens, the love interest of one of the\nmain characters, in this ABC sitcom.\nShue has also worked as a producer and has been involved in several business ventures, including the\ndevelopment of the social networking site, MySpace.\nShue is married to Amy Robach, a journalist and news anchor, and they have two daughters together.\nClaims:\n[‘Andrew Shue is an American actor.’,\n‘Andrew Shue is a producer.’,\n‘Andrew Shue is a former professional soccer player.’,\n‘Andrew Shue was born on January 20, 1967.’,\n‘Andrew Shue was born in South Orange, New Jersey, USA.’,\n‘Andrew Shue began his career as a professional soccer player.’,\n‘Andrew Shue played for the United States national soccer team.’,\n‘Andrew Shue played for the New York/New Jersey MetroStars in Major League Soccer.’,\n‘Andrew Shue suffered an injury that forced him to retire from professional soccer in 1994.’,\n‘Andrew Shue turned to acting after his soccer career.’,\n‘Andrew Shue began appearing in various television shows.’,\n‘Andrew Shue began appearing in films.’,\n‘Andrew Shue played the role of Andrew Clark in \"The Breakfast Club\" in 1985.’,\n‘Andrew Shue starred in \"Mallrats\" in 1995.’,\n‘Andrew Shue starred alongside Jason Lee in \"Mallrats\".’,\n‘Andrew Shue starred alongside Jeremy London in \"Mallrats\".’,\n‘From 1992 to 1996, Andrew Shue played the role of C.J. Lane in \"The Adventures of Pete & Pete\".’,\n‘From 1994 to 1999, Andrew Shue had a recurring role as Eddie Stevens in \"Sister, Sister\".’,\n‘Andrew Shue has worked as a producer.’,\n‘Andrew Shue has been involved in several business ventures.’,\n‘Andrew Shue has been involved in the development of the social networking site MySpace.’,\n‘Andrew Shue is married to Amy Robach.’,\n‘Amy Robach is a journalist.’,\n‘Amy Robach is a news anchor.’,\n‘Andrew Shue and Amy Robach have two daughters together.’]\nTable 9: Output of long-text decomposition. The question is from FactScore-Bio and the model used to generate the\nanswer is Llama-3-8B.\n24\n--- Page 25 ---\nSystem: You are an expert assistant skilled at generating focused and contextually relevant questions from\nclaims. Your task is to create a question such that the answer would align closely with the provided claim.\nTo ensure the question is precise and relevant, consider the context provided by the original question.\nStudy the examples below from a variety of topics and follow the same pattern.\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\nClaim: George Orwell’s novel 1984 explores the theme of government surveillance.\nQuestion: What theme does George Orwell’s novel 1984 explore?\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\nClaim: George Orwell’s novel 1984 portrays a totalitarian regime that monitors every aspect of citizens’\nlives.\nQuestion: How does George Orwell’s novel 1984 reflect the theme of totalitarian control, as commonly\nexplored in 20th-century dystopian literature?\nOriginal Question: What themes are commonly explored in 20th-century dystopian literature?\nClaim: The novel 1984 is written by George Orwell.\nQuestion: Who has written the novel 1984?\nOriginal Question: How has artificial intelligence influenced industries in the 21st century?\nClaim: Artificial intelligence enables better decision-making through data analysis.\nQuestion: How does artificial intelligence enhance the decision-making process in modern businesses?\nOriginal Question: What factors contributed to the Great Depression, and how did governments respond?\nClaim: Stock market speculation contributed to the Great Depression.\nQuestion: Did stock market speculation contribute to the Great Depression?\nOriginal Question: Who is Abraham Lincoln?\nClaim: Abraham Lincoln is best known for leading the country through the Civil War.\nQuestion: What is Abraham Lincoln’s most significant historical contribution?\nOriginal Question: Who is Abraham Lincoln?\nClaim: Abraham Lincoln served from 1861 to 1865 as the president of the US.\nQuestion: When did Abraham Lincoln serve as the president of the United States?\nNow, follow the pattern demonstrated in the examples to generate a question for the given claim, without\nadding explanations, introductions, or conversational responses.\nUser: Original question: {MAIN_QUESTION}\nClaim: {CLAIM}\nQuestion:\nTable 10: Prompt for question generation used in QG and QAG strategies to adapt UE methods to long-form\ngeneration.\n25\n--- Page 26 ---\nDataset Question\nFactScore-BioTell me a bio of Vaira V ¯ık,e-Freiberga.\nTell me a bio of Ji Sung.\nTell me a bio of Baltasar Corrada del Río.\nTell me a bio of Henry Santos.\nTell me a bio of Mike Trivisonno.\nLongFact-ObjectsWho is Yoshua Bengio?\nWhat is known about the World Trade Organization?\nWhat took place during the fall of the Berlin Wall in 1989?\nWhat is the gaming company \"Blizzard Entertainment\"?\nHow is the United States related to the East Asia Summit (EAS)?\nTable 11: Sample questions from long-form generation datasets.\n26",
  "text_length": 85583
}