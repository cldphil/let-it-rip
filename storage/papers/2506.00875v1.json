{
  "id": "http://arxiv.org/abs/2506.00875v1",
  "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint\n  Multilingual Supervised Fine-Tuning",
  "summary": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs.",
  "authors": [
    "Yangfan Ye",
    "Xiaocheng Feng",
    "Zekun Yuan",
    "Xiachong Feng",
    "Libo Qin",
    "Lei Huang",
    "Weitao Ma",
    "Yichong Huang",
    "Zhirui Zhang",
    "Yunfei Lu",
    "Xiaohui Yan",
    "Duyu Tang",
    "Dandan Tu",
    "Bing Qin"
  ],
  "published": "2025-06-01T07:20:55Z",
  "updated": "2025-06-01T07:20:55Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00875v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00875v1  [cs.CL]  1 Jun 2025CC-T UNING : ACross-Lingual Connection Mechanism for Improving\nJoint Multilingual Supervised Fine-Tuning\nYangfan Ye1, Xiaocheng Feng1,2*, Zekun Yuan1, Xiachong Feng3, Libo Qin4,\nLei Huang1Weitao Ma1, Yichong Huang1, Zhirui Zhang, Yunfei Lu5,\nXiaohui Yan5, Duyu Tang5, Dandan Tu5, Bing Qin1,2\n1Harbin Institute of Technology2Peng Cheng Laboratory3The University of Hong Kong\n4Central South University5Huawei Technologies Co., Ltd\n{yfye,xcfeng,qinb}@ir.hit.edu.cn\nAbstract\nCurrent large language models (LLMs) often\nexhibit imbalanced multilingual capabilities\ndue to their English-centric training corpora.\nTo address this, existing fine-tuning approaches\noperating at the data-level (e.g., through data\naugmentation or distillation) typically intro-\nduce implicit cross-lingual alignment, over-\nlooking the potential for more profound, latent-\nlevel1cross-lingual interactions. In this work,\nwe propose CC-T UNING , a novel multilingual\nfine-tuning paradigm that explicitly establishes\na cross-lingual connection mechanism at the la-\ntent level. During training, CC-T UNING fuses\nthe feed forward activations from both English\nand non-English inputs, enabling the model to\nbenefit from both linguistic resources. This\nprocess is facilitated with a trainable Decision\nMaker that identifies beneficial activations. Fur-\nthermore, during inference, a Transform Matrix\nis utilized to simulate the cross-lingual connec-\ntion under monolingual setting through repre-\nsentation transformation. Our experiments on\nsix benchmarks covering 22 languages show\nthatCC-T UNING outperforms vanilla SFT and\noffers a strong latent-level alternative to data-\nlevel augmentation methods. Further analysis\nalso highlights the practicality of CC-T UNING\nand the potential of latent-level cross-lingual\ninteractions in advancing the multilingual per-\nformance of LLMs. (Code link: CC-Tuning)\n1 Introduction\nRecent advancements in large language models\n(LLMs) have demonstrated exceptional capabili-\nties in handling diverse tasks (Dong et al., 2023;\nWei et al., 2022a,b; Shanahan, 2022; Zhao et al.,\n2023; Liu et al., 2023; Huang et al., 2025) while ex-\nhibiting promising generalizability across diverse\nlanguages (Ye et al., 2023; Qin et al., 2024; Huo\net al., 2025). However, significant performance\n*Corresponding Author\n1latent-level : referring to direct manipulation of the\nmodel’s internal representations (e.g., FFN activations)\nVanillaSupervisedFine-TuningMultilingualSupervisedTrainingData(Pairs)(Q1, A1)(Q2, A2)(Q3, A3)(Q4, A4)(Q5, A5)……Langs: Ar, Bn, De, En, Es, Fr,…Parallel Data Translatedinto English(Pairs)(Qen1, Aen1)(Qen2, Aen2)(Qen3, Aen3)(Qen4, Aen4)(Qen5, Aen5)……Langs: EnTranslate\nAugmented Data(Q1, Qen1, A1)(Q2, Qen2, A2)(Q3, Qen3, A3)(Q4, Qen4, A4)(Q5, Qen5, A5)……InputQ’Cross-lingualConnectionQQ’(LatentFusion)AuxiliaryInputQenCC-TUNING\nAVanillaLossComputation(a) Vanilla SFT withData Augmentation usingTranslated Parallel Data(b)Cross-lingualConnectionMechanismwithParallel Data duringSFT (Ours)\n(Q’1, A’1)(Q’2, A’2)(Q’3, A’3)(Q’4, A’4)(Q’5, A’5)……A’VanillaLossComputationInput Q•Combine•Restructure•DataDistillFigure 1: Comparison between vanilla supervised fine-\ntuning with data augmentation at data level (implicit)\nand our method at latent activation level (explicit).\ndisparities persist across languages due to the over-\nwhelming dominance of English in training cor-\npora, making balanced multilingual proficiency an\nongoing research challenge (Touvron et al., 2023;\nZhang et al., 2023; Ye et al., 2024a).\nOne of the prevailing approaches towards these\nchallenges focuses on joint multilingual supervised\nfine-tuning (SFT) (Ouyang et al., 2022), which\nrefers to fine-tuning the model with supervised\ndata spanning multiple languages. While effective\nin principle, these methods encounter the “curse\nof multilinguality” – a paradoxical phenomenon\nwhere expanding language coverage during joint\ntraining leads to performance degradation across\nboth high- and low-resource languages (Conneau\net al., 2020; Wang et al., 2020).\nTo address this, current studies primarily focus\non data-level interventions through parallel corpus\nutilization. Common strategies include: multilin-\ngual data augmentation with English-aligned paral-\nlel examples (Aharoni et al., 2019; Shaham et al.,\n--- Page 2 ---\n2024), explicit translation task formulation (John-\nson et al., 2017; Tang et al., 2020), and response\ndistillation from resource-rich languages (Zhang\net al., 2024). While these methods demonstrate par-\ntial success, their reliance on implicitly introducing\ndata-level text alignment overlooks the potential\nfor deeper, latent-level cross-lingual interactions.\nWe propose CC-T UNING , a novel multilingual\nfine-tuning paradigm that introduces explicit cross-\nlingual connections at the latent activation level by\nfusing feed-forward activations from English and\nnon-English languages (Figure 1). This approach is\ngrounded in recent empirical findings highlighting\nthe significant potential of feed-forward activations\nin improving model’s multilingual performance (Ye\net al., 2024b). During training, our method lever-\nages parallel bilingual inputs and incorporates a\ntrainable Decision Maker to identify linguistically\nbeneficial signals from auxiliary English activa-\ntions, integrating them into the forward propaga-\ntion of non-English inputs. Additionally, during\ninference, an “easy-to-learn” Transform Matrix is\nutilized to simulate the cross-lingual connection\nwithout the parallel bilingual inputs, ensuring the\npracticality of our approach. This latent-level inter-\naction mechanism fundamentally differs from con-\nventional data-level approaches, as it establishes di-\nrect interlingual activation connections rather than\nrelying on statistical correlations in training data.\nTo validate our approach, we conduct extensive\nexperiments across six benchmarks encompassing\nboth natural language understanding and genera-\ntion tasks, spanning 22 languages using two repre-\nsentative LLMs. Our results highlight the superi-\nority of CC-T UNING over vanilla SFT in multilin-\ngual joint learning scenarios. Besides, compared to\ndata-level augmentation or distillation methods that\nleverage parallel data, CC-T UNING offers a highly\neffective alternative for facilitating cross-lingual in-\nteraction. Additionally, our further ablation studies\nand analysis also provide strong evidence of the\npracticality and robustness of CC-T UNING .\n2 Related Work\nMultilingual Large Language Models. Re-\ncently, larger models such as Bloom (Scao et al.,\n2022), Mala-500 (Lin et al., 2024) and Aya\nModel (Üstün et al., 2024) have pushed multilin-\ngual performance further by leveraging the bene-\nfits of greater scale. Generally, multilingual pre-\ntraining and fine-tuning are now the two main-stream methods for improving multilingual capa-\nbilities. Models such as Sabia (Pires et al., 2023),\nChineseLLaMA (Cui et al., 2023), ChineseMix-\ntral (HIT-SCIR, 2024), PolyLM (Wei et al., 2023)\nand PaLM2 (Anil et al., 2023) have been developed\nthrough (continuous) pretraining with large multi-\nlingual corpora or language-specific data. Other\nmodels like BLOOMz (Muennighoff et al., 2022),\nm-LLaMA (Zhu et al., 2023), Camoscio (Santilli\nand Rodolà, 2023), Phoenix (Chen et al., 2023) and\nBode (Garcia et al., 2024) have opted for leverag-\ning multilingual or language-specific data directly\nduring SFT stage to foster cross-lingual alignment.\nMultilingual Supervised Fine-Tuning. Multi-\nlingual SFT is an effective way to enhance the\nmultilingual performance of LLMs. Current re-\nsearch often focuses on data augmentation or distil-\nlation techniques to enrich training data and im-\nprove model generalization across multiple lan-\nguages. For instance, Pan et al. (2024) highlighted\nthe importance of diverse, high-quality data for ma-\nchine translation fine-tuning, while Li et al. (2023)\naddressed \"translationese\" by using Google Trans-\nlate and ChatGPT for multilingual response gen-\neration. In terms of instruction tuning, Shaham\net al. (2024) showed that adding multilingual ex-\namples to English-centric fine-tuning significantly\nboosts multilingual instruction-following, while\nChen et al. (2024) demonstrated the superiority of\nmultilingual tuning over language-specific training.\nTranslation-based fine-tuning has been shown to\nenhance semantic alignment, as argued by Ranaldi\net al. (2024). Similarly, Zhu et al. (2023) combined\ntranslation data, cross-lingual tasks, and scaling\nlaws to optimize multilingual performance. Ad-\nditionally, Zhang et al. (2024) proposed a self-\ndistillation approach leveraging LLMs’ internal\ncapabilities in resource-rich languages to enhance\nmultilingual performance.\nThe above methods primarily focus on enrich-\ning training data with parallel data to foster im-\nplicit cross-lingual alignment. In contrast, our\nCC-T UNING emphasizes improving the training\nparadigm by explicitly incorporating cross-lingual\nlatent interactions into the training process.\n3 Method\nIn this section, we first revisit the vanilla multilin-\ngual supervised fine-tuning paradigm, then present\nthe training implementation of CC-T UNING and its\nspecialized configurations during inference stage.\n--- Page 3 ---\n(1) Training Stage\n[Input] 孔⼦在哪⾥出⽣？[Output] 孔⼦在中国的鲁国出⽣。ChineseQ & A Pair (with Template)AttnFFNAttnFFN\nEmbedding\nLLMAttnFFNAttnFFN\n[Input] Where was Confucius born?[Output]ParallelQ’inEnglish (with Template)Embedding\nLLM\nAdaptive Decision Maker (ALinearLayer)3123…LFeed Forward Activationsfrom LDecoder LayersEmbedding Activationsfrom Chinese Input+\nTranslation\nVanillaLossComputation(2) Inference Stage\n[Input] 孔⼦在哪⾥出⽣？[Output]QuestioninChinese (with Template)Thesamemodelunder training\nMonolingualInputParallelBilingual Inputs\n…Embedding…AttnFFNAttnFFN\nLLM\n…AttnFFNAttnFFN\nLLM\n…123…LTransform Matrix(non-En àEn)123…LAnswer+Embedding\nDecision Maker?\nFigure 2: Overview of the cross-lingual connection mechanism in CC-T UNING . In the training stage, CC-T UNING\nleverages an auxiliary English input alongside the non-English input, while retaining the vanilla loss computation\nwithout introducing additional training objectives. In the inference stage, a transform matrix is used to simulate\ncross-lingual connection in monolingual input scenarios, eliminating the dependence on bilingual parallel input.\n3.1 Multilingual Supervised Fine-Tuning\nMultilingual supervised fine-tuning enables pre-\ntrained models to better perform downstream\ntasks across diverse languages through training\non annotated multilingual instruction dataset D=\n{(xi, yi)}N\ni=1, where Nrepresents the size of the\ndataset, xidenotes the input question or instruc-\ntion, and yiis the corresponding expected output\nor response. The training process is required to\nminimize the following objective of negative log-\nlikelihood of the predicted output with respect to\nthe ground-truth response. θdenotes the parame-\nters of the model.\nLSFT(θ) =1\nNNX\ni=1−logP(yi|xi, θ) (1)\nData Augmentation with Parallel Data. For\nthe multilingual instruction dataset D, we define\nits corresponding English parallel data as Den.\nSeveral previous studies have explored enriching\nthe original training data by merging these two\ndatasets, incorporating additional translation task\nform data constructed from parallel pairs, or uti-\nlizing techniques such as distillation. We collec-\ntively refer to these augmented datasets as Daug=\n{(xaug\ni, yaug\ni)}M\nj=1. These approaches, in essence,\ndo not alter the SFT process; rather, they introduce\nadditional supervised data, as illustrated below:\nLSFT aug(θ) =1\nNNX\ni=1−logP(yi|xi, θ)\n+1\nMMX\nj=1−logP(yaug\ni|xaug\ni, θ)(2)3.2 CC-T UNING\nWe will introduce cross-lingual connection mecha-\nnism in CC-T UNING in detail, focusing on its im-\nplementation during training and inference stages.\n3.2.1 Training with Cross-lingual Connection\nMotivated by the findings in Ye et al. (2024b),\nwhich empirically demonstrate that feed-forward\nactivations from English hold the potential to sig-\nnificantly enhance a model’s performance in non-\nEnglish languages. The cross-lingual connection\nmechanism in CC-T UNING aims to incorporate\nthe above latent interactions into the multilingual\nfine-tuning process, enabling the model to benefit\nfrom both English and non-English resources as\nthe parameters are updated.\nWe denote D={(xi, yi)}N\ni=1as a multilingual\nsupervised instruction dataset, where xirepresents\nthe input question for the i-th data point and yide-\nnotes the corresponding ground-truth response. Be-\nsides, CC-T UNING requires auxiliary parallel data,\nDen={(xi, xen\ni, yi)}N\ni=1, where xen\niis the En-\nglish translation of xi. Generally, the cross-lingual\nconnection mechanism consists of two key opera-\ntions: (1) adaptive decision maker and (2) latent\nfeed forward connection. Notably, these opera-\ntions are executed just before the Response Start\nToken (RST) , which marks the beginning of the\nmodel’s response in the training template. This\nensures that our operations can smoothly introduce\nthe intervention into the response generation pro-\ncess. Assuming the training template is structured\nas “[Input] {question} [output] {answer} ”, these\noperations are executed at the position that is right\nbefore the [output] token.\n--- Page 4 ---\nAdaptive Decision Maker. Given an auxiliary\ninput xen\ni, we first pass it through the model to ex-\ntract its feed-forward activations Fen\ni∈RL×d=\n{fen\ni,l}L\nl=1from Ldecoder layers, where dis the\ndimensionality of the hidden states. Notably, prior\nresearch has shown that not all feed-forward ac-\ntivations contribute equally and some may de-\ngrade performance (Ye et al., 2024b). To mitigate\nthis issue, we introduce a trainable linear layer\nWDM∈Rd×L, referred to as the Decision Maker ,\nwhich adaptively selects the most beneficial layer.\nBy combining Fen\niwith the embedding activations\nei∈Rdofxi, we integrate features from both En-\nglish and non-English inputs. The resulting com-\nbined features are then fed into the Decision Maker\nalong with Gumbel-Softmax (Jang et al., 2016) to\nachieve the identification as follows:\nHi=1\nLLX\nl=1(fen\ni,l+ei)·WDM (3)\nfen\ni,s=LX\nl=1Gumbel-Softmax( Hi)l·fen\ni,l (4)\nwhere fen\ni,s∈Rdrepresents the selected activation\nfrom the s-th layer among the Ldecoder layers.\nLatent Feed Forward Connection. The second\nstep aims to transfer the beneficial activation fen\ni,s\nidentified in the previous step into the forward prop-\nagation process of non-English input. When the\ninput xiis fed into the model, let the output of all\nLdecoders be denoted as Oi={oi,l}L\nl=1, where\neachoi,lshould have been obtained by combining\nthe feed-forward activations fi,land self-attention\nactivations ai,lthrough a residual connection. How-\never, the incorporation of fen\ni,srefines this process\nby connecting itself with the feed-forward activa-\ntionfi,1from the first decoder layer. Formally, this\nmodification can be expressed as:\n˜fi,1=fi,1+fen\ni,s (5)\nThe forward propagation of the input xithen con-\ntinues with this modification. Consequently, the\noriginal decoder outputs {oi,l}L\nl=jwill be altered to\n{˜oi,l}L\nl=jdue to the update of fi,1→˜fi,1, leading\nto new final prediction outcomes ˜oi,L.\nAnd within CC-T UNING , the training objective\nremains the same as the vanilla loss objective in\nEquation 1. During the tuning process, the model it-\nself, along with the Decision Maker , learns to lever-\nage the benefits of both English and non-English\nresources, improving its multilingual capabilities.3.2.2 Inference with Transform Matrix\nUnlike the training stage, our inference process\nis conducted without the need for parallel inputs.\nInstead, we leverages a training-free Transform\nMatrix to simulate the cross-lingual connection.\nThe role of the Transform Matrix WThere is to\nachieve the transformation of Fi={fi,l}L\nl=1→\nFen\ni={fen\ni,l}L\nl=1in the absence of parallel English\ninput xen\ni. Specifically, after training, we firstly\nsample 1,000 parallel pairs (xi, xen\ni)from the\ndatasets DandDen, and collect their feed-forward\nactivations, FiandFen\ni, respectively. These ac-\ntivations are then stacked and denoted as A=\n{fi,l|i= 1, ..., N ;l= 1, ..., L}andB={fen\ni,l|\ni= 1, ..., N ;l= 1, ..., L}. Therefore, Acan be\nmapped into Bas follows through WT:\nA·WT=B (6)\nTo minimize the difference AandB, our objective\nis defined as follows (Least-Squares optimization):\nW∗\nT= argmin\nWTNX\ni=1LX\nl=1\r\rfi,lWT−fen\ni,l\r\r2(7)\nThis problem seeks the optimal W∗\nTthat minimizes\nthe distance between the source and target repre-\nsentations. Hence, the closed-form solution to this\noptimization problem is:\nW∗\nT= NX\ni=1LX\ni=l(fi,l)Tfi,l!−1 NX\ni=1LX\ni=l(fi,l)Tfen\ni,l!\n(8)\nOnce the optimal WThas been learned, it can\nbe applied to the non-English representation to\nmap it to the corresponding English representa-\ntion. This resulting mapped representation Fi·WT,\nthen substitutes Fen\ni={fen\ni,l}L\nl=1in equations 3, 4,\n5, thereby simulating the cross-lingual connection.\nThis alignment effectively eliminates the depen-\ndence for bilingual parallel data and enables the\nsimulation of cross-lingual connection in a mono-\nlingual scenario.\n4 Experiments\n4.1 Setup\nModels. We selected two representative LLMs:\n(1)LLaMA-3.1-8B (Dubey et al., 2024) and (2)\nQwen2.5-7B (Yang et al., 2024).\nTraining Corpus. We totally select 20,236 mul-\ntilingual instruction pairs from aya dataset (Singh\n--- Page 5 ---\nMethodMultilingual Understanding Multilingual Generation\nXNLI XStoryCloze MMMLU MKQA XQuAD XLSum\nLLaMA. Qwen. LLaMA. Qwen. LLaMA. Qwen. LLaMA. Qwen. LLaMA. Qwen. LLaMA. Qwen.\nBaselines\nML-SFT 31.88 48.23 65.23 70.06 40.20 50.05 14.64 14.73 60.42 63.61 12.27 12.40\n+EN 35.02 50.76 65.13 71.63 39.62 48.80 13.28 13.05 57.40 62.34 12.04 12.20\n+MT 35.90 47.05 69.90 70.50 40.68 47.49 13.56 13.54 58.40 64.03 12.89 12.48\n+SDRRL 29.74 52.36 55.82 80.67 28.06 47.28 – – – – – –\nOurs\nCC-T UNING 38.42 51.00 70.60 71.43 40.74 49.65 15.94 14.84 61.85 63.72 12.88 12.50\n(+6.54) (+2.77) (+5.37) (+1.37) (+0.54) (-0.40) (+1.30) (+0.11) (+1.21) (+0.11) (+0.61) (+0.10)\n+EN 32.72 49.48 60.94 64.69 38.73 47.35 14.61 13.56 60.89 62.69 12.78 12.63\n(-2.30) (-1.28) (-4.19) (-6.94) (-0.89) (-1.45) (+1.33) (+0.51) (+3.40) (+0.35) (+0.74) (+0.43)\n+MT 36.44 48.13 73.54 71.39 38.87 49.39 15.59 13.77 61.55 64.26 13.05 12.87\n(+0.54) (+1.08) (+3.64) (+0.89) (-1.81) (+1.90) (+2.03) (+0.23) (+3.10) (+0.23) (+0.16) (+0.39)\n+SDRRL 29.84 53.06 69.19 80.93 37.77 47.87 – – – – – –\n(+0.10) (+0.70) (+13.37) (+0.26) (+9.71) (+0.59) – – – – – –\nTable 1: Main results that are the averages of the performance across all languages involved for each dataset.\nBlue cell indicates better performance than the vanilla ML-SFT under the same training data setting, while\nGray cell indicates the opposite. Bold numbers indicate the best performance. LLaMA. and Qwen. respectively\nrepresent LLaMA-3.1-8B andQwen2.5-7B .\net al., 2024) as our training corpus and the multi-\nlingual training corpus covers more than 60 lan-\nguages, ensuring extensive multilingual coverage.\nOur training processes are conducted on 8 * A800-\nSXM4-80GB with the following settings: batch\nsize=16 ,epochs=3 ,learning rate=1.0e-5 ,warmup\nratio=0.1 , and bf16=true . The implementation is\nbased on LLaMA-Factory (Zheng et al., 2024).\nBaselines. More details are in Appendix A.1.\n•ML-SFT represents vanilla supervised instruc-\ntion tuning (Ouyang et al., 2022) with original\nmultilingual instruction dataset (data size= N).\n•ML-SFT+E Nincorporates the full parallel En-\nglish version of the dataset for training, followed\nby vanilla supervised fine-tuning (data size= 2N).\n•ML-SFT+MT constructs additional translation\ntask form data by pairing the original multilin-\ngual instruction dataset with its parallel English\nversion and then applies supervised instruction\ntuning (data size= 2N).\n•ML-SFT+SDRRL (Zhang et al., 2024) is a\nself-distillation-based method that integrates En-\nglish instruction tuning data and its multilingual\ncode-switched extensions. Additionally, it in-\ncorporates partially translated data and comple-\ntion data for fine-tuning (LLaMA-3.1-8B: data\nsize≈1.2N, Qwen2.5-7B: data size ≈1.6N).\nAnd CC-T UNING (+E N, +MT, +SDRRL) refers\nto our method applying the cross-lingual connec-tion mechanism and its combination with different\nabove mentioned training data settings.\nEvaluation Datasets. We conduct experiments\non 6 benchmarks, which can be categorized into:\n•Multilingual Understanding: (1)XNLI (Con-\nneau et al., 2018), a multilingual natural language\ninference (NLI) dataset, (2) XStoryCloze (Lin\net al., 2022), a multilingual commonsense rea-\nsoning dataset for evaluating story understand-\ning and (3) MMMLU , the multilingual version\nofMMLU (Hendrycks et al., 2020), designed to\nevaluate models’ general knowledge.\n•Multilingual Generation: (1)MKQA (Long-\npre et al., 2021), an open-domain multilin-\ngual question answering evaluation dataset, (2)\nXQuAD (Artetxe et al., 2020), a question answer-\ning dataset and (3) XLSum (Hasan et al., 2021),\na multilingual abstractive summarization bench-\nmark comprising professionally annotated article-\nsummary pairs.\nFor each of the above datasets, we conduct experi-\nments on 10 language subsets, covering a total of\n22 languages. For XNLI ,XStoryCloze ,MMMLU ,\nMKQA and XQuAD datasets, Accuracy metric\nis used for evaluation. And for XLSum dataset,\nROUGE-L scores are reported. We use greedy\ndecoding with a max of 40 new tokens for each\nmodel. Detailed information on the datasets and\nevaluations can be found in Appendix A.2.\n--- Page 6 ---\n4.2 Main Results\nThe average results across the different languages\ninvolved in each dataset are presented in Table 1.\nThe detailed results for different languages can be\nfound in Table 7, 8. Note that the results of apply-\ning+SDRRL to NLG tasks are not reported, as it\nmay lead to deviations from the prompt language\nin model responses, as shown in Appendix A.3.\n(1)CC-T UNING outperforms vanilla SFT in\njoint multilingual learning scenarios. The re-\nsults in Table 1 demonstrate that under the same\nmultilingual training data settings of original data,\n+MT and +SDRRL ,CC-T UNING significantly\noutperforms vanilla SFT in both multilingual under-\nstanding and multilingual generation tasks. How-\never, under the +ENsetting, where more than half\nof the training data is in English, the cross-lingual\nconnection becomes an EN2ENconnection. This\nshift undermines the core goal of CC-T UNING —to\npromote cross-lingual latent interaction—leading\nto a notable decline in performance, which also\nemphasizes CC-T UNING ’s alignment with its mo-\ntivation and use case in joint multilingual learning\nscenarios.\n(2)CC-T UNING with original training data out-\nperforms data augmentation and distillation\nmethods on LLaMA-3.1-8B .As observed on\nLLaMA-3.1-8B ,CC-T UNING , even when trained\nsolely with the original dataset (data size = N),\noutperforms the data augmentation and distilla-\ntion approaches of ML-SFT+E N(data size = 2N),\n+MT (data size = 2N), and +SDRRL (data size ≈\n1.2N), which utilize larger training set. This sug-\ngests that, compared to implicitly introducing cross-\nlingual alignment information at the data level, the\nexplicit latent-level cross-lingual connection mech-\nanism in CC-T UNING provides a compelling alter-\nnative for facilitating cross-lingual interaction.\n4.3 Ablation Studies\nWe perform ablation studies to assess the following\naspects: (1) the effectiveness of the Transform Ma-\ntrix, (2) the necessity of the Decision Maker , and\n(3) the advantages of feed-forward activations in\nfacilitating cross-lingual interactions.\n(1) The Transform Matrix aligns well with the\neffect of using parallel bilingual inputs. We\nverify whether the Transform Matrix WTcan ef-\nfectively achieve the alignment by evaluating the\nmean squared error (MSE) between fi,l·WTandMethod\n(|M|= 1000 )XNLI XStoryCloze MMMLU MKQA XQuAD XLSum\nModel: LLaMA-3.1-8B\nMSE value MSE =1\nN×LPN\ni=1PL\nl=1(1\nd∥fi,l·WT−fen\ni,l∥2\n2)\nCC-T UNING 0.012427 0.013256 0.013196 0.021572 0.015428 0.014314\n+EN 0.014215 0.012734 0.012917 0.018137 0.015919 0.014046\n+MT 0.020413 0.021251 0.023016 0.027770 0.021769 0.025639\n+SDRRL 0.017896 0.019859 0.017098 – – –\nAVG.MSE 0.016238 0.016775 0.016557 0.022493 0.017705 0.018000\n|∆|value |∆|=|Result(Parallel Bilingual Input) - Result(Transform Matrix) |\nCC-T UNING 0.16 0.60 0.03 0.01 0.21 0.28\n+EN 0.08 0.31 0.25 0.01 0.08 0.16\n+MT 0.01 0.23 0.36 0.12 0.07 0.10\n+SDRRL 0.06 0.56 0.11 – – –\nAVG.|∆| 0.08 0.43 0.19 0.05 0.12 0.18\nTable 2: The results of mean squared error between feed-\nforward representations in English and the transformed\nrepresentations after applying the Transform Matrix , as\nwell as the performance difference |∆|between using\nparallel bilingual inputs and applying Transform Matrix .\nLLaMA-3.1-8B Qwen2.5-7B3033363942454851\n38.4251.00\n33.9250.73\n36.7749.44AccuracyPerformance on XNLIW/ DECISION MAKER W/ M EAN POOLING W/ RANDOM POOLING\nLLaMA-3.1-8B Qwen2.5-7B141516 15.94\n14.8414.71 14.6615.67\n14.48AccuracyPerformance on MKQA\nFigure 3: Performance comparisons of using Decision\nMaker ,Mean Pooling andRandom Pooling strategy on\nXNLI andMKQA datasets.\nfen\ni,las well as the performance difference |∆|be-\ntween using parallel bilingual inputs during infer-\nence and applying the Transform Matrix . The re-\nsults in Table 2 show that the MSE value reaches\nthe order of magnitude as low as 10−2, indicating\nthat the Transform Matrix effectively transforms\nfi,lintofen\ni,l. Additionally, the small performance\ndifference |∆|further suggests that the Transform\nMatrix serves as an effective substitute for parallel\nbilingual inputs, achieving great alignment.\n(2) The Decision Maker plays a crucial role. To\nverify the necessity of the Decision Maker , we\nreplaced it with two alternative strategies— Mean\nPooling andRandom Pooling —during both training\nand inference, and compared their performance in\nFigure 3. In Mean Pooling , the feed-forward activa-\ntions from all layers are averaged, while in Random\nPooling , a single activation is randomly selected\nfrom the set of feed-forward activations across all\nlayers. The results demonstrate that the perfor-\nmance with the Decision Maker significantly out-\nperforms the other two strategies, confirming that\n--- Page 7 ---\nLLaMA-3.1-8B Qwen2.5-7B3033363942454851\n38.4251.00\n36.3150.34\n34.3750.60AccuracyPerformance on XNLICC-T UNING (FFN) CC-T UNING (ATTN) CC-T UNING (BLOCK )\nLLaMA-3.1-8B Qwen2.5-7B141516 15.94\n14.8415.48\n14.6915.88\n14.71AccuracyPerformance on MKQA\nFigure 4: Performance comparisons of utilizing feed\nforward activations, self-attention activations and whole\ndecoder block activations for cross-lingual connection\nonXNLI andMKQA datasets.\ntheDecision Maker effectively serves its role in\nbeneficial activation identification and contributes\nto the overall training paradigm of CC-T UNING .\n(3) Feed-forward activations contribute the most\nin cross-lingual connection. In addition to in-\nvestigating cross-lingual connections at the feed-\nforward activation level, we also explored the po-\ntential contributions of self-attention activations\nand whole decoder block activations. Our results,\nas shown in Figure 4, indicate that feed-forward\nactivations have the most pronounced impact on\ncross-lingual connections within the CC-Tuning\nparadigm. This finding highlights the crucial role\nof feed-forward activations in facilitating cross-\nlingual latent interactions, which well match the\nfindings presented in Dai et al. (2022), where FFN\nstores factual knowledge, as well as the motiva-\ntion of cross-lingual feed forward transplantation\noperation in Ye et al. (2024b).\n5 Further Analysis\n5.1 Practicality Analysis\n(1) Is the Transform Matrix difficult to learn?\nFigure 5 presents the variation in MSE values be-\ntween fi,l·WTandfen\ni,las the amount of parallel\ndata,|M|, used to acquire the Transform Matrix\nincreases. We observe that when |M|= 1000 , the\nMSE value starts to converge between 0.01and\n0.02, and subsequently exhibits a stable trend. This\nindicates that only a thousand of parallel data are\nsufficient to effectively align fi,lwithfen\ni,lthrough\ntheTransform Matrix , suggesting that the Trans-\nform Matrix is relatively easy to learn.\n(2) Does incorporating cross-lingual connection\nsubstantially interfere with model training and\nmodel inference? During training , as shown in100 200 300 500 1000 2000 3000 5000 10000 15000 full00.010.020.030.040.050.060.070.080.090.10.110.12\nNum of MSetMSE valueMSE curvesXNLI XStoryCloze MMMLU\nMKQA XQuAD XLSum\nFigure 5: The curves of mean squared error between\nfeed-forward representations in English and the trans-\nformed representations after applying the Transform\nMatrix , as the amount of parallel data used to acquire\ntheTransform Matrix increases.\nFigure 6: The training loss curves of vanilla supervised\nfine-tuning and CC-T UNING under different training\nsettings (models and training data).\nFigure 6, the loss curves of vanilla SFT and CC-\nTUNING are closely aligned, suggesting that the\nincorporation of cross-lingual connection on top of\nvanilla SFT introduces only negligible interference\nto the overall training process. This is primarily\nbecause no additional training objectives are intro-\nduced. In terms of training overhead, our statistics\nshow that the training time for CC-T UNING is ap-\nproximately 1.12∼1.16 times that of vanilla SFT\n(Table 4). Moreover, the additional linear layer\nDecision Maker accounts for only 0.0016% and\n0.0013% of the total parameter count in LLaMA-\n3.1-8B andQwen2.5-7B , respectively—proportions\nso small that they are practically negligible. Dur-\ninginference , the time cost for inference with the\nTransform Matrix is also approximately 1.1 times\nthat of vanilla inference (Table 5).\n--- Page 8 ---\nEnglishArabicChinese\n(b) AfterVanillaSFT(c) AfterCC-Tuning(with Cross-lingual Connection)\n(a) BeforeSFTFigure 7: t-SNE visualizations of output representations\nbyLLaMA-3.1-8B before fine-tuning, after vanilla su-\npervised fine-tuning and after CC-T UNING .\n5.2 Multilingual Representation Analysis\nTo analyze the impact of CC-T UNING on multilin-\ngual representations, we employ t-SNE (Van der\nMaaten and Hinton, 2008) to visualize the repre-\nsentations of 200 sentences sampled from XNLI in\nparallel across English, Arabic, and Chinese.\nAs depicted in Figure 7 (c), after applying CC-\nTUNING , the multilingual representations show a\nsignificantly more compact clustering. This indi-\ncates that CC-T UNING has already facilitated a\ncertain level of cross-lingual interaction through\nthe cross-lingual connection mechanism, allowing\nthe multilingual representations after CC-T UNING\nrequire less extensive sharing with representations\nfrom other languages in high-dimensional space.\nAnd the boundaries between different language\nrepresentations become more distinct, suggesting\nthatCC-T UNING alleviates the mutual dependency\nbetween representations of different languages, en-\nabling the model to exhibit clearer and more dis-\ntinct multilingual modeling capabilities.\n5.3 Beneficial Layer Distribution Analysis\nIn this section, we present the distribution of the\nlayer with the highest probability of being selected\nby the Decision Maker across NLU and NLG tasks,\nas shown in Figure 8. This analysis explores layer-\nwise effectiveness within the cross-lingual connec-\ntion. The distribution results indicate that LLMs\ntend to predominantly utilize the middle layers for\nboth NLU and NLG tasks ( LLaMA-3.1-8B : 19;\nQwen2.5-7B : 17), which suggests that the middle\nlayers may capture more valuable and generalized\nknowledge, potentially acting as a bridge between\nrepresentations in different languages. Addition-\nally, we observe that the beneficial layers identified\nin NLG tasks are more diverse, likely due to the in-\nherent complexity of generation tasks. In contrast,\nNLU tasks—primarily focused on selecting from\npredefined options (e.g., A, B, C, or D)—are less\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32020406080Percentage (%)LLaMA-3.1-8B + NLU tasks ( XNLI ,XStoryCloze ,MMMLU )1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32010203040Percentage (%)LLaMA-3.1-8B + NLG tasks ( MKQA ,XQuAD ,XLSum )1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n2801020304050Percentage (%)Qwen2.5-7B + NLU tasks ( XNLI ,XStoryCloze ,MMMLU )1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n280102030Percentage (%)Qwen2.5-7B + NLG tasks ( MKQA ,XQuAD ,XLSum )\nFigure 8: The distribution of the layer with the highest\nprobability of being selected after the Decision Maker\nover NLU and NLG tasks.\ncomplex, and thus, the layer distribution tend to be\nmore concentrated.\n5.4 Performance on Cross-lingual Task\nWe further conduct additional experiments eval-\nuating the performance of CC-T UNING in cross-\nlingual scenarios on XQuAD under following set-\ntings: (1) “ X-to-English”: the question is given in\nlanguage X, and the model is explicitly prompted\nto respond in English. (2) “English-to- X”: the ques-\ntion is given in English, and the model is explicitly\nprompted to respond in language X.\nThe results in Table 3 show that CC-T UNING\noutperforms vanilla ML-SFT in both settings, high-\nlighting its effectiveness in cross-lingual scenarios.\nThe advantage is more pronounced in the X-to-\nEnglish” setting, where the model is given non-\nEnglish questions. This aligns with the motivation\nbehind CC-T UNING : the models can benefit more\nwhen processing non-English inputs by leverag-\ning the latent activations from English. Moreover,\nthe performance gains on English questions under\n“English-to- X” setting are relatively smaller, which\nis also consistent with the observations under +EN\nsetting in Table 1.\n--- Page 9 ---\nXQuAD (Ask in X, Answer in English) en ar de el hi ru th tr vi zh A VG\nML-SFT 72.61 15.29 30.84 21.60 12.18 15.21 18.82 25.38 33.87 13.95 25.97\nCC-T UNING 74.62 18.82 35.71 24.54 12.27 17.14 25.29 27.23 35.21 17.90 28.87\nXQuAD (Ask in English, Answer in X) en ar de el hi ru th tr vi zh A VG\nML-SFT 72.61 20.34 40.50 18.91 21.09 20.08 17.48 29.24 31.93 27.48 29.97\nCC-T UNING 75.29 18.99 40.34 20.67 23.36 20.34 19.41 29.33 33.45 28.49 30.97\nTable 3: Results on the cross-lingual QA task with LLaMA-3.1-8B . The symbol Xrefers to either the input prompt\nlanguage or the required response language, as specified by the corresponding configuration.\n5.5 Language Confusion Analysis\nLanguage confusion refers to the cases where the\nmodel fails to consistently response in the user’s\ndesired language, or the appropriate language given\nthe context. Here we employ the lid.176.bin model\nfrom fastText , which can identify 176 languages, to\nevaluate the alignment between model responses\nand input languages.\nThe results in Table 6 show that language con-\nfusion phenomenon frequently occured in baseline\nSDRRL . Since SDRRL is designed to facilitate\nknowledge distillation from resource-rich to low-\nresource languages, the training data under this\nsetup often contains inconsistencies between input\nand output languages. While this issue is partially\nmitigated through code-switching and the integra-\ntion of external parallel corpora, we observed that\nit still frequently causes deviations from the prompt\nlanguage in model responses, making SDRRL less\nsuitable for generation tasks. In contrast, CC-\nTUNING , along with other baselines, do not exhibit\nsignificant language confusions.\n6 Conclusion\nIn this paper, we propose CC-T UNING , a novel\nmultilingual fine-tuning paradigm that establishes a\ncross-lingual connection mechanism at latent level\nto address the imbalanced multilingual capabilities\nof current LLMs. During training, CC-T UNING\nfuses the feed forward activations from both En-\nglish and non-English inputs, enabling the model to\nbenefit from both languages. During inference, we\nsimulate the cross-lingual connection using only\nmonolingual input through representation transfor-\nmation techniques. Extensive experiments across\nsix benchmarks covering 22 languages demonstrate\nthatCC-T UNING outperforms vanilla supervised\nfine-tuning and serves as a strong latent-level alter-\nnative to data-level augmentation approaches. Our\nresults also highlight the importance of rethinking\nmultilingual training paradigms beyond superficialdata manipulation, suggesting that deeper architec-\ntural interventions may unlock greater potential in\nLLMs’ multilingual capabilities.\nLimitations\nThis work exhibits several limitations worth not-\ning. Firstly, though several ablation experiments\nare conducted to validate the benefits of our train-\ning paradigm, we believe there is much more to\nexplore and investigate in latent cross-lingual inter-\nactions. Such interactions should not only be lim-\nited to the form discussed in our work. Secondly,\nour experiments were conducted on LLaMA-3.1-\n8B and Qwen2.5-7B. While these models represent\nimportant milestones in open-source LLM devel-\nopment, the evaluation across more LLMs would\nimprove the generalizability of our findings across\nthe broader LLM ecosystem. Thirdly, due to the\ncomputational constraints, we did not conduct com-\nparisons between LLMs of different model sizes\n(particularly larger models), resulting in a lack of\ninsights into the impact of model capacity on per-\nformance.\nAcknowledgements\nXiaocheng Feng is the corresponding author of\nthis work. We thank the anonymous review-\ners for their insightful comments. This work\nwas supported by the National Natural Science\nFoundation of China (NSFC) (grant 62276078,\nU22B2059), the Key R&D Program of Hei-\nlongjiang via grant 2022ZX01A32, and the Funda-\nmental Research Funds for the Central Universities\n(Grant No.HIT.OCEF.2023018). We also thank\nHuawei Technologies Co., Ltd for supporting part\nof the computing resources and funding.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\n--- Page 10 ---\nInProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 3874–3884,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403 .\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 4623–4637, Online. Association\nfor Computational Linguistics.\nPinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, An-\ndrey Kutuzov, Barry Haddow, and Kenneth Heafield.\n2024. Monolingual or multilingual instruction tun-\ning: Which makes a better alpaca. In Findings of the\nAssociation for Computational Linguistics: EACL\n2024 , pages 1347–1356, St. Julian’s, Malta. Associa-\ntion for Computational Linguistics.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan\nWang, Fei Yu, Guiming Chen, Hongbo Zhang,\nJuhao Liang, Chen Zhang, Zhiyi Zhang, et al. 2023.\nPhoenix: Democratizing chatgpt across languages.\narXiv preprint arXiv:2304.10453 .\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing . Association for Computa-\ntional Linguistics.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca. arXiv preprint arXiv:2304.08177 .\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, andZhifang Sui. 2023. A survey for in-context learning.\nArXiv preprint , abs/2301.00234.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783 .\nGabriel Lino Garcia, Pedro Henrique Paiola, Luis Hen-\nrique Morelli, Giovani Candido, Arnaldo Cân-\ndido Júnior, Danilo Samuel Jodas, Luis Afonso,\nIvan Rizzo Guilherme, Bruno Elias Penteado, and\nJoão Paulo Papa. 2024. Introducing bode: A fine-\ntuned large language model for portuguese prompt-\nbased task. arXiv preprint arXiv:2401.02909 .\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 ,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300 .\nHIT-SCIR. 2024. Chinese-mixtral-8x7b: An open-\nsource mixture-of-experts llm. https://github.\ncom/HIT-SCIR/Chinese-Mixtral-8x7B .\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\nACM Transactions on Information Systems , 43(2):1–\n55.\nWenshuai Huo, Xiaocheng Feng, Yichong Huang,\nChengpeng Fu, Baohang Li, Yangfan Ye, Zhirui\nZhang, Dandan Tu, Duyu Tang, Yunfei Lu, et al.\n2025. Enhancing non-english capabilities of english-\ncentric large language models through deep supervi-\nsion fine-tuning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 39, pages\n24185–24193.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categori-\ncal reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144 .\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google‘s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics , 5:339–351.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri\nAji, and Timothy Baldwin. 2023. Bactrian-x:\n--- Page 11 ---\nMultilingual replicable instruction-following mod-\nels with low-rank adaptation. arXiv preprint\narXiv:2305.15011 .\nPeiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André FT\nMartins, and Hinrich Schütze. 2024. Mala-500: Mas-\nsive language adaptation of large language models.\narXiv preprint arXiv:2401.13303 .\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with\nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing , pages 9019–9052,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys , 55(9):1–35.\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021.\nMKQA: A linguistically diverse benchmark for mul-\ntilingual open domain question answering. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1389–1406.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems , 35:27730–27744.\nXingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng\nLiu, Yu Lu, and Shanbo Cheng. 2024. G-DIG: To-\nwards gradient-based DIverse and hiGh-quality in-\nstruction data selection for machine translation. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 15395–15406, Bangkok, Thai-\nland. Association for Computational Linguistics.\nRamon Pires, Hugo Abonizio, Thales Sales Almeida,\nand Rodrigo Nogueira. 2023. Sabiá: Portuguese\nlarge language models. In Brazilian Conference on\nIntelligent Systems , pages 226–240. Springer.\nLibo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen,\nYinghui Li, Lizi Liao, Min Li, Wanxiang Che, and\nPhilip S Yu. 2024. Multilingual large language\nmodel: A survey of resources, taxonomy and fron-\ntiers. arXiv preprint arXiv:2404.04925 .Leonardo Ranaldi, Giulia Pucci, and Andre Fre-\nitas. 2024. Empowering cross-lingual abilities\nof instruction-tuned large language models by\ntranslation-following demonstrations. In Findings of\nthe Association for Computational Linguistics: ACL\n2024 , pages 7961–7973, Bangkok, Thailand. Associ-\nation for Computational Linguistics.\nAndrea Santilli and Emanuele Rodolà. 2023. Camoscio:\nan italian instruction-tuned llama. Preprint ,\narXiv:2307.16456.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 .\nUri Shaham, Jonathan Herzig, Roee Aharoni, Idan\nSzpektor, Reut Tsarfaty, and Matan Eyal. 2024. Mul-\ntilingual instruction tuning with just a pinch of mul-\ntilinguality. In Findings of the Association for Com-\nputational Linguistics: ACL 2024 , pages 2304–2317,\nBangkok, Thailand. Association for Computational\nLinguistics.\nMurray Shanahan. 2022. Talking about large language\nmodels. ArXiv preprint , abs/2212.03551.\nShivalika Singh, Freddie Vargus, Daniel Dsouza,\nBörje F. Karlsson, Abinaya Mahendiran, Wei-Yin\nKo, Herumb Shandilya, Jay Patel, Deividas Mat-\naciunas, Laura OMahony, Mike Zhang, Ramith\nHettiarachchi, Joseph Wilson, Marina Machado,\nLuisa Souza Moura, Dominik Krzemi ´nski, Hakimeh\nFadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib,\nOshan Mudannayake, Zaid Alyafeai, Vu Minh Chien,\nSebastian Ruder, Surya Guthikonda, Emad A. Al-\nghamdi, Sebastian Gehrmann, Niklas Muennighoff,\nMax Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh\nFadaee, and Sara Hooker. 2024. Aya dataset: An\nopen-access collection for multilingual instruction\ntuning. Preprint , arXiv:2402.06619.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and finetuning. arXiv\npreprint arXiv:2008.00401 .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 .\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\net al. 2024. Aya model: An instruction finetuned\nopen-access multilingual language model. arXiv\npreprint arXiv:2402.07827 .\n--- Page 12 ---\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research , 9(11).\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.\n2020. On negative interference in multilingual mod-\nels: Findings and a meta-learning treatment. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4438–4450, Online. Association for Computa-\ntional Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\nArXiv preprint , abs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems , 35:24824–24837.\nXiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei\nZhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei\nCao, Binbin Xie, et al. 2023. Polylm: An open\nsource polyglot large language model. arXiv preprint\narXiv:2307.06018 .\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-\nhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJingren Zhou, Junyang Lin, Kai Dang, Keming Lu,\nKeqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng\nXue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tian-\nhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng\nRen, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan\nQiu. 2024. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115 .\nJiacheng Ye, Xijia Tao, and Lingpeng Kong. 2023. Lan-\nguage versatilists vs. specialists: An empirical revis-\niting on multilingual transfer ability. arXiv preprint\narXiv:2306.06688 .\nYangfan Ye, Xiachong Feng, Xiaocheng Feng, Weitao\nMa, Libo Qin, Dongliang Xu, Qing Yang, Hongtao\nLiu, and Bing Qin. 2024a. GlobeSumm: A chal-\nlenging benchmark towards unifying multi-lingual,\ncross-lingual and multi-document news summariza-\ntion. In Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 10803–10821, Miami, Florida, USA. Associa-\ntion for Computational Linguistics.\nYangfan Ye, Xiaocheng Feng, Xiachong Feng, Libo\nQin, Yichong Huang, Lei Huang, Weitao Ma, Zhirui\nZhang, Yunfei Lu, Xiaohui Yan, et al. 2024b. Xtrans-\nplant: A probe into the upper bound performance\nof multilingual capability and culture adaptability in\nllms via mutual cross-lingual feed-forward transplan-\ntation. arXiv preprint arXiv:2412.12686 .Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and\nGrzegorz Kondrak. 2023. Don’t trust ChatGPT when\nyour question is not in English: A study of multilin-\ngual abilities and types of LLMs. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing , pages 7915–7927, Singa-\npore. Association for Computational Linguistics.\nYuanchi Zhang, Yile Wang, Zijun Liu, Shuo Wang, Xi-\naolong Wang, Peng Li, Maosong Sun, and Yang\nLiu. 2024. Enhancing multilingual capabilities\nof large language models through self-distillation\nfrom resource-rich languages. arXiv preprint\narXiv:2402.12204 .\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. ArXiv preprint ,\nabs/2303.18223.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan\nYe, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.\n2024. Llamafactory: Unified efficient fine-tuning\nof 100+ language models. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 3: System Demonstra-\ntions) , Bangkok, Thailand. Association for Computa-\ntional Linguistics.\nWenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan,\nJingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun\nChen, and Lei Li. 2023. Extrapolating large language\nmodels to non-english by aligning languages. arXiv\npreprint arXiv:2308.04948 .\n--- Page 13 ---\nA Experiment Details\nA.1 Baselines Settings\nThis section introduces the details of different train-\ning data settings.\n•+ENcombines the original multilingual dataset\nDwith its translated parallel English dataset Den,\nresulting in a total training dataset size of N+\nN= 2N.\n•+MT constructs additional translation task form\ndata by pairing the original multilingual dataset\nDwith its translated parallel English dataset Den\nas follows:\n{\n\"instruction\": \"Translate the following sentence from\nEnglish to Spanish.\\n The category corresponds to poli-\ntics.\",\n\"output\": \"La categoría corresponde a política. \"\n}\nNpairs of parallel data from DandDencan be\nconstructed into Nadditional samples of trans-\nlation task form data, resulting in a total training\ndataset size of N+N= 2N.\n•+SDRRL (Zhang et al., 2024) is a self-\ndistillation-based method that integrates English\ninstruction tuning data and its multilingual code-\nswitched extensions. Additionally, it incor-\nporates partially translated data and comple-\ntion data for fine-tuning (LLaMA-3.1-8B: data\nsize≈1.2N, Qwen2.5-7B: data size ≈1.6N).\nA.2 Datasets and Evaluations\nA.2.1 Datasets\nThe language subsets used in the 6 evaluation\ndatasets involved in our experiments and the data\nsize used for each language subset are as follows:\nInvolved Languages (10 languages each dataset)\nXNLI: en, ar, el, hi, ru, sw, th, tr, ur, zh\nXStoryCloze: en, ar, es, eu, hi, id, ru, sw, te, zh\nMMMLU: en, ar, bn, es, hi, id, ko, pt, sw, yo\nXQuAD: en, ar, de, el, hi, ru, th, tr, vi, zh\nMKQA: en, ar, de, ja, ko, pt, ru, tr, vi, zh\nXLSum: en, ar, fr, hi, id, ru, sw, tr, ur, vi\nA total of 22 unique languages are involved\n1.\"...\\n问题：⿊豹队的防守丢了多少分？\\n\\n您的答案：2.\"...\\n问题：贾⾥德在职业⽣涯中有多少次擒杀？\\n\\n您的答案：\"3.\"...\\n问题：卢克·坎克利贡献了多少次擒抱？\\n\\n您的答案：\"4.\"...\\n问题：约什·诺曼拦截了多少球？\\n\\n您的答案：\"5.\"...\\n问题：本赛季谁为球队贡献的擒杀最多？\\n\\n您的答案：\"6.\"...\\n问题：2015年⿊豹队的防守有多少次拦截记录？\\n\\n您的答案：\"7.\"...\\n问题：谁带领⿊豹队擒杀？\\n\\n您的答案：\"8.\"...\\n问题：有多少名⿊豹队防守球员⼊选了职业碗？\\n\\n您的答案：\"9.\"...\\n问题：托⻢斯·戴维斯有多少次迫使掉球？\\n\\n您的答案：\"10.\"...\\n问题：本赛季哪个球员拦截次数最多？\\n\\n您的答案：\"11....1.\"The answer is 308 points.\"2.\"Jared Allen has 136 career sacks.\"3.\"在他们⾝后，⿊豹队的三名⾸发线卫中有两⼈⼊选了职业碗：托⻢斯·戴维斯和卢克·坎克\"4.\"四次\"5.\"Jared Allen\"6.\"The answer is 24.\"7.\"Jared Allen\"8.\"11⼈\"9.\"Thomas Davis forced four fumbles.\"10.\"The answer is: Josh Norman\"11....Questions (ask in Chinese)\nAnswersFigure 9: Examples of the deviations from the prompt\nlanguage in model responses when applying +SDRRL .\nSample Size\nXNLI: 1000×10 = 10000 (parallel)\nXStoryCloze: 1511×10 = 15110 (parallel)\nMMMLU: 1000×10 = 10000 (parallel)\nMKQA: 1000×10 = 10000 (parallel)\nXQuAD: 1190×10 = 11900 (parallel)\nXLSum: 100×10 = 1000 (non-parallel)\nA.2.2 Evaluations\nXNLI ,XStoryCloze , and MMMLU all belong to\nthe multiple-choice category. For these datasets,\na model’s response is considered correct only if it\ncontains the correct option and excludes all other\noptions. For the short QA generative dataset MKQA\nandXQuAD , a model’s answer is deemed correct\nif the gold answer appears in the model’s response.\nA.3 Model Responses with +SDRRL\nThe results of applying +SDRRL to NLG tasks\nare not reported in the main body, as it may lead to\ndeviations from the prompt language in model re-\nsponses. Since +SDRRL aims to achieve distilla-\ntion from resource-rich languages to low-resource\nlanguages, many of the training data’s input and\noutput languages under this setup are inconsistent.\nAlthough this issue is partially mitigated through\ncode-switching and the incorporation of external\nparallel corpora, we still observed that it easily\nleads to deviations from the prompt language in\nmodel responses, making it unsuitable for NLG\ntasks. As in the examples shown in Figure 9, only\n3 of the 10 questions given are correctly answered\nin Chinese, while the rest are all answered in En-\nglish.\n--- Page 14 ---\nTraining Time Cost (h:m:s) LLaMA-3.1-8B Qwen2.5-7B\nML-SFT 01:36:43 01:33:10\nCC-T UNING 01:51:58 01:45:15\nTime Cost Ratio 1.16 1.13\nML-SFT+E N 03:08:25 03:03:02\nCC-T UNING +EN 03:34:28 03:25:30\nTime Cost Ratio 1.14 1.12\nML-SFT+MT 03:08:24 03:04:13\nCC-T UNING +MT 03:34:19 03:25:59\nTime Cost Ratio 1.14 1.12\nML-SFT+SDRRL 01:52:17 02:23:00\nCC-T UNING +SDRRL 02:08:52 02:41:20\nTime Cost Ratio 1.15 1.13\nTable 4: Comparisons of training time cost.\nInference Time Cost (s) LLaMA-3.1-8B Qwen2.5-7B\nvanilla inference 2012.26 1898.89\ninference w/ Transform Matrix 2209.90 2064.50\nTime Cost Ratio 1.10 1.09\nTable 5: Comparisons of inference time cost on the\nArabic subset of XNLI dataset.\nConsistency MKQA XQuAD XLSum\nLLaMA-3.1-8B\nBase Model 0.880 0.988 0.879\nML-SFT 0.879 0.901 0.983\nML-SFT+E N 0.911 0.888 0.988\nML-SFT+MT 0.827 0.888 0.986\nML-SFT+SDRRL 0.360 0.447 0.352\nCC-T UNING 0.972 0.967 0.993\nQwen2.5-7B\nBase Model 0.963 0.915 0.955\nML-SFT 0.996 0.999 0.999\nML-SFT+E N 0.994 0.998 0.998\nML-SFT+MT 0.995 0.998 0.981\nML-SFT+SDRRL 0.541 0.642 0.582\nCC-T UNING 0.996 0.997 0.997\nTable 6: Input and output language consistency results.\n--- Page 15 ---\nModels Dataset: XNLI\nen ar el hi ru sw th tr ur zh Avg\nVanilla Model ( LLaMA-3.1-8B ) 36.20 15.20 23.90 31.80 29.70 28.20 29.70 28.10 24.50 8.90 25.62\nML-SFT ( LLaMA-3.1-8B ) 12.90 35.50 35.80 31.10 34.50 31.20 31.60 37.70 33.10 35.40 31.88\n+EN 46.60 38.70 24.40 32.20 32.70 31.90 32.00 37.60 38.90 35.20 35.02\n+MT 47.20 35.10 22.10 35.80 40.90 31.00 32.90 39.20 36.90 37.90 35.90\n+SDRRL 29.80 29.70 29.70 29.80 29.70 29.80 29.70 29.70 29.80 29.70 29.74\nCC-T UNING (LLaMA-3.1-8B ) 51.10 40.50 38.90 33.20 42.50 30.70 37.10 39.00 35.10 36.10 38.42\n+EN 39.50 32.00 33.40 29.20 34.50 30.00 31.20 31.70 34.50 31.20 32.72\n+MT 48.70 36.20 37.30 30.40 39.70 31.10 32.50 38.00 33.00 37.50 36.44\n+SDRRL 29.70 29.60 29.70 31.00 29.50 29.00 29.70 29.70 30.70 29.80 29.84\nVanilla Model ( Qwen2.5-7B ) 90.20 43.60 40.40 41.80 58.70 11.60 46.90 42.50 33.60 49.00 45.83\nML-SFT ( Qwen2.5-7B ) 60.60 52.60 44.80 45.90 56.60 29.70 51.20 48.10 36.30 56.50 48.23\n+EN 81.70 54.10 39.70 43.30 59.60 30.50 50.70 49.30 39.70 59.00 50.76\n+MT 61.60 49.60 36.70 45.80 56.80 29.70 51.60 48.10 42.00 48.60 47.05\n+SDRRL 81.60 56.60 34.00 51.00 60.20 33.10 55.20 54.10 38.90 58.90 52.36\nCC-T UNING (Qwen2.5-7B ) 78.90 51.80 41.70 44.00 60.10 30.70 52.30 50.60 40.50 59.40 51.00\n+EN 72.00 54.30 43.10 47.00 56.50 28.60 51.70 48.40 35.90 57.30 49.48\n+MT 64.40 51.40 36.70 43.70 57.70 29.70 52.50 49.40 37.90 57.90 48.13\n+SDRRL 83.80 56.30 33.40 46.00 60.00 32.20 58.00 58.30 42.30 60.30 53.06\nModels Dataset: XStoryCloze\nen ar es eu hi id ru sw te zh Avg\nVanilla Model ( LLaMA-3.1-8B ) 49.70 41.69 14.89 28.72 48.44 60.03 36.47 49.70 6.02 19.72 35.54\nML-SFT ( LLaMA-3.1-8B ) 88.62 65.32 21.91 64.86 70.81 83.39 43.61 62.54 63.40 87.82 65.23\n+EN 77.04 40.44 65.45 59.36 77.10 79.62 49.44 60.49 55.46 86.90 65.13\n+MT 91.73 77.70 85.04 60.82 75.78 80.48 62.14 57.84 20.91 86.57 69.90\n+SDRRL 72.93 65.32 45.80 20.32 68.63 64.13 66.05 45.14 49.64 60.29 55.82\nCC-T UNING (LLaMA-3.1-8B ) 89.34 73.73 64.26 51.09 79.48 79.81 70.22 58.24 55.33 84.45 70.60\n+EN 69.36 66.71 75.38 25.74 62.48 75.78 49.77 50.36 49.24 84.58 60.94\n+MT 87.43 73.99 87.62 57.91 82.06 82.86 76.44 59.43 38.65 89.01 73.54\n+SDRRL 86.96 71.67 70.42 34.61 80.61 77.17 77.63 50.56 66.18 76.04 69.19\nVanilla Model ( Qwen2.5-7B ) 85.97 85.84 91.40 18.07 78.76 69.89 91.33 17.94 55.92 75.84 67.09\nML-SFT ( Qwen2.5-7B ) 92.12 78.89 93.51 52.95 79.48 79.48 71.21 37.06 28.92 86.96 70.06\n+EN 78.23 54.27 91.00 56.25 81.80 87.36 71.34 44.74 61.02 90.27 71.63\n+MT 82.06 56.12 92.19 57.64 82.20 88.15 73.92 29.52 57.78 85.44 70.50\n+SDRRL 93.85 88.42 94.51 62.61 82.00 89.15 93.05 52.88 62.01 88.22 80.67\nCC-T UNING (Qwen2.5-7B ) 91.59 81.60 91.00 54.86 77.96 80.68 78.82 35.94 37.59 84.25 71.43\n+EN 39.38 37.46 90.40 55.26 79.55 86.70 85.24 45.00 42.55 85.31 64.69\n+MT 65.32 66.64 91.66 55.06 81.67 86.43 75.12 52.75 53.47 85.77 71.39\n+SDRRL 93.45 90.87 92.26 57.58 82.26 88.68 94.51 57.25 59.03 93.45 80.93\nModels Dataset: MMMLU\nen ar bn es hi id ko pt sw yo Avg\nVanilla Model ( LLaMA-3.1-8B ) 45.40 28.20 17.70 11.30 25.10 26.40 25.00 11.70 16.20 0.60 20.76\nML-SFT ( LLaMA-3.1-8B ) 57.40 41.60 31.70 51.20 37.60 44.70 39.40 51.70 20.70 26.00 40.20\n+EN 56.80 35.90 32.00 50.90 36.00 40.80 40.50 48.10 29.80 25.40 39.62\n+MT 59.20 37.80 33.10 51.50 37.60 42.80 42.50 48.10 28.30 25.90 40.68\n+SDRRL 53.70 32.20 23.00 27.80 35.30 30.50 27.20 36.70 12.80 1.40 28.06\nCC-T UNING (LLaMA-3.1-8B ) 57.50 41.30 33.40 51.70 37.60 43.30 41.70 46.80 27.00 27.10 40.74\n+EN 56.50 38.10 30.80 49.90 37.00 40.70 39.00 47.30 26.80 21.20 38.73\n+MT 55.70 36.80 30.70 49.60 36.10 39.90 38.70 48.40 26.70 26.10 38.87\n+SDRRL 53.10 38.60 33.40 46.40 36.50 41.10 35.70 47.20 31.70 14.00 37.77\nVanilla Model ( Qwen2.5-7B ) 68.20 53.50 43.70 64.00 47.40 60.90 46.00 62.40 17.90 1.30 46.53\nML-SFT ( Qwen2.5-7B ) 69.80 53.30 42.00 65.60 41.10 59.50 55.70 62.60 28.60 22.30 50.05\n+EN 65.90 53.20 37.90 64.60 40.30 56.80 55.70 62.10 28.20 23.30 48.80\n+MT 60.50 51.40 40.00 63.90 39.50 58.30 49.00 63.00 28.80 20.50 47.49\n+SDRRL 66.00 46.00 38.30 60.70 41.50 56.90 52.10 58.60 29.90 22.80 47.28\nCC-T UNING (Qwen2.5-7B ) 69.10 52.80 40.50 65.10 41.20 59.10 54.90 62.30 30.90 20.60 49.65\n+EN 67.10 54.50 38.10 64.20 41.90 55.50 53.90 61.30 24.10 12.90 47.35\n+MT 66.60 53.10 40.30 64.70 41.70 60.50 53.60 63.10 29.90 20.40 49.39\n+SDRRL 66.30 50.60 39.30 60.30 41.60 56.30 52.30 57.50 28.10 26.40 47.87\nTable 7: The detailed performance results of different language subsets on NLU tasks (XNLI, XStoryCloze,\nMMMLU) across all involved models and baselines.\n--- Page 16 ---\nModels Dataset: MKQA\nen ar de ja ko pt ru tr vi zh Avg\nVanilla Model ( LLaMA-3.1-8B ) 22.50 5.20 3.50 3.50 3.00 4.80 4.90 15.70 6.80 5.70 7.56\nML-SFT ( LLaMA-3.1-8B ) 33.60 4.90 23.30 9.10 6.00 20.70 10.00 15.60 14.30 8.90 14.64\n+EN 26.00 6.30 18.80 9.70 5.70 19.40 10.00 15.60 13.00 8.30 13.28\n+MT 29.20 5.80 19.50 9.10 5.70 17.10 10.70 15.50 14.40 8.60 13.56\n+SDRRL – – – – – – – – – – –\nCC-T UNING (LLaMA-3.1-8B ) 32.00 6.00 24.10 10.90 6.30 22.40 10.50 17.80 18.20 11.20 15.94\n+EN 27.70 6.40 20.20 11.10 7.30 20.50 9.50 17.20 15.50 10.70 14.61\n+MT 32.10 6.90 21.90 10.70 7.30 21.10 10.10 17.70 17.30 10.80 15.59\n+SDRRL – – – – – – – – – – –\nVanilla Model ( Qwen2.5-7B ) 1.00 6.60 8.50 10.40 8.30 7.20 7.50 10.10 15.70 15.20 9.05\nML-SFT ( Qwen2.5-7B ) 30.30 6.60 19.10 11.20 8.90 19.70 9.40 12.10 15.80 14.20 14.73\n+EN 27.80 6.90 14.80 10.80 7.40 17.50 8.10 10.10 14.70 12.40 13.05\n+MT 27.10 6.90 16.10 9.60 7.90 19.40 8.60 11.00 14.70 14.10 13.54\n+SDRRL – – – – – – – – – – –\nCC-T UNING (Qwen2.5-7B ) 30.3 7.2 18.60 11.6 8.5 20.5 8.3 12.9 15.80 14.7 14.84\n+EN 27.60 5.90 15.10 10.80 7.60 19.20 9.60 12.90 13.60 13.30 13.56\n+MT 29.30 6.50 16.10 11.20 7.90 18.30 8.50 12.50 13.30 14.10 13.77\n+SDRRL – – – – – – – – – – –\nModels Dataset: XQuAD\nen ar bn es hi id ko pt sw yo Avg\nVanilla Model ( LLaMA-3.1-8B ) 72.18 52.86 58.07 47.73 61.26 43.87 46.97 51.93 53.53 68.32 55.67\nML-SFT ( LLaMA-3.1-8B ) 72.61 56.13 64.62 52.18 60.00 47.73 58.49 53.70 64.87 73.87 60.42\n+EN 63.28 53.45 62.02 51.09 57.73 46.39 58.40 50.84 61.18 69.66 57.40\n+MT 71.76 53.78 60.84 50.84 58.99 49.33 54.03 47.73 65.21 71.51 58.40\n+SDRRL – – – – – – – – – – –\nCC-T UNING (LLaMA-3.1-8B ) 75.29 55.29 64.96 51.34 62.27 52.10 60.42 54.20 67.82 74.79 61.85\n+EN 69.08 58.32 64.03 52.77 60.59 51.51 60.25 52.69 66.64 73.03 60.89\n+MT 77.73 55.29 63.45 53.61 61.34 52.18 56.72 53.11 68.24 73.78 61.55\n+SDRRL – – – – – – – – – – –\nVanilla Model ( Qwen2.5-7B ) 53.19 71.26 71.01 50.17 49.92 56.39 64.62 57.98 77.31 89.24 64.11\nML-SFT ( Qwen2.5-7B ) 79.92 66.97 70.08 40.00 46.39 53.95 64.96 56.05 72.77 85.04 63.61\n+EN 74.29 64.54 69.41 36.13 47.39 54.37 64.03 59.41 73.19 80.59 62.34\n+MT 79.33 65.13 69.33 41.01 50.67 52.61 67.56 58.24 72.69 83.70 64.03\n+SDRRL – – – – – – – – – – –\nCC-T UNING (Qwen2.5-7B ) 79.24 64.12 71.34 39.75 47.06 53.61 65.71 57.73 74.03 84.62 63.72\n+EN 72.18 64.45 68.40 41.01 47.31 54.37 66.30 58.99 72.94 80.92 62.69\n+MT 77.98 67.31 71.26 39.75 49.41 52.86 69.33 57.82 72.27 84.62 64.26\n+SDRRL – – – – – – – – – – –\nModels Dataset: XLSum\nen ar fr hi id ru sw tr ur vi Avg\nVanilla Model ( LLaMA-3.1-8B ) 6.60 3.88 11.91 1.02 5.63 7.62 3.53 5.74 1.54 9.59 5.71\nML-SFT ( LLaMA-3.1-8B ) 24.36 9.67 18.66 1.94 13.72 14.47 8.05 11.07 6.64 14.14 12.27\n+EN 22.46 10.62 19.66 2.97 13.72 14.02 6.76 7.14 5.77 17.27 12.04\n+MT 25.74 11.06 19.50 3.97 14.78 14.74 7.56 9.58 7.16 14.78 12.89\n+SDRRL – – – – – – – – – – –\nCC-T UNING (LLaMA-3.1-8B ) 25.00 10.87 19.46 3.02 13.46 15.55 8.63 10.01 7.20 15.63 12.88\n+EN 23.76 10.26 21.45 3.67 14.30 14.45 8.94 9.94 6.15 14.92 12.78\n+MT 27.57 11.38 21.08 3.23 13.34 15.71 9.14 10.88 4.41 13.73 13.05\n+SDRRL – – – – – – – – – – –\nVanilla Model ( Qwen2.5-7B ) 10.45 3.59 10.86 0.00 5.43 6.89 2.73 3.54 3.09 4.21 5.08\nML-SFT ( Qwen2.5-7B ) 24.13 12.20 22.10 0.33 14.89 16.10 5.95 8.04 5.47 14.74 12.40\n+EN 23.75 11.70 20.14 0.33 14.97 15.61 6.90 8.51 5.78 14.36 12.20\n+MT 26.72 12.32 21.47 0.67 14.00 15.29 5.66 8.73 5.12 14.78 12.48\n+SDRRL – – – – – – – – – – –\nCC-T UNING (Qwen2.5-7B ) 23.22 10.75 22.21 0.62 14.47 17.61 6.47 8.37 5.43 15.84 12.50\n+EN 25.06 12.79 19.58 0.33 14.71 15.63 7.12 10.62 5.39 15.01 12.63\n+MT 25.84 11.46 22.62 1.00 15.77 16.43 5.69 9.37 5.06 15.46 12.87\n+SDRRL – – – – – – – – – – –\nTable 8: The detailed performance results of different language subsets on NLG tasks (MKQA, XQuAD, XLSum)\nacross all involved models and baselines.",
  "text_length": 66058
}