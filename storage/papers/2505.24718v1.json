{
  "id": "http://arxiv.org/abs/2505.24718v1",
  "title": "Reinforcing Video Reasoning with Focused Thinking",
  "summary": "Recent advancements in reinforcement learning, particularly through Group\nRelative Policy Optimization (GRPO), have significantly improved multimodal\nlarge language models for complex reasoning tasks. However, two critical\nlimitations persist: 1) they often produce unfocused, verbose reasoning chains\nthat obscure salient spatiotemporal cues and 2) binary rewarding fails to\naccount for partially correct answers, resulting in high reward variance and\ninefficient learning. In this paper, we propose TW-GRPO, a novel framework that\nenhances visual reasoning with focused thinking and dense reward granularity.\nSpecifically, we employs a token weighting mechanism that prioritizes tokens\nwith high informational density (estimated by intra-group variance),\nsuppressing redundant tokens like generic reasoning prefixes. Furthermore, we\nreformulate RL training by shifting from single-choice to multi-choice QA\ntasks, where soft rewards enable finer-grained gradient estimation by\ndistinguishing partial correctness. Additionally, we propose question-answer\ninversion, a data augmentation strategy to generate diverse multi-choice\nsamples from existing benchmarks. Experiments demonstrate state-of-the-art\nperformance on several video reasoning and general understanding benchmarks.\nNotably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over\nVideo-R1) and 65.8\\% on MMVU. Our codes are available at\n\\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.",
  "authors": [
    "Jisheng Dang",
    "Jingze Wu",
    "Teng Wang",
    "Xuanhui Lin",
    "Nannan Zhu",
    "Hongbo Chen",
    "Wei-Shi Zheng",
    "Meng Wang",
    "Tat-Seng Chua"
  ],
  "published": "2025-05-30T15:42:19Z",
  "updated": "2025-05-30T15:42:19Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24718v1",
  "full_text": "arXiv:2505.24718v1 [cs.CV] 30 May 2025Reinforcing Video Reasoning with Focused Thinking Jisheng Dang1,2,5†Jingze Wu1†, Teng Wang3∗, Xuanhui Lin2, Nannan Zhu1, Hongbo Chen1,Wei-Shi Zheng1,Meng Wang4,Tat-Seng Chua5 1Sun Yat-sen University2Lanzhou University3University of Hong Kong 4Hefei University of Technology5National University of Singapore Abstract Recent advancements in reinforcement learning, particularly through Group Rela- tive Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group variance), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we pro- pose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state- of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4% accuracy on CLEVRER (18.8% improvement over Video-R1) and 65.8% on MMVU. Our codes are available at https://github.com/longmalongma/TW-GRPO. 1 Introduction Recent advances in reinforcement learning (RL) for large language models (LLMs) have yielded significant improvements in reasoning capabilities. DeepSeek-R1 [ 1] demonstrated that pure RL optimization can substantially improve model reasoning, while subsequent works [ 2;3;4;5;6] extended these benefits to multimodal scenarios. Notable examples include VideoR1 [ 7], which introduced T-GRPO for video spatiotemporal reasoning, and VideoChat-R1 [ 8] that leverages GRPO- based multi-task joint fine-tuning. These improvements show promising progress in understanding of fine-grained video details and multi-step reasoning. Although RL-based approaches excel in optimizing verifiable metrics, critical challenges persist in refining reasoning quality and reward granularity for complex multimodal tasks. First, while chain- of-thought (CoT) reasoning has proven effective for solving language-based intricate problems [ 9], its application to MLLMs often results yields verbose, unfocused thinking chains ( e.g., overthinking [ 10]). Building upon this, current training objectives fail to prioritize semantically critical spatio-temporal cues, which may obscure pivotal information and hurt learning efficiency. Second, existing methods rely on sparse, binary rewards derived from single-choice question-answer (QA) tasks [ 7;11;4]. These rewards assign maximum credit for fully correct answers and none otherwise, disregarding partial correctness. Recent work on video grounding [ 8], shows that soft reward signals enable †Equal contribution.∗Corresponding author <think> Let ’s analyze... </think> not... thepurple <think>Important UnimportantB 0 (b) Our method(a) Existing method Important Unimportant Let ’s analyze... </think> collide. B D 1 B 0.5 Question: \"Which of the following will not happen without the yellow object?\" A. The red sphere collides with the cyan cube. B. The gray object and the cyan sphere collide. C. The red object and the gray object collide. D. The gray cube collides with the purple object. the image each option coll with object idesTime (c) Model performanceNext -GQA MMVU CLEVRER VideoMME TempCompassMVBench VideoQwen2.5VL -7B Video -R1 VideoChat -R1 Our method50.476.1 65.8 63.3 73.355.1Soft reward AnswerBinary reward AnswerVanilla thinkingVanilla thinking Focused thinkingFocused thinking will cyan sphere collide. not the and grey object the <think> Let ’s analyze... </think> the image will cyan sphere collide. not the and grey object the so they doFigure 1: TW-GRPO integrates focused thinking and soft multi-level rewards for multi-choice QA. Unlike vanilla thinking which assigns uniform token importance, focused thinking highlight critical tokens to dominate loss calculation. By shifting single-choice QA’s binary rewards to multi-choice QA’s multi-level rewards, TW-GRPO enables fine-grained gradient estimation and training efficiency. finer-grained optimization. However, such approaches remain underexplored in mainstream video QA tasks, where single-choice formats lack naturally defined multi-level reward signals. To address these limitations, we propose TW-GRPO, a novel framework that enhances GRPO via token weighting and multi-grained rewards. As shown in Figure 1, our dynamic weighting mechanism prioritizes tokens with high informational density during loss computation. Specifically, we estimate token importance by analyzing intra-group distributional divergence across token positions, focusing the model on content critical to reasoning outcomes rather than generic phrases ( e.g., prefatory statements and repeated verifications). By prioritizing these tokens, the model learns to generate concise, task-aware reasoning chains while avoiding redundant or irrelevant details. Furthermore, we reformulate RL training using multi-choice QA tasks, replacing sparse binary rewards with multi-level ones. Our approach distinguishes between partially correct and fully incorrect answers, enabling finer-grained gradient estimation and stabilizes policy updates. To mitigate multi-choice data scarcity, we introduce Question-Answer Inverse (QAI), that converts single-choice tasks into multi-choice formats by negating questions and inverting answers. Experiments demonstrate TW-GRPO’s superiority on multiple video reasoning and general un- derstanding benchmarks. As shown in Table 1, our model achieves state-of-the-art accuracy on CLEVRER [ 12], NExT-GQA [ 13], and MMVU [ 14], outperforming Video-R1 with a clear margin by 18.8%, 1.8%, and 1.6%, respectively. With focused thinking, qualitative analysis reveals condensed reasoning chains focused on critical visual or logical cues. Multi-level rewards also reduce reward variance during training. Our main contributions are summarized as follows: •We propose dynamic token weighting, a mechanism prioritizing tokens with high informa- tional density during loss computation, enabling concise, task-focused reasoning chains. •We propose multi-grained reward modeling using multi-choice QA tasks with partial cor- rectness evaluation, improving gradient estimation and policy stability. •We propose question-answer inverse, a data augmentation converting single-choice QA into multi-choice formats via question negation and answer inversion, mitigating data scarcity. 2 2 Related Works 2.1 Reinforcement Learning in MLLMs The reasoning abilities of LLMs have been a central focus of recent research, with efforts aimed at enhancing their capacity for complex, multi-step problem-solving tasks. RL has been a key driver of this progress, with works such as OpenAI-o1 [ 15] and DeepSeek-R1 [ 1] achieving notable results. The latter adopts GRPO [ 9], a RL method that extends Proximal Policy Optimization (PPO) [ 16] by eliminating the critic model and estimating relative quality through group-wise response comparisons, enabling efficient policy optimization. For MLLMs, numerous efforts [ 7;6;17;11;5;8;18] have applied GRPO techniques with verifiable reward mechanisms to improve visual reasoning performance. However, existing GRPO-based methods operate at the sequence level and lack mechanisms to distinguish informative tokens. And as shown in Figure 1, generic phrases like “Let’s think...” are unnecessary for optimizing the policy model. Ignoring this variation can lead to misaligned optimization signals, resulting in verbose or redundant reasoning. To address this issue, we propose a token-level extension of GRPO that models token importance, enabling the policy to focus on critical spatiotemporal cues and improving reasoning quality. 2.2 MLLMs for Video Understanding Video understanding is a crucial capability for MLLMs, enabling them to interpret and reason over dynamic visual content [ 19;20;21;22;23]. Recent advancements have resulted in the develop- ment of MLLMs specifically designed to improve video understanding tasks. Recent models such as VideoLLaMA2 [ 19] improve video-language alignment through spatiotemporal modeling and multimodal integration. Building on the success of DeepSeek-R1 [ 1], the GRPO algorithm [ 7;8] has been widely adopted in video reasoning tasks. However, existing RL-based frameworks rely on sparse, binary reward signals [ 7], offering no distinction between partially correct and entirely incorrect responses in video QA. Recently, VideoChat-R1 [ 8] introduced an IoU-based soft reward for video grounding, providing continuous feedback that has proven effective in improving learning stability and precision. Inspired by this, we explore the use of soft reward mechanisms in Video-QA. Specifically, we reformulate the RL objective as a multi-choice classification problem, enabling multi-level reward assignment and fine-grained policy optimization. 3 Methodology 3.1 Preliminary As shown in Figure 2, we focus on the task of multi-choice Video-QA, where the model is required to select the correct answer from a set of candidate options based on both the video content and the question. To enhance the performance of MLLMs on this task, one of the most advanced RL-based method, the GRPO [ 9] has been introduced. For an input query q, GRPO samples Gcandidate responses o={o1,..., o G}from the policy distribution. The rule-based reward model evaluates these responses to obtain reward scores {R1,..., R G}. The relative quality of each response is then computed through standardization: ˆAi=Ri−mean( {Ri}G i=1) std({Ri}G i=1), (1) where ˆAidenotes the normalized advantage of the i-th response within the group. The optimization objective combines response quality improvement with policy regularization: JGRPO(θ) =E(q,a)∼D,{oi}G i=1∼πθold(·|q) \" 1 GGX i=11 |oi||oi|X t=1 min\u0010 ri,t(θ)ˆAi,t,clip\u0010 ri,t(θ),1−ε,1 +ε\u0011 ˆAi,t\u0011 −βD KL(πθ||πref)!#,(2) where ri,t(θ) =πθ(oi,t|q, oi,<t) πθold(oi,t|q, oi,<t). (3) 3 NormalizationPolicy model Question: “Which of the following will not happen without the yellow object? ” VideoVision EncoderLLM Decoder Tokens......Rule-based reward Group computationFormat reward Mutli-answer soft reward Template format check Content type check1. Reward partial correctness 2. Penalize wrong outputs Output: 0 Adjusted output: 0.5 Token-level importance weighted Token-level weight distribution Weighted Tokens Tokens (Uniform weights)Important Unimportant Let ’s analyze on Based will cyan... sphere collide. analysis notthis the and......Update Cal. LossPolicy grad. optimizationAnswer: Let me think ……. <answer> B</answer> 1A 2A GA i 1 2 G i 1 2 GR mean({R,R,,R })Astd({R,R,,R }) Output... grey object the the Let ’s analyze on Based will cyan... sphere collide. analysis notthis the and... grey object the the Token PositionWeight1R 2R GR Figure 2: Overview of the TW-GRPO framework. The diagram shows the key steps in a forward pass, starting from the video input, generating possible completions, and calculating the reward with adjustments for the final objective and model updates. Specifically, a multi-level soft reward is incorporated into the reward calculation, providing partial correctness feedback. These signals are then integrated into the final objective, where token-level importance weighting is applied, allowing the model to prioritize more informative tokens and improve overall performance. Despite its efficiency, existing GRPO algorithms do not differentiate between token positions during optimization, as shown in Equation 2. This leads the model to expend unnecessary effort on uninformative tokens, such as generic phrases like “Let’s think...”, which are less useful for optimizing the policy compared to tokens that describe critical spatiotemporal cues. In addition, since QA tasks are typically formulated as single-choice classification problems, existing approaches often adopt binary reward signals. However, in video grounding tasks [ 8], such binary signals have been shown to be less effective than continuous reward. Nevertheless, how to incorporate multi-level rewards into single-choice QA tasks remains unexplored. To address these two issues, we propose the TW-GRPO framework, as illustrated in Figure 2. In order to address the overlooked variation in token importance, we introduce a token-level importance wighting mechanism in Section 3.2, which guides the model to focus on informative tokens. Further- more, to overcome the limitations of binary rewards in single-choice QA, we reformulate the task as amulti-answer setting, where a question may have one or more correct options. Then, a crafted multi-level soft reward is designed to provide partial credit, enabling finer-grained policy learning, which is introduced in Section 3.3. 3.2 Token-Level Importance Wighting In this section, we address how to model token-level importance, enabling the policy to distinguish informative tokens better and optimize more effectively. As usual, the fine-grained assessment of reasoning quality typically requires an auxiliary critic model [ 11], which introduces additional parameters and undermines one of GRPO’s main advantages. Inspired by recent work [ 24;25], which demonstrates that key reasoning tokens can be identified based on token-level distributional differences, we propose a lightweight approach grounded in the concept of information entropy. The key insight is that token positions where candidate outputs exhibit higher divergence from the 4 expected distribution are more likely to carry more information. This allows us to estimate token importance without introducing extra model components. Specifically, we purpose the token importance weight wtto quantify the information content of each token position. In detail, the Kullback-Leibler (KL) divergence DKLmeasures the discrepancy between the probability distribution of the token at position tin the candidate output oiand the expected distribution at the same position. For token position t, we calculate the divergence Dtas: Dt=GX i=1DKL\u0000 p(oi,t) E[ot]\u0001, (4) where Gdenotes the number of candidate outputs, and E[ot]is the expected probability distribution at token position t, which computed by averaging the probability distributions of each candidate output, with the missing tokens filled in using the uniform distribution U(V)to account for variable sequence lengths. This filling process ensures that all token sequences, regardless of their length, contribute fairly to the divergence calculation, preventing bias towards longer sequences and capturing the meaningful information in shorter sequences. To ensure numerical stability and comparable importance scores across different positions, we normalize the divergence measurements using min-max normalization: wt= (1 + α)·Dt−Dmin Dmax−Dmin. (5) In this formulation, αis a hyperparameter that controls the scaling of token importance. To ensure comparability, the raw divergence scores are normalized, mapping them to a standard range while preserving their relative differences. Moreover, the addition of the constant offset (1 +α)guarantees that tokens with low divergence retain non-zero weights, thus preventing any position from being entirely ignored during training. Consequently, the resulting weights wtenable position-sensitive optimization by modulating the learning signal according to token-level informativeness. JTW-GRPO (θ) =E(q,a)∼D,{αi}G i=1∼πθold(·|q) \" 1PG i=1|αi|GX i=1|αi|X t=1min\u0010 wt·ri,t(θ)ˆAi,t,clip\u0000 ri,t(θ),1−ε,1 +ε\u0001ˆAi,t\u0011#.(6) Here, we normalize the number of outputs |oi|to balance their contributions to the overall loss. Notably, our method does not require additional evaluation models to assess each step of the model’s output. It only requires simple distance calculations to guide the model in applying varying levels of attention to different reasoning positions. After addressing the first issue, we focus on the second challenge: the need for multi-level reward signals in QA tasks. 3.3 Multi-Answer Soft Reward In this section, we address the inefficiency of reward signals in QA tasks caused by the single-choice formulation. Our solution consists of two main steps. First, inspired by multi-choice formats in standardized testing, we reformulate the single-choice QA task as a textitmulti-answer setting, where each question contains at least one correct answer, and possibly more. Second, leveraging this multi-choice question, we introduce a multi-level soft reward that assigns partial credit based on the overlap between the predicted and ground-truth answers. This enables more informative reward feedback for reinforcement learning. We describe the details of each component below. Redefine Multi-choice QA Task. In Video-QA tasks, standard benchmarks such as NExT- GQA [ 13] are typically formulated as single-choice questions, where each question has exactly one correct answer. As a result, evaluation is based on 0/1 accuracy, which inherently limits the ability to generate multi-level soft rewards. Interestingly, multi-answer questions, which are often used in the most challenging sections of standardized exams, align well with the needs. These questions offer both suitable difficulty and the presence of at least one correct option, making them well-suited to our training objective. Therefore, we reformulate the single-choice QA task as a multi-answer one. Unfortunately, this raises a new challenge: obtaining suitable multi-answer data for training. To the best of our knowledge, only a limited number of existing datasets, such as the counterfactual reasoning task in CLEVRER, address multi-choice problems. 5 To mitigate this scarcity, we introduce question-answer inversion, a novel data augmentation technique that transforms single-choice questions into multi-answer questions in general datasets. For example, in the NExT-GQA [ 13] dataset, a question such as “Why did the boy pick up one present from the group and move to the sofa?” is modified by changing \"did\" to \"didn’t,\" thereby transforming it from a five-choice, single-answer question into a five-choice, multiple-answer question. To prevent the model from incorrectly associating negation with the selection of multiple correct answers, we introduce a mechanism that randomly removes correct options, ensuring that the model remains challenged and is required to reason more carefully. Finally, by performing random question-answer inversion on the original dataset, we construct the final multi-choice NExT-GQA dataset, which may contain more than one correct answer, thereby increasing task complexity and fostering more sophisticated reasoning and deeper understanding of the underlying context. However, while introducing multi-choice questions improves the training environment for RL, it also presents significant challenges. The increased complexity causes traditional accuracy reward mechanisms, based on binary accuracy (0 or 1), to exhibit significant reward variance between single-choice and multi-choice questions. As shown in Figure 3, the GRPO model trained on the single-choice dataset (GRPO(single-choice)) exhibits a notably lower reward standard deviation compared to the GRPO model trained on the multi-choice dataset (GRPO(fixed reward)). This higher variance complicates model convergence, making stable improvements in reasoning ability more difficult. Therefore, a key challenge is optimizing the fixed reward mechanism to mitigate this variance and ensure consistent progress in reasoning with the more complex multi-choice dataset. Multi-Level Soft Reward. To address this, we draw inspiration from the IoU reward used in grounding tasks such as VideoChat-R1 [ 8], where a continuous-valued reward reflects the degree of temporal overlap between predicted and ground-truth intervals. This soft feedback enables the model to capture partial correctness and optimize both precision and recall, rather than relying solely on exact matches. Analogously, we proposed the multi-level soft reward, which assigns graded credit to partially correct predictions and penalises completely incorrect ones. The reward is defined as: Rsoft=( |P| |G|,ifP⊆G, 0, ifP̸⊆G.(7) Here, Pdenotes the predicted set and Gthe ground truth set. If P⊆G, the reward is |P|/|G|; otherwise, it is 0. As shown in Figure 1, if the ground truth is {B, D}and the model predicts {B}, the reward is 1/2, reflecting partial correctness. Predictions including elements outside Greceive a reward of 0, penalizing false positives. This design ensures that the model is proportionally rewarded for partially correct answers, improving fine-grained gradient estimation and policy stability. 4 Experiment 4.1 Experiment Setup We train our model based on Qwen2.5-VL-7B using two NVIDIA H800 GPUs with a lightweight setup of 500 RL steps on 1,000 CLEVRER counterfactual train datasets. Each frame is processed at a resolution of 128×28×28during training. For reasoning, the frame resolution is increased to 256×28×28while maintaining a maximum of 16 frames to improve performance. Evaluation is conducted on six video benchmarks covering general understanding and reasoning: MVBench [ 27], TempCompass [ 28], VideoMME [ 29], MMVU [ 14], NExT-GQA [ 13], and CLEVRER [ 12]. Detailed settings for all experiments are provided in Appendix C. 4.2 Main Results Superior Performance of TW-GRPO. As shown in Table 1, TW-GRPO consistently outperforms existing models in both video reasoning and general understanding tasks, achieving better results with fewer training samples. Specifically, in reasoning tasks such as CLEVRER, NExT-GQA, and MMVU, TW-GRPO demonstrates significant improvement over the original GRPO models that do not utilize soft rewards and token-level weighting. On CLEVRER, it reaches 50.4% accuracy, surpassing the next best (Video-R1) by over 18%. It also beats Video-R1 and VideoChat-R1 on NExT-GQA and MMVU by 1.8% and 1.6%, respectively. For general video understanding tasks, 6 Table 1: Comparison of model performance on both video reasoning and general video benchmarks. Models TrainingVideo Reasoning Benchmark Video General Benchmark CLEVRER cfNExT-GQA MMVU mcMVBench TempCompass VideoMME (wo sub) Baseline LLaMA-VID  - - - - 41.9 45.6 - VideoLLaMA2  - - - 44.8 54.6 - 47.9 LongV A-7B  - - - - - 56.9 52.6 Video-UTR-7B  - - - - 58.8 59.7 52.6 Kangeroo-8B  - - - - 61.1 62.5 56.0 Qwen2.5-VL-7B (Zero-Shot) - 30.5 75.9 65.4 63.3 72.5 56.5 Qwen2.5-VL-7B (CoT)  - 27.7 73.4 63.0 57.4 72.2 53.1 Supervised Finetuning Qwen2.5-VL-7B (SFT)  165K SFT 29.0 69.0 61.3 59.4 69.2 52.8 Reinforcement Learning Finetuned Video-R1  165K SFT + 4K RL 31.6 74.3 64.2 62.7 72.6 57.4 VideoChat-R1  18K RL 29.2 76.0 64.2 63.1 72.9 52.4 GRPO 1K RL 41.1 75.2 65.1 62.8 71.9 55.9 TW-GRPO (Ours) 1K RL 50.4 76.1 65.8 63.3 73.3 55.1 0 100 200 300 400 500 Training Step0.00.20.40.60.8Reward STD (a)GRPO(fixed reward) GRPO(soft reward) GRPO(single-choice) TW-GRPO(fixed reward) TW-GRPO 0 100 200 300 400 500 Training Step100150200250300350Completion Length (b)GRPO(fixed reward) GRPO(soft reward) GRPO(single-choice) TW-GRPO(fixed reward) TW-GRPO Figure 3: Training dynamics of different GRPO variants. (a) TW-GRPO achieves faster con- vergence in reward standard deviation, indicating more stable and efficient learning. (b) It also produces consistently shorter output lengths, reflecting more concise and effective reasoning than other methods. TW-GRPO demonstrates competitive performance, even with fewer training resources. In MVBench, TW-GRPO matches the zero-shot performance of Qwen2.5-VL-7B (63.3%), while outperforming both Video-R1 and VideoChat-R1. In TempCompass, TW-GRPO leads with a 73.3% accuracy, surpassing the best-performing baseline by 0.4%. Though its performance in VideoMME is slightly lower, TW-GRPO still outperforms VideoChat-R1 by 2.7%. Even under identical training conditions, TW-GRPO significantly improves over GRPO across five benchmarks. These results highlight the effectiveness and robustness of our method. By enhancing reinforcement learning with token-level importance weighting and multi-level reward strategies, it provides more efficient and stable policy learning, thereby boosting model performance across diverse tasks. Training Dynamics and Convergence Behavior. Figure 3 illustrates the training dynamics of different GRPO variants. Figure 3(a) shows that TW-GRPO achieves faster convergence in reward standard deviation, indicating more stable learning. This stability is attributed to the introduction of multi-level soft reward and token-weighting strategies, which help the model handle ambiguous questions more effectively. In detail, traditional GRPO suffers from slow convergence on multi-choice tasks due to fixed accuracy rewards. In contrast, our soft reward strategy reduces the reward standard deviation, leading to more stable optimization. Furthermore, the token-level importance weighting mechanism, which helps the model focus on more informative tokens, thereby improving optimization efficiency and speeding up convergence. As shown in Figure 3(b), TW-GRPO produces shorter output sequences, suggesting it learns to reason more concisely. This reflects a more substantial alignment between the reward objective and the final model behaviour, confirming the effectiveness of our proposed training design. Due to this crafted design, TW-GRPO achieves smoother convergence with fewer tokens in the generated outputs, reflecting more concise reasoning. 7 Video Question: Based on the measurements from the demonstration video, what is the approximate density of the stone in grams per cubic centimeter (g/ cm³)? A. 2.0 g/cm³. B. 2.2 g/cm³. C. 2.5 g/cm³. D. 2.7 g/cm³. E. 3.0 g/cm³. T-GRPO:<think>Let me think about this for a moment. From the video, we know the following: The mass of the stone is 230 g. The volume of the displaced water is 100 cm ³. Using the formula: ρ=230g/ 100cm³=2.3 g/cm³. Then compares: A. 2.0 — too low. B. 2.2 — too low. C. 2.5 — close. D. 2.7 — higher. E. 3.0 — much higher. Oh, I see. The closest value to our calculated density of 2.3 g/cm³ is 2.5. Therefore, the correct answer should be C. Let's break it down one more time to ensure. The calculated density is 2.3 g/cm ³, which is very close to 2.5 g/cm ³. So, the most accurate choice is C. Final verification: The density of the stone is approximately 2.5 g/cm ³, which matches option C. Final answer: C. </think> <answer>D </answer>TW-GRPO:<think> The video shows two key pieces of information: the weight of the stone when not in water and its apparent weight when submerged. The initial weight of the stone is given as 230 g, which is the mass of the stone. When the stone is submerged, its apparent weight decreases to 138 g, indicating that some force is acting upwards — buoyancy. The difference between these weights gives us the buoyant force: 230 g - 138 g = 92 g. Since water has a density of 1 g/cm ³...means the stone displaced approximately 92cm ³ of water. Then, using the formula: Density=Mass/Volume=230g/9 2cm³≈2.5gm/cm³. </think> <answer>C</answer> ① ② ③① ② Figure 4: Comparison of reasoning paths from T-GRPO and TW-GRPO on MMVU samples. Qualitative Analysis of Reasoning Path. We compare T-GRPO and TW-GRPO on a physics-based density estimation task from the MMVU dataset. As shown in Figure 4, a stone is first weighed in air (230 g) and then submerged in water, where its apparent weight drops to 138 g. Solving the task requires applying Archimedes’ principle to derive the displaced volume from the 92 g buoyant force and compute the density as mass over volume. The T-GRPO model attempts this but incorrectly assumes a volume of 100 cm31, leading to a calculated density of 2.3 g/cm3. It then mistakenly claims 2.5 g/cm3is the closest answer, despite 2.2 g/cm3being numerically closer 2. Although the model attempts to refine its performance through reflection, it remains anchored to an incorrect conclusion, resulting in substantial token inefficiency. Moreover, it ultimately selects a value of 2.7g/cm ³, contradicting its prior (and redundant) estimate. In contrast, TW-GRPO trained model accurately extracts the key values from the video, applies physical principles to infer volume from buoyant force 1, and correctly matches the result to the provided answer choices 23. This example illustrates TW-GRPO’s improved capacity for grounded, causal, and quantitative reasoning based on dynamic visual cues. Additionally, we also provide more visual samples in Appendix E. 4.3 Ablation Study To better understand the contributions of each component in our method, we conducted ablation studies to assess the improvements brought by indeterminate selection and TW-GRPO. In addition to using accuracy as an evaluation metric, we compute soft accuracy based on Equation 7, enabling a more fine-grained assessment of the model’s performance on multi-choice datasets. We evaluate the necessity of adding uncertain options by assessing the performance of the training model on datasets with different problem types. Specifically, the multi-choice training data used for the STAR and NExT-GQA datasets is generated through our question-Answer inversion method. As shown in Table 2, for datasets such as CLEVRER, NExT-GQA, and STAR [ 34], multi-choice training outperforms training with only single-choice questions. The results demonstrate that incorporating multi-choice questions increases the complexity of the training set, thereby enhancing the model’s reasoning ability. For instance, the accuracy of the model trained with multi-choice questions on the CLEVRER dataset is 41.1%, which is 8.5% higher than that of the GRPO model trained with only single-choice questions. The counterfactual reasoning subset of the CLEVRER dataset [ 12] includes samples involving object dynamics and hypothetical outcome predictions. This provides a more complex and realistic training environment for models.. Next, we investigate the effectiveness of TW-GRPO under different training setups, reward strategies, and token weighting configurations, as summarized in Table 3. We first observe that training on multi-choice question types yields better performance than single-choice only settings, especially on CLEVRER, where multi-choice training improves accuracy by over 10% compared to single-choice training. Regarding reward design, soft rewards consistently outperform fixed rewards across settings, 8 Table 2: Ablation study on data construction with different sources and sampling strategies. We evaluate the effect of using single-choice and multi-choice questions from CLEVRER, NExT-GQA, and STAR datasets. Setting Single. Multiple Choice All MMVU Acc. (%) Acc. (%) Soft. (%) Acc. (%) Soft. (%) Acc. (%) CLEVRER cfSource GRPO (single) 51.5 18.2 50.9 32.6 51.2 62.7 GRPO (multi-choice) 50.4 32.3 55.4 41.1 52.7 65.1 NExT-GQA Source GRPO (single-choice) 48.0 12.3 45.5 30.7 45.7 64.3 GRPO (multi-choice) 63.5 9.6 44.9 36.7 54.7 64.6 STAR Source GRPO (single-choice) 65.1 2.6 41.4 29.3 51.5 64.8 GRPO (multi-choice) 65.9 1.5 40.8 29.1 51.5 66.2 Table 3: Ablation study on training type, reward design, and the token-level weighting mechanism (TW-GRPO). We evaluate performance on CLEVRER and MMVU under different settings. Setting Single. Multiple Choice All MMVU Acc. (%) Acc. (%) Soft. (%) Acc. (%) Soft. (%) Acc. (%) Training Problem Type (TW-GRPO) Single-choice 60.4 22.8 55.9 38.9 57.8 63.7 Multi-choice 60.9 42.5 64.4 50.4 62.9 65.8 Reward Design (TW-GRPO, multi-choice) Fixed reward 64.9 26.0 54.2 42.6 58.7 65.0 Soft reward 60.9 42.5 64.4 50.4 62.9 65.8 Effect of Token Weighting (multi-choice, Fixed Reward) GRPO 50.4 32.3 55.4 41.1 52.7 65.1 TW-GRPO 64.9 26.0 54.2 42.6 58.7 65.0 Effect of Token Weighting (multi-choice, Soft Reward) GRPO 58.3 28.1 57.6 41.2 57.9 64.6 TW-GRPO ( α= 0) 64.7 36.6 60.5 48.6 62.3 62.1 TW-GRPO 60.9 42.5 64.4 50.4 62.9 65.8 improving soft accuracy by up to 8.7% in multi-choice tasks. This indicates that multi-answer soft reward provides more informative and stable supervision signals, especially for ambiguous or partially correct answers. Finally, we assess the effect of token weighting. In both fixed and soft reward settings, TW-GRPO shows notable gains over GRPO. For example, with soft reward, TW-GRPO improves multiple-choice soft accuracy from 57.6% to 64.4%. When token weights are removed (TW-GRPO) ( α= 0)), performance drops, confirming that token-level importance modeled is crucial for fine-grained optimization. And the analysis of hyperparameters is provided in Appendix D.1. 5 Conclusions In this work, we present TW-GRPO, a novel reinforcement learning framework that advances video reasoning in MLLMs. While prior approaches have improved model accuracy, two core limitations remain: the inability to distinguish token-level contributions and the inefficiency of binary reward signals. TW-GRPO addresses these challenges by incorporating token-level importance weighting and introducing multi-answer soft rewards that grant partial credit for partially correct responses. Extensive experiments across six benchmarks, along with comprehensive ablation studies, validate the effectiveness of our approach. We hope this insight provides a foundation for future research in fine-grained video reasoning with MLLMs. 9 References D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,” arXiv preprint arXiv:2501.12948, 2025. L. Chen, L. Li, H. Zhao, Y. Song, and Vinci, “R1-v: Reinforcing super generalization ability in vision-language models with less than $3,” https://github.com/Deep-Agent/R1-V, 2025, accessed: 2025-02-02. W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin, “Vision- r1: Incentivizing reasoning capability in multimodal large language models,” arXiv preprint arXiv:2503.06749, 2025. F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, T. Han, B. Shi, W. Wang, J. He et al., “Mm- eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning,” arXiv preprint arXiv:2503.07365, 2025. H. Zhou, X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh, “R1-zero’s\" aha moment\" in visual reasoning on a 2b non-sft model,” arXiv preprint arXiv:2503.05132, 2025. Y. Peng, G. Zhang, M. Zhang, Z. You, J. Liu, Q. Zhu, K. Yang, X. Xu, X. Geng, and X. Yang, “Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl,” arXiv preprint arXiv:2503.07536, 2025. K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue, “Video-r1: Reinforc- ing video reasoning in mllms,” arXiv preprint arXiv:2503.21776, 2025. X. Li, Z. Yan, D. Meng, L. Dong, X. Zeng, Y. He, Y. Wang, Y. Qiao, Y. Wang, and L. Wang, “Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning,” arXiv preprint arXiv:2504.06958, 2025. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., “Deepseekmath: Pushing the limits of mathematical reasoning in open language models,” arXiv preprint arXiv:2402.03300, 2024. D. Jiang, R. Zhang, Z. Guo, Y. Li, Y. Qi, X. Chen, L. Wang, J. Jin, C. Guo, S. Yan et al., “Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency,” arXiv preprint arXiv:2502.09621, 2025. J. Zhang, J. Huang, H. Yao, S. Liu, X. Zhang, S. Lu, and D. Tao, “R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization,” arXiv preprint arXiv:2503.12937, 2025. K. Yi, C. Gan, Y. Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum, “Clevrer: Collision events for video representation and reasoning,” arXiv preprint arXiv:1910.01442, 2019. J. Xiao, A. Yao, Y. Li, and T. S. Chua, “Can i trust your answer? visually grounded video ques- tion answering,” in 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Y. Zhao, L. Xie, H. Zhang, G. Gan, Y. Long, Z. Hu, T. Hu, W. Chen, C. Li, J. Song et al., “Mmvu: Measuring expert-level multi-discipline video understanding,” arXiv preprint arXiv:2501.12380, 2025. A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney et al., “Openai o1 system card,” arXiv preprint arXiv:2412.16720, 2024. R. Zheng, S. Dou, S. Gao, Y. Hua, W. Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, Y. Zhou et al., “Secrets of rlhf in large language models part i: Ppo,” arXiv preprint arXiv:2307.04964, 2023. Y. Zhan, Y. Zhu, S. Zheng, H. Zhao, F. Yang, M. Tang, and J. Wang, “Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning,” arXiv preprint arXiv:2503.18013, 2025. 10  M. Chen, G. Chen, W. Wang, and Y. Yang, “Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization,” arXiv preprint arXiv:2505.12346, 2025. Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao et al., “Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms,” arXiv preprint arXiv:2406.07476, 2024. F. Shu, L. Zhang, H. Jiang, and C. Xie, “Audio-visual llm for video understanding,” arXiv preprint arXiv:2312.06720, 2023. C. Yan, H. Wang, S. Yan, X. Jiang, Y. Hu, G. Kang, W. Xie, and E. Gavves, “Visa: Reasoning video object segmentation via large language models,” in European Conference on Computer Vision. Springer, 2024, pp. 98–115. X. Zhang, D. Peng, Y. Zhang, Z. Guo, C. Wu, C. Chen, W. Ke, H. Meng, and M. Sun, “Towards self-improving systematic cognition for next-generation foundation mllms,” arXiv preprint arXiv:2503.12303, 2025. Y. Zhang, Y. Liu, Z. Guo, Y. Zhang, X. Yang, C. Chen, J. Song, B. Zheng, Y. Yao, Z. Liu et al., “Llava-uhd v2: an mllm integrating high-resolution feature pyramid via hierarchical window transformer,” arXiv preprint arXiv:2412.13871, 2024. E. Bigelow, A. Holtzman, H. Tanaka, and T. Ullman, “Forking paths in neural text generation,” arXiv preprint arXiv:2412.07961, 2024. Z. Lin, T. Liang, J. Xu, X. Wang, R. Luo, C. Shi, S. Li, Y. Yang, and Z. Tu, “Critical tokens matter: Token-level contrastive estimation enhence llm’s reasoning capability,” arXiv preprint arXiv:2411.19943, 2024. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu et al., “Dapo: An open-source llm reinforcement learning system at scale,” arXiv preprint arXiv:2503.14476, 2025. K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo et al., “Mvbench: A comprehensive multi-modal video understanding benchmark,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 22 195– 22 206. Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou, “Tempcompass: Do video llms really understand videos?” arXiv preprint arXiv:2403.00476, 2024. C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang et al., “Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis,” arXiv preprint arXiv:2405.21075, 2024. Y. Li, C. Wang, and J. Jia, “Llama-vid: An image is worth 2 tokens in large language models,” inEuropean Conference on Computer Vision. Springer, 2024, pp. 323–340. P. Zhang, K. Zhang, B. Li, G. Zeng, J. Yang, Y. Zhang, Z. Wang, H. Tan, C. Li, and Z. Liu, “Long context transfer from language to vision,” arXiv preprint arXiv:2406.16852, 2024. E. Yu, K. Lin, L. Zhao, Y. Wei, Z. Zhu, H. Wei, J. Sun, Z. Ge, X. Zhang, J. Wang et al., “Unhackable temporal rewarding for scalable video mllms,” arXiv preprint arXiv:2502.12081, 2025. J. Liu, Y. Wang, H. Ma, X. Wu, X. Ma, X. Wei, J. Jiao, E. Wu, and J. Hu, “Kangaroo: A powerful video-language model supporting long-context video input,” arXiv preprint arXiv:2408.15542, 2024. B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan, “Star: A benchmark for situated reasoning in real-world videos,” arXiv preprint arXiv:2405.09711, 2024. S. Agarwal, Z. Zhang, L. Yuan, J. Han, and H. Peng, “The unreasonable effectiveness of entropy minimization in llm reasoning,” arXiv preprint arXiv:2505.15134, 2025. 11  Q. Zhang, H. Wu, C. Zhang, P. Zhao, and Y. Bian, “Right question is already half the answer: Fully unsupervised llm reasoning incentivization,” arXiv preprint arXiv:2504.05812, 2025. X. Zhao, Z. Kang, A. Feng, S. Levine, and D. Song, “Learning to reason without external rewards,” arXiv preprint arXiv:2505.19590, 2025. Z. Kang, X. Zhao, and D. Song, “Scalable best-of-n selection for large language models via self-certainty,” arXiv preprint arXiv:2502.18581, 2025. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, S. Huang, K. Rasul, and Q. Gallouédec, “Trl: Transformer reinforcement learning,” https://github.com/huggingface/ trl, 2020. 12 Part I Appendix Table of Contents A Discussion on Entropy-based Measurement 14 B Detailed of TW-GRPO 14 B.1 Detailed of Optimization Objective........................ 14 B.2 Token-Level Importance Modeling........................ 15 C Detailed Experimental Setup 18 D Additional Experiment Results 18 D.1 Effect of Sampling Diversity on Approximation Validity............ 18 D.2 Sensitivity Analysis on Token-Level Importance Weighting Mechanism.... 19 E Additional Visualization Results 21 E.1 Analysis of Reasoning Path............................ 21 F Discussion 23 F.1 Limitations.................................... 23 13 A Discussion on Entropy-based Measurement We note that some concurrent studies [ 35;18;36;37] have consistently highlighted the value of entropy-based signals as intrinsic measures that significantly enhance policy optimization in LLM reasoning. Similarly, our method also integrates entropy into the optimization process, but introduces a novel mechanism centered on token-level informativeness through distributional divergence. Entropy Minimization in Reasoning Optimization. Works such as EMPO [ 36] and SEED- GRPO [ 18] adopt a semantic perspective, where entropy is computed over clusters of sampled completions. EMPO minimizes entropy among latent semantic groups formed from multiple outputs, encouraging global consistency across generations. SEED-GRPO builds upon this by adjusting GRPO updates based on the entropy level of input prompts—assigning larger updates to low-entropy (confident) inputs and smaller updates to uncertain ones. These approaches frame entropy as a measure of reasoning confidence at the sequence level. Token-Level Entropy and Local Uncertainty. While EMPO and SEED-GRPO focus on semantic- level aggregation, recent work has revisited the benefits of entropy minimization at the token level. Agarwal et al. [ 35] introduced EM-RL and EM-FT as two forms of token-level entropy-based optimization. In EM-RL, token-level entropy is used as the sole reward signal in reinforcement learning, promoting deterministic generation at each step. Meanwhile, EM-FT applies direct token- level entropy minimization as a fine-tuning objective, reinforcing confidence locally across generation trajectories. Importantly, these approaches highlight that entropy minimization can be interpreted as a mechanism to exploit pretrained confidence priors embedded in LLMs. Self-Certainty and Internal Feedback. Complementary to entropy-based reward signals, Zhao et al. proposed INTUITOR, which replaces verifiable external rewards in GRPO with an intrinsic measure called self-certainty [ 38], formulated as the KL divergence between the model’s output distribution and a uniform distribution at each token step. By encouraging high self-certainty scores, the model aligns itself to more confident and consistent generation patterns, improving both in- domain reasoning and out-of-domain generalization. This work reinforces the idea that token-level distributional sharpness, much like entropy or self-certainty, can serve as a rich intrinsic feedback signal in reasoning-centric LLM optimization. Our Approach: Informativeness via Distributional Divergence. Similar to the above works, our method also incorporates entropy-based signals into the GRPO optimization process. However, we adopt a distinct perspective by modeling token-level informativeness through distributional divergence. Specifically, we compute the Kullback-Leibler (KL) divergence between the predicted distribution at each token position and the expected distribution aggregated across multiple candidate outputs. This divergence reflects the variability or uncertainty associated with each position, and serves as a proxy for its relative importance in the reasoning process. This approach aligns with the intuition behind self-certainty [ 37;38], where confident and consistent predictions are indicative of stronger reasoning. In our case, token positions with low divergence typically exhibit stable distributions across samples, suggesting that the model has already formed confident decisions at these positions. Further optimization of such low-information tokens is often redundant and provides limited gains. In contrast, tokens with high divergence correspond to areas of uncertainty or critical decision points that are more likely to influence the overall output. By assigning higher weights to these informative positions, our method selectively amplifies learning signals where they are most needed, resulting in more effective and interpretable optimization within the GRPO framework. B Detailed of TW-GRPO B.1 Detailed of Optimization Objective We begin by reviewing the Group Relative Policy Optimization (GRPO) framework [ 9]. Given an input query q, GRPO samples Gcandidate responses o={o1,..., o G}from the policy distribution πθold. A rule-based reward model then assigns scalar reward scores {R1,..., R G}to these responses. 14 To quantify the relative quality of each response, the rewards are standardized within the group: ˆAi=Ri−mean( {Ri}G i=1) std({Ri}G i=1), (8) where ˆAidenotes the normalized advantage of the i-th response within the group. The GRPO objective encourages response quality improvements while regularizing the policy through a KL- divergence term: JGRPO(θ) =E(q,a)∼D,{oi}G i=1∼πθold(·|q) \" 1 GGX i=11 |oi||oi|X t=1 min\u0010 ri,t(θ)ˆAi,t,clip\u0010 ri,t(θ),1−ε,1 +ε\u0011 ˆAi,t\u0011 −βD KL(πθ||πref)!#,(9) where ri,t(θ) =πθ(oi,t|q, oi,<t) πθold(oi,t|q, oi,<t). (10) Although GRPO successfully eliminates the need for a critic model, as required in PPO [ 39], recent findings [ 26] reveal that its sample-level optimization and KL-divergence regularization may restrict the model’s reasoning capacity, particularly in complex generation tasks. Motivated by these insights, we adopt a token-level policy gradient objective inspired by DAPO [ 26], and remove the KL penalty to enable more flexible optimization dynamics. The resulting optimized objective is defined as: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q) \" 1PG i=1|oi|GX i=1|oi|X t=1min\u0010 ri,t(θ)ˆAi,t,clip\u0000 ri,t(θ),1−ε,1 +ε\u0001ˆAi,t\u0011#.(11) Building upon this token-level optimization objective, we now introduce our token-level importance modeling approach, which aims to explicitly identify and leverage the most influential tokens in the optimization process. B.2 Token-Level Importance Modeling B.2.1 Theoretical Analysis and Motivation In the context of exploring multi-modal reward functions such as R1, recent implementations often follow the design choices in R1-V [ 2] and TRL [ 40], which simplify the optimization formulation to improve training efficiency. Specifically,by assuming that the policy undergoes relatively small updates, i.e., ri,t(θ)∈(1−ϵ,1 +ϵ), the objective can be simplified as: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1|oi|GX i=1|oi|X t=1\u0010 ri,t(θ)ˆAi,t\u0011#, (12) by padding all sampled responses oito a uniform length omax= max i|oi|using a uniform distribution, and applying the commutative law of summation, we can derive from Eq 12 that: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=1\u0010 ri,t(θ)ˆAi,t\u0011#. (13) Since the adopted reward model operates at the sample level, it assigns rewards based solely on the complete sampled response and is inherently independent of individual token positions. As a result, the advantage computed in Eq 8 is also position-invariant for token index t, i.e., the following holds: ˆAi=ˆAi,t, (14) so, the Eq 12 can be expressed as JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=1\u0010 ri,t(θ)ˆAi\u0011#. (15) 15 when we define ¯rt≜mean ({ri,t}G i), Eq 15 is equal to: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=1\u0010 [(ri,t(θ)−¯rt) + ¯rt]ˆAi\u0011#.(16) This can be separated into two terms: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=1(ri,t(θ)−¯rt)ˆAi+1PG i=1omaxomaxX t=1GX i=1¯rtˆAi#.(17) Due to the group-wise normalization of advantages (cf. Eq 8), we have: GX i=1ˆAi= 0, (18) which causes the second term in Eq 17 to vanish. Hence, the objective simplifies to: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=1(ri,t(θ)−¯rt)ˆAi#. (19) To simplify the expression of ri,t, we aim to approximate its denominator in a way that is both computationally efficient and statistically stable. To justify this approximation, we make the following assumption: Assumption B.1 We assume that (i) the number of sampled trajectories Gis sufficiently large, and (ii) the policy πθproduces relatively stable outputs across similar histories. That is, for a fixed input q, the conditional distributions πθ(· |q, oj,<t)exhibit only mild variation across different j. As empirically verified in Section D.1, this assumption is foundational for our method to achieve strong performance. Under this assumption, the individual conditional likelihoods πθold(oi,t|q, oi,<t) can be reasonably approximated by their average over the sample set. Specifically, we define the empirical distribution under the old policy at position tas: πemp θold,t:=1 GGX j=1πθold(oj,t|q, oj,<t). (20) We then approximate the denominator of ri,tusing this empirical estimate: ri,t(θ)≈πθ(oi,t|q, oi,<t) πemp θold,t. (21) Similarly, the mean importance ratio can be approximated as: ¯rt≈1 GGX j=1πθ(oj,t|q, oj,<t) πemp θold,t=πemp θ,t πemp θold,t, (22) where we define: πemp θ,t:=1 GGX j=1πθ(oj,t|q, oj,<t), (23) as the empirical distribution of the current policy at position t. Substituting into the difference (ri,t−¯rt), we obtain: ri,t−¯rt≈1 πemp θold,t\u0010 πθ(oi,t|q, oi,<t)−πemp θ,t\u0011. (24) 16 Substituting the approximation into Eq 19, we obtain the final reformulated objective: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxGX i=1omaxX t=11 πemp θold,t\u0010 πθ(oi,t|q, oi,<t)−πemp θ,t\u0011 ˆAi#,(25) which is equivalent to a re-ordered form: JGRPO′(θ)=E(q,a)∼D,{oi}G i=1∼πθold(·|q)\" 1PG i=1omaxomaxX t=1GX i=11 πemp θold,t\u0010 πθ(oi,t|q, oi,<t)−πemp θ,t\u0011 ˆAi#.(26) From Eq 26, we observe that the model’s update at each token position tis driven by the difference between the current policy’s prediction and the empirical distribution, scaled by the trajectory’s advantage ˆAi. This leads to the following interpretations: •Single-Trajectory View: For a fixed trajectory i, the advantage ˆAiremains constant across all positions t. Thus, the model is encouraged to adjust the prediction πθ(oi,t)at each token position proportionally to how much it deviates from the average distribution. Tokens with greater deviation from the average receive stronger updates, enabling the model to focus optimization on informative, distinctive tokens within the trajectory. •Multi-Sample View: When updates are aggregated from multiple trajectories, the signs ofˆAivary. This introduces a cancellation effect: tokens with larger deviations from the empirical distribution may contribute opposing gradients due to differing signs of ˆAi. Consequently, even if a position consistently demonstrates informative token choices, the gradient signals can be diminished or nullified due to interference across samples. This reduces the model’s ability to reliably identify and leverage truly informative positions. To mitigate this issue, we propose a Token-Level Importance Weighting (TW) strategy that selectively emphasizes positions exhibiting higher variability across sampled responses. The core insight is that token positions with greater distributional divergence indicate a mismatch between the current and optimal policies, suggesting these positions play a more influential role in model behavior. Our approach amplifies the learning signals associated with these informative tokens to counteract potential gradient cancellation effects, which enables more stable and effective policy optimization by preserving the influence of critical token positions. B.2.2 Token Importance via Information Content. To more precisely quantify the informativeness of each token position, we introduce an information- theoretic measure: the token-level information content. Specifically, we propose to use the average Kullback-Leibler (KL) divergence across trajectories at each position tto measure the degree of variation between individual token distributions and the expected distribution at that position. This provides a principled way to assess how “surprising” or “diverse” the predictions are at each step, which correlates with the position’s importance to learning. Formally, we compute a token-level divergence score Dtas: Dt=GX i=1DKL\u0000 p(oi,t) E[ot]\u0001, (27) where Gdenotes the number of sampled outputs, p(oi,t)is the probability distribution over tokens at position tfor trajectory i, andE[ot]is the expected distribution at position t, estimated by averaging the predicted distributions across all trajectories. To accommodate variable-length sequences, any missing tokens are filled using a uniform distribution U(V)over the vocabulary V. This ensures all positions are comparably represented, and prevents bias toward longer sequences. To normalize across positions and maintain stable optimization, we apply min-max normalization: wt= (1 + α)·Dt−Dmin Dmax−Dmin, (28) 17 Table A1: Sensitivity analysis on sampling number and temperature on CLEVRER and MMVU datasets. Metrics include accuracy and soft accuracy under different settings. SettingSingle. Multiple Choice All MMVU Acc. (%) Acc. (%) Soft Acc. (%) Acc. (%) Soft Acc. (%) Acc. (%) Sampling Number 4 56.2 26.0 55.2 38.9 55.6 65.9 8 60.9 42.5 64.4 50.4 62.9 65.8 12 65.1 37.2 62.8 49.1 63.8 63.2 Temperature 0.5 63.4 33.5 61.0 46.3 62.1 65.3 1.0 60.9 42.5 64.4 50.4 62.9 65.8 1.5 51.7 38.7 61.5 44.3 57.4 63.7 where αis a hyperparameter controlling the baseline importance of low-divergence positions. The additive constant (1 + α)ensures that no token position receives a zero weight, thus preventing complete gradient suppression at any location. B.2.3 Final Objective with Token-Level Importance Weighting We integrate these token-level weights into Eq 11 to form the Token-Level Importance Weighting GRPO (TW-GRPO) objective: JTW-GRPO (θ) =E(q,a)∼D,{oi}G i=1∼πθold(·|q) \" 1PG i=1|oi|GX i=1|oi|X t=1min\u0010 wt·ri,t(θ)ˆAi,t,clip\u0000 ri,t(θ),1−ε,1 +ε\u0001ˆAi,t\u0011#.(29) C Detailed Experimental Setup Evaluation is conducted on six video benchmarks covering general understanding and reason- ing: MVBench [ 27], TempCompass [ 28], VideoMME [ 29], MMVU [ 14], NExT-GQA [ 13], and CLEVRER [ 12]. MVBench, TempCompass, and VideoMME emphasize general video understand- ing, combining visual perception and temporal comprehension without explicit reasoning focus. CLEVRER, NExT-GQA, and MMVU assess complex spatiotemporal and multimodal reasoning over dynamic videos. Together, these benchmarks comprehensively evaluate both general video under- standing and fine-grained multimodal reasoning capabilities, ensuring a well-rounded assessment of the model’s performance. For all evaluations, we followed the decoding configuration used in the official Qwen2.5-VL demo, with top_p = 0.001 and temperature = 0.01. We adopted the same experimental settings as in Video- R1, including the sampling temperature and top_p, and set the batch size to 16. For the NExT- GQA, MMVU, MVBench, TempCompass, and VideoMME datasets, we used the prompt words from Video R1. For the CLEVRER dataset, we adopt the simple prompting strategy designed for our indefinite-choice setting: \"Output the thinking process in <think></think> and the final answer (letters separated by commas, if multiple) in <answer></answer> tags.\" In particular, we evaluate and train on its most challenging subset—counterfactual questions. For the other benchmarks, we follow the evaluation setup of Video-R1, conducting experiments on a partial subset of VideoMME and the multiple-choice question split of MMVU. D Additional Experiment Results D.1 Effect of Sampling Diversity on Approximation Validity As shown in Table A1, we conduct a sensitivity analysis on two key generation hyperparameters: the number of sampled trajectories and the decoding temperature. These factors directly influence the 18 0.0 0.4 0.5 0.6 0.7 0.9 6062646668Score 62.163.764.5 64.365.8 62.7MMVU 0.0 0.4 0.5 0.6 0.7 0.9 596061626364656667Score 62.663.0 61.864.3 63.362.9MVBench 0.0 0.4 0.5 0.6 0.7 0.9 72747678Score 75.1 75.3 73.776.0 76.1 74.3Next-GQA 0.0 0.4 0.5 0.6 0.7 0.9 6870727476Score 72.2 71.572.972.373.3 72.3T empCompass 0.0 0.4 0.5 0.6 0.7 0.9 5051525354555657Score 53.9 53.9 52.354.655.1 53.6VideoMME 0.0 0.4 0.5 0.6 0.7 0.9 626364656667686970Score 65.265.565.066.366.7 65.2 65.265.565.066.366.7 65.2Average VideoR1 Avg: 66.2 VideoChatR1 Avg: 65.7Figure A1: Analysis of the influence of the TW-GRPO weighting coefficient αon model performance. validity of Assumption B.1, which requires (i) a sufficiently large but not excessively large sampling size, and (ii) the old policy to behave consistently across similar contexts. We observe that when the number of sampled trajectories is too small (e.g., 4), model performance degrades notably. This is expected, as a small sample size violates the assumption of empirical distribution reliability: it introduces high variance into the denominator approximation in importance weighting. Conversely, when the number of samples is too large (e.g., 12), performance again drops, likely due to increased inter-sample variability—larger sample sizes lead to more diverse histories oj,<t, thereby increasing the heterogeneity in πθold(· |q, oj,<t), and breaking the assumption of policy stability across contexts. Temperature exhibits a similar trade-off. When the temperature is too low (e.g., 0.5), model outputs become overly deterministic, reducing the diversity needed for advantage computation to distinguish informative tokens via deviation. On the other hand, very high temperatures (e.g., 1.5) induce excessive randomness, which again leads to large variability across trajectories and violates the assumption of stability in the policy’s behavior. In summary, both the sample size and the generation temperature must be carefully tuned to maintain a balance: enough diversity to support optimization via sample-level deviation, but not so much that it invalidates the Assumption B.1. This results empirically support the necessity of Assumption B.1 for TW-GRPO to be effective. D.2 Sensitivity Analysis on Token-Level Importance Weighting Mechanism We conduct sensitivity studies to better understand how the design of the TW-GRPO mechanism affects model performance. In particular, we focus on two key components: (i) the weight coefficient α, and (ii) the positional scope over which token-level importance weighting is applied. D.2.1 Effect of Weight Coefficient α We analyze the impact of the TW-GRPO weighting hyperparameter αon model performance. As shown in Figure A1, the results across different datasets exhibit similar trends: performance improves asαincreases up to a point and then begins to degrade. Most datasets achieve optimal results around α= 0.7, with exceptions such as VideoMME, where α= 0.5yields the best outcome. These results suggest that smaller αvalues under-emphasize the contribution of important tokens during optimization, limiting the potential of TW-GRPO. On the other hand, excessively large α values lead to over-concentration on high-deviation tokens, potentially ignoring broader contextual information—analogous to the “blind men and the elephant” problem. Thus, careful tuning of αis critical; we use α= 0.7as the default setting in all main experiments. 19 0 50 100 150 200 250 Iteration050100150200250300350400Token Position 0.00.20.40.60.81.0 WeightFigure A2: Visualization of token-level importance weights across all positions on the first 300 training samples. The area above the white dashed line corresponds to padding tokens. Table A2: Performance of TW-GRPO under different token-weighting position strategies across video reasoning and general benchmarks. Weighting PositionVideo Reasoning Benchmark Video General Benchmark CLEVRER cfNExT-GQA MMVU mcMVBench TempCompass VideoMME (wo sub) None (No Weighting) 48.6 75.1 62.1 62.6 72.2 53.9 Padding Only 47.0 74.7 64.3 62.7 72.6 53.7 Content Only 46.0 76.1 66.2 63.4 72.7 55.7 All Positions 50.4 76.1 65.8 63.3 73.3 55.1 D.2.2 Effect of Weighting Token Positions In practice, since the model samples multiple completions per prompt and each sampled response may vary in length, we apply padding to align all sequences to a uniform maximum length. This enables consistent token-wise operations, such as weighting, across different completions. In this section, we evaluate the impact of token-level importance weighting positions: (i) None, which applies no weighting to any token positions; (ii) Padding Only, which applies weighting exclusively to the padded token postions introduced to match sequence lengths; (iii) Content Only, which applies weighting only to positions that contain valid content across all completions (i.e., tokens that are present in every sampled sequence before padding); (iv) All Positions, which applies weighting uniformly to all token positions, including both content and padding. Table A2 shows that the All Positions configuration consistently achieves the best performance across all datasets. It provides substantial gains on CLEVRER, MMVU, and VideoMME, indicating that the token-level importance signals captured by TW-GRPO are beneficial in padding regions and throughout the full token sequence. This suggests that incorporating signals from all token positions allows for more robust optimization, especially in settings with variable-length outputs. The Content Only strategy performs competitively on several benchmarks, such as NExT-GQA, MMVU, and VideoMME, and even outperforms other strategies in some cases. This suggests that focusing solely on content postions helps capture critical reasoning information, even though the importance weights assigned to content tokens are significantly lower than those involving padding tokens, as shown in Figure A2. However, its performance on CLEVRER is inferior to that of All Postions, suggesting that excluding padding may overlook useful auxiliary signals. In cases with variable output lengths, padding positions may carry important alignment or reasoning-related information, especially when short responses are padded to match longer ones. As shown in Figure A2, although tokens in padding regions often receive relatively high importance weights, the Padding Only strategy yields only slight improvements over None on a few benchmarks, such as MMVU, and remains consistently less effective than the All Positions configuration. This suggests that restricting the weighting to padding positions fails to capture critical information 20 0 50 100 150 200 250 300 350 400 450 Training Iteration0.5 0.00.51.01.52.0Average Dt1e8 EMA (span=50) Average DtFigure A3: Visualization of Dtacross Content Only on the First 500 Training Samples. embedded in the content regions, which is more effective in comparison, as shown in Table A2, resulting in limiting the overall effectiveness of token-level optimisation. Overall, the consistent improvements obtained with All Postions confirm the importance of a global token-weighting mechanism for maximizing TW-GRPO performance. In terms of token-level distinction, TW-GRPO assigns near-zero importance to initial tokens such as generic openings like “The video shows...”, which appear consistently across samples and contribute little to task-specific reasoning. Their low variance and minimal distributional divergence naturally lead to low importance weights. In contrast, tokens in the middle of the sequence exhibit significantly higher and more variable weights. These positions often correspond to reasoning-critical content, including causal links, temporal dynamics, or attribute comparisons, which vary substantially across completions. This indicates that TW-GRPO can effectively identify semantically meaningful token-level differences and direct optimization toward the most informative regions. By enabling finer granularity and position-aware gradient updates, the model benefits from all available token-level signals, leading to overall performance gains. D.2.3 Analysis of DtDuring Training To further understand how TW-GRPO leverages token-level information throughout training, we visualize the evolution of the average divergence score Dtacross iterations, as shown in Fig. A3. As training progresses, Dtgradually flattens, indicating a decline in variance across token distributions. This trend reflects a core intuition: when a model becomes more confident, it generates increasingly consistent token-level predictions across samples. Such convergence in output distributions, as quantified by the declining Dt, has been shown to align with improved reasoning performance in prior work [ 35;36;37], where deterministic or low-entropy behavior is positively correlated with predictive accuracy and generalization. To encourage this desirable behavior during training, TW- GRPO explicitly incorporates token-level uncertainty into the optimization process. Our method uses KL divergence across samples to identify uncertain or unstable token positions. High divergence indicates locations where the model remains uncertain, and TW-GRPO amplifies learning signals at these positions. As training continues and these tokens become more stable, their weights naturally decrease, resulting in a smooth convergence of Dt. This dynamic behavior facilitates broad exploration during early training and enables more focused convergence in later stages, thereby validating the effectiveness of our token-level importance weighting strategy in enhancing reasoning optimization. E Additional Visualization Results E.1 Analysis of Reasoning Path We compare models trained with T-GRPO and TW-GRPO on representative samples from the CLEVRER and MMVU datasets. In Figure A4, the model trained with T-GRPO focuses on local object motion but misses broader contextual factors such as the blocking role of the metal cylinder 1and the dynamically introduced sphere 2. It does not simulate hypothetical changes like object removal 3, which limits the scope of its prediction. The TW-GRPO-trained model, in contrast, integrates the late-appearing sphere into its reasoning 1and simulates the removal of the green cylinder to infer its effect on future trajectories 2, demonstrating stronger temporal and causal 21 Video T-GRPO:<think>Let's break down the scenario step by step: 1. Initial Setup: We have a yellow cube... 2. Metal Cylinder's Role: The metal cylinder is not directly involved in the interactions described. It is just there and does not affect the other objects' movements or collisions. 3. Yellow Object's Movement: The yellow cube moves... 4. Gray Cylinder's Movement: The gray cylinder is moving... 5. Sphere's Presence: The sphere is not mentioned as being part of the initial setup, so its presence is irrelevant to the current interactions. 6. Collision Analysis:... but it is not clear if it will collide with the sphere... - The yellow object is moving towards the gray cylinder, which is moving towards the cyan cube, so it is possible that the yellow object will collide with the cyan cube after the gray cylinder has already collided with it. Given the current movements and the lack of direct interaction with the sphere, the most likely scenario is that the yellow object will collide with the cyan cube after the gray cylinder has already collided with it. \\n</think>\\n <answer>C, D</answer>TW-GRPO:<think>The video shows a sequence of objects moving on a flat surface. Initially, there is a yellow cube, a gray cylinder, and a green cylinder. As time progresses, the yellow cube moves closer to the sphere that appears later in the scene. The gray cylinder and cyan cube also move, but they do not collide directly with each other or with the yellow cube until after the yellow cube has moved past them. The scenario suggests that without the green cylinder, which is stationary, the yellow cube would continue its trajectory and likely collide with the sphere due to its position relative to the sphere's entry point. </think>\\n <answer>A, D </answer> ① ② Question: Without the metal cylinder, what will happen? A. The yellow object collides with the sphere. B. The yellow object and the green rubber cylinder collide. C. The yellow object collides with the cyan cube. D. The gray cylinder and the cyan cube collide. ③① ②Figure A4: Comparison of reasoning paths from T-GRPO and TW-GRPO on CLEVRER samples. TW-GRPO accurately reasons about dynamically introduced objects and counterfactual outcomes (e.g., object removal), enabling stronger causal reasoning. Video: Question: What are the precursors of the cells in the video? A. Eosinophils. B. Basophils. C. Monocytes. D. Primary lymphocyte. E. Megakaryocytes. T-GRPO:<think>Let me think about this for a moment. The question is asking about the precursors of...First, let's consider the context of the video...This suggests that the focus is on the immune system and the cells that play a role in it. Now, let's look at the options: A. Eosinophils - These are granulocytes...B. Basophils - These are also granulocytes that... C. Monocytes - These are precursor cells that differentiate into macrophages and dendritic cells. They are involved in the immune response and can be found in the bone marrow. D. Primary lymphocyte - This term is not standard in biology. It seems like there might be a typo or misunderstanding here. E. Megakaryocytes - These are precursor cells that...Hmm, it seems that the most logical choice is C,... Oh, I see. The correct answer should be C, as it is the only option that fits the context of the video and the immune system. Let's break it down one more time to verify... After this reflection, I am confident that the correct answer is C. Final answer: C...Let's double -check the options one last time to ensure there are no mistakes. Yes, C is the only logical choice.</think>\\n <answer>D</answer>TW-GRPO:<think> The video seems to illustrate a process...The options provided suggest different types of blood cells: eosinophils...Eosinophils and basophils are typically involved in allergic reactions and i n f l a m m a t i o n, w h i l e m o n o c y t e s c a n differentiate into macrophages, which are crucial for phagocytosis. Primary lymphocytes are involved in the adaptive immune response, and megakaryocytes are responsible for platelet production. Given that the video shows cells reacting to an injury, it would make sense that these could be monocytes, which are known to migrate to si tes o f in fl amm ati on an d en gul f pathogens. </think>\\n <answer>C </answer>① ② ③ ① Figure A5: Comparison of reasoning paths from T-GRPO and TW-GRPO on MMVU samples. TW-GRPO achieves more accurate conclusions by leveraging stronger causal reasoning and better alignment with visual and medical knowledge. understanding. A similar pattern appears in Figure A5, the T-GRPO trained model initially reasons about monocytes correctly 1, but exhibits inconsistency when evaluating distractor options 2,3. TW-GRPO maintains a coherent reasoning chain by aligning immune activity with monocyte function and contextual visual cues 1. These examples highlight how TW-GRPO enhances the accuracy of reasoning by integrating dynamic, contextual, and causal relationship, leading to more precise and reliable conclusions. 22 F Discussion F.1 Limitations Although our method demonstrates strong performance across multiple tasks, it still has several limitations. First, due to considerations of training efficiency and computational resources, we follow the same training and evaluation settings as Video-R1 by uniformly sampling each video into 16 frames. However, the frame sampling strategy used in the VideoChat-R1 paper differs from ours. To address this, we utilize the publicly released weights of VideoChat-R1 and report the test results under their original settings. For transparency and reproducibility, we provide the full benchmark results and corresponding evaluation logs for both our method and VideoChat-R1 in the anonymous repository. Second, the proposed Question-Answer Inversion method currently relies on direct string matching, which requires inputs to conform to a specific format. This constraint may limit its generalizability to more diverse or unstructured question-answer pairs. Finally, while our optimization approach achieves strong empirical results, it relies on Assumption B.1 being satisfied. As a result, it imposes certain requirements on the sampling configuration of the model, particularly the number of sampled responses and the temperature setting during inference. 23",
  "text_length": 75839
}