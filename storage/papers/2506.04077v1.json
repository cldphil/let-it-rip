{
  "id": "http://arxiv.org/abs/2506.04077v1",
  "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions",
  "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.",
  "authors": [
    "Chung-Chun Wang",
    "Jhen-Ke Lin",
    "Hao-Chien Lu",
    "Hong-Yun Lin",
    "Berlin Chen"
  ],
  "published": "2025-06-04T15:42:53Z",
  "updated": "2025-06-04T15:42:53Z",
  "categories": [
    "cs.CL",
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04077v1",
  "full_text": "--- Page 1 ---\nA Novel Data Augmentation Approach for Automatic Speaking Assessment on\nOpinion Expressions\nChung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen\n1Dept. Computer Science and Information Engineering, National Taiwan Normal University, Taiwan\n{takala, jacob, howchien, buffett, berlin }@ntnu.edu.tw\nAbstract\nAutomated speaking assessment (ASA) on opinion expres-\nsions is often hampered by the scarcity of labeled recordings,\nwhich restricts prompt diversity and undermines scoring relia-\nbility. To address this challenge, we propose a novel training\nparadigm that leverages a large language models (LLM) to gen-\nerate diverse responses of a given proficiency level, converts\nresponses into synthesized speech via speaker-aware text-to-\nspeech synthesis, and employs a dynamic importance loss to\nadaptively reweight training instances based on feature distri-\nbution differences between synthesized and real speech. Subse-\nquently, a multimodal large language model integrates aligned\ntextual features with speech signals to predict proficiency scores\ndirectly. Experiments conducted on the LTTC dataset show\nthat our approach outperforms methods relying on real data or\nconventional augmentation, effectively mitigating low-resource\nconstraints and enabling ASA on opinion expressions with\ncross-modal information.\nIndex Terms : Automated speaking assessment, Opinion Ex-\npression, Data Augmentation\n1. Introduction\nIn recent years, technologies for computer-assisted language\nlearning (CALL), such as automated speaking assessment\n(ASA), have made significant strides to meet the growing de-\nmand for scalable and objective evaluation of second-language\n(L2) speaking proficiency in both academic and professional\ncontexts [1, 2, 3]. In L2 speaking examinations, test-takers are\noften required to express opinions on a given topic or to partici-\npate in open-ended discussions, tasks that are crucial for assess-\ning learners’ communicative competence and critical thinking\nin languages such as English [4]. ASA systems that provide\nimmediate feedback and support large-scale scoring are becom-\ning commonplace in both CALL applications and formal testing\nscenarios.\nHowever, traditional human scoring is time-consuming,\nlabor-intensive, and prone to inconsistency among raters,\na problem that is especially acute in high-stakes opinion-\nexpression tasks such as those found in TOEFL, IELTS, and\nGEPT [5, 6, 7]. CALL platforms, through their immediate feed-\nback and interactive instruction, have evolved from early aca-\ndemic prototypes into mature intelligent classroom assistants\nand robust online testing systems, effectively alleviating many\nof these challenges. By reducing the burden on human raters\nand improving scoring consistency, effective ASA for opinion-\nexpression tasks has figured prominently in diverse use cases of\nlanguage teaching and assessment.\nEarly ASA approaches combine automatic speech recogni-\ntion (ASR) with hand-crafted acoustic and linguistic features,such as pronunciation accuracy [8, 9], fluency [10], prosody\n[11], rhythm [12], and others. These features play a funda-\nmental role in many automated scoring tasks and became the\ncornerstone of the field [6, 13, 14, 15, 16]. A few pioneer-\ning systems extract measures of pronunciation fluency, lexical\nrichness, grammatical accuracy, content relevance, and the like\nfrom L2 speech, and then draw on statistical models to predict\na composite proficiency score.\nMore recently, multi-task and multimodal architectures\nhave been introduced to capture complementary information in-\nherent in both the text content and speech signal of a spoken\nresponse. The emergence of self-supervised speech encoders\n(e.g., wav2vec 2.0 [5, 7, 17]), Transformer-based text models\n[18], and multimodal large language model (MLLM) such as\nPhi-4 multimodal1gives rise to a variety of end-to-end model-\ning frameworks that learn response representations jointly from\nraw audio and textual transcripts, markedly improving scoring\naccuracy and generalization. Nevertheless, these models still\nrely excessively on large amounts of annotated training datasets\nthat are frequently scarce for ASA on opinion expressions, thus\noften leading to overfitting on limited prompts.\nTo alleviate data scarcity, prior studies explore combining\ntext-to-speech (TTS) techniques with LLM–driven data aug-\nmentation. These approaches have shown some success on var-\nious speech and language processing tasks, such as automatic\nspeech recognition (ASR) [19, 20], emotion recognition [21],\ntext classification [22, 23], and intent recognition [24]. For the\npurpose of spoken content generation, these methods typically\nuse LLMs to generate diverse texts, thereby increasing content\nvariety, and subsequently employ TTS systems to synthesize\nspeech from the generated texts, enriching speaker characteris-\ntics, prosody, and content. Because synthesized and real texts\nand audio waves differ in their acoustic feature distributions,\nthere is good reason to apply dynamic instance reweighting or\nmetric-learning techniques to adaptively adjust the importance\nof each synthesized instance based on its proximity to real coun-\nterparts in a embedding space, thus narrowing the domain gap\n[22]. However, to our knowledge, there is still a dearth of work\ninvestigating the synergistic effect of combining the aforemen-\ntioned methods and their extensions to automated scoring of\nspoken opinion expressions.\nIn view of this, we present a unified framework for spo-\nken opinion-expression assessment under the constraint of lim-\nited resources, which combines generative data augmentation\nwith neural multimodal modeling. First, we develop a two-step\naugmentation pipeline in which an LLM generates proficiency-\nconditioned text responses [25] and a high-fidelity TTS system\nsynthesizes these text responses into spoken utterances while re-\n1Microsoft Phi-4 multimodal https://arxiv.org/abs/2503.01743arXiv:2506.04077v1  [cs.CL]  4 Jun 2025\n--- Page 2 ---\nFigure 1: Pipeline for synthesized speech generation. We\nbuild in-context prompts from kreal transcripts and condition\na multi-speaker TTS model on LLM-generated text ˆRl,iand a\nspeaker embedding vlto produce learner-like audio.\ntaining non-native prosody and natural disfluencies [20]. Next,\nwe introduce a novel dynamic importance loss that builds on a\nrecently proposed data-weighting principle [22] for training of\nour scoring model. This training loss automatically increases\nthe contribution of synthesized instances, so as to bridge do-\nmain gaps or proficiency distribution imbalance and meanwhile\ncurb overreliance on the generated data. Finally, the backbone\nof our scoring model is a Phi-4 multimodal1finetuned with\nLoRA adapters [26], which directly fuses text prompt, as well\nas raw audio and ASR transcript of a spoken response, to pre-\ndict the corresponding proficiency score. Our approach delivers\nrobust performance on both seen and unseen prompts despite\nthat only limited labeled instances are made available. Our ap-\nproach not only attains superior performance on low-resource\nspeaking assessment but also offers a replicable paradigm for\nother multimodal, low-resource speaking assessment tasks. In\nsummary, our key contributions are at least three-fold:\n•Unified Low-resource Modeling Framework: Combining\ntexts generated from a large language model and their corre-\nsponding high-fidelity synthesized speech utterances to sub-\nstantially expand L2 training data.\n•Dynamic Importance Loss: Leveraging an adaptive loss\nmechanism that effectively mitigates domain and distribution\nmismatches between synthesized and real data.\n•Multimodal Scoring: Developing an efficient end-to-end\nscoring model that fuses speech and text embeddings for ro-\nbust opinion-expression scoring.\n2. Methodology\nOur methodology advances low-resource opinion-expression\nassessment through three key innovations. First, we intro-\nduce a proficiency-aware text synthesis module that leverages\nan LLM to generate response texts closely aligned with each\nproficiency level. This module constructs in-context promptsby sampling and permuting transcripts of real test-takers’ re-\nsponses, ensuring that generated responses faithfully mimic au-\nthentic test-takers’ language competence. Second, we design a\nspeaker-conditioned voice-cloning pipeline, leveraging embed-\ndings extracted from real recordings to guide a multi-speaker\nTTS model to produce synthesized utterances that preserve non-\nnative prosody and natural hesitations. Third, we design and im-\nplement a novel dynamic importance loss, which automatically\nadjusts the importance weight of each training instance based on\nthe relative confidence of a fixed “quality” model and the train-\nable “target” model. This concept parallels the data-weighting\nidea of Kuo et al. in text data generation [22], but our formula-\ntion is the first to integrate it into a fully end-to-end multimodal\nscoring network. We implement this loss within our scoring\nmodel, a Phi-4 multimodal1, enhanced with LoRA adapters\n[26] for parameter-efficient fine-tuning. Together, these three\ninnovations—rich text augmentation, authentic speech cloning,\nand principled loss reweighting—curate a training dataset that\nbalances the diversity and fidelity factors between synthesized\nand real data, yielding robust scoring performance under strin-\ngent data constraints.\n2.1. LLM-Based Text Synthesis\nTo simulate responses from test-takers of different proficiency\nlevels and enrich the diversity of our synthesized training in-\nstances, we leverage the in-context learning capability of an\nLLM. For each target level l, we first randomly select ktran-\nscribed responses from the genuine training dataset, denoted as\n{Rl,1, Rl,2, . . . , R l,k}. We then concatenate the task prompt Q\nwith these examples in order to form an input prompt:\nPl=\u0002\nQ;Rl,1;Rl,2;. . .;Rl,k\u0003\n, (1)\naccompanied by the instruction, “Please generate your response\nin the style of the above examples.” During decoding, we set a\nrelatively high temperature τso that\nˆRl,i∼LLM\u0000\nPl;τ\u0001\n, i= 1, . . . , n, (2)\nwhere ˆRl,iis the i-th synthesized response and nis the number\nof instances generated per level. By shuffling the order of the\nexamples and repeating this generation process across multiple\nrounds, we obtain a synthesized dataset of text responses that\nremains aligned with the prompt’s content while exhibiting rich\nvariation in vocabulary, syntax, and viewpoints.\n2.2. Voice Cloning Synthesis\nWhen converting a synthesized texts ˆRl,iinto the corresponding\nrealistic speech of a test-taker, we observed that a standard TTS\nmodel tends to produce over-perfect audio which lacks natural\ncharacteristics of of non-native speakers, such as disfluencies,\nmispronunciations, and prosodic variability, since it is typically\ntrained on corpora of native speakers. To remedy this, we em-\nploy a multi-speaker TTS conditioned on disparate speaker em-\nbeddings rather than a single fixed voice.\nFor each generated text ˆRl,i, we randomly pick up a short\nreference audio clip Al,jfrom a real speaker at the same pro-\nficiency level l. A speaker encoder Eis then used to map this\nclip to an embedding:\nvl,j=E(Al,j), (3)\nwhich encodes voice identity, speaking rate, rhythm, and typical\nlearner disfluencies. We feed both the text and embedding into\nthe multi-speaker TTS model Gto produce a cloned waveform:\n--- Page 3 ---\nFigure 2: Two-stage training workflow. Stage 1 fine-tunes a Quality Model fQon real data. Stage 2 trains the Target Model fTon\nmixed real and synthesized instances using the ratio riand weight wito dynamically reweight each instance’s loss.\nˆAl,i∼G\u0000ˆRl,i, vl,j\u0001\n, i= 1, . . . , n. (4)\nBy randomly pairing each gl,iwith different vl,j, we en-\nsure the synthesized audio faithfully reproduces learner-specific\ntraits, in contrast to the uniform, overly clean output generated\nby a conventional single-speaker TTS model.\n2.3. End-to-End Multimodal Training with Dynamic Im-\nportance Reweighting\nAfter preparing synthesized texts and cloned speech, we intro-\nduce a new dynamic importance reweighting loss that automat-\nically adjusts the weigh of each training instance according to\nits fidelity to real learner behavior. This loss computes the ra-\ntio of confidence scores produced by a fixed “quality model”\nand by our trainable “target model,” then uses that ratio to am-\nplify high-quality synthesized instances and suppress poorly\nmatched ones. At the same time, real instances receive an el-\nevated base weight to ensure the model remains grounded in\nauthentic speech distributions. We integrate this loss into a neu-\nral scoring model built upon the Phi-4 multimodal1backbone\nand fine-tuned with LoRA adapters [26] for maximal parameter\nefficiency. The training phase unfolds in two stages:\nStage 1. Quality Model Calibration We first fine-tune\nthe Phi-4 multimodal1backbone on real prompt–audio–score\ntriplets alone, using standard cross-entropy to obtain a Quality\nModel fQ. This model establishes a reliable reference distribu-\ntion over genuine responses of test-takers.\nStage 2. Target Model Optimization Next, we train a sep-\narate Target Model fTon the union of real and synthesized in-\nstances. For each instance xiwith true label yi, we compute\npQ,i=PrfQ(yi|xi)and pT,i=PrfT(yi|xi), (5)\nwhich denote the confidence that each of these two model as-\nsigns to the correct label. We then define the importance ratio :\nri=pQ,i\npT,i, (6)\nwhich will be employed to amplify those synthesized instances\nthat most closely match real-data behavior. To prevent overre-\nliance on generated data—whose fluency often exceeds natural\nFigure 3: Score distribution for real data.\nlearner speech—we introduce a real-instance weight α > 1.\nThe combined per-instance loss is\nℓi=−riwilog\u0000\npT,i\u0001\n, w i=(\nα, x i∈ D real,\n1, x i∈ D syn.(7)\nWe optimize the average of these losses over all Ntraining in-\nstances:\nL=1\nNNX\ni=1ℓi. (8)\nBy introducing this novel training loss, we ensure that the\naugmentation of synthesized instances enriches the training set\nwithout distorting the model’s prediction capability on the au-\nthentic speech. By doing so, the estimated multimodal scoring\nmodel is anticipated to achieve robust performance under low-\nresource constraints.\n3. Experiments and Results\n3.1. Dataset\nWe used the LTTC GEPT Intermediate–Advanced\nopinion-expression dataset, which comprises 1,200\nprompt–response–score triples evenly drawn from four\nprompt types. Among them, 300 triples of the fourth type\n--- Page 4 ---\nTable 1: Comparison of baseline models and our approach. End-to-End models are marked with “ ✓”.\nModel End-to-End Seen Test Unseen Test\nAcc. Binary Acc. Acc. Binary Acc.\nWav2vec2 [17] ✓ 61.80% 65.10% 55.33% 57.52%\nBERT [18] 75.28% 78.65% 63.00% 67.89%\nWav2vec2+BERT 73.33% 77.78% 64.00% 67.89%\nPhi-4mm1(only real) ✓ 73.33% 77.78% 63.33% 68.00%\nOur Model ✓ 76.67% 81.11% 64.00% 68.33%\nserved as an unseen test set; the remaining 900 triples were\nsplit 8:1:1 into training, validation and seen-test sets. Each\nspoken response was rated 1–5 by two experts; we average\nand floor their scores (no responses scored 1), treating 4–5 as\npassing and 1–3 as failing. All recordings of spoken responses\nwere manually transcribed. We then generated a synthesized\ncorpus matching the size and score distribution of the training\nset, with utterance lengths varying according to the generated\ntext. Figure 3 illustrates the final score distributions.\n3.2. Experimental Setup\nWe generate synthesized texts with OpenAI o4-mini2using up\nto ten shuffled in-context examples per prompt and decoding\nat temperature τ= 1.5. For each text, we sample a real refer-\nence clip at the same proficiency level, extract its 512-D speaker\nembedding via Coqui-ai XTTSv23, and synthesize learner-like\nspeech with a multi-speaker TTS. Our scoring backbone is a\nPhi-4 multimodal1equipped with LoRA adapters [26]. We first\nfine-tune a Quality Model on real prompt–speech–score triplets\nfor three epochs using AdamW ( β1= 0.9, β2= 0.95, ϵ=\n10−7), gradient clipping at 1.0, learning rate α= 2 , batch\nsize16. We then train the Target Model on the combined real\nand synthesized corpus under the same settings, applying a real-\nsample weight α= 2in our dynamic importance loss.\n3.3. Experiment Results\nTable 1 compares our proposed model with strong single-\nmodality and dual-modality baselines on both the seen and\nunseen prompts. The baselines include an acoustics-only\nwav2vec2 [17] system, a text only BERT [18] model, and a Phi-\n4 multimodal1finetuned on the real data alone.\nOn the test set of seen prompts, our model achieves the\nhighest overall accuracy and pass–fail accuracy among all meth-\nods. By combining speech and text through LLM-driven aug-\nmentation and dynamic reweighting, it alleviate the downsides\nthe systems that rely either on purely acoustic or purely textual\ninformation. It notably outperforms wav2vec in grading consis-\ntency and robustness, while surpasseing BERT despite BERT’s\nstrong contextual encoding of transcripts. Although fine-tuning\nPhi-4 multimodal1model on real data alone yields competi-\ntive results, our pipeline with synthesized augmentation and the\ndynamic importance reweighting loss delivers the best perfor-\nmance, confirming the practical effectiveness of our contribu-\ntions.\nOn the the test et of unseen prompt, our model maintains\nits advantage and incurs a smaller performance drop than any\nbaseline. The synergy of diverse synthesized data and adap-\ntive loss weighting enables more dependable scoring of novel\n2OpenAI o4-mini https://platform.openai.com/docs/models/o4-mini\n3Coqui-ai XTTSv2 https://github.com/coqui-ai/TTSprompts, alleviating under- or over-scoring on unfamiliar con-\ntent. These results show that our unified training framework\nnot only boosts accuracy on familiar prompts but also enhances\nrobustness across different prompts.\n3.4. Ablation Studies\nTo isolate the effects of synthesized augmentation and real-\ninstance weighting, we compare three variants in Table 2: train-\ning on synthesized data alone, mixing real and synthesized with\nequal weight, and mixing with an elevated real-sample weight\nofα= 2 . Training on synthesized data by itself performs\npoorly, demonstrating that synthesized speech cannot substitute\nentirely for authentic speech. Introducing real training instances\nalongside synthesized ones recovers almost all of the perfor-\nmance lost in the first variant, validating the value of mixed-data\ntraining. Finally, increasing the weight on real instances yields\nthe best results on both seen and unseen prompts, confirming\nthat our dynamic importance reweighting effectively balances\nsynthesized diversity against fidelity to genuine speech.\nTable 2: Ablation results on the seen and unseen test sets.\nVariant Seen Acc. Unseen Acc.\nOnly synthesized 42.22% 46.67%\nMix ( α= 1) 72.22% 63.67%\nMix ( α= 2) 76.67% 64.00%\n4. Conclusion\nIn this paper, we have proposed a unified, low-resource train-\ning framework for opinion-expression assessment that inte-\ngrates LLM-driven text synthesis, speaker-conditioned voice\ncloning, and a novel dynamic importance reweighting loss\nwithin an end-to-end multimodal model. Empirical results on\nthe LTTC GEPT Intermediate–Advanced dataset have shown\nthat our approach outperforms strong single- and dual-modality\nbaselines and generalizes robustly to both seen and unseen\nprompts, demonstrating the promise of generative augmentation\nand adaptive loss reweighting for scalable, accurate automated\nspeaking assessment under data-scarcity scenarios.\nLimitations and Future Work Despite these gains, accent\nand hesitation mismatches persist and our experiments are lim-\nited to one language. The skewed mid-range score distribution\nalso impairs extreme-level discrimination. Future work will\nadapt the framework to additional languages and domains, ad-\ndress class imbalance, and explore lightweight on-device mod-\nels alongside advanced augmentation strategies.\n--- Page 5 ---\n5. Acknowledgement\nThis work was supported by the Language Training and Testing\nCenter (LTTC), Taiwan. Any findings and implications in the\npaper do not necessarily reflect those of the sponsor.\n6. References\n[1] M. Eskenazi, “An overview of spoken language technology for\neducation,” Speech communication , vol. 51, no. 10, pp. 832–844,\n2009.\n[2] B. Lin and L. Wang, “Attention-based multi-encoder automatic\npronunciation assessment,” in ICASSP 2021-2021 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP) . IEEE, 2021, pp. 7743–7747.\n[3] K. Zechner and X. Xi, “Towards automatic scoring of a test of\nspoken language with heterogeneous task types,” in Proceedings\nof the third workshop on innovative use of NLP for building edu-\ncational applications , 2008, pp. 98–106.\n[4] S. Bann `o, K. M. Knill, M. Matassoni, V . Raina, and M. Gales,\n“Assessment of l2 oral proficiency using self-supervised speech\nrepresentation learning,” in 9th Workshop on Speech and Lan-\nguage Technology in Education (SLaTE) , 2023, pp. 126–130.\n[5] T.-H. Lo, F.-A. Chao, T.-i. Wu, Y .-T. Sung, and B. Chen, “An\neffective automated speaking assessment approach to mitigating\ndata scarcity and imbalanced distribution,” in Findings of\nthe Association for Computational Linguistics: NAACL 2024 ,\nK. Duh, H. Gomez, and S. Bethard, Eds. Mexico City,\nMexico: Association for Computational Linguistics, Jun. 2024,\npp. 1352–1362. [Online]. Available: https://aclanthology.org/\n2024.findings-naacl.86/\n[6] Y . Qian, P. Lange, K. Evanini, R. Pugh, R. Ubale, M. Mulholland,\nand X. Wang, “Neural approaches to automated speech scoring of\nmonologue and dialogue responses,” in ICASSP 2019-2019 IEEE\ninternational conference on acoustics, speech and signal process-\ning (ICASSP) . IEEE, 2019, pp. 8112–8116.\n[7] H.-C. Lu, C.-C. Wang, J.-K. Lin, and B. Chen, “Development\nof an english oral assessment system with the gept dataset,” in\n2024 27th Conference of the Oriental COCOSDA International\nCommittee for the Co-ordination and Standardisation of Speech\nDatabases and Assessment Techniques (O-COCOSDA) . IEEE,\n2024, pp. 1–6.\n[8] L. Chen, K. Evanini, and X. Sun, “Assessment of non-native\nspeech using vowel space characteristics,” in 2010 IEEE Spoken\nLanguage Technology Workshop . IEEE, 2010, pp. 139–144.\n[9] K. Takai, P. Heracleous, K. Yasuda, and A. Yoneyama, “Deep\nlearning-based automatic pronunciation assessment for second\nlanguage learners,” in HCI International 2020-Posters: 22nd In-\nternational Conference, HCII 2020, Copenhagen, Denmark, July\n19–24, 2020, Proceedings, Part II 22 . Springer, 2020, pp. 338–\n342.\n[10] H. Strik and C. Cucchiarini, “Automatic assessment of second lan-\nguage learners’ fluency,” 1999.\n[11] E. Coutinho, F. H ¨onig, Y . Zhang, S. Hantke, A. Batliner, E. N ¨oth,\nand B. Schuller, “Assessing the prosody of non-native speakers of\nenglish: Measures and feature sets,” 2016.\n[12] K. Kyriakopoulos, K. M. Knill, and M. J. Gales, “A deep learning\napproach to automatic characterisation of rhythm in non-native\nenglish speech.” ISCA, 2019.\n[13] Z. Yu, V . Ramanarayanan, D. Suendermann-Oeft, X. Wang,\nK. Zechner, L. Chen, J. Tao, A. Ivanou, and Y . Qian, “Using\nbidirectional lstm recurrent neural networks to learn high-level\nabstractions of sequential features for automated scoring of non-\nnative spontaneous speech,” in 2015 IEEE Workshop on Auto-\nmatic Speech Recognition and Understanding (ASRU) . IEEE,\n2015, pp. 338–345.\n[14] L. Chen, J. Tao, S. Ghaffarzadegan, and Y . Qian, “End-to-end\nneural network based automated speech scoring,” in 2018 IEEE\ninternational conference on acoustics, speech and signal process-\ning (ICASSP) . IEEE, 2018, pp. 6234–6238.[15] L. Chen, K. Zechner, S.-Y . Yoon, K. Evanini, X. Wang, A. Louk-\nina, J. Tao, L. Davis, C. M. Lee, M. Ma et al. , “Automated scoring\nof nonnative speech using the speechrater sm v. 5.0 engine,” ETS\nResearch Report Series , vol. 2018, no. 1, pp. 1–31, 2018.\n[16] J. Park and S. Choi, “Addressing cold start problem for end-to-end\nautomatic speech scoring,” in Interspeech 2023 , 2023, pp. 994–\n998.\n[17] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec\n2.0: A framework for self-supervised learning of speech repre-\nsentations,” Advances in neural information processing systems ,\nvol. 33, pp. 12 449–12 460, 2020.\n[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in Proceedings of the 2019 conference of the North\nAmerican chapter of the association for computational linguis-\ntics: human language technologies, volume 1 (long and short pa-\npers) , 2019, pp. 4171–4186.\n[19] K. Yang, T.-Y . Hu, J.-H. R. Chang, H. S. Koppula, and O. Tuzel,\n“Text is all you need: Personalizing asr models using controllable\nspeech synthesis,” in ICASSP 2023-2023 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2023, pp. 1–5.\n[20] A. Fazel, W. Yang, Y . Liu, R. Barra-Chicote, Y . Meng, R. Maas,\nand J. Droppo, “Synthasr: Unlocking synthetic data for speech\nrecognition,” in Interspeech 2021 , 2021, pp. 896–900.\n[21] H. Tang, X. Zhang, J. Wang, N. Cheng, and J. Xiao, “Emomix:\nEmotion mixing via diffusion models for emotional speech syn-\nthesis,” in Interspeech 2023 , 2023, pp. 12–16.\n[22] H.-Y . Kuo, Y .-H. Liao, Y .-C. Chao, W.-Y . Ma, and P.-J.\nCheng, “Not all LLM-generated data are equal: Rethinking data\nweighting in text classification,” in The Thirteenth International\nConference on Learning Representations , 2025. [Online].\nAvailable: https://openreview.net/forum?id=oI5tZaWkF9\n[23] K. M. Yoo, D. Park, J. Kang, S.-W. Lee, and W. Park, “GPT3Mix:\nLeveraging large-scale language models for text augmentation,”\ninFindings of the Association for Computational Linguistics:\nEMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t.\nYih, Eds. Punta Cana, Dominican Republic: Association for\nComputational Linguistics, Nov. 2021, pp. 2225–2239. [Online].\nAvailable: https://aclanthology.org/2021.findings-emnlp.192/\n[24] G. Sahu, P. Rodriguez, I. Laradji, P. Atighehchian, D. Vazquez,\nand D. Bahdanau, “Data augmentation for intent classification\nwith off-the-shelf large language models,” in Proceedings of\nthe 4th Workshop on NLP for Conversational AI , B. Liu,\nA. Papangelis, S. Ultes, A. Rastogi, Y .-N. Chen, G. Spithourakis,\nE. Nouri, and W. Shi, Eds. Dublin, Ireland: Association\nfor Computational Linguistics, May 2022, pp. 47–57. [Online].\nAvailable: https://aclanthology.org/2022.nlp4convai-1.5/\n[25] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith,\nD. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning language\nmodels with self-generated instructions,” in Proceedings of the\n61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , A. Rogers, J. Boyd-\nGraber, and N. Okazaki, Eds. Toronto, Canada: Association\nfor Computational Linguistics, Jul. 2023, pp. 13 484–13 508.\n[Online]. Available: https://aclanthology.org/2023.acl-long.754/\n[26] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang,\nL. Wang, W. Chen et al. , “Lora: Low-rank adaptation of large\nlanguage models.” ICLR , vol. 1, no. 2, p. 3, 2022.",
  "text_length": 27227
}