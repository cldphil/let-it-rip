{
  "id": "http://arxiv.org/abs/2506.04214v1",
  "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation",
  "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/",
  "authors": [
    "Tingle Li",
    "Baihe Huang",
    "Xiaobin Zhuang",
    "Dongya Jia",
    "Jiawei Chen",
    "Yuping Wang",
    "Zhuo Chen",
    "Gopala Anumanchipalli",
    "Yuxuan Wang"
  ],
  "published": "2025-06-04T17:57:26Z",
  "updated": "2025-06-04T17:57:26Z",
  "categories": [
    "cs.CV",
    "cs.LG",
    "cs.MM",
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04214v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04214v1  [cs.CV]  4 Jun 2025Sounding that Object: Interactive Object-Aware Image to Audio Generation\nTingle Li1Baihe Huang1Xiaobin Zhuang2Dongya Jia2Jiawei Chen2Yuping Wang2Zhuo Chen2\nGopala Anumanchipalli1Yuxuan Wang2\nAbstract\nGenerating accurate sounds for complex audio-\nvisual scenes is challenging, especially in the pres-\nence of multiple objects and sound sources. In\nthis paper, we propose an interactive object-aware\naudio generation model that grounds sound gener-\nation in user-selected visual objects within images.\nOur method integrates object-centric learning into\na conditional latent diffusion model, which learns\nto associate image regions with their correspond-\ning sounds through multi-modal attention. At test\ntime, our model employs image segmentation to\nallow users to interactively generate sounds at\ntheobject level. We theoretically validate that\nour attention mechanism functionally approxi-\nmates test-time segmentation masks, ensuring\nthe generated audio aligns with selected objects.\nQuantitative and qualitative evaluations show that\nour model outperforms baselines, achieving bet-\nter alignment between objects and their associ-\nated sounds. Project site: https://tinglok.\nnetlify.app/files/avobject/ .\n1. Introduction\nHumans naturally perceive the world as ensembles of dis-\ntinct objects and their associated sounds (Bregman, 1994).\nFor example, in a busy city street (Figure 1), we can identify\nsounds from multiple objects, such as honks, footsteps, and\nchatters. However, replicating such object-level specificity\nremains challenging for computational models. Despite\nnotable advances in audio generation (Van Den Doel et al.,\n2001; Kong et al., 2019; Yang et al., 2023), existing methods\noften generate holistic soundscapes that fail to accurately\nreproduce the distinct sounds of specific objects (Pijanowski\net al., 2011). In complex scenes, models may either forget\nsubtle sounds (e.g., footsteps) or bind co-occurring events\n1University of California, Berkeley2ByteDance Inc. Corre-\nspondence to: Tingle Li <tingle@eecs.berkeley.edu>.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).(e.g., crowd noise and wind) even when only one is intended,\nleading to inaccurate sound textures (McDermott & Simon-\ncelli, 2011).\nRecent progress in vision-based models (Sheffer & Adi,\n2023) relies on analyzing the entire visual scene to pro-\nduce a single soundtrack, but this broad perspective may\noverlook subtle yet important sound sources. Text-based\nmodels (Liu et al., 2023), on the other hand, face diffi-\nculties when a prompt represents multiple events, either\nomitting certain sounds or conflating them with others due\nto entangled feature correlations (Wu et al., 2023). While\nmanually reweighting individual sound events in the dif-\nfusion latent (Xue et al., 2024) can mitigate these issues,\nit remains labor-intensive and impractical for large-scale\napplications. Fundamentally, these challenges arise because\nreal-world sounds are often imbalanced andconfounding in\ncomplex scenes, making it difficult to disentangle distinct\nsound sources.\nTo overcome these limitations, we propose an interactive\nobject-aware audio generation model that grounds each\ngenerated sound in a specific visual object. Inspired by\nhow humans parse complex soundscapes (Gaver, 1993), our\nmodel not only processes the overall scene context (e.g., a\ncity street) but also decouples separate events (e.g., honks,\nfootsteps). Drawing on object-centric learning (Greff et al.,\n2019), we build our model upon a conditional audio genera-\ntion framework (Liu et al., 2023) and introduce multi-modal\ndot-product attention (Vaswani et al., 2017) to learn sound-\nobject associations through self-supervision (Zhao et al.,\n2018; Afouras et al., 2020), which fundamentally overcomes\nthe problem of forgetting orbinding sound events.\nTo provide finer control and interactivity, we leverage seg-\nmentation masks (Kirillov et al., 2023) to convert user\nqueries into attention maps at test time, allowing users to\nselect specific objects in an image (e.g., car shapes) to gen-\nerate the corresponding sounds (e.g., engine sounds) with\nsimple mouse clicks. Since these masks guide the model to\nfocus on objects of interest, even subtle sound events can\nbe captured more accurately than with scene-wide analysis\nalone. Moreover, because the entire image still informs\nthe generation process, selecting multiple objects naturally\nblends their sounds into a consistent environment, rather\n1\n--- Page 2 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nInput ImageAudio generated to match user-selected objects\nFigure 1: Interactive object-aware audio generation . We generate sound aligned with specific visual objects in complex\nscenes. Users can select one or more objects in the scene using segmentation masks, and our model generates audio\ncorresponding to the selected objects. Here, we show a busy street with multiple sound sources (left). After training, our\nmodel generates object-specific audio (right), such as crowd noise for people, engine sounds for cars, and blended audio for\nmultiple objects.\nthan merely layering independent audio clips.\nThrough quantitative evaluations and human perceptual stud-\nies, we show that our model generates more complete and\ncontextually relevant soundscapes than existing baselines.\nAdditionally, we provide qualitative results and theoretical\nanalysis to demonstrate that our object-grounding mecha-\nnism is functionally equivalent to segmentation masks. In\nsummary, our contributions include:\n•An interactive object-aware audio generation model that\nlinks sounds to user-selected visual objects via masks.\n•A mechanism that replaces attention with segmentation\nmasks at test time, allowing fine-grained control over\nwhich objects, and thus which sounds, are present in the\ngenerated audio.\n•Empirical and theoretical validation demonstrating our\nmodel outperforms baselines in sound-object alignment\nand user controllability while maintaining audio quality.\n2. Related Work\nObject discovery. Object-centric learning aims to rep-\nresent visual scenes as compositions of discrete objects,\nenabling models to understand and manipulate individual\nentities within a scene. Unsupervised object discovery meth-\nods have been developed to decompose visual scenes into\nobject representations without explicit annotations (Greff\net al., 2019; Burgess et al., 2019; Locatello et al., 2020).\nIn the audio-visual realm, prior studies have explored au-\ndio localization (Arandjelovic & Zisserman, 2018; Rou-\nditchenko et al., 2019; Chen et al., 2021b; Mo & Morgado,\n2022; Hamilton et al., 2024), separation (Zhao et al., 2018;\nAfouras et al., 2020), and spatialization (Li et al., 2024b) by\nusing the correspondence between visual objects and their\ncorresponding audio. In concurrent work, SSV2A (Guoet al., 2024) introduces bounding boxes from external object\ndetectors to generate audio from multiple sound sources. In\ncontrast, our model interactively generates object-specific\nsound, without requiring explicit object segmentations and\nrepresentations during training.\nPredicting sound from images and text. Generating\nsounds from visual and textual inputs has gained notable\nattention recently. Image-based methods focus on synthe-\nsizing sounds from visual cues such as physical interactions\n(Van Den Doel et al., 2001; Owens et al., 2016), human\nmovements (Gan et al., 2020; Su et al., 2021; Ephrat & Pe-\nleg, 2017; Prajwal et al., 2020; Hu et al., 2021), musical\ninstrument performances (Koepke et al., 2020), and content\nfrom open-domain images and videos (Zhou et al., 2018;\nIashin & Rahtu, 2021; Sheffer & Adi, 2023; Luo et al., 2023;\nTang et al., 2023; Wang et al., 2024; Tang et al., 2024; Xing\net al., 2024; Zhang et al., 2024; Cheng et al., 2024a; Chen\net al., 2024). These approaches typically generate audio\nthat corresponds to the entire visual scene without isolating\nindividual sound sources, resulting in holistic sound gen-\neration. Text-based methods aim to produce sounds from\ntextual descriptions using generative models like GANs and\ndiffusion models (Yang et al., 2023; Kreuk et al., 2023; Liu\net al., 2023; Huang et al., 2023b; Saito et al., 2025; Evans\net al., 2025). However, when prompts contain multiple\nsound events, these methods often struggle to capture all\nthe desired audio elements (Wu et al., 2023). Unlike these\nmodels, our method generates sounds for user-selected one\nor more objects within images. This offers enhanced control\nand precision in audio generation.\nAudio-visual learning. Many works have focused on\naudio-visual associations due to their inherent correspon-\ndence in videos. A line of works explores the semantic\n2\n--- Page 3 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\ncorrespondence, identifying which sounds and visuals are\ncommonly associated with one another (Arandjelovic &\nZisserman, 2017). This includes representation learning\n(Morgado et al., 2021; Huang et al., 2023a), source local-\nization (Chen et al., 2021b; Harwath et al., 2018; Chen\net al., 2023b), audio stylization (Chen et al., 2022a; Li et al.,\n2024a), as well as scene classification (Chen et al., 2020;\nGemmeke et al., 2017; Du et al., 2023a) and generation (Li\net al., 2022b; Sung-Bin et al., 2023). Other studies leverage\nspatial correspondence between audio and visual streams\n(Owens & Efros, 2018; Korbar et al., 2018; Patrick et al.,\n2021) to tackle tasks like source separation (Zhao et al.,\n2018; 2019; Ephrat et al., 2016; Gao et al., 2018; Li et al.,\n2020; Chen et al., 2023a; Dong et al., 2022; Cheng et al.,\n2024b), Foley sound synthesis (Owens et al., 2016; Du et al.,\n2023b), and audio spatialization (Gao & Grauman, 2019;\nMorgado et al., 2018; Yang et al., 2020). Inspired by these\nworks, we aim to generate sound from user-selected objects\nwithin images.\n3.Interactive Object-Aware Audio Generation\nOur goal is to generate sound from user-selected objects\nwithin an image in an interactive way. We cast this problem\nby learning the correlation between audio and its correspond-\ning visual scene and then using this correlation to predict\nthe sound from the activated region. To achieve this, we:\n(i) fine-tune an off-the-shelf conditional audio generation\nmodel for sound synthesis; (ii) train an audio-guided vi-\nsual object grounding model to isolate the desired object;\n(iii) theoretically demonstrate the equivalence between the\nsegmentation mask and our grounding model.\n3.1. Conditional Audio Generation Model\nConditional latent diffusion model. We adopt a pre-\ntrained conditional latent diffusion model (Liu et al., 2023)\nto generate audio conditioned on textual inputs. Building\nupon latent diffusion models (Ho et al., 2020; Rombach\net al., 2022), our model operates in the latent space to im-\nprove computational efficiency. Specifically, given a text\nprompt tqdescribing the desired sound and a noise vec-\ntorϵ∼ N(0,I), the model iteratively denoises the latent\nvariables over Nsteps to generate the corresponding audio.\nOur model is trained to predict the added noise at each\ndenoising step n, conditioned on the textual input tq. The\ntraining objective minimizes the difference between the\npredicted noise and the true noise:\nLθ=Ez0,tq,ϵ∼N(0,I),n∥ϵ−ϵθ(zn, n,tq)∥2\n2, (1)\nwhere z0is the latent representation of the ground truth\naudio, znis the noisy latent at step n, andϵθis the denoising\nmodel parameterized by θ.Mel-spectrograms compression. We compress mel-\nspectrograms into a lower-dimensional latent space using a\nvariational autoencoder (V AE) (Kingma & Welling, 2013).\nThe V AE encodes the mel-spectrogram a∈RT×Finto a\nlatent representation z∈RT′×F′×d, where T′andF′are\nreduced temporal and frequency dimensions, and dis the\ndimensionality of the latent embeddings.\nTextual representation. We represent the textual input\ntqusing a pre-trained text encoder from CLAP (Elizalde\net al., 2023), which maps the text into an embedding space\nEt(tq)∈RL, where Ldenotes the embedding dimension.\nThese text embeddings capture semantic information about\nthe desired sound and are used to condition the diffusion\nmodel through cross-attention mechanisms (Vaswani et al.,\n2017).\nClassifier-free guidance. We employ classifier-free guid-\nance (CFG) (Ho & Salimans, 2022) to encourage the model\nto learn both conditional and unconditional denoising. Dur-\ning training, we randomly omit the conditioning input tq\nwith a 10% probability. At test time, we use a guidance\nscale λ≥1to interpolate between the conditional and\nunconditional predictions:\n˜ϵθ(zn, n,tq) =λ·ϵθ(zn, n,tq)+(1−λ)·ϵθ(zn, n,∅),(2)\nwhere ϵθ(zn, n,∅)is the unconditional prediction.\nWaveform reconstruction. After generating the latent\nrepresentation of the audio, we reconstruct the correspond-\ning waveform. The decoder part of the V AE transforms the\nlatent representation z0back into a mel-spectrogram. Sub-\nsequently, a pre-trained HiFi-GAN neural vocoder (Kong\net al., 2020a) is used to synthesize the time-domain audio\nwaveform from the mel-spectrogram, producing the final\naudio output.\n3.2. Text-Guided Visual Object Grounding Model\nVisual representation. To ground the visual objects corre-\nsponding to the desired sound, we extract features from the\ninput image using a pre-trained visual encoder. Specifically,\nwe utilize CLIP (Radford et al., 2021) to encode the image\ninto a set of visual patches embeddings Ev(iq)∈RP×L,\nwhere iqis the input image, Pis the number of patches, and\nLdenotes the embedding dimension (matching that of the\ntext embeddings). These embeddings capture both semantic\nand spatial information of the visual scene.\nScaled dot-product attention. We employ scaled dot-\nproduct attention (Vaswani et al., 2017) to fuse the textual\nand visual inputs, allowing the model to focus on specific\nobjects within the scene. Before computing the attention,\nthe text embeddings Et(tq)and patch embeddings Ev(iq)\n3\n--- Page 4 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nLatent Decoder\nOutput SpectrogramLatent DiffusionHiFi-GANVocoderStylized AudioLatent EncoderRef Spectrogram\n+\nAttention“A baby laughs and a puppy barks. ”Text PromptText EncoderMLPImage PromptImage Encoder\nMask / Attention Map\nFigure 2: Model architecture. We encode the reference spectrogram via a pre-trained latent encoder. An image and text\nprompt are processed by separate encoders, and their embeddings are fused using an attention mechanism to highlight\nrelevant objects. We then feed these conditioned features and noisy latent into a latent diffusion model to generate the\nobject-specific audio. Finally, the latent decoder reconstructs the spectrogram, and a pre-trained HiFi-GAN vocoder\ngenerates the final audio waveform. At test time, we replace the attention with a user-provided segmentation mask, and the\nlatent encoder for the reference spectrogram is notused.\nare linearly projected to obtain the query, key, and value\nmatrices. Specifically, we compute:\nQ=Et(tq)WQ,K=Ev(iq)WK,V=Ev(iq)WV,\n(3)\nwhere WQ,WK, andWVare learnable projectors.\nWe then compute the attention weights between the pro-\njected text and each projected image patch, grounding the\ntext in the visual domain:\nAttention (Q,K,V) =softmax\u0012QK⊤\n√dk\u0013\nV, (4)\nwhere dkis the dimensionality of the key embedding.\nAfter obtaining the attention output, we apply an MLP layer\n(Murtagh, 1991) to further refine the fused representations,\nwhich enables the model to attend to image regions corre-\nsponding to the text input. In this way, we integrate the\nimages iqwith the diffusion process, allowing the model to\nlearn to focus on the relevant regions in the image through\nself-supervision.\nLearnable positional encoding. To enhance the model’s\nability to localize objects within the image, we incorporate\nlearnable positional encodings (Devlin, 2018) into the at-\ntention mechanism. These encodings are added to the key\nand value embeddings, providing spatial information about\nthe image patches. By learning positional information, the\nmodel can better distinguish between objects in different\nlocations, improving grounding precision.Segmentation mask at test time. After training, we sub-\nstitute the attention weights derived from the scaled dot-\nproduct attention with segmentation masks generated by the\nsegment anything model (SAM) (Kirillov et al., 2023). We\nrescale the raw outputs of SAM into a normalized mask\nmq∈RP, matching the mean and variance of the attention\nweights. This allows us to generate the desired object’s\nsound by focusing on the regions specified by the segmenta-\ntion mask. Since SAM’s masks can be obtained using either\ntext prompts or point clicks, our model supports interactive\nimage-to-audio generation, allowing users to intuitively se-\nlect objects of interest and generate their associated sounds.\n3.3. Theoretical Analysis\nOne may notice that our training pipeline uses both text and\nimage encoders, but the test-time computation involves only\nthe image encoder, where the softmax attention weights\nare replaced by the segmentation masks. This indicates an\nout-of-distribution generalization ability (Lin et al., 2023),\nwhere our model trained on the softmax attention weights\ncomputed by CLAP & CLIP embeddings (Equation 4) is\nable to generalize well on the segmentation masks computed\nby SAM. We hypothesize that this ability is rooted in the\nalignment of contrastive losses and the dot-product attention\nmechanism. Recall that the InfoNCE loss (Gutmann &\nHyvärinen, 2010; Oord et al., 2018) for the text encoder in\n4\n--- Page 5 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\ncontrastive learning is given by Lt(Et,Ev) =:\nExT,xI\n1:N\"\n−logexp\u0000\n⟨Ev(xT),Et(xI\n1)⟩/τ\u0001\nPN\nj=1exp\u0000\n⟨Ev(xT),Et(xI\nj)⟩/τ\u0001#\n(5)\nwhere (xT, xI\n1)is the matching text-image pair, and\nxI\n2, . . . , xI\nNare the negative image samples associated with\nxT. Notice that if we substitute xTwith the text input tq,\nxI\n1:Nwith the image patches iq, and xI\n1with the matching\nimage patch (with the text input), then the loss in Equation 5\nbecomes the Maximum Likelihood Estimation (MLE) loss\nof the softmax attention weights in Equation 4 (under proper\nscaling in the exponents). Therefore, the encoders Ev,Et\nare able to assign high attention weights to image patches\nthat match with textual inputs, and low attention weights\nto irrelevant image patches, working effectively as the seg-\nmentation mask at test time . As such, the audio generation\nmodel is trained with the ability to focus only on the selected\nobjects by segmentation masks.\nIn the following theorem, we formalize the above argument\ninto a test-time error guarantee. We let fdenote the com-\nposition of the trained MLP layers and the trained audio\ngeneration model, such that fmaps an attention output aq\nto an audio output sq(on query q), and let vdenote the\nvalue metric that maps a sound-image-mask tuple (s, i, m )\nto a real number v(s, i, m )∈R. Our goal is to bound the\nfollowing test error\nerrtest:=Eq[v(f∗(pqV∗),iq, pq)−v(f(mqV),iq,mq)]\ni.e. the expected (over the randomness of test query q) gap\nbetween (i) the value v(f∗(pqV∗),iq, pq)achieved by the\noptimal model (f∗,V∗)and ground-truth mask pq, and (ii)\nthe value v(f(mqV),iq,mq)of the trained model (f,V)\nusing SAM segmentation mqat test time. Here, f∗and\nV∗are the ground-truth counterpart of fand value ma-\ntrix,pq∈∆Pis the (normalized) ground-truth mask of\nquery qsuch that pq,k=P(tq|iq,k)PP\nl=1P(tq|iq,l)for patch index\nk∈ {1, . . . , P },aqrepresents the attention output com-\nputed by Equation 4. Note that f(mqV), the audio output\nof the trained model, depends on the segmentation mask\nmqinstead of the ground-truth mask pqor text input tq.\nTheorem 3.1. Letϵsam:=Eq[∥mq−pq∥ℓ1]denote the\nexpected ℓ1error of the segmentation model. Let ϵf, ϵV\ndenote the expected error of fandVunder the pre-trained\nCLAP & CLIP embeddings respectively, and ϵcontrast denote\nthe expected contrastive loss of the encoders, more precisely,\nϵf=Eq[v(f∗(aq),iq, pq)]−E[v(f(aq),iq, pq)],\nϵV=∥V−V∗∥∞\nϵcontrast =Eq,d∼pq\"\n−logexp (⟨Ev(tq),Et(iq,d)⟩Σ)PP\nk=1exp (⟨Ev(tq),Et(iq,k)⟩Σ)#\n−Eq,d∼pq[−logpq,d].where ⟨·,·⟩Σis the local inner product under Σ :=\nWK(WQ)⊤/√dk(note that ϵcontrast is simply the differ-\nence between the model’s InfoNCE loss and the optimal\nInfoNCE loss, under the similarity metric ⟨·,·⟩Σ). Suppose\n∥V∗∥∞,∥V∥∞≤Bv,visLv-Lipschitz, and f, f∗are\nLf-Lipschitz, then we have\nerrtest≤Lv·Lf·\u0000\nϵV+Bv·\u0000\nϵsam+ 2√\n2ϵcontrast\u0001\u0001\n+Lv·ϵsam+ϵf. (6)\nDue to space constraints, the proof is deferred to the Ap-\npendix F. On the right hand side of Equation 6, the error\nterms ϵV, ϵsam, ϵcontrast , ϵfhave been minimized by mas-\nsive training (Radford et al., 2021; Elizalde et al., 2023;\nKirillov et al., 2023); furthermore, the regularity parameters\nLv, Lf, Bvare standard in learning theory literature (An-\nthony & Bartlett, 1999; Neyshabur et al., 2015; Bartlett\net al., 2017) and can be bounded with guarantees (Tsuzuku\net al., 2018; Combettes & Pesquet, 2020; Fazlyab et al.,\n2019). Consequently, Theorem 3.1 implies that the test-time\nerror can be effectively upper bounded, hence supporting\nthe substitution of attention weights derived from scaled\ndot-product attention with segmentation masks generated by\nthe segmentation model during testing. Our theory is further\ncorroborated by empirical findings in Section 4.3, where we\nobserve that using dot-product attention weights achieves\nperformance on par with using segmentation masks, while\nadditive attention fails completely.\n4. Experiments\n4.1. Experiment Setup\nDataset. We use AudioSet (Gemmeke et al., 2017) as our\nprimary data source, which consists of 4,616 hours of video\nclips, each paired with corresponding labels and captions.\nTo ensure audio-visual correspondence, we perform several\npreprocessing steps similar to Sound-VECaps (Yuan et al.,\n2024). This reduces the dataset to 748 hours of video for\ntraining. We then evaluate models on the AudioCaps (also a\nsubset of AudioSet) (Kim et al., 2019), a widely used bench-\nmark dataset for audio generation. Please see Appendix B\nfor more details on the dataset.\nModel architecture. We employ the pre-trained V AE\nand HiFi-GAN vocoder in AudioLDM (Liu et al., 2023).\nThe V AE is configured with a latent dimensionality dof 8\nchannels. For embedding extraction, we utilize the “ViT-\nB/32\" CLAP audio encoder (Elizalde et al., 2023) and the\nCLIP image encoder (Radford et al., 2021). These em-\nbeddings are then incorporated into the U-Net-based diffu-\nsion model through cross-attention. We implement a linear\nnoise schedule consisting of N= 1000 diffusion steps,\nfrom β1= 0.0015 toβN= 0.0195 . The DDIM sampling\n5\n--- Page 6 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nMethod ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑) OVL (↑) RET (↑) REI (↑) REO (↑)\nGround Truth / / / / 0.962 4.12±0.06 4.02 ±0.05 4.06 ±0.07 /\nRetrieve & Separate (Zhao et al., 2018) 0.276 4.051 1.572 1.550 0.764 2.73±0.02 2.54 ±0.05 2.76 ±0.04 2.49 ±0.04\nAudioLDM 1 (Liu et al., 2023) 0.336 3.576 1.537 1.545 0.724 2.83±0.07 3.09 ±0.03 2.92 ±0.02 2.18 ±0.04\nAudioLDM 2 (Liu et al., 2024) 0.513 2.976 1.162 1.779 0.743 2.98±0.04 3.19 ±0.02 3.09 ±0.03 2.47 ±0.01\nCaptioning (Li et al., 2022a) 0.587 2.778 1.364 1.901 0.773 2.84±0.03 3.15 ±0.04 3.05 ±0.06 2.63 ±0.05\nMake-an-Audio (Huang et al., 2023b) 0.309 3.555 1.443 1.673 0.712 2.74±0.08 3.06 ±0.05 2.89 ±0.05 2.08 ±0.04\nIm2Wav (Sheffer & Adi, 2023) 0.499 3.602 1.526 1.872 0.798 2.88±0.05 3.12 ±0.04 3.01 ±0.05 2.48 ±0.06\nSpecVQGAN (Iashin & Rahtu, 2021) 0.611 2.515 1.142 1.965 0.825 2.94±0.04 3.26 ±0.03 3.11 ±0.06 2.51 ±0.04\nDiff-Foley (Luo et al., 2023) 0.683 1.908 0.783 2.010 0.842 3.09±0.06 3.43 ±0.05 3.32 ±0.03 2.52 ±0.06\nCoDi (Tang et al., 2023) 0.672 1.954 0.856 1.936 0.833 3.00±0.04 3.32 ±0.03 3.31 ±0.05 2.34 ±0.02\nSeeing & Hearing (Xing et al., 2024) 0.668 1.923 0.794 1.954 0.722 3.08±0.05 3.38 ±0.04 3.28 ±0.06 2.49 ±0.04\nFoleyCrafter (Zhang et al., 2024) 0.732 1.760 0.665 2.007 0.811 3.19±0.02 3.48 ±0.03 3.32 ±0.04 2.60 ±0.04\nSSV2A (Guo et al., 2024) 0.806 1.265 0.525 2.100 0.893 3.22±0.02 3.50 ±0.03 3.35 ±0.02 3.48 ±0.06\nOurs 0.859 1.271 0.517 2.102 0.891 3.31±0.04 3.62 ±0.05 3.48 ±0.04 3.74 ±0.07\nTable 1: Quantitative comparison of our method and baselines across different objective and subjective metrics. The\nsubjective OVL, RET, REI, and REO scores are presented with 95% confidence intervals.\nmethod (Song et al., 2020) is used with 200 steps to facili-\ntate efficient generation. At test time, we apply CFG with a\nguidance scale λset to 2.0.\nTraining configuration. To facilitate parallel training,\neach video’s soundtrack is either truncated or zero-padded\nto achieve a fixed duration of 10 seconds and then converted\nto a 16 kHz sample rate. We apply a 512-point discrete\nFourier transform with a frame length of 64 ms and a frame\nshift of 10 ms. For each video, a single visual frame is\nrandomly chosen to serve as the input image. The model\nis then trained using the AdamW optimizer (Loshchilov &\nHutter, 2017) with a batch size of 64, a learning rate of\n10−4,β1= 0.95,β2= 0.999,ϵ= 10−6, and a weight\ndecay of 10−3over 300 epochs.\nEvaluation metrics. We use both objective and subjective\nmetrics (see Appendix C for more evaluation details) to\nevaluate the performance of our model. For the objective\nevaluation, we employ several metrics, including Sound\nEvent Accuracy (ACC), which leverages the PANNs model\n(Kong et al., 2020b) to predict and sample sound event logits\nbased on the annotated labels and then compute the mean\naccuracy across the dataset. We also measure the semantic\nalignment between the output and target using four estab-\nlished metrics: (i) Fréchet Audio Distance (FAD) (Kilgour\net al., 2019), which quantifies how close the generated audio\nis to the real audio in latent space; (ii) Kullback-Leibler Di-\nvergence (KL), which assesses the alignment of distributions\nbetween the generated and target audio; (iii) Inception Score\n(IS) (Salimans et al., 2016), which evaluates the diversity\nof the generated audio; (iv) Audio-Visual Correspondence\n(A VC) (Arandjelovic & Zisserman, 2017), which measures\nhow well the resulting audio match the visual context.\nFor subjective evaluation, we conduct a human study to\nassess the quality and relevance of the generated audio. We\npresent both the holistic samples and the object-selectedsamples. Each participant is provided with an input image,\nalong with the corresponding generated audio, and is asked\nto rate each sample on a scale from 1 to 5 based on sev-\neral criteria: (i) Overall Quality (OVL), which evaluates\nthe general quality of the audio; (ii) Relevance to the Text\nPrompt (RET), which assesses how well the audio matches\nany associated text description; (iii) Relevance to the Input\nImage (REI), which judges the alignment between the audio\nand the visual content; (iv) Relevance to the Selected Ob-\nject (REO), which focuses on how well the generated audio\naligns with a specific object in the visual scene.\nBaselines. We compare our method with several baseline\nmodels, each of which is adapted for our task: (i) Retrieve &\nSeparate (Zhao et al., 2018), a two-stage object-aware model\nthat first retrieves audio based on a text prompt (Elizalde\net al., 2023), then separates the object-specific audio from\nthe specified visual object (Zhao et al., 2018); (ii) Audi-\noLDM 1 & 2 (Liu et al., 2023; 2024), which we fine-tune on\nour dataset for a fair comparison; (iii) Captioning (Li et al.,\n2022a), a cascade model that takes input image, generates\ncaptions and feeds them to a pre-trained AudioLDM 2; (iv)\nMake-an-Audio (Huang et al., 2023b), which supports ei-\nther text or image prompts for audio generation. We extract\nits image branch and fine-tune it on our dataset; (v) Im2Wav\n(Sheffer & Adi, 2023), an image-guided open-domain audio\ngeneration model that operates auto-regressively. Since the\noriginal model generates only 4 seconds of audio, we retrain\nit on our dataset to better suit our task; (vi) SSV2A and\n(vii) CoDi, which are sound-source-aware and any-to-any\ngenerative models respectively. We use their image-to-audio\nbranch for comparison; (viii) SpecVQGAN (Iashin & Rahtu,\n2021), (ix) Diff-Foley (Luo et al., 2023), (x) Seeing & Hear-\ning (Xing et al., 2024), (xi) FoleyCrafter (Zhang et al., 2024),\nwhich are video-to-audio generative models. We modify\nthem by using static images (randomly sampling a single\nframe from each video clip) as input and fine-tuning them\non our dataset.\n6\n--- Page 7 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\n“Distant chatter with cars passing”“Train arriving with people talking”\nOursAudioLDM 2Make-an-AudioIm2WavSpecVQGANDiff-FoleyAudioLDM 1\n“Jet roaring with crowd cheering”\nText“Dog growls with goose honks”Mask\nImageGenerated Audio\nPrompt Type\nTarget Audio\nFigure 3: Qualitative model comparison . We show audio generation results for our method and the baselines, each of\nwhich is conditioned on an image, text, or segmentation mask.\n4.2. Comparison to Baselines\nQuantitative results. Table 1 compares our approach to\nbaselines on the AudioCaps dataset (see Appendix E for\nevaluations on another dataset), showing that our model out-\nperforms across metrics and generates high-quality audio.\nIn particular, our method achieves the highest ACC scores,\ndemonstrating its ability to generate sounds closely linked\nto visual objects in the scene. Among baselines, SSV2A\nperforms competitively, likely due to its object-level speci-\nficity from the external object detector. Diff-Foley, Seeing &\nHearing, and FoleyCrafter perform competitively, likely due\nto their contrastive representations enhancing audio-visual\nconsistency. Make-an-Audio, Im2Wav, and SpecVQGAN\nachieve reasonable A VC scores but underperform on FAD\nand KL, suggesting limitations in audio quality. AudioLDM,\nCaptioning, and CoDi show lower ACC and FAD metrics,\nlikely reflecting that CLAP text embeddings fail to represent\ncomplex audio events. Retrieve & Separate struggles with\nretrieval in multi-source scenes, limiting its performance in\ncomplex scenarios. These results demonstrate our model’s\nstrength in leveraging object-level cues to generate contex-\ntually relevant sounds.\nFor subjective evaluation, we randomly select 100 gener-\nated samples from the test set, including 50 with manually\ncreated segmentation masks for specific objects (see Ap-\npendix C). These samples are then rated by 50 participants.\nOur model achieves the highest average ratings across all\nmeasures, with a significant lead in REO, indicating better\nalignment between generated sounds and objects in the im-\nage. Interestingly, baselines achieve similar REO scores,\nsuggesting limited ability to link audio to object-level vi-\nsual cues. Moreover, our model consistently outperforms in\nOVL, RET, and REI, further validating the objective metrics\nand demonstrating improved contextual alignment.\nQualitative results. Figure 3 compares our method with\ngenerative baselines on the AudioCaps dataset. In the firstMethod Time ( ↓) Attempts ( ↓) Satisfaction ( ↑)\nAudioLDM 1 (Liu et al., 2023) 7.34 3.20 2.00 ±0.88\nAudioLDM 2 (Liu et al., 2024) 5.10 2.40 2.80 ±1.04\nFoleyCrafter (Zhang et al., 2024) 3.00 2.80 3.00 ±1.96\nSSV2A (Guo et al., 2024) 2.95 1.80 3.40 ±1.42\nOurs 2.67 1.60 3.60 ±0.68\nTable 2: Interaction satisfaction evaluation of user-driven\naudio generation methods. We report average time (min-\nutes), number of attempts, and satisfaction score (with 95%\nconfidence intervals).\nexample, where a dog and a goose are present, baselines\ngenerate only dog growls, missing the goose honks, while\nour method captures both sounds, demonstrating its object-\naware capability. Similarly, in the second and third exam-\nples, baselines produce only partial sound events, whereas\nour model generates the complete soundscape. In the fi-\nnal example, featuring a small jet in the background with\na cheering crowd, vision-based models fail to detect the\njet due to its small size, generating only crowd and wind\nnoises, while text-based models struggle to combine mul-\ntiple sounds. Our approach captures all relevant sounds,\nhighlighting its ability to generate accurate audio aligned\nwith complex visual scenes. For a more direct experience,\nplease view the results video on the project webpage.\nInteraction satisfaction. We conduct another human\nstudy focusing on user-driven audio generation, compar-\ning our method to text-based baselines (we exclude those\nthat do not allow user prompting). We ask 5 experienced\nparticipants to generate “baby laughs and puppy barks” from\na single image (the one in Figure 2), and we measure the\naverage time taken, the number of attempts required, and\na 5-point subjective satisfaction score. As shown in Table\n2, text-based baselines often miss one of the sounds and\nrequire multiple prompt adjustments, leading to higher time\nand lower satisfaction. Our method, by contrast, consis-\ntently requires fewer attempts, takes less time, and achieves\n7\n--- Page 8 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nMethod ACC ( ↑) FAD ( ↓) KL ( ↓) IS ( ↑) A VC ( ↑)\n(i) Frozen Diffusion 0.692 1.543 1.047 1.943 0.733\n(ii) Muiti-Head Attn. 0.415 2.238 1.903 2.115 0.887\n(iii) Additive Attn. 0.103 15.747 7.425 1.343 0.137\n(iv) Txt-Img Attn. 0.856 1.270 0.520 2.097 0.890\n(v) Aud-Img Attn. 0.634 1.761 1.232 1.731 0.692\n(vi) Mask Training 0.763 1.446 0.742 1.947 0.797\nOurs 0.859 1.271 0.517 2.102 0.891\nTable 3: Quantitative ablation studies on the AudioCaps\ndataset.\nhigher satisfaction, even for participants already familiar\nwith prompting.\n4.3. Ablation Study and Analysis\nTable 3 summarizes the ablation experiments. We explore\nthe following model variations: (i) freezing the latent dif-\nfusion weights rather than fine-tuning them; (ii) replacing\nsingle-head attention with multi-head attention; (iii) altering\nthe attention mechanism from dot-product to additive atten-\ntion; (iv) using text-image attention instead of segmentation\nmasks during inference; (v) substituting text-image atten-\ntion with audio-image attention; (vi) using segmentation\nmasks instead of attention during training. We also show\nadditional results in Appendix D.\nEffect of freezing diffusion weights. We test the impact\nof freezing the latent diffusion model weights instead of\nfine-tuning them during training. We observe that freezing\nthe weights degrades the performance, which suggests that\nfine-tuning is required to achieve more coherent audio.\nImpact of attention head. We compare our single-\nhead attention mechanism with the multi-head counterpart\n(Vaswani et al., 2017). The multi-head approach enhances\nthe alignment between textual inputs and the generated\naudio, leading to a stronger correspondence between text\ndescriptions and sound outputs. However, this improve-\nment reduces controllability when specifying specific audio\ncharacteristics based on the segmentation mask. We con-\njecture that this limitation arises because each head in the\nmulti-head attention focuses on different regions of the in-\nput (V oita et al., 2019; Hamilton et al., 2024). While this\nstrategy increases text-audio alignment, the lack of a clear\ndefinition for each head’s specific scope reduces the inter-\npretability of the final results. This likely contributes to the\nmasking results deviating from expectations.\nEvaluation of attention scoring mechanism. We assess\nthe role of the attention scoring function by replacing dot-\nproduct attention with the additive one (Bahdanau, 2014).\nThe additive variant collapses significantly, indicating that\nsegmentation masks are not a suitable replacement for this\nattention. As explained by the theory in Section 3.3, this\nInputImage\nAttention\nMaskInputImageAttentionMaskFigure 4: Visualization results . We visualize the difference\nbetween attention maps and segmentation masks using im-\nages from Places (Zhou et al., 2017) and text prompts from\nBLIP (Li et al., 2022a).\ncould be because addition operations are not compatible\nwith the contrastive losses used by CLAP & CLIP and seg-\nmentation masks generated by SAM, which disrupts our\ngrounding model.\nChoice of attention modality. We investigate the effec-\ntiveness of text-image attention compared to an adapted\naudio-image attention model (Li et al., 2024b). Results\nshow a decline in performance, which could be attributed\nto the inherent limitations of the CLAP model in represent-\ning overlapping audio. This limitation probably introduces\nnoise, thereby weakening the model’s ability to form audio-\nvisual associations essential for audio generation.\nRole of masking during training and inference. We\ncompare the text-image attention to segmentation masks at\ntest time. Results show that this attention achieves compa-\nrable performance to segmentation masks, suggesting both\nmethods provide similar guidance (Section 3.3). Notably,\nusing segmentation masks in both training and testing de-\ngrades performance. We hypothesize that masking entire\nobject regions imposes an overly rigid prior, as sound is typ-\nically emitted from specific parts (e.g., a dog’s head rather\nthan its tail). From a probabilistic viewpoint, hard masks\nsampled from the ground-truth distribution exhibit high vari-\nance, whereas soft attention, empowered by CLIP & CLAP,\ndirectly approximates the ground-truth distribution. This\nallows the model to focus on sound-relevant regions while\nmaintaining audio accuracy at test time.\n4.4. Cross-dataset Evaluation\nVisualization between grounding and masking. In Fig-\nure 4, we visualize the comparison between the attention\nmaps generated by our model and the segmentation masks\nproduced by SAM. For this, we use images from Places\n(Zhou et al., 2017) and text prompts derived from BLIP (Li\net al., 2022a). To visualize the attention maps, we apply\nbilinear interpolation to match the resolution of the segmen-\ntation masks. Our results show a strong alignment between\nour model’s attention maps and the segmentation masks,\nproviding empirical evidence for the theory in Section 3.3\nand the findings of the ablation study in Section 4.3. While\nthe segmentation masks represent a form of hard attention,\n8\n--- Page 9 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nMaskOutput AudioInput ImageMaskOutput AudioInput Image\nFigure 5: Interactive audio generation . Our model gener-\nates object-specific sounds in the city (left) and beach (right)\nscenes, and composes a complete soundscape when one or\nmore objects are selected.\nInput ImageMaskOutput AudioMaskOutput AudioInput Image\nFigure 6: Generating soundscapes from visual texture\nchanges. . We generate different soundscapes by manipulat-\ning the visual textures of the same scene, such as changing\nweather (left) or materials (right).\ndirectly highlighting entire selected objects, our model gen-\nerates softattention maps that probabilistically focus on\nthe sound-relevant areas within each object. This similarity\nindicates that, through training, our model learns to capture\nobject-specific regions similar to those identified by segmen-\ntation, achieving the desired grounding in a flexible manner.\nFurthermore, this observation suggests that attention maps\ncan be replaced with segmentation masks at test time.\nInteractive audio generation. We ask whether our model\nwill generate object-specific sounds by isolating individual\nobjects within a scene. As shown in Figure 5, we use the\nsame image for each scene, separating different objects\n(cars, people, seagulls, etc.) to generate corresponding audio\noutputs. The results illustrate that our model successfully\nlearns to generate distinct sounds for each object, such as car\nengines or footsteps, reflecting their unique sound textures.\nFurthermore, when multiple objects are selected together,\nour model is able to generate the entire soundscape that\nrepresents the scene property. This capability highlights our\nmodel’s strength in interactively synthesizing audio.\nSound adaptation to visual texture changes. We explore\nwhether our method can generate soundscapes that adapt to\nchanges in visual textures, inspired by audio-visual video\nediting (Lee et al., 2023). Starting with images from the\nPlaces (Zhou et al., 2017) and Greatest Hits (Owens et al.,\n2016) datasets, we apply an off-the-shelf image translation\nmodel (Park et al., 2020; Li et al., 2022b) to create paired\nscenes (e.g., sunny-rainy, water-grass), and then overlay full-image segmentation masks on top. As illustrated in Figure 6,\nour model generates context-appropriate soundscapes. For\ninstance, it generates rain sounds for dark skies, wind sounds\nfor clear skies, water splashing for watery surfaces, and\ngrass crunching for grassy areas. This demonstrates that our\nmodel successfully captures variations in visual textures to\ngenerate corresponding audio.\nBalancing the volume of different objects. We find in\nFigure 5 that specifying each object separately tends to\nassign a similar volume to all sources. However, when mul-\ntiple objects are selected, our method dynamically accounts\nfor context. For example, if a large car dominates the scene,\nits siren may overwhelm subtle ambient sounds, creating\na more realistic blend instead of flattening everything to\nequal volume. Moreover, we quantitatively confirm this\ncontext-driven behavior in Table 1, 7, and 10, where our\nobject-aware method better reflects how certain sources can\noverpower others or combine to create natural audio events.\nInteractions among multiple objects. We show in Figure\n6 that our method captures interactions, like a stick splash-\ning water, instead of generating only generic water flowing\nsounds. These results indicate our model’s ability to handle\nbasic multi-object interactions from static images.\n5. Conclusion\nIn this paper, we proposed an interactive object-aware audio\ngeneration model, focusing on aligning generated sounds\nwith specific visual objects in complex scenes. To achieve\nthis, we developed a diffusion model grounded in object-\ncentric representations, enhancing the association between\nobjects and their corresponding sounds via multi-modal\nattention. Theoretical analysis demonstrates that our object-\ngrounding mechanism is functionally equivalent to seg-\nmentation masks. Quantitative and qualitative evaluations\nshow that our model surpasses baselines in sound-object\nalignment, enabling cross-dataset generalization and user-\ncontrollable synthesis. We hope our work not only advances\ncontrollable audio generation but also inspires further ex-\nploration into the relationships between objects and sounds.\nWe will release code and models upon acceptance.\nLimitations and broader impacts. Our model shows\npromising results in generating object-specific sounds from\nimages but has certain limitations. First, relying on static\nimages makes it challenging to produce non-stationary audio\nsynchronized with dynamic events, such as impact sounds\n(Figure 6). Second, it may lack precise control over the type\nof sound generated for similar objects, leading to ambiguity\n(e.g., a car might produce a siren or engine noise in Figure\n3). Lastly, while useful for content creation like filmmaking,\nour model could be misused to generate misleading videos.\n9\n--- Page 10 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nAcknowledgment\nWe thank Ziyang Chen, Hao-Wen Dong, Yisi Liu, and\nZhikang Dong for their helpful discussions, and the anony-\nmous reviewers for their valuable feedback.\nImpact Statement\nThis paper introduces an interactive object-aware audio gen-\neration model. It is trained on a publicly available dataset,\ni.e., AudioSet, which does not contain personally identifi-\nable information. We have taken steps to ensure compliance\nwith data usage policies, and our model does not involve\nhuman subjects or raise privacy concerns. We believe our\nwork poses minimal negative ethical impacts and societal\nimplications, as it focuses on enhancing sound-object align-\nment in a controlled research environment. However, we\nencourage responsible use of our model, particularly when\napplied to real-world scenarios.\nReferences\nAfouras, T., Owens, A., Chung, J. S., and Zisserman, A.\nSelf-supervised learning of audio-visual objects from\nvideo. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XVIII 16 , pp. 208–224. Springer, 2020.\nAnthony, M. and Bartlett, P. L. Neural Network Learning:\nTheoretical Foundations . Cambridge University Press,\n1999.\nArandjelovic, R. and Zisserman, A. Look, listen and learn.\nInProceedings of the IEEE international conference on\ncomputer vision , pp. 609–617, 2017.\nArandjelovic, R. and Zisserman, A. Objects that sound.\nInProceedings of the European conference on computer\nvision (ECCV) , pp. 435–451, 2018.\nBahdanau, D. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 ,\n2014.\nBartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally-\nnormalized margin bounds for neural networks. Advances\nin neural information processing systems , 30, 2017.\nBregman, A. S. Auditory scene analysis: The perceptual\norganization of sound . MIT press, 1994.\nBurgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins,\nI., Botvinick, M., and Lerchner, A. Monet: Unsupervised\nscene decomposition and representation. arXiv preprint\narXiv:1901.11390 , 2019.\nChen, C., Gao, R., Calamia, P., and Grauman, K. Visual\nacoustic matching. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition ,\npp. 18858–18868, 2022a.\nChen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vg-\ngsound: A large-scale audio-visual dataset. In ICASSP\n2020-2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pp. 721–725.\nIEEE, 2020.\nChen, H., Xie, W., Afouras, T., Nagrani, A., Vedaldi, A.,\nand Zisserman, A. Audio-visual synchronisation in the\nwild. arXiv preprint arXiv:2112.04432 , 2021a.\nChen, H., Xie, W., Afouras, T., Nagrani, A., Vedaldi, A.,\nand Zisserman, A. Localizing visual sounds the hard way.\nInProceedings of the Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2021b.\nChen, J., Zhang, R., Lian, D., Yang, J., Zeng, Z., and Shi,\nJ. iquery: Instruments as queries for audio-visual sound\nseparation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 14675–\n14686, 2023a.\nChen, S., Wu, Y ., Wang, C., Liu, S., Tompkins, D., Chen,\nZ., and Wei, F. Beats: Audio pre-training with acoustic\ntokenizers. arXiv preprint arXiv:2212.09058 , 2022b.\nChen, Z., Qian, S., and Owens, A. Sound localization\nfrom motion: Jointly learning sound direction and camera\nrotation. arXiv preprint arXiv:2303.11329 , 2023b.\nChen, Z., Seetharaman, P., Russell, B., Nieto, O., Bour-\ngin, D., Owens, A., and Salamon, J. Video-guided fo-\nley sound generation with multimodal controls. arXiv\npreprint arXiv:2411.17698 , 2024.\nCheng, H. K., Ishii, M., Hayakawa, A., Shibuya, T.,\nSchwing, A., and Mitsufuji, Y . Taming multimodal joint\ntraining for high-quality video-to-audio synthesis. arXiv\npreprint arXiv:2412.15322 , 2024a.\nCheng, X., Zheng, S., Wang, Z., Fang, M., Zhang, Z.,\nHuang, R., Ma, Z., Ji, S., Zuo, J., Jin, T., et al. Om-\nnisep: Unified omni-modality sound separation with\nquery-mixup. arXiv preprint arXiv:2410.21269 , 2024b.\nCombettes, P. L. and Pesquet, J.-C. Lipschitz certificates for\nlayered network structures driven by averaged activation\noperators. SIAM Journal on Mathematics of Data Science ,\n2(2):529–557, 2020.\nCramer, A. L., Wu, H.-H., Salamon, J., and Bello, J. P. Look,\nlisten, and learn more: Design choices for deep audio\nembeddings. In ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 3852–3856. IEEE, 2019.\n10\n--- Page 11 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018.\nDong, H.-W., Takahashi, N., Mitsufuji, Y ., McAuley, J.,\nand Berg-Kirkpatrick, T. Clipsep: Learning text-queried\nsound separation with noisy unlabeled videos. arXiv\npreprint arXiv:2212.07065 , 2022.\nDu, C., Teng, J., Li, T., Liu, Y ., Yuan, T., Wang, Y ., Yuan, Y .,\nand Zhao, H. On uni-modal feature learning in supervised\nmulti-modal learning. In International Conference on\nMachine Learning , pp. 8632–8656. PMLR, 2023a.\nDu, Y ., Chen, Z., Salamon, J., Russell, B., and Owens,\nA. Conditional generation of audio from video via foley\nanalogies. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 2426–\n2436, 2023b.\nElizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H.\nClap learning audio concepts from natural language su-\npervision. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 1–5. IEEE, 2023.\nEphrat, A. and Peleg, S. Vid2speech: speech reconstruction\nfrom silent video. In 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) ,\npp. 5095–5099. IEEE, 2017.\nEphrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson, K., Has-\nsidim, A., Freeman, W. T., and Rubinstein, M. Looking to\nlisten at the cocktail party: A speaker-independent audio-\nvisual model for speech separation. ACM Transactions\non Graphics (TOG) , 37(4), 2016.\nEvans, Z., Parker, J. D., Carr, C., Zukowski, Z., Taylor, J.,\nand Pons, J. Stable audio open. In ICASSP 2025-2025\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , pp. 1–5. IEEE, 2025.\nFazlyab, M., Robey, A., Hassani, H., Morari, M., and Pap-\npas, G. Efficient and accurate estimation of lipschitz\nconstants for deep neural networks. Advances in neural\ninformation processing systems , 32, 2019.\nGan, C., Huang, D., Chen, P., Tenenbaum, J. B., and Tor-\nralba, A. Foley music: Learning to generate music from\nvideos. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XI 16 , pp. 758–775. Springer, 2020.\nGao, R. and Grauman, K. 2.5 d visual sound. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pp. 324–333, 2019.Gao, R., Feris, R., and Grauman, K. Learning to separate ob-\nject sounds by watching unlabeled video. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npp. 35–53, 2018.\nGaver, W. W. What in the world do we hear?: An ecolog-\nical approach to auditory event perception. Ecological\npsychology , 5(1):1–29, 1993.\nGemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,\nLawrence, W., Moore, R. C., Plakal, M., and Ritter, M.\nAudio set: An ontology and human-labeled dataset for\naudio events. In 2017 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp.\n776–780. IEEE, 2017.\nGirdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V .,\nJoulin, A., and Misra, I. Imagebind: One embedding\nspace to bind them all. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\npp. 15180–15190, 2023.\nGreff, K., Kaufman, R. L., Kabra, R., Watters, N., Burgess,\nC., Zoran, D., Matthey, L., Botvinick, M., and Lerchner,\nA. Multi-object representation learning with iterative\nvariational inference. In International conference on\nmachine learning , pp. 2424–2433. PMLR, 2019.\nGuo, W., Wang, H., Ma, J., and Cai, W. Gotta hear them\nall: Sound source aware vision to audio generation. arXiv\npreprint arXiv:2411.15447 , 2024.\nGutmann, M. and Hyvärinen, A. Noise-contrastive estima-\ntion: A new estimation principle for unnormalized statisti-\ncal models. In Proceedings of the thirteenth international\nconference on artificial intelligence and statistics , pp.\n297–304. JMLR Workshop and Conference Proceedings,\n2010.\nHamilton, M., Zisserman, A., Hershey, J. R., and Free-\nman, W. T. Separating the\" chirp\" from the\" chat\": Self-\nsupervised visual grounding of sound and language. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 13117–13127, 2024.\nHarwath, D., Recasens, A., Surís, D., Chuang, G., Torralba,\nA., and Glass, J. Jointly discovering visual objects and\nspoken words from raw sensory input. In Proceedings of\nthe European conference on computer vision (ECCV) , pp.\n649–665, 2018.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598 , 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems , 33:6840–6851, 2020.\n11\n--- Page 12 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nHu, C., Tian, Q., Li, T., Yuping, W., Wang, Y ., and Zhao, H.\nNeural dubber: Dubbing for videos according to scripts.\nAdvances in neural information processing systems , 34:\n16582–16595, 2021.\nHuang, P.-Y ., Sharma, V ., Xu, H., Ryali, C., Fan, H., Li,\nY ., Li, S.-W., Ghosh, G., Malik, J., and Feichtenhofer, C.\nMavil: Masked audio-video learners, 2023a.\nHuang, R., Huang, J., Yang, D., Ren, Y ., Liu, L., Li, M.,\nYe, Z., Liu, J., Yin, X., and Zhao, Z. Make-an-audio:\nText-to-audio generation with prompt-enhanced diffusion\nmodels. In International Conference on Machine Learn-\ning (ICML) , 2023b.\nIashin, V . and Rahtu, E. Taming visually guided sound\ngeneration. In The British Machine Vision Conference\n(BMVC) , 2021.\nIashin, V ., Xie, W., Rahtu, E., and Zisserman, A. Synch-\nformer: Efficient synchronization from sparse cues. In\nICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp.\n5325–5329. IEEE, 2024.\nKilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M.\nFréchet audio distance: A reference-free metric for evalu-\nating music enhancement algorithms. In INTERSPEECH ,\npp. 2350–2354, 2019.\nKim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Gen-\nerating captions for audios in the wild. In Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short\nPapers) , pp. 119–132, 2019.\nKim, T. K. T test as a parametric statistic. Korean journal\nof anesthesiology , 68(6):540–546, 2015.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114 , 2013.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo,\nW.-Y ., et al. Segment anything. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npp. 4015–4026, 2023.\nKoepke, A. S., Wiles, O., Moses, Y ., and Zisserman, A.\nSight to sound: An end-to-end approach for visual piano\ntranscription. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 1838–1842. IEEE, 2020.\nKong, J., Kim, J., and Bae, J. Hifi-gan: Generative ad-\nversarial networks for efficient and high fidelity speech\nsynthesis. Advances in Neural Information Processing\nSystems , 33:17022–17033, 2020a.Kong, Q., Xu, Y ., Iqbal, T., Cao, Y ., Wang, W., and Plumb-\nley, M. D. Acoustic scene generation with conditional\nsamplernn. In ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 925–929. IEEE, 2019.\nKong, Q., Cao, Y ., Iqbal, T., Wang, Y ., Wang, W., and\nPlumbley, M. D. Panns: Large-scale pretrained audio\nneural networks for audio pattern recognition. IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, 28:2880–2894, 2020b.\nKorbar, B., Tran, D., and Torresani, L. Cooperative learn-\ning of audio and video models from self-supervised syn-\nchronization. In Proceedings of the Advances in Neural\nInformation Processing Systems , 2018.\nKreuk, F., Synnaeve, G., Polyak, A., Singer, U., Défossez,\nA., Copet, J., Parikh, D., Taigman, Y ., and Adi, Y . Audio-\ngen: Textually guided audio generation. In International\nConference on Learning Representations (ICLR) , 2023.\nLee, S. H., Kim, S., Yoo, I., Yang, F., Cho, D., Kim, Y .,\nChang, H., Kim, J., and Kim, S. Soundini: Sound-\nguided diffusion for natural video editing. arXiv preprint\narXiv:2304.06818 , 2023.\nLi, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping\nlanguage-image pre-training for unified vision-language\nunderstanding and generation. In International confer-\nence on machine learning , pp. 12888–12900. PMLR,\n2022a.\nLi, T., Lin, Q., Bao, Y ., and Li, M. Atss-net: Target speaker\nseparation via attention-based neural network. In Inter-\nspeech , pp. 1411–1415, 2020.\nLi, T., Liu, Y ., Owens, A., and Zhao, H. Learning vi-\nsual styles from audio-visual associations. In European\nConference on Computer Vision , pp. 235–252. Springer,\n2022b.\nLi, T., Wang, R., Huang, P.-Y ., Owens, A., and Anu-\nmanchipalli, G. Self-supervised audio-visual soundscape\nstylization. In Proceedings of the European Conference\non Computer Vision , 2024a.\nLi, Z., Zhao, B., and Yuan, Y . Cyclic learning for binaural\naudio generation and localization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 26669–26678, 2024b.\nLin, Y ., Chen, M., Wang, W., Wu, B., Li, K., Lin, B., Liu,\nH., and He, X. Clip is also an efficient segmenter: A\ntext-driven approach for weakly supervised semantic seg-\nmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 15305–\n15314, 2023.\n12\n--- Page 13 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nLiu, H., Chen, Z., Yuan, Y ., Mei, X., Liu, X., Mandic, D.,\nWang, W., and Plumbley, M. D. Audioldm: Text-to-audio\ngeneration with latent diffusion models. In International\nConference on Machine Learning (ICML) , 2023.\nLiu, H., Yuan, Y ., Liu, X., Mei, X., Kong, Q., Tian, Q.,\nWang, Y ., Wang, W., Wang, Y ., and Plumbley, M. D.\nAudioldm 2: Learning holistic audio generation with\nself-supervised pretraining. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing , 2024.\nLocatello, F., Weissenborn, D., Unterthiner, T., Mahendran,\nA., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf,\nT. Object-centric learning with slot attention. Advances in\nneural information processing systems , 33:11525–11538,\n2020.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101 , 2017.\nLuo, S., Yan, C., Hu, C., and Zhao, H. Diff-foley: Syn-\nchronized video-to-audio synthesis with latent diffusion\nmodels. arXiv preprint arXiv:2306.17203 , 2023.\nMcDermott, J. H. and Simoncelli, E. P. Sound texture per-\nception via statistics of the auditory periphery: evidence\nfrom sound synthesis. Neuron , 71(5):926–940, 2011.\nMcHugh, M. L. Interrater reliability: the kappa statistic.\nBiochemia medica , 22(3):276–282, 2012.\nMo, S. and Morgado, P. Localizing visual sounds the easy\nway. In European Conference on Computer Vision , pp.\n218–234. Springer, 2022.\nMorgado, P., Vasconcelos, N., Langlois, T., and Wang, O.\nSelf-supervised generation of spatial audio for 360 video.\nInAdvances in Neural Information Processing Systems ,\n2018.\nMorgado, P., Vasconcelos, N., and Misra, I. Audio-visual\ninstance discrimination with cross-modal agreement. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 12475–12486, 2021.\nMurtagh, F. Multilayer perceptrons for classification and\nregression. Neurocomputing , 2(5-6):183–197, 1991.\nNeyshabur, B., Tomioka, R., and Srebro, N. Norm-based\ncapacity control in neural networks. In Conference on\nlearning theory , pp. 1376–1401. PMLR, 2015.\nOord, A. v. d., Li, Y ., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748 , 2018.\nOwens, A. and Efros, A. A. Audio-visual scene analy-\nsis with self-supervised multisensory features. In Pro-\nceedings of the European conference on computer vision\n(ECCV) , pp. 631–648, 2018.Owens, A., Isola, P., McDermott, J., Torralba, A., Adelson,\nE. H., and Freeman, W. T. Visually indicated sounds. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition , pp. 2405–2413, 2016.\nPark, T., Efros, A. A., Zhang, R., and Zhu, J.-Y . Con-\ntrastive learning for unpaired image-to-image translation.\nInComputer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart IX 16 , pp. 319–345. Springer, 2020.\nPatrick, M., Huang, P.-Y ., Misra, I., Metze, F., Vedaldi,\nA., Asano, Y . M., and Henriques, J. F. Space-time crop\n& attend: Improving cross-modal video representation\nlearning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pp. 10560–10572, 2021.\nPijanowski, B. C., Villanueva-Rivera, L. J., Dumyahn, S. L.,\nFarina, A., Krause, B. L., Napoletano, B. M., Gage, S. H.,\nand Pieretti, N. Soundscape ecology: the science of sound\nin the landscape. BioScience , 61(3):203–216, 2011.\nPrajwal, K., Mukhopadhyay, R., Namboodiri, V . P., and\nJawahar, C. Learning individual speaking styles for ac-\ncurate lip to speech synthesis. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 13796–13805, 2020.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning , pp. 8748–8763. PMLR, 2021.\nRavi, N., Gabeur, V ., Hu, Y .-T., Hu, R., Ryali, C., Ma, T.,\nKhedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.\nSam 2: Segment anything in images and videos. arXiv\npreprint arXiv:2408.00714 , 2024.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pp.\n10684–10695, 2022.\nRouditchenko, A., Zhao, H., Gan, C., McDermott, J.,\nand Torralba, A. Self-supervised audio-visual co-\nsegmentation. In ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 2357–2361. IEEE, 2019.\nSaito, K., Kim, D., Shibuya, T., Lai, C.-H., Zhong, Z.,\nTakida, Y ., and Mitsufuji, Y . Soundctm: Unifying score-\nbased and consistency models for full-band text-to-sound\ngeneration. In The Thirteenth International Conference\non Learning Representations , 2025.\n13\n--- Page 14 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V .,\nRadford, A., and Chen, X. Improved techniques for\ntraining gans. Advances in neural information processing\nsystems , 29, 2016.\nSheffer, R. and Adi, Y . I hear your true colors: Image\nguided audio generation. In ICASSP 2023-2023 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pp. 1–5. IEEE, 2023.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 , 2020.\nSu, J., Ahmed, M., Lu, Y ., Pan, S., Bo, W., and Liu, Y .\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing , 568:127063, 2024.\nSu, K., Liu, X., and Shlizerman, E. How does it sound?\nAdvances in Neural Information Processing Systems , 34:\n29258–29273, 2021.\nSung-Bin, K., Senocak, A., Ha, H., Owens, A., and Oh, T.-H.\nSound to visual scene generation by audio-to-visual latent\nalignment. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 6430–\n6440, 2023.\nTang, Z., Yang, Z., Zhu, C., Zeng, M., and Bansal, M. Any-\nto-any generation via composable diffusion. Advances\nin Neural Information Processing Systems , 36:16083–\n16099, 2023.\nTang, Z., Yang, Z., Khademi, M., Liu, Y ., Zhu, C., and\nBansal, M. Codi-2: In-context interleaved and interactive\nany-to-any generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npp. 27425–27434, 2024.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288 ,\n2023.\nTsuzuku, Y ., Sato, I., and Sugiyama, M. Lipschitz-margin\ntraining: Scalable certification of perturbation invariance\nfor deep neural networks. Advances in neural information\nprocessing systems , 31, 2018.\nVan Den Doel, K., Kry, P. G., and Pai, D. K. Foleyautomatic:\nphysically-based sound effects for interactive simulation\nand animation. In Proceedings of the 28th annual confer-\nence on Computer graphics and interactive techniques ,\npp. 537–544, 2001.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems , 30, 2017.V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.\nAnalyzing multi-head self-attention: Specialized heads\ndo the heavy lifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418 , 2019.\nWang, H., Ma, J., Pascual, S., Cartwright, R., and Cai, W.\nV2a-mapper: A lightweight solution for vision-to-audio\ngeneration by connecting foundation models. In Proceed-\nings of the AAAI Conference on Artificial Intelligence , pp.\n15492–15501, 2024.\nWu, H.-H., Nieto, O., Bello, J. P., and Salamon, J. Audio-\ntext models do not yet leverage natural language. In\nICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp.\n1–5. IEEE, 2023.\nXing, Y ., He, Y ., Tian, Z., Wang, X., and Chen, Q. Seeing\nand hearing: Open-domain visual-audio generation with\ndiffusion latent aligners. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npp. 7151–7161, 2024.\nXue, J., Deng, Y ., Gao, Y ., and Li, Y . Auffusion: Leveraging\nthe power of diffusion and large language models for text-\nto-audio generation. arXiv preprint arXiv:2401.01044 ,\n2024.\nYang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y .,\nand Yu, D. Diffsound: Discrete diffusion model for text-\nto-sound generation. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , 2023.\nYang, K., Russell, B., and Salamon, J. Telling left from right:\nLearning spatial correspondence of sight and sound. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 9932–9941, 2020.\nYuan, Y ., Jia, D., Zhuang, X., Chen, Y ., Liu, Z., Chen,\nZ., Wang, Y ., Wang, Y ., Liu, X., Plumbley, M. D., et al.\nImproving audio generation with visual enhanced caption.\narXiv preprint arXiv:2407.04416 , 2024.\nZhang, Y ., Gu, Y ., Zeng, Y ., Xing, Z., Wang, Y ., Wu, Z.,\nand Chen, K. Foleycrafter: Bring silent videos to life\nwith lifelike and synchronized sounds. arXiv preprint\narXiv:2407.01494 , 2024.\nZhao, H., Gan, C., Rouditchenko, A., V ondrick, C., Mc-\nDermott, J., and Torralba, A. The sound of pixels. In\nProceedings of the European conference on computer\nvision (ECCV) , pp. 570–586, 2018.\nZhao, H., Gan, C., Ma, W.-C., and Torralba, A. The sound of\nmotions. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pp. 1735–1744, 2019.\n14\n--- Page 15 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Tor-\nralba, A. Places: A 10 million image database for scene\nrecognition. IEEE transactions on pattern analysis and\nmachine intelligence , 40(6):1452–1464, 2017.\nZhou, Y ., Wang, Z., Fang, C., Bui, T., and Berg, T. L. Visual\nto sound: Generating natural sound for videos in the\nwild. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 3550–3558, 2018.\n15\n--- Page 16 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nA. Results Video\nWe provide a results video on the project webpage, which\nshowcases our model’s ability to generate sounds based on\nthe masked object prompts. Specifically, this video demon-\nstrates the following:\n•Our model can interactively generate object-specific\nsounds within complex scenes.\n•Despite being trained on the AudioSet (Gemmeke et al.,\n2017), our model can be successfully applied to out-of-\ndomain visual scenes, including those from the Places\ndataset (Zhou et al., 2017), the Greatest Hits dataset\n(Owens et al., 2016), and even random web images.\n•Our model can capture variations in visual textures to\ngenerate corresponding audio.\n•Our model can capture diverse objects within an image\nand generate sounds more accurately than the baselines.\nB. Dataset Preprocessing Details\nB.1. Dataset Refinement\nWe use the AudioSet (Gemmeke et al., 2017) as the primary\nsource for this task. The original dataset comprises 4,616\nhours of video clips, each paired with corresponding labels\nand captions. Inspired by Sound-VECaps (Yuan et al., 2024),\nwe apply the following refinement steps to adapt the dataset\nfor our use.\nAudio-visual matching. To ensure strong correspondence\nbetween audio and visual inputs, we train an audio-visual\nmatching model (Figure 8), which consists of a 6-layer\nnon-causal transformer with a rotary positional embedding\nmechanism (Su et al., 2024). Visual embeddings are ex-\ntracted using the ViT-B/16 Transformer module from CLIP\n(Radford et al., 2021), while audio embeddings are gen-\nerated using the BEATs model (Chen et al., 2022b). Both\nembeddings are then passed through a 3-layer MLP to match\na 768-dimensional space. The model is trained in a self-\nsupervised manner (Owens & Efros, 2018; Korbar et al.,\n2018), treating audio-visual pairs from the same tempo-\nral instance as matches and those from different videos as\nmismatches, which allows the model to learn audio-visual\ncorrespondences without human annotations.\nFor training efficiency, the videos are standardized to 8\nframes per second, with each frame resized to 224x224 pix-\nels. During the evaluation, our model achieves an accuracy\nof 91% for matching scenarios and 85% for non-matching\nscenarios on a set of 100 matched and 100 mismatched sam-\nples, indicating its effectiveness in capturing audio-visual\nalignment. We use this model to score each clip in the Au-\ndioSet, with results shown in Figure 7. A threshold of 0.6 is\nthen applied to filter the dataset.\n0.0 0.2 0.4 0.6 0.8 1.0\nMatch Score020000400006000080000100000120000140000CountFigure 7: Distribution of matching scores. We present the\nscores for audio-visual pairs in the AudioSet.\nViTsBEATsAdaptorAdaptorTransformerClassifierInput AudioInput Video\nFigure 8: Model architecture of audio-visual matching.\nWe train a model to quantify the correspondence between a\nvideo and its corresponding soundtrack.\nCaption rephrasing. To ensure captions to focus exclu-\nsively on visible sounding objects, we utilize Llama (Tou-\nvron et al., 2023) with a tailored prompt (Figure 9). Given\nthe video and audio captions, our prompt instructs the model\nto generate a single sentence highlighting the common fea-\ntures between the audio and visual content. The prompt\nemphasizes including only events present in both modali-\nties, while excluding modality-specific details such as overly\nspecific visual features. The model is guided to capture\nthe order and parallel occurrence of events using temporal\nmarkers like “and then”, “followed by”, and “while”. This\nprocess enhances the consistency between audio and visual\ndescriptions.\nAudio filtering. We filter out clips containing human vo-\ncalizations (e.g., singing, talking), voiceovers, and music\nusing a sound event detection model (Kong et al., 2020b)\nand the metadata from AudioSet. This step ensures that\nthe remaining audio data largely consists of ambient and\ncontext-specific sounds that are more likely to align with\nthe visual content.\nAfter applying these refinement steps, the dataset is reduced\nto 748 hours of video clips that most likely contain contin-\nuous sounds throughout each clip and exhibit high audio-\nvisual correspondence.\n16\n--- Page 17 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nRole-System:You are a helpful assistant for identifying audio-visual events and generating sentences. Your task is to identify the overlapping or common features between a 10-second audio and the corresponding visual description, and help the user to generate a single sentence of caption that represents this intersection.The caption feature is a sentence generated by an audio-caption model: {enclap_caption}.The label feature is several audio events that happened in the audio: {audio_label}.Lastly, the user is given several sentences which are the image description of the scene for each second, connected by “and then”.Please identify all the audio events and visual elements based on all three features and try to conclude in one single sentence to describe this scene with the shared audio-visual events or actions that present sound and sight together.Please emphasize time features to present the order of each event, such as “and then” , “followed by” , “after” for order; “and”,  “while” etc., for parallel events.Intersection Focus:•Based on the first caption feature, you might need to change or alter any wrong audio event, improve the sentence with more features, such as the weather, the emotion of any people, the description of the car and so on.•Keep only the features that are common between the audio and visual descriptions. If an event or element is mentioned in both the audio and the visual description, include it in the final caption.•Omit any feature or detail that is present in only one modality. This includes removing overly specific visual details, such as the color, shape, any text or label, name and what people are writing and so on, that do not align with the audio description and vice versa.Please ensure that the final caption accurately reflects the common elements of the audio-visual scene, maintaining the order of occurrence, and capturing the shared background, foreground, and context.Role-User:The descriptions of the frames are: {frame_caption}\nFigure 9: Prompt for Llama . We extract common features between the audio and visual caption using Llama, ensuring the\nresulting caption focuses on events present in both modalities while avoiding overly specific details.\nVehicle\n31.6%\nAnimal19.7%\nEnvironment15.8%Human6.3%T ool 4.7%Engine\n3.9%Impact\n2.4%Other\n15.5%\nFigure 10: Categorical distribution of the filtered Au-\ndioSet. We show top 8 categories derived from AudioSet\nannotations.\nB.2. Dataset Configuration\nFigure 10 shows the top-8 categorical distributions, derived\nfrom AudioSet (Gemmeke et al., 2017) annotations. We\nuniformly sample 48 hours across these categories for the\ntest set, with the remaining used for training. Notably, there\nis no overlap between training and testing videos. As most\nclips contain multiple sound sources, we randomly select\n100 examples from the test set to assess our model’s ability\nto generate object-specific sounds through human evaluation.\nFor 50 of these samples, we manually create object masks by\nsplitting each caption into object snippets and then randomlyselecting one to guide SAM in generating the mask.\nC. Additional Evaluation Details\nACC. We use the PANNs model *(Kong et al., 2020b) to\ncompute ACC for each audio clip, leveraging annotations\nfrom AudioSet. Each audio clip is first processed through\nthe pre-trained PANNs model to obtain logit values for all\nsound event classes, excluding classes like “Speech\" and\n“Music.\" We then sample the logits for each clip based on\nits annotated labels in AudioSet. Since these logits are\nsoftmax outputs, they represent the model’s confidence in\neach sound event, allowing us to interpret them as accuracy\nscores. Finally, we compute the mean of these sampled\nlogits across all clips to determine the overall ACC score.\nFAD, KL, and IS. We measure FAD, KL, and IS using\nthe AudioLDM-Eval toolbox†. The reference and gener-\nated audio files are organized into separate folders, and the\ntoolbox is run in paired mode.\nA VC. We measure A VC using a two-stream network\n(Arandjelovic & Zisserman, 2017). One stream extracts\naudio features, while the other extracts visual features. We\nuse OpenL3‡(Cramer et al., 2019) to obtain these features\nand compute the cosine similarity for each image-audio pair.\n*https://github.com/qiuqiangkong/\naudioset_tagging_cnn\n†https://github.com/haoheliu/audioldm_\neval\n‡https://github.com/marl/openl3\n17\n--- Page 18 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nScale ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑)\nλ= 1.0 0.413 2.021 0.914 1.336 0.674\nλ= 1.5 0.657 1.558 0.762 1.617 0.751\nλ= 2.0 0.859 1.271 0.517 2.102 0.891\nλ= 2.5 0.807 1.440 0.589 2.012 0.853\nλ= 3.0 0.796 1.482 0.576 2.023 0.841\nTable 4: Quantitative results under different CFG scales.\nSpecifically, we employ the “env” content type model with\na 512-dimensional linear spectrogram representation.\nHuman evaluation. We conducted a human evaluation\nto assess the quality and relevance of the generated audio\nusing Amazon Mechanical Turk§. The interface for this\nstudy is shown in Figure 11. Each participant was presented\nwith an input image and the corresponding generated audio,\nthen rated each sample on a scale from 1 to 5 based on\nthe following criteria: (i) Overall Quality (OVL), assess-\ning the general audio quality; (ii) Relevance to Input Text\n(RET), measuring the alignment of the audio with the asso-\nciated text description; (iii) Relevance to Input Image (REI),\nevaluating how well the audio corresponds to the visual con-\ntent; and (iv) Relevance to Selected Object (REO), focusing\non the alignment of the audio with a specific object in the\nimage.\nWe randomly selected 100 samples for evaluation, each rated\nby 50 unique participants to ensure reliability. These sam-\nples included both (50%) holistic and (50%) object-specific\ncases. To control for random responses, we incorporated a\nset of noise-only samples. Consistently low scores for these\ncontrol samples confirmed the reliability of participants. Ad-\nditionally, we ensured that each participant spent at least\n90 seconds evaluating each sample to guarantee thoughtful\nassessment.\nTo further validate our results, we computed the inter-rater\nreliability using Cohen’s kappa (McHugh, 2012), which\nindicated a substantial agreement among raters ( κ= 0.78).\nFurthermore, we conducted a statistical significance test\n(paired t-test) (Kim, 2015) between our model and base-\nlines for each criterion, confirming that the improvements\nreported are statistically significant ( p <0.01). The final\nscores presented in the main paper are the mean ratings\nacross all participants.\nD. Additional Results\nDifferent CFG scales. We evaluate our model’s perfor-\nmance across CFG scales ranging from 1.0 to 3.0. As shown\nin Table 4, there is a consistent improvement in metrics as\nλincreases from 1.0 to 2.0, reaching peak performance at\nλ= 2.0. However, further increasing λbeyond 2.0 results\n§https://www.mturk.com/Threshold ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑)\n0.4 0.521 1.874 0.888 1.432 0.696\n0.5 0.743 1.536 0.691 1.625 0.774\n0.6 0.859 1.271 0.517 2.102 0.891\n0.7 0.845 1.387 0.612 1.987 0.882\n0.8 0.812 1.501 0.664 2.005 0.879\nTable 5: Quantitative results under different audio-visual\nmatching scores.\nMethod ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑)\nw/o PE 0.787 1.493 0.674 1.913 0.779\nw/ PE (Ours) 0.859 1.271 0.517 2.102 0.891\nTable 6: Model performance comparison with and without\npositional encoding.\nin a gradual decline across most metrics.\nDifferent thresholds of audio-visual matching. We test\nour model’s performance across different audio-visual\nmatching thresholds, varying from 0.4 to 0.8 (Figure 7).\nThe same held-out test set is used to assess the metrics, with\nresults presented in Table 5. We empirically find that the\nmodel achieves optimal performance at a threshold of 0.6.\nEffect of positional encoding. We assess the impact of\npositional encoding (PE) on our model’s performance. As\nshown in Table 6, removing positional encoding leads to a\nsignificant degradation across all metrics, highlighting its\nimportance in the model’s overall performance.\nImpact of overall scene context. We examine whether\ncapturing the overall scene context benefits audio generation.\nTo this end, we compare the Captioning & Mix baseline,\nwhere each detected object in the image is captioned sep-\narately, passed to AudioLDM to generate individual audio\nclips, and subsequently mixed, against the Captioning base-\nline (as described in Section 4.1) that leverages the full\nscene. As shown in Table 7, although Captioning & Mix\nyields more accurate audio events (ACC), the perceptual\nmetrics (FAD, KL, IS, and A VC) consistently favor the full-\nscene Captioning method. These results suggest that context\nawareness is crucial for generating high-quality audio.\nChoice of segmentation module. We replace SAM with\nSAM 2 (Ravi et al., 2024), a more sophisticated segmen-\ntation method, and evaluate it on the test set. We show in\nTable 8 that this substitution leads to further gains in genera-\ntion accuracy and quality, which confirms that more precise\nsegmentation masks benefit our method and aligns well with\nTheorem 3.1.\nSynchformer-based metric. Inspired by Synchformer’s\ncontrastive pre-training (Iashin et al., 2024), we employ\n18\n--- Page 19 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nFigure 11: Human evaluation interface. We show the interface used for the subjective evaluation of generated audio\nsamples. Participants are presented with input text, an image, and a corresponding audio sample, and are instructed to rate\nthe audio on four criteria. All ratings must be completed before advancing to the next sample.\nMethod ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑)\nCaptioning & Mix 0.643 7.634 2.511 1.443 0.645\nCaptioning 0.587 2.778 1.364 1.901 0.773\nTable 7: Model performance comparison with and without\nmixing strategy.\nMethod ACC ( ↑) FAD ( ↓) KL ( ↓) IS ( ↑) A VC ( ↑)\nw/ SAM 0.859 1.271 0.517 2.102 0.891\nw/ SAM 2 0.881 1.153 0.472 2.295 0.936\nTable 8: Evaluation comparison between SAM and SAM 2\nmodules.\nImageBind (Girdhar et al., 2023) to measure audio-visual\nmatching on static images. By extracting features from both\nmodalities and computing cosine similarity, we show in\nTable 9 that our method consistently outperforms baselines\non this metric.\nE. Additional Dataset Evaluations\nVGG-Sound dataset. To further evaluate our method, we\nevaluate it on the VGG-Sound dataset (Chen et al., 2020),\nwhich contains in-the-wild audio-visual data collected from\nYouTube. Following (Chen et al., 2021a), we use VGG-\nSound Sync, a 14-hour subset that contains about 5,000Method IB ( ↑)\nGround Truth 0.66\nRetrieve & Separate (Zhao et al., 2018) 0.29\nAudioLDM 1 (Liu et al., 2023) 0.24\nAudioLDM 2 (Liu et al., 2024) 0.27\nCaptioning (Li et al., 2022a) 0.31\nMake-an-Audio (Huang et al., 2023b) 0.19\nIm2Wav (Sheffer & Adi, 2023) 0.33\nSpecVQGAN (Iashin & Rahtu, 2021) 0.37\nDiff-Foley (Luo et al., 2023) 0.39\nOurs 0.45\nTable 9: Comparison of ImageBind (IB) scores across dif-\nferent methods.\nvideo clips with better audio-visual synchronization, for test-\ning. To obtain captions aligned with this dataset, we apply\nthe same refinement procedure described in Appendix B.1.\nWe assess our model and all baselines (trained on AudioSet)\nusing the VGG-Sound Sync dataset. As presented in Ta-\nble 10, our method outperforms all baselines across multiple\nmetrics, particularly in ACC. These results indicate that our\nmodel generates more accurate audio that captures the com-\nplexity of each scene, while preserving audio quality.\nImageHear dataset. We also evaluate our method on the\nImageHear dataset (Sheffer & Adi, 2023), an image-to-\naudio benchmark comprising 100 web-sourced images span-\nning 30 visual categories (2–8 images per class). Although\n19\n--- Page 20 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nMethod ACC (↑) FAD (↓) KL(↓) IS(↑) A VC (↑)\nGround Truth / / / / 0.986\nRetrieve & Separate (Zhao et al., 2018) 0.143 4.731 1.726 1.782 0.713\nAudioLDM 1 (Liu et al., 2023) 0.256 3.876 1.634 1.901 0.634\nAudioLDM 2 (Liu et al., 2024) 0.401 3.114 1.137 1.915 0.687\nCaptioning (Li et al., 2022a) 0.491 2.378 1.089 2.001 0.763\nMake-an-Audio (Huang et al., 2023b) 0.395 3.436 1.571 1.876 0.721\nIm2Wav (Sheffer & Adi, 2023) 0.412 3.005 1.474 1.894 0.747\nSpecVQGAN (Iashin & Rahtu, 2021) 0.544 2.722 1.015 1.916 0.796\nDiff-Foley (Luo et al., 2023) 0.607 1.834 0.941 2.161 0.851\nOurs 0.761 1.112 0.675 2.342 0.898\nTable 10: Additional quantitative comparison of our method and baselines on the VGG-Sound Sync dataset.\nMethod CS ( ↑) ACC ( ↑)\nMake-an-Audio (Huang et al., 2023b) 27.44 0.77\nIm2Wav (Sheffer & Adi, 2023) 9.53 0.49\nSpecVQGAN (Iashin & Rahtu, 2021) 18.98 0.49\nDiff-Foley (Luo et al., 2023) 35.12 0.86\nOurs 47.37 0.88\nTable 11: Additional comparison of our method and base-\nlines on the ImageHear dataset.\neach image contains only a single object, which does not\nalign well with our object-aware setting, our method contin-\nues to outperform all baselines in both clip-score (CS) and\nACC, as reported in Table 11.\nF. Proof of Theorem 3.1\nProof. For notation simplicity, let uq∈∆Pdenote the\nsoftmax attention weight computed on query qsuch that\nuq,l=exp(⟨Ev(tq),Et(iq,l)⟩Σ)PP\nk=1exp(⟨Ev(tq),Et(iq,k)⟩Σ). We first state the follow-\ning lemma.\nLemma F.1. Under the same conditions in Theorem 3.1 of\nthe main paper, we have\nEq[∥uq−pq∥ℓ1]≤√\n2ϵcontrast\nProof. Notice that\nϵcontrast\n=Eq,d∼pq\"\n−logexp (⟨Ev(tq),Et(iq,d)⟩Σ)PP\nk=1exp (⟨Ev(tq),Et(iq,k)⟩Σ)#\n−Eq,d∼pq[−logpq,d]\n=Eq,d∼pq\u0014\nlogpq,d\nuq,d\u0015\n=Eq[DKL(pq,d∥uq,d)]\nwhere DKLdenotes the KL distance. By Pinsker’s inequal-ity and Cauchy-Schwarz inequality,\nϵcontrast =Eq[DKL(pq,d∥uq,d)]\n≥1\n2·Eq\u0002\n∥pq,d−uq,d∥2\nℓ1\u0003\n≥1\n2·(Eq[∥pq,d−uq,d∥ℓ1])2.\nIt follows that\nEq[∥uq−pq∥ℓ1]≤√\n2ϵcontrast .\nReturning to the proof of Theorem 3.1 in the main paper, let\nsq:=f(aq) =f(uqV)denote the audio output on query q\nby the trained model. We decompose errtestby\nerrtest\n=Eq[v(f∗(pqV∗),iq, pq)]−Eq[v(f∗(uqV∗),iq, pq)]| {z }\nA\n+Eq[v(f∗(uqV∗),iq, pq)]−Eq[v(f∗(aq),iq, pq)]| {z }\nB\n+Eq[v(f∗(aq),iq, pq)]−Eq[v(f(aq),iq, pq)]| {z }\nC\n+Eq[v(f(aq),iq, pq)]−Eq[v(f(aq),iq,mq)]| {z }\nD\n+Eq[v(f(aq),iq,mq)]−Eq[v(f(mqV),iq,mq)]| {z }\nE.\nBy Lemma F.1 and ∥V∗∥∞≤Bv, we have\nA≤Eq[Lv·Lf·Bv· ∥uq−pq∥ℓ1]\n≤Lv·Lf·Bv·√\n2ϵcontrast .\nSince∥V∗−V∥∞≤ϵVand∥uq∥1= 1, we have\nB=Eq[v(f∗(uqV∗),iq, pq)]−Eq[v(f∗(uqV),iq, pq)]\n≤Lv·Lf·ϵV.\n20\n--- Page 21 ---\nSounding that Object: Interactive Object-Aware Image to Audio Generation\nBy definition, C≤ϵf. Using the definition ϵsam=\nEq[∥mq−pq∥ℓ1], we have\nD≤Eq[Lv· ∥mq−pq∥ℓ1]\n≤Lv·ϵsam.\nand using ∥V∥∞≤Bvwith Lemma F.1,\nE≤Eq[Lv·Lf·Bv· ∥mq−uq∥ℓ1]\n≤Lv·Lf·Bv·(ϵsam+√\n2ϵcontrast ).\nCombining, we have\nerrtest≤Lv·Lf·\u0010\nϵV+Bv·\u0000\nϵsam+ 2√\n2ϵcontrast\u0001\u0011\n+Lv·ϵsam+ϵf.\nThis completes the proof.\n21",
  "text_length": 87035
}