{
  "id": "http://arxiv.org/abs/2506.01116v1",
  "title": "ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive\n  Uncertainty Estimation",
  "summary": "Large Language Models (LLMs) are widely used across various scenarios due to\ntheir exceptional reasoning capabilities and natural language understanding.\nWhile LLMs demonstrate strong performance in tasks involving mathematics and\ncoding, their effectiveness diminishes significantly when applied to\nchemistry-related problems. Chemistry problems typically involve long and\ncomplex reasoning steps, which contain specific terminology, including\nspecialized symbol systems and complex nomenclature conventions. These\ncharacteristics often cause general LLMs to experience hallucinations during\nthe reasoning process due to their lack of specific knowledge. However,\nexisting methods are struggling to effectively leverage chemical expertise and\nformulas. Moreover, current uncertainty estimation methods, designed to\nmitigate potential reasoning errors, are unable to precisely identify specific\nsteps or key knowledge. In this work, we propose a novel framework called\nChemAU, which incorporates our adaptive uncertainty estimation method that\napplies different uncertainty values based on the position of reasoning steps\nwithin the whole reasoning chain. Leveraging this method, ChemAU identifies\ngaps in chemistry knowledge and precisely supplements chemical expertise with\nthe specialized domain model, thereby correcting and updating the previously\nflawed reasoning chain. Our experiments with three popular LLMs across three\nchemistry datasets demonstrate that ChemAU significantly enhances both\nreasoning accuracy and uncertainty estimation.",
  "authors": [
    "Xinyi Liu",
    "Lipeng Ma",
    "Yixuan Li",
    "Weidong Yang",
    "Qingyuan Zhou",
    "Jiayi Song",
    "Shuhao Li",
    "Ben Fei"
  ],
  "published": "2025-06-01T18:45:49Z",
  "updated": "2025-06-01T18:45:49Z",
  "categories": [
    "cs.AI",
    "q-bio.QM"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01116v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01116v1  [cs.AI]  1 Jun 2025CHEM AU: H ARNESS THE REASONING OF LLM S IN CHEMICAL\nRESEARCH WITH ADAPTIVE UNCERTAINTY ESTIMATION\nA P REPRINT\nXinyi Liu1, Lipeng Ma1, Yixuan Li1, Weidong Yang1,∗, Qingyuan Zhou1, Jiayi Song1, Shuhao Li1, Ben Fei2,∗\n1Fudan University,2The Chinese University of Hong Kong\n24210240236@m.fudan.edu.cn, lpma21@m.fudan.edu.cn, wdyang@fudan.edu.cn, benfei@cuhk.edu.hk\nABSTRACT\nLarge Language Models (LLMs) are widely used across various scenarios due to their exceptional rea-\nsoning capabilities and natural language understanding. While LLMs demonstrate strong performance\nin tasks involving mathematics and coding, their effectiveness diminishes significantly when applied\nto chemistry-related problems. Chemistry problems typically involve long and complex reasoning\nsteps, which contain specific terminology, including specialized symbol systems and complex nomen-\nclature conventions. These characteristics often cause general LLMs to experience hallucinations\nduring the reasoning process due to their lack of specific knowledge. However, existing methods are\nstruggling to effectively leverage chemical expertise and formulas. Moreover, current uncertainty\nestimation methods, designed to mitigate potential reasoning errors, are unable to precisely identify\nspecific steps or key knowledge. In this work, we propose a novel framework called ChemAU , which\nincorporates our adaptive uncertainty estimation method that applies different uncertainty values\nbased on the position of reasoning steps within the whole reasoning chain. Leveraging this method,\nChemAU identifies gaps in chemistry knowledge and precisely supplements chemical expertise with\nthe specialized domain model, thereby correcting and updating the previously flawed reasoning chain.\nOur experiments with three popular LLMs across three chemistry datasets demonstrate that ChemAU\nsignificantly enhances both reasoning accuracy and uncertainty estimation.\n1 Introduction\nIn recent years, large language models (LLMs) have undergone dramatic advancement and demonstrated remarkable\nutility across multiple fields Topsakal and Akinci [2023], Achiam et al. [2023], Guo et al. [2025], such as natural\nlanguage processingZubiaga [2024], computer visionSapkota et al. [2024], legal and medical fields Li et al. [2024a],\nGoyal et al. [2024]. Beyond these applications, reasoning techniques such as chain-of-thought (CoT) Wei et al. [2022],\nself-reflection Renze and Guven [2024] have been developed to significantly enhance the inferential capabilities of\nLLMs, revealing their substantial potential in scientific domains. Notably, LLMs have been applied to assist in various\nchemistry tasks, including molecular property prediction Qian et al. [2023] and experimental protocol design Huang\net al. [2024a]. These applications demonstrate that LLMs possess significant potential for supporting chemical research\nand addressing chemistry-related problems.\nResearch in chemistry-focused LLMs predominantly follows two main approaches. The first approach follows the\npre-train and fine-tune paradigm, developing domain-specific models from scratch. These models are first pre-trained\non specialized chemical data, such as SMILES or SELFIES molecular Wang et al. [2019], Honda et al. [2019], Bagal\net al. [2021], to learn domain-specific features. They are then fine-tuned on task-specific datasets to optimize for\nobjectives like chemical toxicity prediction and drug solubility prediction Axelrod and Gomez-Bombarelli [2022].\nHowever, unlike general-purpose LLMs, which leverage vast amounts of general text data, these models are typically\nsmaller in scale and rely on limited, high-quality datasets curated through domain expertise Zhang et al. [2025]. This\nreliance on narrowly focused data restricts their scalability and flexibility, as they are optimized for specific tasks and\noperate within strictly defined input-output formats.\n∗Corresponding authors\n--- Page 2 ---\nChemAU A P REPRINT\n< Step 1 > The compound \"Yellow Prussiate of Potash\" is a common \nname for potassium ferricyanide , which has the chemical formula \nK3[Fe(CN)6]. \n< Step 2 > The molar mass of potassium ferricyanide is the sum of \nthe atomic masses of its constituent elements: K (39.09 g/mol), Fe \n(55.85 g/mol), and C (12.01 g/mol), N (14.01 g/mol), and 6x(12.01 \ng/mol + 14.01 g/mol) for the six cyanide groups. This gives a molar \nmass of 329.24 g/mol. \n< Step 3 > To find the number of moles in 80 grams of potassium \nferricyanide , we can divide the mass by the molar mass: 80 g / \n329.24 g/mol = 0.243 mol.\nHow many moles are there in 80 grams of \"Yellow Prussiate of Potash\"?\nLogit\n1         0.72\n2                   0.81\n3                   1.00\nStep\nFigure 1: Chemistry-specific tokens exhibit a distinctive pattern during the reasoning process: as reasoning steps\nincrease, their logit values progressively rise. In this chemistry problem, the LLM generates three consecutive reasoning\nsteps, with the chemistry token “ potassium ferricyanide ” appearing in each step. The data reveals that in the first\nreasoning step, the logit value for this token is 0.72. In the second step, it increases to 0.81. And by the third step, the\nlogit value further grows to 1.0. This phenomenon clearly demonstrates that as the reasoning chain increases, the logit\nvalues of specific chemistry tokens show an upward trend.\nThe second approach leverages general LLMs by instruction-tuning them with domain-specific chemical knowl-\nedge Zhang et al. [2024a], Li et al. [2025]. This method enhances the model’s expertise in chemistry while retaining\nits general-purpose capabilities. Unlike traditional small-scale models, these chemistry-specialized LLMs support\ndiverse input formats, flexible task requirements, and coherent dialogue capabilities. However, their large parameter\nsizes and the need for extensive domain-specific training data significantly increase computational costs and demand\nsubstantial resources. Moreover, this approach is typically feasible only for open-source LLMs, limiting broader\napplicability. Another emerging LLM-based approach is Retrieval-Augmented Generation (RAG) Asai et al. [2023],\nwhich integrates domain-specific knowledge into LLMs through information retrieval mechanisms rather than parameter\nupdates. While RAG reduces the need for extensive fine-tuning, the retrieved knowledge fragments often lack coherence\nand accuracy, introducing cognitive noise into the reasoning process. This fragmentation can negatively affect the\nmodel’s performance on complex chemistry tasks, where precise and contextual knowledge is crucial.\nTo address these challenges, we propose a novel framework that synergizes the powerful reasoning capabilities of\ngeneral LLMs with the specialized domain knowledge of chemistry-specific models. Drawing inspiration from recent\nadvancements in uncertainty estimation Liu et al. [2025], Huang et al. [2023], our framework incorporates a step-by-step\nuncertainty estimation mechanism. This mechanism dynamically evaluates the confidence of the general LLM at each\nreasoning step, identifying when to invoke the specialized model for domain-specific knowledge supplementation. By\nleveraging the complementary strengths of general and specialized models, our framework ensures both accuracy and\nreliability in tackling complex chemistry problems.\nRecent research on uncertainty estimation in LLMs has introduced methods such as token-level probability Ling et al.\n[2024], self-verbalized Tang et al. [2024], and semantic-similarity approaches Chen and Mueller [2023], which typically\nrely on predefined thresholds to classify responses as reliable or unreliable. However, in chemistry-specific tasks,\nwe observe a unique phenomenon: the logit values of chemistry-specific tokens, initially low, progressively increase\nas reasoning advances. This behavior, as illustrated in Figure 1, arises because LLMs, primarily trained on general\ncorpora, struggle to represent domain-specific symbols and nomenclature effectively. Over time, the model begins\nto treat these tokens as “active vocabulary”, artificially inflating their probabilities to maintain thematic consistency.\nThis phenomenon undermines the accuracy of existing uncertainty estimation methods, particularly in domains with\nunique terminologies like chemistry. To address this issue, we propose a novel step-wise uncertainty estimation method\nthat dynamically adjusts uncertainty values based on the position of each reasoning step within the reasoning chain.\nThis approach allows the model to better identify when domain-specific expertise is needed, facilitating more effective\ncollaboration between general and specialized models.\n2\n--- Page 3 ---\nChemAU A P REPRINT\nThe molecular weight of potassium ferricyanide \n(K4[Fe(CN)6]) calculated by summing the atomic \nweights of its constituent elements: K, Fe, C, N.The compound \"Yellow Prussiate of Potash\" is a \ncommon name for potassium ferricyanide, \nwhich has the chemical formula K3[Fe(CN)6].\nChemical\nKnowledge sOriginal Question: How many moles are there in 80 \ngrams of \"Yellow Prussiate of Potash\"?\nStart Thinking. General LLM\nStep1\nStep2\nThe molar mass of potassium ferricyanide is the \nsum of the atomic masses of its constituent \nelements: K (39.09 g/mol), Fe (55.85 g/mol), and \nC (12.01 g/mol), N (14.01 g/mol), and 6x(12.01 \ng/mol + 14.01 g/mol) for the six cyanide groups. \nThis gives a molar mass of 329.24 g/mol.Knowledge \nIncorrect .\nStep3Continue \nWrong \nReasoning.\nGet Final \nAnswer. \n(Wrong)Vanilla Reasoning Process\nTo find the number of moles in 80 grams of \npotassium ferricyanide, we divide the mass by \nthe molar mass: 80 g / 329.24 g/mol ≈ 0.243 mol .General LLM\nStep1\nStep2\nStep3\nThe atomic weight of K is 39.09 g/mol, Fe is \n55.85 g/mol, C is 12.01 g/mol, and N is 14.01 \ng/mol. The molecular weight of K4[Fe(CN)6] is \n4(39.09) + 55.85 + 6(12.01) + 6(14.01) = 156.36 \n+ 55.85 + 72.06 + 84.06 = 368.33 g/mol.\nStep4\nTo find the number of moles in 80 grams of \npotassium ferricyanide, we divide the mass by the \nmolecular weight: 80 g / 368.33 g/mol ≈ 0.217 mol.ChemAU Reasoning \nProcess (Ours)\nUncertain\nChemistry Model\nUncertain \nEstimation\nStart Thinking.\nKnowledge \nIncorrect .\nContinue \nCorrect \nReasoning \nSteps.\nGet Final \nAnswer. \n(Correct)\nCalculate \nscores\nCertainUncertain \nEstimation\nCalculate \nscoresPotassium ferricyanide has the \nchemical formula K4[Fe(CN)6].\nThe compound \"Yellow Prussiate of Potash\" is a \ncommon name for potassium ferricyanide, \nwhich has the chemical formula K3[Fe(CN)6].\nFigure 2: Overview of our proposed LLM reasoning framework with Adaptive Uncertainty estimation specifically\ndesigned for Chem istry problems ( ChemAU ). The left side illustrates the reasoning process of a conventional general\nLLM when solving chemistry problems, which failed to detect the error in the chemical formula “ K3[Fe(CN)6]”,\nultimately resulting in an incorrect inference. The right side demonstrates how our proposed framework successfully\nidentifies the error in Step1 through an uncertainty estimation mechanism, subsequently redirecting the query to a\nspecialized chemistry domain model to obtain accurate domain knowledge. Specifically, the correct molecular formula\n“K4[Fe(CN)6]”. This accurate information is then reintroduced into the general LLM, ultimately yielding the correct\nanswer.\nAs shown in Figure 2, we introduce ChemAU , a novel LLM reasoning framework with Adaptive Uncertainty estimation\nspecifically designed for Chem istry reasoning tasks. ChemAU employs a general LLM to generate a reasoning chain\nfor a given chemistry question and sequentially evaluates the uncertainty of each reasoning step. Steps identified with\nhigh uncertainty—often associated with unfamiliar chemistry-specific tokens—trigger the specialized chemistry model\nto analyze the accuracy of the current step and provide relevant domain knowledge. The provided domain-specific\nknowledge is then integrated into the reasoning process to guide subsequent steps, ensuring accurate and contextually\nrelevant outputs.\nWe evaluate the ChemAU framework with three popular general LLMs ( Qwen2.5-7B-Instruct Yang et al. [2024], LLaMA-\n3-8B-Instruct Grattafiori et al. [2024], DeepSeek-R1-Distill-Qwen-14B Guo et al. [2025]) across three distinct chemistry\ndatasets (GPQA Rein et al. [2024], MMLU-Pro Wang et al. [2024], SuperGPQA Du et al. [2025]). Experimental results\ndemonstrate that our proposed framework significantly improves the performance of LLMs on chemistry problems. Our\ncontributions can be listed as follows:\n•We propose an LLM reasoning framework with Adaptive Uncertainty estimation specifically designed for\nChem istry problems ( ChemAU ), which combines the powerful reasoning capabilities of general LLMs with\nthe precise domain knowledge of specialized chemistry models. To the best of our knowledge, this is the first\nframework to introduce a model collaboration strategy for chemistry reasoning tasks.\n•We identify a unique phenomenon in chemistry-specific problems where the logit values of domain tokens\nprogressively rise during reasoning, leading to inaccurate uncertainty estimation with existing methods. To\naddress this, we propose a step-wise uncertainty estimation method that dynamically adjusts uncertainty values\nbased on their position within the reasoning chain.\n•Extensive experiments across multiple general LLMs and chemistry datasets demonstrate that ChemAU\nsignificantly improves performance in chemistry reasoning tasks, highlighting its potential for advancing\ndomain-specific applications.\n2 Related Work\nUncertainty Estimation of LLMs. Due to the substantial computational expense of running inference on LLMs, the\nLLM community has largely moved away from traditional uncertainty estimation methods used for learned models Bala-\n3\n--- Page 4 ---\nChemAU A P REPRINT\nbanov and Linander [2024], Shorinwa et al. [2024]. Instead, researchers have developed less computationally demanding\napproximate techniques that leverage the distinctive architecture of LLMs to evaluate model uncertainty Azaria and\nMitchell [2023], Yang et al. [2024], Huang et al. [2023], Zhang et al. [2024b]. These uncertainty estimation approaches\nfor LLMs can be generally divided into two categories: white-box models and black-box models Fadeeva et al. [2023].\nWhite-box uncertainty estimation methods leverage access to intermediate outputs from the underlying system, such\nas the probability distributions across generated tokens, to evaluate and quantify the model’s uncertainty Manakul\net al. [2023], Fadeeva et al. [2024]. In contrast, black-box uncertainty estimation methods assess a model’s uncertainty\nsolely from its responses to input prompts. These methods typically require evaluating the similarity between multiple\nresponses generated by either a single LLM or an ensemble of LLMs Liu et al. [2019], Reimers and Gurevych [2019],\nZhang et al. [2019]. However, the characteristic of black-box methods requiring multiple generations determines that\ntheir usage consumes substantial computational resources. Our proposed uncertainty estimation method only requires a\nsingle generation for white-box models, greatly improving computational efficiency.\nThere are mainly two kinds of uncertainty: aleatoric uncertainty and epistemic uncertainty Hou et al. [2023]. Aleatoric\nuncertainty occurs in situations where there is inherent randomness and noise in the data-generating process, which\nmeans it is inevitable Min et al. [2020], Kuhn et al. [2022]. In contrast, epistemic uncertainty can be reduced or\neliminated because it occurs due to a lack of knowledge or limited training data. Our proposed framework focuses on\naddressing the problem of inaccurate responses generated by LLMs due to knowledge deficiencies.\nApplications of LLMs in Chemistry. LLMs have gained impressive success in natural language understanding\nand complex question reasoning Hadi et al. [2023], Li et al. [2024b]. Its strong abilities have been applied in various\nkinds of chemistry related tasks Jablonka et al. [2024], Li et al. [2024c], Boiko et al. [2023]. The current approach for\nchemistry-specific LLMs, like ChemLLM Zhang et al. [2024a] and ChemDFM Zhao et al. [2024], involves collecting\ndata from research papers and textbooks, pre-training domain knowledge on general LLMs, and then fine-tuning\nwith chemical instructions. These models perform excellently on chemistry-related tasks and demonstrate significant\nadvantages in terms of accuracy and depth of domain knowledge Huang et al. [2024b]. However, precisely because\nof their heightened focus on chemistry domain knowledge and tasks, they may emphasize factual knowledge and\nprofessional task performance over reasoning processes, potentially limiting their reasoning capabilities compared to\ngeneral LLMs.\n3 Method\nChem ical Research with Adaptive Uncertainty Estimation ( ChemAU ) aims to enhance the reasoning performance of\nLLMs in the chemistry domain through uncertainty estimation and precise supplementation of missing knowledge. As\ndepicted in Figure 2, we first input a chemistry problem into the general LLM to generate an initial reasoning chain.\nSubsequently, we perform uncertainty estimation individually for each reasoning step. For the step with high uncertainty,\nwe utilize domain-specific models to provide accurate chemistry knowledge related to the concepts mentioned. This\naccurate information, along with the previous correct reasoning steps, is then input to the general LLM, prompting the\nmodel to regenerate a more reliable subsequent reasoning chain. Our method consists of three components: Adaptive\nUncertainty Estimation, Extraction and Supplementation of Chemistry Knowledge in a Single Reasoning Step, and\nAdjustment of Reasoning Steps.\n3.1 Adaptive Uncertainty Estimation\nFor general problems, probability-based methods commonly employ length-normalized scoring for uncertainty measure-\nment Malinin and Gales [2020], wherein equal weighting is applied to all tokens in the generated sequence, with these\nweights being inversely correlated to the total sequence length. To mitigate the impact of probability from function\nwords such as “the”, “an”, and “of” on uncertainty estimation, several studies have proposed determining tokens’\nweights based on their semantic contribution to the entire sentence Bakman et al. [2024], Duan et al. [2023]. However,\nmodels detecting semantic similarity may struggle to discern differences in domain-specific knowledge between\nsentences due to the lack of chemistry expertise in their pre-training data. More importantly, they may incorrectly\nidentify two sentences as semantically equivalent even when their key components are fundamentally different, leading\nto a misjudgment, as shown in Appendix.\nAnother probability-based method uses the maximum value of the negative log probabilities of all tokens as an\nuncertainty estimation metric Manakul et al. [2023], Shorinwa et al. [2024]:\nMax(−log p) =max\nj−log(pj), (1)\nwhich evaluates by identifying the token with the lowest likelihood. However, this method does not perform well\nin chemistry domain. Based on our observations, we find a notable phenomenon: when LLMs generate responses\n4\n--- Page 5 ---\nChemAU A P REPRINT\nto chemistry-specific inquiries, chemistry-specialized tokens typically exhibit markedly low logit values upon initial\nappearance. However, as the chain of reasoning extends and develops, the frequency of these specialized terms tends\nto rise, accompanied by a corresponding increase in their logit scores, as illustrated in Figure 1. This phenomenon\nmay stem from the fact that LLMs undergo training mainly on general textual data, while the chemistry domain\noften incorporates unique symbolic systems and specialized expressions that appear infrequently in common texts,\nresulting in limited representation learning. As the reasoning process continues, the model progressively recognizes\nthese chemistry-specific terms as contextually relevant tokens, consequently enhancing their prediction probabilities to\npreserve topical coherence.\nThe methods mentioned above demonstrate good performance on general problems, typically employing a predefined\nthreshold where expressions with uncertainty above it are considered potentially erroneous. However, according to\nour findings, applying a fixed threshold to reasoning steps at different positions within the reasoning chain presents\nsignificant limitations when facing chemistry-related problems.\nTo address these challenges, we propose a novel uncertainty estimation method specifically designed for LLMs’\nreasoning in the chemistry domain. This method dynamically assigns distinct uncertainty values to each reasoning step\nbased on its position within the overall reasoning chain:\nUi(R,Pi) =max\nj−log(pij) +α(LR−i), (2)\nwhere Uiquantifies the uncertainty of the i-th reasoning step, Rrepresents the complete reasoning chain, αrepresents\na predefined constant, LRrepresents the number of reasoning steps, and Piindicates the probabilities of all tokens\n(denoted as pijfor each token) within the i-th step. If the uncertainty exceeds a predefined threshold θ, formally\nexpressed as: ifUi(R, P i)> θ, indicating that this reasoning step exhibits a high likelihood of containing potential\nerrors and requires further processing.\n3.2 Extraction and Supplementation of Chemistry Knowledge in a Single Reasoning Step\nFor a potentially erroneous step, we decompose it into multiple units of atomic chemistry knowledge for analysis and\ncorrection. Through this fine-grained decomposition, we can precisely identify and supplement knowledge deficiencies\nin the chemistry domain that LLMs exhibit during reasoning chain generation. Specifically, we construct a chemistry\nknowledge dataset and use it to perform instruction fine-tuning on Qwen2.5-1.5B-Instruct Yang et al. [2024], developing\na specialized chemistry domain model. This model evaluates the accuracy of input chemistry knowledge points. When\nit detects inaccuracies or incompleteness, it promptly provides corresponding precise and comprehensive chemistry\nknowledge in response. By feeding decomposed atomic-level chemistry knowledge points into this chemical-specialized\nmodel, its output can accurately remedy knowledge gaps in the general LLM within the chemistry domain, effectively\ncorrecting the current reasoning step. This strategy, instead of exhaustively verifying all chemistry knowledge, allows\nus to concentrate computational resources only on the steps that are most likely to contain errors, precisely identifying\nand supplementing missing knowledge where needed, thereby improving the overall efficiency of reasoning.\n3.3 Adjustment of Reasoning Steps\nChemistry problems typically exhibit the following characteristics:\n• They involve long chains of thought, which require multiple reasoning steps to solve a problem.\n• The reasoning steps are tightly interconnected, where each step usually serves as the foundation for the next.\nAs shown in the example provided in Figure 3, the correctness of the “ moles ” in < Step 3 > directly depends on\nthe accuracy of “ molar mass ” from < Step 2 >, which bases on the exactness of “ chemical formula ” from < Step 1\n>. Therefore, the certainty for earlier reasoning steps should be stricter, precisely matching the dynamic uncertainty\nestimation approach introduced in Section 3.1 that assigns higher uncertainty values to reasoning steps at earlier\npositions, rather than applying the same value across all reasoning steps.\nFor the entire reasoning process, we propose a reasoning framework shown in Figure 2. First, we input a specific\nchemistry problem into the general LLM to generate an initial reasoning chain. Subsequently, we implement a\nsequential detection strategy throughout the reasoning chain, evaluating each step’s uncertainty value in order. When\nthe uncertainty value is low, we consider the step correct and add it to the set of confirmed reasoning steps. Once we\nidentify a reasoning step whose uncertainty value exceeds the predetermined threshold, we exclusively focus on that\nparticular step, temporarily setting aside subsequent reasoning processes. For this high-uncertainty step, we perform\natomic chemistry knowledge extraction, generating corresponding expertise through the chemistry-specific model\nmentioned in Section 3.2 to bridge the knowledge gap in the general LLM. Then, we reintroduce the newly acquired\n5\n--- Page 6 ---\nChemAU A P REPRINT\nReasoning Chain\n< Step 1 > The compound \"Yellow Prussiate of Potash\" is a common name for potassium ferricyanide, \nwhich has the chemical formula K3[Fe(CN)6]. \n< Step 2 > The molar mass of potassium ferricyanide is the sum of the atomic masses of its \nconstituent elements: K (39.09 g/mol), Fe (55.85 g/mol), and C (12.01 g/mol), N (14.01 g/mol), and \n6x(12.01 g/mol + 14.01 g/mol) for the six cyanide groups. This gives a molar mass of 329.24 g/mol. \n< Step 3 > To find the number of moles in 80 grams of potassium ferricyanide, we can divide the \nmass by the molar mass: 80 g / 329.24 g/mol ≈0.243 mol.\nFigure 3: In chemistry problem-solving, reasoning steps are closely interconnected, with each subsequent step typically\nbuilding upon the results of the previous one. For instance, in the illustrated example, the molar amount “0.243”\n(calculated in < Step 3 >) is derived from the molar mass “329.24” (determined in < Step 2 >), which in turn is based on\nthe molecular formula “ K3[Fe(CN)6]” (identified in < Step 1 >). Consequently, errors in the first step will propagate\nthrough the entire reasoning chain, compromising the accuracy of all subsequent calculations, similar to a domino\neffect.\nchemistry knowledge, the original chemistry problem, and the confirmed reasoning steps back into the general LLM to\ncontinue generating a new reasoning chain. This iterative process continues, forming a complete reasoning procedure\nthat adjusts to the chemistry domain problem.\n4 Experiment\n4.1 Experimental Settings\nModels. In our experiments, we utilize three different series of open-source LLMs, including Qwen2.5-7B-\nInstruct Yang et al. [2024], LLaMA-3-8B-Instruct Grattafiori et al. [2024], and DeepSeek-R1-Distill-Qwen-14B Guo\net al. [2025], with model sizes of 7B, 8B, and 14B. Additionally, based on our constructed chemistry domain dataset,\nwe perform instruction fine-tuning on the Qwen2.5-1.5B-Instruct Yang et al. [2024] to serve as the domain-specific\nmodel for our experiments.\nDatasets. We utilize three distinct datasets, specifically GPQA Rein et al. [2024], MMLU-Pro Wang et al. [2024], and\nSuperGPQA Du et al. [2025], from which we extract chemistry-related questions for our experimental analysis.\nThey contain complex chemistry domain problems that require both multi-step reasoning processes and specialized\nknowledge, creating a strong contrast with commonsense questions and providing an ideal testing scenario for evaluating\nLLMs’ reasoning capabilities in chemistry.\nBaselines. We evaluate all models using Chain-of-Thought (CoT) prompting to encourage step-by-step reasoning,\napplying identical prompting templates across all experiments to ensure fair comparison. Templates are shown in\nAppendix. For each dataset, we measure the answer accuracy as our primary evaluation metric.\n•General LLMs Performance. In Table 1, we present the performance of general LLMs on chemistry\nreasoning tasks without any specialized enhancement. This baseline serves as a reference point to demonstrate\nthe effectiveness of our uncertainty-driven adaptive reasoning framework.\n•Domain-Specific Model Performance. We evaluate the performance of our constructed chemistry domain\nmodel by directly answering questions without the reasoning support of general LLMs. As shown in Table 1,\nwhen the domain model independently addresses complex chemistry problems, its accuracy rate is relatively\nlow, strongly proving that the domain model’s value lies not in directly enhancing the general model’s reasoning\nprocess, but rather in providing specialized chemistry domain knowledge.\n•Retrieval-Augmented Generation Approach. We explore the Retrieval-Augmented Generation (RAG)\napproach by retrieving relevant chemistry knowledge based on specific problems and feeding both the problems\nand retrieved knowledge to the general model to generate reasoning steps and answers. However, as shown in\nFigure 4, the performance using RAG is actually inferior to using the general model alone. This may primarily\n6\n--- Page 7 ---\nChemAU A P REPRINT\nbecause the knowledge retrieved by RAG is too broad and may not precisely identify the domain knowledge\nthat the model actually lacks or needs. When the provided knowledge is not closely relevant, it may mislead\nthe model’s reasoning process, resulting in decreased accuracy. This experiment further demonstrates the\nimportance of precisely identifying and supplementing the critical domain knowledge that the general model\nlacks during the reasoning process, which is precisely the role of the domain model triggered by uncertainty\ndetection.\nTable 1: Performance Comparison of General LLMs and Domain-Specific Model on Chemistry Reasoning Tasks. This\ntable presents the accuracy (%) of three instruction-tuned open-source LLMs ( LLaMA-3 ,Qwen2.5 ,DeepSeek-R1 ) and\nthe self-constructed chemistry domain model across three chemistry reasoning datasets. These results establish baseline\nperformance for evaluating the effectiveness of our proposed adaptive uncertainty-driven reasoning framework.\nDatasets LLaMA-3 Qwen2.5 DeepSeek-R1 Domain Model\nGPQA 20.43% 19.35% 22.58% 6.45%\nMMLU-Pro 27.44% 36.85% 49.60% 11.25%\nSuperGPQA 15.48% 13.79% 16.28% 9.89%\nLLaMA-3 Qwen2.5 DeepSeek-R1515253545Performance\nRAG General Model ChemAU22.5820.4339.78\n20.4319.3531.18\n6.4522.5823.65\n(a) GPQA\nLLaMA-3 Qwen2.5 DeepSeek-R12030405060Performance\nRAG General Model ChemAU30.95\n27.4453.56\n33.3336.8547.31\n21.6349.6052.06 (b) MMLU-Pro\nLLaMA-3 Qwen2.5 DeepSeek-R1010203040Performance\nRAG General Model ChemAU15.65 15.4838.72\n13.39 13.7926.28\n9.6616.2820.23 (c) SuperGPQA\nFigure 4: Performance comparison of different reasoning approaches across three chemistry datasets. Each sub-figure\nshows the accuracy (%) of three different reasoning methods (RAG, General LLMs only, and our proposed framework\nChemAU) on a specific dataset. The results demonstrate that ChemAU consistently outperforms other approaches\nacross all tested general models on all evaluation datasets.\n4.2 Performance Evaluation on Different LLM Backbones\nAs illustrated in Figure 4, our method significantly improves the accuracy of general LLMs on chemistry domain\nproblems. For instance, on the MMLU-Pro dataset, our framework achieves an accuracy of 53.56% when using\nLLaMA-3 as general model, representing a 26.12% improvement over using the model alone and 22.60% improvement\nover using the RAG approach. Notably, this performance even surpasses that of the larger 14B parameter model,\ndemonstrating the effectiveness of our framework.\n4.3 Comparison of Uncertainty Estimation Methods\nFigure 5 illustrates the performance comparison between two uncertainty estimation methods within our framework.\nThe results clearly demonstrate that our proposed dynamic uncertainty estimation method consistently outperforms\nMax(−logp)across various general LLMs and datasets, substantially enhancing the overall framework performance.\nThese findings confirm that our uncertainty estimation method can more precisely and promptly identify the missing\ndomain knowledge in the reasoning chain, thereby effectively improving the model’s accuracy.\n5 Ablation Studies\nNecessity of the Domain Model. To investigate whether the specialized chemistry domain model provides chemistry\nknowledge that effectively compensates for the knowledge gaps in the general LLM, thereby enhancing its ability\nto solve chemistry reasoning problems, rather than the general model simply improving through self-reflection and\niterative optimization to increase chemistry reasoning accuracy, we design and conduct an ablation experiment targeting\nthe domain model component. This experiment aims to verify whether the key role of domain models lies in providing\n7\n--- Page 8 ---\nChemAU A P REPRINT\nGPQA MMLU-Pro SuperGPQA25354555Performance\nMax(p) Our method30.1039.7846.5253.56\n29.4538.72\n(a) LLaMA-3\nGPQA MMLU-Pro SuperGPQA25354555Performance\nMax(p) Our method30.1031.1844.4147.31\n21.7026.28 (b) Qwen2.5\nGPQA MMLU-Pro SuperGPQA204060Performance\nMax(p) Our method22.58 23.6551.27 52.06\n19.44 20.23 (c) DeepSeek-R1\nFigure 5: Performance comparison of uncertainty estimation methods on chemistry reasoning tasks. The figure\npresents accuracy results (%) across three general LLMs evaluated on three chemistry datasets. Our proposed dynamic\nuncertainty estimation method (dark bars) consistently outperforms the Max(−logp)approach (light bars) across all\nexperimental settings, with a maximum improvement of 9.68%. The dynamic method demonstrates superior capability\nin identifying high-uncertainty reasoning steps where domain knowledge integration is required.\nspecialized chemistry knowledge that general models lack, rather than simply facilitating the general model’s self-\nimprovement process.\nIn this experiment, we remove the chemistry domain model from our proposed reasoning framework while maintaining\nthe uncertainty estimation method to identify high-risk reasoning steps. Instead of leveraging domain expertise for\nknowledge correction, we provide the general model with only the original reasoning steps, potential error information,\nand the original chemistry problem, requesting it to regenerate the reasoning chain iteratively. As shown in Figure 6,\nthis ablation study demonstrates a significant performance decline compared to our complete reasoning framework,\nwith accuracy rates dropping by up to 16.39%. These results empirically validate that general LLMs, even when\nprompted with uncertainty information, struggle to independently correct domain-specific reasoning errors. They often\npossess knowledge gaps or conceptual misunderstandings in specialized domains that cannot be resolved through simple\nre-reasoning processes. Our findings strongly highlight the necessity of integrating domain-specific knowledge through\nspecialized models, confirming that our proposed framework effectively bridges the gap between general reasoning\ncapabilities and specialized domain expertise in solving complex chemistry problems.\nNecessity of Step-wise Uncertainty Detection. To validate the efficacy of our step-wise approach, we design an\nablation experiment that examines the necessity of fine-grained uncertainty estimation and targeted knowledge injection\nduring the reasoning process. Our proposed framework fundamentally innovates through its step-wise methodology that\nboth identifies specific reasoning steps with high uncertainty and precisely supplements chemistry-specific knowledge\nexactly where needed. We hypothesize that treating the entire reasoning chain as a unit may compromise both\nuncertainty detection and knowledge supplementation processes. When evaluated as a whole, critical knowledge\ndeficiencies at individual steps might be obscured, and the domain model may not be able to provide precise, step-\nspecific knowledge corrections. This could lead to either overlooking crucial knowledge gaps or introducing broadly\ngeneralized information that fails to address specific reasoning errors, ultimately resulting in less effective knowledge\nintegration and compromising the overall reasoning quality.\nIn this ablation study, instead of performing uncertainty estimation on individual reasoning steps, we treat the entire\nreasoning chain as a single unit, which is directly fed into the domain model to obtain relevant knowledge. Subsequently,\nthe original question, along with this knowledge supplement, is reintroduced to the general model to regenerate the\nentire reasoning chain. As illustrated in Figure 6, this variant performs worse than our proposed step-wise framework,\nwhich validates that fine-grained uncertainty detection at each reasoning step enables more precise identification of\nknowledge gaps in the general model’s reasoning. The step-wise approach allows for targeted and timely domain\nknowledge injection exactly where it is needed, avoiding overwhelming the model with potentially irrelevant domain\ninformation.\n6 Conclusion\nIn this paper, we observe that LLMs often suffer from hallucinations and poor performance when answering domain-\nspecific questions due to insufficient specialized knowledge. To address this issue, we propose a reasoning framework\nfor the chemistry domain that incorporates a novel adaptive uncertainty estimation method. Experimental results\ndemonstrate that our framework significantly improves the accuracy of LLMs on chemistry-related questions, enabling\nsmaller models to achieve or even exceed the performance of models with larger parameter configurations. Limitations\nand Future Work. The proposed adaptive uncertainty estimation method is only applicable to open-source LLMs. For\n8\n--- Page 9 ---\nChemAU A P REPRINT\nGPQA MMLU-Pro SuperGPQA2030405060Performance\nChemAU Without Domain Model39.7853.56\n38.72\n25.8038.69\n22.32\nGPQA MMLU-Pro SuperGPQA25354555Performance\nStep-wise Approach Chain-level Approach39.78\n29.0353.56\n44.15\n38.72\n28.26\nFigure 6: (Left) The comparisons of the accuracy (%) of our complete reasoning framework against a variant without\nthe specialized chemistry domain model across different chemistry datasets. The variant still identifies high-uncertainty\nreasoning steps but is resolved through iterative general model re-reasoning rather than domain knowledge integration.\nResults show significant performance degradation (up to 14.87% accuracy drop) when domain expertise is removed.\n(Right) The comparisons of the accuracy results (%) of our proposed step-wise approach against a variant that treats\nthe entire reasoning chain as a single unit across different general models and chemistry datasets. In this chain-level\napproach, the complete reasoning chain is evaluated as one unit before domain knowledge integration, rather than\nassessing uncertainty at individual reasoning steps. Results demonstrate that the chain-level method diminishes the\nstep-wise approach’s effectiveness by an average of 10.20%.\nblack-box models, this method cannot be directly applied since it’s impossible to access the logit values of generated\ntokens. Future research could develop more universally methods.\n9\n--- Page 10 ---\nChemAU A P REPRINT\nAppendix\nA Performance between Different Uncertainty Estimation Methods\nIn this section, we compare existing token-based uncertainty estimation methods for open-source LLMs. Token-based\nuncertainty estimation methods typically utilize the logit values of tokens provided by open-source LLMs. Following\nstatistical principles, the basic method is to multiply the logit values of all generated tokens to reflect the overall\nconfidence of the LLM in the entire generated sentence. The formula can be expressed as follows:\nBase (i) =LiY\nj=1pij, (3)\nwhere Lirepresents the length of generated tokens, and pijrepresents the logit value of each token. For consistent\nexpression, we convert it to logarithmic form, which can be expressed as follows:\nLogBase(i) =LiX\nj=1log(pij). (4)\nHowever, it typically exhibits poor performance when evaluating long-form responses, as the product of token\nprobabilities inherently diminishes with response length, even when these longer responses are semantically equivalent\nto their shorter counterparts. To mitigate this limitation, researchers have developed several variant methods to\neffectively reduce the dependency between the metrics and the sequence length. The comparison of these methods is\nshown in Figure 7.\nA.1 Length-Normalized Scoring Function\nThe length-normalized method aims to distribute uncertainty across each token to mitigate the impact of sentence length\non uncertainty calculations Manakul et al. [2023]. This method can be mathematically expressed as follows:\nLN(i) =LiY\nj=1pij1\nLi. (5)\nThe corresponding logarithmic form is:\nLogLN(i) =1\nLiLiX\nj=1log(pij), (6)\nwhich is essentially identical to the base method mentioned above, with the only difference being that it applies an\naveraging process according to sentence length, ensuring that sentences of varying lengths can be evaluated using\ncomparable uncertainty assessment criteria.\nA.2 Semantic Contribution Weighting\nThe length-normalized approach treats each token equally, meaning they contribute identically to the uncertainty\nestimation value. In reality, different words can have varying impacts on a sentence’s meaning within the question\ncontext, particularly for connective words such as “the,” “an,” “of,” and similar tokens, while the key tokens are the\nones truly answering the question. Therefore, some researchers suggest that when calculating uncertainty values, tokens\nshould be assigned different weights based on their semantic contribution level. They typically employ additional\nsemantic detection models to specifically compare the semantic similarity between the original sentence and the\nsentence with certain tokens removed. If the similarity is high, it indicates that the token does not significantly affect\nthe sentence’s meaning, thus, it should be assigned a lower weight in uncertainty estimation. If the similarity is low, it\nsuggests that the token is crucial to the essential meaning of the sentence, therefore, it should be given a higher weight\nin uncertainty estimation Duan et al. [2023]. The specific formula is as follows:\nSCW (i) =LiY\nj=1pijwij, (7)\n10\n--- Page 11 ---\nChemAU A P REPRINT\nWhat is the largest planet in the Solar System?\nJupiter is the largest planet in the Solar System. \nJupiter\n0.8is\n0.98the\n0.99largest\n0.95planet\n0.97in\n0.99the\n0.99Solar\n0.98System\n0.98\nBase                ς𝑗=19𝑝𝑖𝑗= 0.589\nThe long sentence length results in an \nunderestimated value\nLength-Normalized   ς𝑗=19𝑝𝑖𝑗1\n9= 0.94Excessive use of conjunctions causes \nabnormally high value\nMeaning -Aware       ς𝑗=19𝑝𝑖𝑗𝑤𝑖𝑗= 0.826\n Appropriate values\nJupiter\n0.994is\n0.001the largest planet\n0.003the Solar System\n0.001in\n0.001\nFigure 7: Different uncertainty estimation methods demonstrate varying performance on the same sentence in everyday\nlanguage corpora. The Base method is sensitive to sentence length influence, often resulting in underestimated\nuncertainty values that may incorrectly classify reliable answers as unreliable. While the Length-Normalized method\naverages values based on token count from the Base method, the abundance of conjunctions with high logit values\ntends to produce overestimated uncertainty values, potentially misclassifying unreliable answers as trustworthy. In\ncontrast, the Meaning-Aware method calculates weights according to each token’s semantic contribution, yielding more\nreasonable and accurate uncertainty assessment values.\nwhere wijrepresents the weight of token pij. The corresponding logarithmic form is:\nLogSCW(i) =wijLiX\nj=1log(pij). (8)\nHowever, this method performs poorly on chemistry problems for the same reason we mentioned in the main text\nregarding why LLMs don’t perform as well on chemistry questions as they do on general topics: semantic similarity\ndetection models are primarily trained on everyday general corpora, with relatively limited chemistry-related content.\nConsequently, these models cannot accurately identify the semantic impact and importance of key chemistry tokens\nwithin a sentence. More critically, even when key chemistry tokens express entirely different meanings, these models\nfail to detect the semantic differences, as illustrated in Figure 8.\nB Computer Resources and Experiment Details\nFor both general LLMs and the chemistry domain model, inference is performed with a temperature of 0.3 and top-k\nsampling, retrieving 4 candidate tokens per position. The general LLM is configured with a maximum sequence length\nof 1024 tokens, while the chemistry domain model uses a reduced maximum sequence length of 100 tokens. GPU\nmemory utilization is set to 0.6 for the general LLM and 0.2 for the chemistry domain model. The basic threshold\n11\n--- Page 12 ---\nChemAU A P REPRINT\nThe ortho -chlorophenol  exhibits distinct hydrogen bonding \ninteractions due to the proximity of its chlorine atom to the \nhydroxyl group\nThe para -chlorophenol  exhibits distinct hydrogen bonding \ninteractions due to the proximity of its chlorine atom to the \nhydroxyl groupSentence Pair Similarity Score\n0.868\nThe 2-chloro -4-nitrobenzoic  acid demonstrates exceptional \nreactivity in nucleophilic substitution reactions\nThe 4-chloro -2-nitrobenzoic  acid demonstrates exceptional \nreactivity in nucleophilic substitution reactions0.998\nFundamental Difference\nThe chlorine atoms in ortho -chlorophenol and para -\nchlorophenol occupy different positions , which allows \northo -chlorophenol to form intramolecular hydrogen \nbonds while para -chlorophenol cannot, resulting in \nsignificant differences in their chemical reactivity and \nphysical properties. \nThe structural difference between 2 -chloro -4-nitrobenzoic \nacid and 4 -chloro -2-nitrobenzoic acid lies in the exchanged \npositions of the chlorine atom and nitro group . This \nstructural difference causes 4 -chloro -2-nitrobenzoic acid to \nundergo decarboxylation more readily under basic \nconditions, a reaction property that 2 -chloro -4-nitrobenzoic \nacid does not possess.\nFigure 8: Models designed to assess semantic similarity perform poorly when evaluating sentences containing specific\nchemical terminology. We utilize the sentence-transformers/all-MiniLM-L6-v2 for testing. The two examples above\nclearly illustrate this issue: despite these sentences having fundamental differences in chemical expressions that should\nclassify them as completely unrelated, the model fails to identify their significant distinctions and instead incorrectly\nevaluate them as semantically almost identical. This indicates notable limitations in current models when processing\ntext from specialized chemistry domains.\nvalue is set to -1.5, while the hyperparameter α, which incorporates the relative positions of reasoning steps into the\nuncertainty estimation model, is set to -0.08. All experiments are conducted using NVIDIA A100 (80GB) GPUs.\nC Prompt Template\nFor all experiments, we employ identical prompt templates to ensure fairness and reliability in our evaluations.\nSpecifically, we utilize two distinct prompt templates throughout our experiments. The first is for guiding general LLMs\nto generate reasoning on initial chemistry problems. The second is for reintroducing the acquired knowledge, correct\nreasoning steps, and original chemistry questions back into the general LLM to continue generating more accurate\nreasoning processes, as shown in Figures 9 and 11.\nD Addition to Experiment and Ablation Section\nFigure 12 demonstrates the comparison of different knowledge augmentation strategies. Precise and relevant knowledge\ncan effectively guide models toward correct reasoning, while broad or irrelevant knowledge misleads models, resulting\nin erroneous reasoning outcomes. This finding explains why the RAG approach performed worse than using the general\nLLM alone in our experiments.\nFor the ablation experiments, we still conduct ablation studies on the domain model and step-wise uncertainty detection\nusing Qwen2.5 as the general LLM, with results shown in Figure 10. Consistent with the results obtained using\nLLaMA-3 as the general model in the main text, both ablation experiments yield lower performance than the complete\nframework, which aligns with the conclusions drawn in Section 5 in the main text.\n12\n--- Page 13 ---\nChemAU A P REPRINT\n<|start_header_id |>system <|end_header_id >\nYou are a scientific research assistant. Help me solve this multiple choice  question with step-by-step reasoning .\nFor each reasoning step, you MUST:\n1. Start the line with EXACTLY ' --' (two hyphens)\n2. Focus on a single, atomic piece of domain knowledge or logical inference\n3. Break down complex domain knowledge into multiple atomic steps rather than combining several concepts in one step\nAfter completing all reasoning steps, state your final answer using only the option number from the \"Options\" part in this \nformat: [n] where n is the correct option number (note that numbering may start from 0).\nEXAMPLE FORMAT :\n-- First reasoning step here\n-- Second reasoning step here\n-- Third reasoning step here\nFinal Answer: [x]\n<|start_header_id |>user<|end_header_id >\n[Question ]\n[Options ]\nFigure 9: A prompt template designed to guide the general LLM in generating an initial reasoning chain for chemistry\nproblems. In this template, the \"step-by-step reasoning\" instruction encourages the model to generate CoT style\nreasoning processes, using double hyphens (‘- -’) as the starting identifier for each reasoning step, and emphasizing\nthat each reasoning step should focus on atomic chemistry knowledge points. This fine-grained division facilitates\nthe subsequent uncertainty estimation method in identifying and locating the missing chemistry knowledge in general\nLLMs. “Question” and “Options” part in user should be filled with the chemistry problem. Since the DeepSeek-R1\nseries models do not recommend the separation of system and user roles, the system content is directly integrated into\nthe user section .\nThe formula for Yellow Prussiate of Potash is \nKAg(CN)2…\nHow many moles are there in 80 grams of \n“Yellow Prussiate of Potash” ？\nThe molecular weight of potassium ferricyanide \n(K4[Fe(CN)6]) can be calculated by …\nPotassium ferricyanide has the chemical \nformula K4[Fe(CN)6]\nAccurate Knowledge\nHow many moles are there in 80 grams of \n“Yellow Prussiate of Potash” ？\nPotassium cyanide are needed to produce \nYellow Prussiate of Potash\nIrrelevant Knowledge\nFigure 12: Knowledge Augmentation Comparison. Precise and relevant knowledge augmentation facilitates correct\nreasoning in models, while broad or irrelevant knowledge augmentation tends to induce model hallucinations, conse-\nquently leading to erroneous reasoning.\n13\n--- Page 14 ---\nChemAU A P REPRINT\nGPQA MMLU-Pro SuperGPQA1020304050Performance\nChemAU Without Domain Model21.5031.1847.31\n41.60\n26.28\n17.24\nGPQA MMLU-Pro SuperGPQA20304050Performance\nStep-wise Approach Chain-level Approach31.18\n26.8847.31 47.22\n26.28\n22.83\nFigure 10: (Left) The comparisons of the accuracy (%) of our complete reasoning framework against a variant without\nthe specialized chemistry domain model across different chemistry datasets with Qwen2.5 as general LLM. (Right)\nThe comparisons of the accuracy results (%) of our proposed step-wise approach against a variant that treats the entire\nreasoning chain as a single unit across different general models and chemistry datasets with Qwen2.5 as general LLM.\n<|start_header_id |>system <|end_header_id >\nYou are a scientific research assistant. \nAccording to the “ Question” , “Initial Reasoning Steps”  and  “ Knowledge ” provided by the user,  continue the reasoning \nprocess step-by-step to solve this multiple choice  question. \nFor each reasoning step, you MUST:\n1. Start the line with EXACTLY ' --' (two hyphens)\n2. Focus on a single, atomic piece of domain knowledge or logical inference\n3. Break down complex domain knowledge into multiple atomic steps rather than combining several concepts in one step\nAfter completing all reasoning steps, state your final answer using only the option number from the \"Options\" part in this \nformat: [n] where n is the correct option number (note that numbering may start from 0). \nEXAMPLE FORMAT :\n-- First reasoning step here\n-- Second reasoning step here\n-- Third reasoning step here\nFinal Answer: [x]\n<|start_header_id |>user<|end_header_id >\n[Question ]\n[Options ]\n[Initial Reasoning Steps ]\n[Knowledge ]\nFigure 11: A prompt template designed to guide the general LLM in regenerating the reasoning chain for chemistry\nproblems according to the “ Initial Reasoning Steps ” and “ Knowledge ”. This template is based on the previous template\nwith modifications only to the sections outlined in red. Since the DeepSeek-R1 series models do not recommend the\nseparation of system and user roles, the system content is directly integrated into the user section .\n14\n--- Page 15 ---\nChemAU A P REPRINT\nReferences\nOguzhan Topsakal and Tahir Cetin Akinci. Creating large language model applications utilizing langchain: A primer\non developing llm apps fast. In International Conference on Applied Engineering and Natural Sciences , volume 1,\npages 1050–1056, 2023.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948 , 2025.\nArkaitz Zubiaga. Natural language processing in the era of large language models, 2024.\nRanjan Sapkota, Zhichao Meng, and Manoj Karkee. Synthetic meets authentic: Leveraging llm generated datasets\nfor yolo11 and yolov10-based apple detection through machine vision sensors. Smart Agricultural Technology , 9:\n100614, 2024.\nHaitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu,\net al. Legalagentbench: Evaluating llm agents in legal domain. arXiv preprint arXiv:2412.17259 , 2024a.\nSagar Goyal, Eti Rastogi, Sree Prasanna Rajagopal, Dong Yuan, Fen Zhao, Jai Chintagunta, Gautam Naik, and Jeff\nWard. Healai: A healthcare llm for effective medical documentation. In Proceedings of the 17th ACM International\nConference on Web Search and Data Mining , pages 1167–1168, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-\nof-thought prompting elicits reasoning in large language models. Advances in neural information processing systems ,\n35:24824–24837, 2022.\nMatthew Renze and Erhan Guven. Self-reflection in llm agents: Effects on problem-solving performance. arXiv\npreprint arXiv:2405.06682 , 2024.\nChen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can large language models empower molecular\nproperty prediction? arXiv preprint arXiv:2307.07443 , 2023.\nKaixuan Huang, Yuanhao Qu, Henry Cousins, William A Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman,\nMengdi Wang, and Le Cong. Crispr-gpt: An llm agent for automated design of gene-editing experiments. arXiv\npreprint arXiv:2404.18021 , 2024a.\nSheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised\npre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on\nbioinformatics, computational biology and health informatics , pages 429–436, 2019.\nShion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint for low data drug\ndiscovery. arXiv preprint arXiv:1911.04738 , 2019.\nViraj Bagal, Rishal Aggarwal, PK Vinod, and U Deva Priyakumar. Molgpt: molecular generation using a transformer-\ndecoder model. Journal of chemical information and modeling , 62(9):2064–2076, 2021.\nSimon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property\nprediction and molecular generation. Scientific Data , 9(1):185, 2022.\nQiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li,\nZhuoyi Xiang, et al. Scientific large language models: A survey on biological & chemical domains. ACM Computing\nSurveys , 57(6):1–38, 2025.\nDi Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli\nOuyang, et al. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852 , 2024a.\nJunxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui\nXiong, et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In Proceedings\nof the AAAI Conference on Artificial Intelligence , volume 39, pages 415–423, 2025.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate,\nand critique through self-reflection. In The Twelfth International Conference on Learning Representations , 2023.\nXiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and Hua Wei. Uncertainty quantification and\nconfidence calibration in large language models: A survey. arXiv preprint arXiv:2503.15850 , 2025.\n15\n--- Page 16 ---\nChemAU A P REPRINT\nYuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. Look\nbefore you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint\narXiv:2307.10236 , 2023.\nChen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi\nMatsuda, Jie Ji, et al. Uncertainty decomposition and quantification for in-context learning of large language models.\narXiv e-prints , pages arXiv–2402, 2024.\nZhisheng Tang, Ke Shen, and Mayank Kejriwal. An evaluation of estimative uncertainty in large language models.\narXiv preprint arXiv:2405.15185 , 2024.\nJiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model via intrinsic and extrinsic\nconfidence assessment. arXiv preprint arXiv:2308.16175 , 2, 2023.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\nHuang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Let-\nman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 ,\n2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael,\nand Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language\nModeling , 2024.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj,\nXuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.\nInThe Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2024.\nXinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong\nJin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint\narXiv:2502.14739 , 2025.\nOleksandr Balabanov and Hampus Linander. Uncertainty quantification in fine-tuned llms using lora ensembles. arXiv\npreprint arXiv:2402.12264 , 2024.\nOla Shorinwa, Zhiting Mei, Justin Lidard, Allen Z Ren, and Anirudha Majumdar. A survey on uncertainty quan-\ntification of large language models: Taxonomy, open research challenges, and future directions. arXiv preprint\narXiv:2412.05563 , 2024.\nAmos Azaria and Tom Mitchell. The internal state of an llm knows when it’s lying. arXiv preprint arXiv:2304.13734 ,\n2023.\nCaiqi Zhang, Fangyu Liu, Marco Basaldella, and Nigel Collier. Luq: Long-text uncertainty quantification for llms.\narXiv preprint arXiv:2403.20279 , 2024b.\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil\nVasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, et al. Lm-polygraph: Uncertainty estimation\nfor language models. arXiv preprint arXiv:2311.07383 , 2023.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection\nfor generative large language models. arXiv preprint arXiv:2303.08896 , 2023.\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii\nTsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, et al. Fact-checking the output of large language\nmodels via token-level uncertainty quantification. arXiv preprint arXiv:2403.04696 , 2024.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 ,\n2019.\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint\narXiv:1908.10084 , 2019.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation\nwith bert. arXiv preprint arXiv:1904.09675 , 2019.\nBairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. Decomposing uncertainty for large\nlanguage models through input clarification ensembling. arXiv preprint arXiv:2311.08718 , 2023.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-\ndomain questions. arXiv preprint arXiv:2004.10645 , 2020.\n16\n--- Page 17 ---\nChemAU A P REPRINT\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous questions with generative\nlanguage models. arXiv preprint arXiv:2212.07769 , 2022.\nMuhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh,\nNaveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. A survey on large language models: Applications, challenges,\nlimitations, and practical usage. Authorea Preprints , 2023.\nYuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang\nWang, Yi Sun, et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv\npreprint arXiv:2401.05459 , 2024b.\nKevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language\nmodels for predictive chemistry. Nature Machine Intelligence , 6(2):161–169, 2024.\nJiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li. Empowering molecule\ndiscovery for molecule-caption translation with large language models: A chatgpt perspective. IEEE transactions on\nknowledge and data engineering , 2024c.\nDaniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language\nmodels. Nature , 624(7992):570–578, 2023.\nZihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, et al.\nChemdfm: A large language foundation model for chemistry. arXiv preprint arXiv:2401.14818 , 2024.\nYuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong\nLiang, Yi Li, et al. Chemeval: A comprehensive multi-level chemical evaluation for large language models. arXiv\npreprint arXiv:2409.13989 , 2024b.\nAndrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. arXiv preprint\narXiv:2002.07650 , 2020.\nYavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, and Salman\nAvestimehr. Mars: Meaning-aware response scoring for uncertainty estimation in generative llms. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n7752–7767, 2024.\nJinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu.\nShifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models.\narXiv preprint arXiv:2307.01379 , 2023.\n17",
  "text_length": 63886
}