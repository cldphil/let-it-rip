{
  "id": "http://arxiv.org/abs/2506.05289v1",
  "title": "AliTok: Towards Sequence Modeling Alignment between Tokenizer and\n  Autoregressive Model",
  "summary": "Autoregressive image generation aims to predict the next token based on\nprevious ones. However, existing image tokenizers encode tokens with\nbidirectional dependencies during the compression process, which hinders the\neffective modeling by autoregressive models. In this paper, we propose a novel\nAligned Tokenizer (AliTok), which utilizes a causal decoder to establish\nunidirectional dependencies among encoded tokens, thereby aligning the token\nmodeling approach between the tokenizer and autoregressive model. Furthermore,\nby incorporating prefix tokens and employing two-stage tokenizer training to\nenhance reconstruction consistency, AliTok achieves great reconstruction\nperformance while being generation-friendly. On ImageNet-256 benchmark, using a\nstandard decoder-only autoregressive model as the generator with only 177M\nparameters, AliTok achieves a gFID score of 1.50 and an IS of 305.9. When the\nparameter count is increased to 662M, AliTok achieves a gFID score of 1.35,\nsurpassing the state-of-the-art diffusion method with 10x faster sampling\nspeed. The code and weights are available at\nhttps://github.com/ali-vilab/alitok.",
  "authors": [
    "Pingyu Wu",
    "Kai Zhu",
    "Yu Liu",
    "Longxiang Tang",
    "Jian Yang",
    "Yansong Peng",
    "Wei Zhai",
    "Yang Cao",
    "Zheng-Jun Zha"
  ],
  "published": "2025-06-05T17:45:10Z",
  "updated": "2025-06-05T17:45:10Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05289v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05289v1  [cs.CV]  5 Jun 2025AliTok: Towards Sequence Modeling Alignment\nbetween Tokenizer and Autoregressive Model\nPingyu Wu1,2,∗, Kai Zhu1,2,†, Yu Liu2, Longxiang Tang2,∗\nJian Yang1, Yansong Peng1,2,∗, Wei Zhai1,†, Yang Cao1, Zheng-Jun Zha1\n1University of Science and Technology of China2Tongyi Lab\nFigure 1: 256×256 samples of class-conditional generation on ImageNet using our AliTok-XL model (662M).\nAbstract\nAutoregressive image generation aims to predict the next token based on previous\nones. However, existing image tokenizers encode tokens with bidirectional de-\npendencies during the compression process, which hinders the effective modeling\nby autoregressive models. In this paper, we propose a novel Aligned Tokenizer\n(AliTok), which utilizes a causal decoder to establish unidirectional dependencies\namong encoded tokens, thereby aligning the token modeling approach between the\ntokenizer and autoregressive model. Furthermore, by incorporating prefix tokens\nand employing two-stage tokenizer training to enhance reconstruction consistency,\nAliTok achieves great reconstruction performance while being generation-friendly.\nOn ImageNet-256 benchmark, using a standard decoder-only autoregressive model\nas the generator with only 177M parameters, AliTok achieves a gFID score of\n1.50 and an IS of 305.9 . When the parameter count is increased to 662M, AliTok\nachieves a gFID score of 1.35, surpassing the state-of-the-art diffusion method with\n10×faster sampling speed. The code and weights are available at AliTok.\n1 Introduction\nThe next-token prediction of autoregressive (AR) models, especially GPT-style decoder-only trans-\nformer models [ 1,32,33], have achieved remarkable success and dominated the field of NLP, owing\n∗Work done during their internships at Tongyi Lab.\n†Kai Zhu and Wei Zhai are the corresponding authors.\nPreprint. Under review.\n--- Page 2 ---\nFigure 2: Reconstruction vsgeneration with different transformer-based tokenizers. Images are compressed\ninto raster-scan order 1D sequences by the tokenizers. AR are standard decoder-only autoregressive models. All\nexperiments are conducted under the same conditions. Poor results are indicated in green, while the best results\nare highlighted in bold . Tok. : Tokenizer. Acc: Training accuracy. See supplementary material for details.\nto their simplicity and scalability. Following this NLP paradigm, generating images through a fixed\n(raster-scan) order using a unidirectional decoder-only autoregressive model has garnered widespread\nattention [36, 7, 34], showcasing a promising path for multi-modal unification [38, 30, 10, 30].\nUnfortunately, as demonstrated in previous literature [ 21,15], the inherent bi-directional property\nof visual sequence makes it difficult for the raster-scan decoder-only autoregressive model ( e.g.,\nLlamaGen [ 27]) to achieve excellent performance, since unidirectional transformer is hard to model\nbi-directional sequences. Consequently, recent works have shifted focus towards paradigms like\nmasked autoregressive [ 15,4,43,6] and next-scale prediction [ 29,31,7], which employs bidirectional\nattention in autoregressive models and demonstrates a superior choice for visual modeling. However,\nthese approaches complicate visual generation and diverge from traditional autoregressive paradigm,\nincreasing the challenges for multi-modal unification. In contrast to existing works that modify the\nmodel to fit the visual sequence properties, we propose converting sequences into a unidirectional\nformat to better suit the capabilities of decoder-only autoregressive models .\nUnlike natural language, which is inherently compact and allows for one-to-one mapping between\nwords and indices with a non-parametric tokenizer, images are high-dimensional and contain sig-\nnificant redundancy, requiring a learnable tokenizer to effectively eliminate the redundancy. Due\nto the continuity of spatial information, redundancy exists not only within individual patches, but\nalso between adjacent patches. This implies that, in the compression process, multiple tokens must\ncollaborate to effectively remove overall redundancy and ensure a compact representation. This\nprocess naturally leads to a high degree of mutual dependency among encoded tokens, where each\ntoken relies on the complementary information provided by other related tokens to fully convey its\nmeaning, while simultaneously offering necessary context for others. Therefore, the process of image\ncompression leads to complex bidirectional dependencies among the encoded tokens, hindering the\nsubsequent autoregressive models from modeling them effectively.\nWith this observation, we propose to establish better unidirectional dependencies among the encoded\ntokens. An intuitive approach is to construct a causal tokenizer encoder so that the preceding\nimage patches cannot access the latter image patches during encoding. After experimentation, we\nsurprisingly find that although this approach could lead to unsmooth transitions between reconstructed\nimage patches and diminished compression efficiency (rFID: 0.98 →1.56), it effectively facilitates the\nmodeling of generative models, as evidenced by the significant increase in training accuracy (Acc:\n5.4%→11.2%) in Fig. 2. To achieve better reconstruction result, we consider a bidirectional encoder\nto be essential, which enables the tokenizer to comprehend the full semantics and content of each\npatch within an image, thus supporting more effective compression. As an alternative, we propose\nsuppressing the non-causality of the encoder by employing a causal decoder. Through the training\nprocess of the causal decoder, it imposes the non-causal encoder to output latent representations\nthat better adhere to unidirectional dependencies, since this alignment can help the tokenizer to\nachieve lower reconstruction losses. As illustrated in Fig. 2, this design achieves superior generative\nperformance (gFID: 2.96 →1.88) while maintaining well reconstruction ability (rFID: 0.98 →1.07).\nIn this paper, we propose a simple yet effective AliTok tokenizer, which unleashes the power of\nautoregressive models by aligning the token modeling approach between the tokenizer and the\nautoregressive model. Specifically, in the encoding stage, the image information is compressed into\n1D latent tokens via a bidirectional transformer. During the decoding, the causal mask is applied to\ninduce the encoded tokens to exhibit forward-dependent, and the images are reconstructed along the\n2\n--- Page 3 ---\nFigure 3: Sampling time and gFID (w/o cfg and\nw/ cfg). Sampling time is evaluated on an A800.raster-scan order. However, this unidirectional depen-\ndency generally results in poor performance in the\nfirst row of the reconstructed image due to the lack of\nsufficient reference tokens. To address this problem,\nwe introduce prefix tokens specific to the first row to\ncarry extra reconstruction information for compensa-\ntion. Finally, we further enhance reconstruction per-\nformance while preserving generation-friendly prop-\nerties through a two-stage tokenizer training process.\nBy freezing the well-trained encoder and codebook,\nwe retrain a bidirectional transformer as the decoder\nto improve reconstruction continuity and smoothness.\nBuilding on AliTok, we employ a standard decoder-\nonly autoregressive model as the generative model\nand conduct experiments. As illustrated in Fig. 3,\nwith an autoregressive model comprising only 318M\nparameters, our method already surpasses all methods\non gFID (w/o cfg), including recent state-of-the-art\ndiffusion model LightningDiT [ 40]. Upon increasing\nthe parameter count to 662M, we achieve the same\n1.35 gFID (w/ cfg) as LightningDiT, while providing\na 10×faster sampling speed. In summary, the contributions of this paper include:\n1.We reveal a key factor restricting autoregressive model efficacy: conventional tokenizers tend to\nestablish bidirectional dependencies within encoded tokens, which fundamentally conflicts with\nthe unidirectional nature of autoregressive models.\n2.We propose a simple yet effective design for image tokenizer that enables the encoded tokens to\nbe more easily modeled by autoregressive models while maintaining high reconstruction fidelity.\n3.Based on the proposed tokenizer, a standard decoder-only autoregressive model beats state-of-\nthe-art diffusion models.\n2 Related work\nAutoregressive visual generation. Inspired by GPT-style language models [ 1,32,33], early au-\ntoregressive methods [ 35,14] compress images into discrete-valued tokens and predict the next\ntoken along the raster-scan order. Toward this direction, extensive efforts have been made to en-\nhance the generation quality by improving the quantization techniques [ 5,46,17,41,14] and model\nstructure [ 27,16]. Nevertheless, this paradigm remains constrained by a bottleneck, as it struggles\nto model the inherently bidirectional visual sequence using unidirectional transformers. To this\nend, MaskGIT [ 4] first applies the masked transformer model based on bidirectional attention to\nvisual generation. MAR [ 15] further implements masked autoregressive modeling in a continuous\nvalue domain and achieves remarkable performance. V AR [ 31] proposes a next-scale prediction\nparadigm, where each scale is predicted using bidirectional attention and can access information\nfrom the previous scales. RAR [ 42] proposes to maximize the probability in all directions during the\nautoregressive training. RandAR [ 21] introduces arbitrary token order modeling to a decoder-only\nautoregressive model. In contrast to these works that attempt to construct models with better global\nmodeling capabilities, we focus on the modeling object of the model, i.e.,encoded tokens, and\nachieve excellent performance using only standard decoder-only autoregressive models.\nImage tokenizer , a type of Variational Autoencoder (V AE) [ 12], plays an important role in image\ngeneration domain [ 40,37]. By mapping high-dimensional image pixels to a low-dimensional\nlatent space, the image tokenizer significantly enhances the training efficiency of generative models.\nAmong them, VQ-V AE [ 35] utilizes quantization techniques to discretize continuous latent features\nfor autoregressive image generation. VQ-GAN [ 5] further improves the reconstruction quality by\nintroducing an adversarial loss. MagViT-v2 [ 41] and FSQ [ 20] improve quantization technology and\nefficiently increase codebook size. Besides, TiTok [ 43] proposes a transformer-based tokenizer, which\nencodes 2D image patches into a 1D sequence. FlexTok [ 2] proposes to encode images into latent\ntokens with semantic granularity ranging from coarse to fine-grained. Despite numerous efforts, there\n3\n--- Page 4 ---\nFigure 4: Two-stage training process of the proposed AliTok. Stage 1: Training an image tokenizer with a\ncausal decoder. Stage 2: Freezing the encoder and codebook of the tokenizer, training the autoregressive model\nand retraining a bidirectional tokenizer decoder.\nhas been limited exploration on how to design a tokenizer that simultaneously achieves high-quality\nreconstruction while being more conducive to subsequent autoregressive generation.\n3 Method\n3.1 Preliminaries\nThe objective of the autoregressive model is to predict the next token in sequence with only the\nprevious tokens visible. Denoting the 1D image sequence xas[x1, . . . , x T], the autoregressive model\nneeds to model the joint probability distribution of the current token xtbased on the previous tokens\n[x1, . . . , x t−1]. The generation process of the sequence xcan be expressed as:\npθ(x) =TY\ni=1pθ(xi|x1, . . . , x i−1), (1)\nwhere pθis the parameterized autoregressive model. Since the generation of each token is conditioned\nsolely on the preceding tokens, this requires the sequence xitself to possess well unidirectional\ndependencies to be more effectively modeled by autoregressive models.\nImage tokenizer. Since the image pixel space is inherently high-dimensional and redundant, a\nhigh compression rate ( e.g., 16×16) tokenizer is required to perform dimensionality reduction.\nHowever, commonly used tokenizers suffer from the problem that the image encoding process\nlearns compact representations based on the information from the entire image, leading to mutual\ndependencies between the encoded token and the surrounding associated encoded tokens, as analyzed\nin Introduction (Sec. 1). This makes the encoded tokens no longer satisfy the causal condition, i.e.,\nthe current token should only depend on itself and previous tokens.\n3.2 AliTok tokenizer\nTo make the encoded tokens obey the unidirectional dependency, we choose the transformer architec-\nture to build the tokenizer since the attention mask in the attention module can flexibly implement the\ncausal association. For the encoder, we employ a bidirectional transformer to enhance reconstruction\nperformance. We observe that a causal encoder can lead to mismatched details between adjacent re-\nconstructed patches and poor visual perception. This occurs because the causal encoder, relying only\non current and previous patches, struggles to fully comprehend the semantic information contained in\nthe current patch, resulting in inefficient feature compression. Moreover, such reconstruction losses\nare irreparable. For the decoder, we initially use a causal transformer in the first stage to impose\nunidirectional dependency among the encoded tokens. In the second stage, we freeze the encoder and\ncodebook, and retrain a bidirectional transformer to enhance reconstruction performance.\nDefinition. As shown in Fig. 4, the overall framework of AliTok is based on vanilla vision transformer\n(ViT) architecture. For the input image I∈Rh×w×3, the transformer splits it into non-overlapping\nimage patches and flattens them into a 1D patch token sequence P∈R(H×W)×Dby linear projection.\nHere, H=h/f,W=w/f,Dis the number of channels, and fis the patch size, which is set to 16.\n4\n--- Page 5 ---\nSimultaneously, we introduce K+H×Wlatent tokens in the input space (with K= 17, H×W=\n256), with the first Ktokens serving as prefix tokens, and the remaining H×Wlatent tokens\ncorresponding to H×Wpatch tokens. After concatenating the latent tokens with the patch tokens,\nthe sequence is fed into the encoder ( Enc) for tokenization, which compresses the information of the\nimage patches into 1D latent tokens. Subsequently, we discard the encoded patch tokens and retain\nonly the encoded latent tokens, denoted Z∈R(K+H×W)×D. Finally, the encoded latent tokens Z\nundergo quantization ( Quant ) [35] (see supplementary material for details) and pass through the\ndecoder ( Dec) to produce the reconstructed image patches {ˆpn∈R1×D, n= 1,2, ..., K +H×W}.\nCausal decoder. To suppress bi-directional feature dependencies among encoded latent tokens, we\npropose to construct a causal decoder and realize the reconstruction process of encoded latent tokens\ninto image patches following a raster-scan order. In addition, we introduce an additional learnable\nclass token at the front of the sequence in the de-tokenization process, aligning with the token number\nof autoregressive model. The decoding process can be represented as follows:\nˆpi= Dec(Quant( z1, . . . , z i)), i = 1,2, . . . , K +H×W. (2)\nIn this way, the tokenizer decoding process can be well aligned with the autoregressive process, both\nin terms of token quantity and dependency relationships, as illustrated in Fig. 4(b) and (c). This\nalignment allows the encoded tokens to be more easily modeled by the autoregressive model.\nFor reconstruction loss, we use the conventional mean squared error (MSE) loss Lmse, perceptual loss\nLperc, quantization loss [ 46]Lquant , and adversarial loss Ladvas constraints, following TiTok [ 11]:\nLrecon =Lmse+Lperc+Lquant +λLadv, (3)\nwhere λis set to 0.1. In particular, we employ the regular GAN network in the SD V AE [ 23] instead of\nusing the frozen DINO discriminator [ 3,31], even though the latter improves rFID significantly [ 39].\nPrefix token. However, a causal decoder forces latent tokens to only reference features from\npreceding tokens, leading to poor reconstruction of the first row (16 ×256 pixels) of the image,\nespecially the first reconstructed image patch (16 ×16 pixels), due to the lack of sufficient preceding\ntokens to provide dependable features. To solve this problem, an intuitive solution is to increase an\nextra row to the training image, i.e., using images with 272 ×256 resolution for training, and crop\nout the unsatisfactory first row in the final generated result. But this simple approach may result in\ngenerating images with incomplete objects, as information in the topmost row is lost during cropping.\nTo enhance the reconstruction quality of the first row, we propose to add a few prefix tokens as\nsupplementary aids to the latent tokens. These prefix tokens are designed to encapsulate coarse\ninformation from the first row, thus alleviating the reconstruction challenges for subsequent latent\ntokens. As shown in Fig. 4(a) and (b), we incorporate 17 additional prefix tokens specifically to assist\nthe reconstruction process of the first 16 image patches. The first two prefix tokens are allocated to\nfocus on the first image patch, given its considerable reconstruction difficulty, while the remaining\n15 prefix tokens correspond to individual image patches. For constraint implementation, the first 16\nimage patches (with the initial patch replicated twice) serve as the ground truth for the decoded prefix\ntokens. MSE loss is employed as the loss function, denoted by auxiliary loss Laux.\nRetraining a bidirectional decoder. In the second stage, we freeze the encoder and codebook of\nthe tokenizer and retrain a decoder using a bidirectional transformer, to improve detail consistency\nand smoothing. As shown in Fig. 4(d), in addition to converting the attention mechanism in the\ndecoder to bidirectional form, we integrate 32 buffer tokens [ 15] to increase computational load.\nMeanwhile, the loss previously imposed on prefix tokens is removed to allow the decoder to focus\non reconstruction quality. This two-stage training strategy enables our AliTok tokenizer to not only\ngenerate generation-friendly encoded tokens, but also maintain good reconstruction performance.\n3.3 Autoregressive model\nOur decoder-only autoregressive architecture follows the standard design in LlamaGen [ 27], which\nuses RMSNorm [ 45] for pre-normalization and applies 2D rotary positional embeddings (RoPE) [ 26]\nat each layer. Building upon LlamaGen, we make only a few modifications. First, since our method\nhas 17 extra prefix tokens and need to model 273 tokens, 2D RoPE cannot be used directly. Therefore,\n5\n--- Page 6 ---\nTable 1: ImageNet 256 ×256 conditional generation. “Diff.”: Diffusion. “Mask.”: Masked transformer\nmodels. Pre.: Precision. Rec.: Recall. RAR does not report results w/o cfg. Thus, we test it using the weights\nprovided in the original paper, adjusting the temperature at intervals of 0.01 to select the best gFID w/o cfg.\nType GeneratorTraining\nepochs#Para.w/o cfg w/ cfg\ngFID↓ IS↑ gFID↓ IS↑ Pre.↑ Rec.↑\nDiff.DiT-XL [22] 1400 675M 9.62 121.5 2.27 278.2 0.83 0.57\nSiT-XL [18] 800 675M 8.61 131.7 2.06 270.3 0.82 0.59\nREPA [44] 800 675M 5.90 157.8 1.42 305.7 0.80 0.65\nLightningDiT [40] 800 675M 2.17 205.6 1.35 295.3 0.79 0.65\nV ARV AR-d20 [31] 350 600M − − 2.57 302.6 0.83 0.56\nV AR-d24 [31] 350 1.0B − − 2.09 312.9 0.82 0.59\nV AR-d30 [31] 350 2.0B − − 1.92 323.1 0.82 0.59\nMask.MaskGIT [4] 300 227M 6.18 182.1 − − − −\nMAGVIT-v2 [41] 1080 307M 3.65 200.5 1.78 319.4 − −\nTiTok-S-128 [43] 800 287M 4.44 168.2 1.97 281.8 − −\nMARMAR-B [15] 800 208M 3.48 192.4 2.31 281.7 0.82 0.57\nMAR-L [15] 800 479M 2.60 221.4 1.98 290.3 0.81 0.60\nMAR-H [15] 800 943M 2.35 227.8 1.55 303.7 0.81 0.62\nCausal\nARLlamaGen-XL [27] 300 775M − − 2.62 244.1 0.80 0.57\nLlamaGen-XXL [27] 300 1.4B − − 2.34 253.9 0.80 0.59\nLlamaGen-3B [27] 300 3B 9.38 112.9 2.18 263.3 0.81 0.59\nRAR-B [42] 400 261M 7.12 124.8 1.95 290.5 0.82 0.58\nRAR-L [42] 400 461M 5.39 149.1 1.70 299.5 0.81 0.60\nRAR-XL [42] 400 955M 3.72 179.9 1.50 306.9 0.80 0.62\nRAR-XXL [42] 400 1.5B 3.26 193.3 1.48 326.0 0.80 0.63\nCausal\nARAliTok-B 800 177M 2.39 177.9 1.50 305.9 0.78 0.64\nAliTok-L 800 318M 1.98 197.0 1.42 326.6 0.78 0.65\nAliTok-XL 400 662M 1.84 228.3 1.35 318.8 0.79 0.64\nwe use 1D RoPE for the prefix tokens and 2D RoPE for the remaining 256 tokens. Secondly, we\nintroduce the QK-Norm operation [ 30] in the attention module to stabilize the large-scale model\ntraining, aligning with recent multi-modal autoregressive models [ 38,19,30]. We do not conduct\nextensive exploration of autoregressive models and only use the conventional architecture to verify\nthe effectiveness of the proposed tokenizer.\n4 Experiments\n4.1 Experimental Setup\nModels and evaluations. The design of the proposed tokenizer is based on TA-TiTok [ 11], utilizing\nViT-B [ 43] for the encoder and ViT-L for the decoder. We set the vocabulary size to 4096 and\nadopt the online feature clustering method [ 46] to ensure that the codebook utilization is 100%. For\nsimplicity, we call the autoregressive models trained using our AliTok tokenizer as AliTok-B/L/XL,\ncomprising three different sizes, more details are provided in supplementary material. Following\ncommon practice, FID (including rFID and gFID) [ 8], IS [ 25], Precision, and Recall are adopted as\nmetrics. We generate 50,000 images to test the gFID and adopt the evaluation code from RAR [ 42].\nWe do not use Top-k or Top-p strategies [ 31] in the sampling process. For rFID evaluation, we use\nthe ImageNet-1k val set to test FID, consistent with other methods [ 27,43,40]. During the testing\nphase, KV-cache is employed to enhance sampling speed.\nDataset and training details. We train our image tokenizer from scratch on ImageNet-1K [ 24],\nrather than using larger datasets like OpenImages [ 13] as other methods do [ 31,15]. The tokenizer is\ntrained for 600K steps in the first stage and 300K steps in the second stage on 32 A800-80G GPUs.\nFor the autoregressive model training, we employ tencrop [ 28] for data augmentation and cache [ 15]\nthe encoded results of the tokenizer to reduce training time. We train the base model and large model\nfor 800 epochs and the XL model for 400 epochs, with a batch size of 2048 and a learning rate of\n6\n--- Page 7 ---\nFigure 5: 256×256 samples generated by our models of different sizes.\n4e-4. For the first 100 epochs, the learning rate is linearly warmed up, then decayed using a cosine\ndecay schedule down to 1e-5, following RAR [ 42]. During training, class conditioning is randomly\ndropped with a probability of 0.1 to support the use of classifier-free guidance (cfg) [9].\n4.2 Main Results\nGeneration comparison. In Table 1, we report the comparison with state-of-the-art methods on the\n256×256 ImageNet-1K benchmark, including results both without classifier-free guidance (cfg) [ 9]\nand with cfg. Utilizing our AliTok image tokenizer, even a standard autoregressive model as the\ngenerator demonstrates exceptional performance. Specifically, our AliTok-B model achieves a gFID\n(w/ cfg) of 1.50, using less than 6% of the parameter count compared to raster-order counterpart\nLlamaGen-3B [ 27], which achieves a gFID of 2.18 with a 3B model. Furthermore, our AliTok-L\nmodel, with a parameter count of 318M, attains a gFID (w/ cfg) of 1.42, eclipsing all existing\nautoregressive methods and outperforming the state-of-the-art autoregressive model RAR-XXL (1.5B\nparameters) [ 42] in both IS and gFID. Additionally, AliTok-L also achieves a gFID (w/o cfg) of\n1.98, surpassing all methods, including the state-of-the-art diffusion model LightningDiT [ 40]. Upon\nincreasing the parameter count to 662M, our AliTok-XL beats all competing methods, particularly\nin gFID w/o cfg, markedly outperforming the previous best method LightningDiT (1.84 vs2.17).\nWhen using the cfg, our AliTok-XL achieves the same excellent gFID (1.35) as the LightningDiT,\nbut outperforms it in the IS metrics by 23.5. To the best of our knowledge, this represents the first\ntime a standard autoregressive model beats state-of-the-art diffusion models.\nTable 2: Sampling speed comparison. The throughput\n(number of images generated per second) is evaluated at\nFP32 precision on an A800 GPU with a batch size of 64\n(not including extra batch from cfg). All methods are\ntested using their original codebases. gFID: gFID w/ cfg.\nMethod Type #Para. gFID ↓images/sec ↑\nV AR-d30 [31] V AR 2.0B 1.92 12.3\nAliTok-B (ours) AR 177M 1.50 11.0\nMAR-H [15] MAR 943M 1.55 0.3\nRAR-XXL [42] AR 1.5B 1.48 5.0\nAliTok-L (ours) AR 318M 1.47 10.0\nLightningDiT [40] Diff. 675M 1.35 0.6\nAliTok-XL (ours) AR 662M 1.35 6.3Sampling efficiency comparison. In Table 4.2,\nwe list the sampling speeds of our model vari-\nants and compare them with the best results\nof other methods with similar gFID. Benefit-\ning from the excellent performance and the\nimplementation of KV-cache, our AliTok can\nachieve superior generation performance with\nfaster sampling speeds. Specifically, AliTok-L\nimproves throughput by 33.3 ×and 2.0 ×com-\npared to MAR-H [ 15] and RAR-XXL [ 42], re-\nspectively. Compared to the state-of-the-art dif-\nfusion model LightningDiT [ 40], our AliTok-\nXL model demonstrates a substantial improve-\nment in image generation efficiency, requiring\nless than 10% of the time needed by Light-\nningDiT to produce an image.\nGeneration visualization. We further show the generation results under different model sizes in\nFig. 5. It can be observed that even the smallest model is capable of producing high-quality visual\nresults and reasonably generating complex structures, such as the coral reef and altar. As the model\nsize increases, the generated images exhibit enhanced detail and more intricate structures, illustrated\nby the realistic texture of the feathers of the parrot, the fine-grain structure of the space shuttle and\nthe altar, demonstrating a superior level of realism in image quality. More results generated by the\ndifferent model sizes are provided in the supplementary material.\n7\n--- Page 8 ---\nFigure 6: Training curves . (a) training loss (b) training error rate (%) (c) gFID scores w/o cfg (d) gFID scores\nw/ cfg varies with training steps and model parameters. 250,000 training steps corresponds to 400 epochs.\nTable 3: Ablation studies of main components on AliTok-Base model. In experiments (A)-(D), considering\nthe significant training resources and time required for ablating each component, we train the tokenizer for 300K\nsteps and the autoregressive model for 200 epochs. In experiments (E) and (F), the same weight as in the main\nexperiment is used to explore the boost from the two-stage training of the tokenizer. The tokenizer encoder and\ndecoder in experiment (A) are both bidirectional transformer, noted as baseline tokenizer. Loss and Acc. denote\nthe final loss and accuracy during the generator training phase. Causal Dec: Causal tokenizer decoder. Prefix:\nadding prefix tokens to the latent tokens. Laux: MSE loss imposed on prefix tokens. Two-stage: Two-stage\ntraining of the tokenizer decoder.\nTraining SettingTokenizer Setting AR Training Evaluation\nCausal Dec Prefix Laux Two-stage Loss↓Acc.↑gFID↓IS↑rFID↓\nTokenizer trained\nfor 300K steps &\nAR 200 epochs(A) 5.75 5.4% 2.96 270.6 0.98\n(B) ✓ 5.18 10.7% 1.88 285.6 1.07\n(C) ✓ ✓ 5.23 9.5% 1.87 284.8 1.01\n(D) ✓ ✓ ✓ 4.96 13.5% 1.83 286.1 1.03\nBest rFID & (E) ✓ ✓ ✓ 4.89 13.9% 1.53 304.7 0.91\nAR 800 epochs (F) ✓ ✓ ✓ ✓ 4.89 13.9% 1.50 305.9 0.84\nTraining curves. As shown in Fig. 6, we explore the training behavior of the proposed AliTok.\nWith the increase in model parameters and training steps, both the training loss and training error\nrate decrease significantly and steadily. Additionally, larger models achieve lower loss and superior\ngFID scores with fewer training steps. Experiments verify that our generative model effectively\ninherits the scaling ability of autoregressive models. However, the gFID curves indicate that after 400\ntraining epochs, neither our AliTok-B model nor AliTok-L model shows clear signs of convergence.\nTherefore, we extend the training duration for these two models to 800 epochs to further improve the\nperformance. More analysis is provided in discussion (Sec. 4.4) and supplementary material.\nTable 4: Reconstruction quality. Size: vo-\ncabulary size.\nTokenizer Tokens Size rFID ↓\nMaskGIT [4] 256 1024 1.97\nTiTok-B [43] 64 4096 1.70\nV AR [31] 680 4096 0.92\nAliTok (Stage 1) 273 4096 0.91\nAliTok (Stage 2) 273 4096 0.84Reconstruction performance. We report the reconstruc-\ntion performance of different methods in Table 4.2, where\nV AR tokenizer employs 680 encoded tokens, all interpo-\nlated from an initial set of 256 encoded tokens. Com-\npared to V AR tokenizer, we achieve better rFID result.\nThis improvement is partly due to the additional prefix to-\nkens and the use of online feature clustering method [ 46].\nBased on it, our two-stage training strategy significantly\nenhances reconstruction performance. The results demon-\nstrate that our tokenizer not only facilitates generation but\nalso achieves excellent reconstruction ability.\n4.3 Ablation Studies\nAblation studies of main components. In Table 3, we ablate the main components of the proposed\nAliTok. After applying the causal constraint to the tokenizer decoder, the training accuracy is substan-\ntially increased by 5.3%, thus dramatically improving generative performance (gFID: 2.96 →1.88).\nThis indicates that forcing the encoded tokens to be more consistent with unidirectional dependencies,\nthey can be more easily modeled by a decoder-only autoregressive model. When prefix tokens are\nintroduced without the constraint of Laux(MSE loss imposed on prefix tokens), it fails to alleviate\nthe poor reconstruction issue observed in the first row (Figs. 7(e)) and also hardly improves the\ngenerative performance. This experimental result aligns with other studies [ 43,2], where increasing\n8\n--- Page 9 ---\nFigure 7: Reconstruction visualization comparison . (a) original image. (b) and (c) are comparison results.\n(d)-(g) illustrate incremental additions of the proposed modules on the baseline. Obvious errors are boxed in red.\nthe number of tokens enhances reconstruction ability but may degrade generative performance, as the\nautoregressive model has to deal with more tokens. With the addition of Laux, the prefix tokens are\nforced to convey coarse information about the first row of the image. This allows the generation of\nthe first row in a coarse to fine process and partially mitigates the low confidence issue during the\ngeneration of the first few tokens, thus contributing positively to generative performance. Finally, we\nexplore the improvements achieved by retraining a bidirectional decoder while using the same autore-\ngressive model weights. The two-stage approach significantly improves reconstruction performance\n(rFID: 0.91 →0.84) and also enhances the generative performance.\nAblation studies of reconstruction visualization. We further visualize the reconstruction results\nafter applying different modules on the baseline tokenizer in Fig. 7. As illustrated in Fig. 7(c),\nmodifying the tokenizer encoder to the causal form results in discrepancies in color and texture\nbetween image patches, making it difficult to achieve natural and continuous textures, along with the\nappearance of grid artifacts, such as in the keys on the keyboard. This is because the causal encoder\nprevents each image patch from perceiving the completely surrounding area during the compression\nphase, leading to inefficient compression capabilities. Conversely, altering the decoder to a causal\nform yields generally natural reconstruction results except for the first row where the reconstruction\nquality is still poor, usually with color deviations and loss of detail, as shown in Fig. 7(d). To address\nthis issue, the introduction of prefix tokens and Lauxfacilitates the reconstruction of the first row by\nproviding supplementary information. Furthermore, retraining a decoder with bidirectional attention\ncan effectively improve reconstruction smoothness and rectify certain detail inaccuracies, e.g., the\nunnatural lines in Fig. 7(f) can be well corrected. Through above designs, our final reconstruction\nresults (Fig. 7(g)) can be visually close to the baseline (Fig. 7(b)).\n4.4 Discussion\nWith the same codebook size of 4096, V AR-d30 model (2B parameters) [ 31] achieves a training\naccuracy of 6.8%, whereas our 662M model reaches a significantly higher training accuracy of\n15.8%. In this context, the reconstruction performance of the discrete tokenizer becomes a critical\nbottleneck for the generative performance. To test the limiting value of gFID that can be achieved\nunder this tokenizer, we randomly select 50 real images per class from the training set and calculate\nthe gFID after reconstruction (differently from rFID evaluation), obtaining a FID of 1.21. This\nimplies that even if the accuracy in the generation phase is close to 100% (which is not possible), the\ngeneration performance still cannot reach the upper limit of 1.21. This experimental result explains\nthe phenomenon observed in Fig. 6, where the training loss and error rate of AliTok-XL continue to\ndecrease steadily, yet the gFID metric struggles to achieve further improvement. Consequently, for\nfuture work, we consider it necessary to train an AliTok tokenizer with a larger codebook, e.g., 16384\nin LlamaGen [27], to further improve the reconstruction and generation performance.\n5 Conclusion\nIn this paper, we reveal the reason preventing existing autoregressive models from performing well:\nconventional tokenizers lead to bi-directional dependencies in the encoded tokens, conflicting with\nthe uni-directional dependencies of autoregressive models. To address this challenge, we propose\nto employ a causal decoder that establishes forward dependency among encoded tokens, aligning\nwith the modeling approach of autoregressive models. By integrating prefix tokens and employing a\ntwo-stage training strategy, we further significantly improve reconstruction performance and visual\nquality. Our novel tokenizer unleashes the power of autoregressive models, enabling a standard\ndecoder-only autoregressive model to surpass state-of-the-art diffusion models.\n9\n--- Page 10 ---\nFigure 8: More generation results on the ImageNet 256 ×256 benchmark using our AliTok-XL.\n10\n--- Page 11 ---\nA. Limitations and Societal Impacts\nLimitations. As analyzed in the discussion, since the codebook size of the tokenizer is only 4096,\nthis limits the upper bound of the generation performance. When employing larger generative models\nand increasing training steps, although there is a noticeable decline in training loss and error rates, the\ngeneration metrics is difficult to achieve further improvement. Therefore, this paper does not explore\nlarger generative models, and leaves the exploration of training a larger codebook for an AliTok-like\ngeneration-friendly tokenizer to future work.\nSocietal Impacts. The aim of this paper is to advance the development of autoregressive image\ngeneration using a standard decoder-only generative model, facilitating the unification of multi-modal\nframeworks, such as image generation and understanding. Our paper can unlock the potential of\nautoregressive models through a sequence modeling aligned tokenizer, surpassing state-of-the-art\ndiffusion models while utilizing less training and testing time. In the negative side, our method with\nhigh training accuracy may be more susceptible to dataset bias. Additionally, due to the reconstruction\nlimitations of discrete tokenizer, it is challenging to generate overly complex entities, such as faces\nand hands, poses challenges that need to be considered in practical applications.\nB. Introduction Details\nExperiments details. In the experiments depicted in Fig. 2 of the main paper, we train each tokenizer\nfor 300K steps and the autoregressive models for 200 epochs using AliTok-B autoregressive model.\nFor the tokenizer using our approach in Fig. 2, which is consistent with the model in Table 3(b)\nof the ablation experiment. For the tokenizer using conventional approach, we adopt a structure\nsimilar to our approach, only replacing the decoder with a bidirectional transformer. For the intuitive\napproach using a causal encoder, we consider the following factors to design the tokenizer: Firstly,\nit is challenging to compress the information within image patches into latent tokens with causal\nlogic. Secondly, for causal encoder, adding more learnable tokens does not effectively increase\ncomputational load, as the direction of interaction is limited. Therefore, we remove the 256 latent\ntokens from the input space of the encoder and treat the encoded patch tokens as the encoding result.\nTo ensure fairness, we relocate the 256 removed latent tokens to the input space of the bidirectional\ndecoder. Concurrently, to maintain the computational load of the encoder, we reverse the model\nsizes of the encoder and decoder by using a ViT-L model for the encoder and a ViT-B model for the\ndecoder. In this way, we ensure that the computational effort of each tokenizer is the same and the\ndesign is reasonable. In addition, the codebook utilization of each tokenizer is 100% to avoid the\nimpact on the training accuracy.\nC. Method Details\nVector quantization. After the latent tokens pass through the encoder, a continuous encoded latent\nvector zis produced. The nearest vector to z, , based on Euclidean distance, is then queried from\nthe discrete codebook Zto serve as the quantized result. This vector quantization process can be\nformalized by the following equation:\nQuant (z) = argmin\nzi∈Z∥z−zi∥. (4)\nPositional embedding in autoregressive models. We employ 1D RoPE for the 17 prefix tokens,\nwith positions ranging from 0 to 16. Following this, we use 2D PoPE for the remaining 16 ×16 tokens,\nwith positions extending from 17 to 32.\nHyperparameters. The loss applied to the prefix tokens consists only of the auxiliary MSE loss,\nwhile the remaining latent tokens are constrained by MSE, perceptual, and adversarial losses. To\nensure that the prefix tokens faithfully focus on the designated regions, we use a large coefficient α\n(set to 10), for the auxiliary loss, to make the loss magnitudes for both types of tokens remain similar.\nThe total loss for the tokenizer in the first stage is:\nLoss =α×Nprefix\nNremain×Laux+Lrec, (5)\n11\n--- Page 12 ---\nWhereNprefix\nNremainis used to balance the two types of losses based on the number of tokens, as the\ncalculation of the losses involves averaging. Here, Nprefix represents the number of prefix tokens\n(equal to 17), and Nremain represents the number of remaining latent tokens (equal to 256).\nD. Experiments\nModel configurations. For simplicity, we call the autoregressive models trained using our AliTok\ntokenizer as AliTok-B/L/XL, comprising three different sizes. For all autoregressive models, we set\nthe number of attention heads to 16. Specifically, AliTok-B has a depth of 24 and a width of 768,\nAliTok-L has a depth of 24 and a width of 1024, and AliTok-XL has a depth of 32 and a width of\n1280, following the RAR [42] configuration.\nReconstruction performance and time. In Table 5, we compare our AliTok with V AR tokenizer,\nincluding reconstruction quality and reconstruction time. To more comprehensively reflect reconstruc-\ntion quality, in addition to rFID, we also introduce common reconstruction metrics such as PSNR,\nSSIM, and LPIPS. Compared to V AR tokenizer, AliTok significantly improves PSNR and SSIM\nmetrics. However, we perform worse on the LPIPS metric, which might be attributed to our use of\nthe ConvNext-S model for calculating perceptual loss instead of the LPIPS network (VGG model).\nOur transformer-based tokenizer employs a large patch size ( e.g., 16) and utilizes fewer tokens in\nthe decoder, necessitating a larger number of parameters for enhanced computational capacity. Even\nso, our computational load (MACs) and reconstruction time remain significantly less than that of\nV AR tokenizer, with only 45.3% of the MACs and 25.6% of the decoding time compared to V AR\ntokenizer. This illustrates the performance and speed advantages of our tokenizer. Additionally, our\ntwo-stage training significantly improves SSIM and LPIPS metrics compared to the first stage, while\nintroducing only a minor increase in computational load.\nTable 5: Reconstruction performance and time . PSNR, SSIM, LPIPS are tested using random\n5000 images from the ImageNet val dataset. MACs are measured using a tensor of size (3, 256,\n256). Reconstruction time is evaluated using images with a batch size of 64, a resolution of 256, and\nBfloat16 precision. Rec. Time: reconstruction time (including both encoder and decoder). Dec. Time:\ntokenizer decoder time.\nTokenizer Tokens Size PSNR↑SSIM↑LPIPS↓rFID↓#Para.↓MACs↓Rec. Time ↓Dec. Time ↓\nV AR [31] 680 4096 22.75 0.6545 0.104 0.92 109M 303.8G 0.68s 0.39s\nOurs (Stage 1) 273 4096 26.27 0.6602 0.140 0.91 390M 128.2G 0.18s 0.08s\nOurs (Stage 2) 273 4096 26.86 0.6858 0.129 0.84 390M 137.5G 0.20s 0.10s\nImprovements of training 800 epochs. In Table 6, we show the enhancements achieved by increasing\nthe training epochs from 400 to 800, which significantly reduces gFID scores both w/o cfg and w/ cfg.\nWe further plot the training curves on the right side of Table 6. It is evident that the training loss and\nerror rate continue to decrease linearly without convergence, suggesting that extending the number of\ntraining epochs may yield further improvements. However, since we use cosine learning rate decay\nrather than a fixed learning rate, additional epochs require training from scratch. Consequently, we\nhave not further explored the improvements that might result from extending the training period.\nTable 6: Improvements and curves of training 800 epochs . 500,000 training steps corresponds to\n800 epochs. Acc.: final training accuracy. Since we employ cosine learning rate decay instead of a\nfixed learning rate, we train the models for 800 epochs from scratch rather than continuing training\nbased on the weights from the 400 epochs.\nModel Epochs Loss Acc.gFID gFID\nw/o cfg w/ cfg\nAliTok-B400 4.93 13.7% 2.69 1.64\n800 4.89 13.9% 2.39 1.50\nAliTok-L400 4.81 14.3% 2.13 1.47\n800 4.76 14.6% 1.98 1.42\n12\n--- Page 13 ---\nMore generation results. We present more generation results of our AliTok-XL model in Fig. 8.\nAs can be seen, our method is capable of producing detailed textures and realistic image quality. In\nFig. 10, Fig. 11, Fig. 12, Fig. 13, we present various images generated by AliTok-B and AliTok-XL\nacross different categories. Even our smallest model can generate great visual outcomes.\nMore reconstruction results. Fig. 9 presents additional comparisons of reconstruction results, with\nenlarged views of the detailed regions from both the intuitive approach (baseline + causal encoder)\nand our AliTok approach. From the enlarged region, it is obvious that the causal encoder causes\ntexture mismatches between different reconstructed patches with discontinuities and detail deviations.\nNotably, significant grid artifacts persist even after the usage of adversarial loss. In contrast, our\nAliTok method achieves continuous and realistic reconstruction results. Furthermore, as seen in\nFig. 9(d), solely applying the causal decoder on the baseline leads to color deviation and blurriness\nin the first row of the reconstructed images due to poor reconstruction quality, especially in the first\nimage patch. In certain scenes, like the edge of the plate in the first row, unnatural transitions may\nalso be present. While after implementing our proposed series of improvements, including prefix\ntokens, auxiliary loss, and a two-stage training strategy, this issue is significantly alleviated. Our\nAliTok (Fig. 9(e)) can ultimately achieve similar reconstruction quality and visual perception as the\nbaseline approach (Fig. 9(b)).\nFigure 9: More reconstruction result comparisons . The same regions in (c) and (e) are enlarged for\ndetailed comparison.\nHyperparameters for sampling. We list the sampling hyperparameter settings in Table 7. When cfg\nis not used, we employ a temperature hyperparameter. When cfg is used, we apply pow-cosine as the\nguidance schedule, following RAR [42].\nModel Temperature Scaler Power Guidance Scale\nAliTok-B 0.98 w/o cfg\nAliTok-L 0.99 w/o cfg\nAliTok-XL 0.97 w/o cfg\nAliTok-B 1.00 1.8 16\nAliTok-L 1.00 0.7 6\nAliTok-XL 1.00 1.4 13Table 7: Hyperparameters for sam-\npling include without and with cfg.\nWhen cfg is used, we employ pow-\ncosine as the guidance schedule, follow-\ning RAR [42].\n13\n--- Page 14 ---\nBald eagle (22)\nGoldfish (1)\nCheck (436)AliTok -B (177M) AliTok -XL (662M)Figure 10: More generation results. The left images are generated by AliTok-B (177M), and the\nright images are generated by AliTok-XL (662M).\n14\n--- Page 15 ---\nCoral reef (973)\nSchooner (780)\nBoathouse (449)AliTok -B (177M) AliTok -XL (662M)Figure 11: More generation results. The left images are generated by AliTok-B (177M), and the\nright images are generated by AliTok-XL (662M).\n15\n--- Page 16 ---\nFountain (562)\nIce cream (780)\nAgaric (980)AliTok -B (177M) AliTok -XL (662M)\nFigure 12: More generation results. The left images are generated by AliTok-B (177M), and the\nright images are generated by AliTok-XL (662M).\n16\n--- Page 17 ---\nSpace shuttle (812)\nLorikeet (90)\nKnot (616)AliTok -B (177M) AliTok -XL (662M)Figure 13: More generation results. The left images are generated by AliTok-B (177M), and the\nright images are generated by AliTok-XL (662M).\n17\n--- Page 18 ---\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023.\n[2]Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, O ˘guzhan Fatih Kar, Elmira\nAmirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images\ninto 1d token sequences of flexible length. arXiv preprint arXiv:2502.13967 , 2025.\n[3]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the IEEE/CVF international conference on computer vision , pages 9650–9660, 2021.\n[4]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked\ngenerative image transformer. In CVPR , 2022.\n[5]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In CVPR , 2021.\n[6]Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun,\nKaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models\nwith continuous tokens. arXiv preprint arXiv:2410.13863 , 2024.\n[7]Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing\nLiu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis.\narXiv preprint arXiv:2412.04431 , 2024.\n[8]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems , 30, 2017.\n[9]Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598 , 2022.\n[10] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv\npreprint arXiv:2410.21276 , 2024.\n[11] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-\nChieh Chen. Democratizing text-to-image masked generative models with compact text-aware\none-dimensional tokens. arXiv preprint arXiv:2501.07730 , 2025.\n[12] Diederik P Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013.\n[13] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset,\nShahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images\ndataset v4: Unified image classification, object detection, and visual relationship detection at\nscale. International journal of computer vision , 128(7):1956–1981, 2020.\n[14] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive\nimage generation using residual quantization. In CVPR , 2022.\n[15] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image\ngeneration without vector quantization. In NeurIPS , 2024.\n[16] Wenze Liu, Le Zhuo, Yi Xin, Sheng Xia, Peng Gao, and Xiangyu Yue. Customize your visual\nautoregressive recipe with set autoregressive modeling. arXiv preprint arXiv:2410.10511 , 2024.\n[17] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-\nmagvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv\npreprint arXiv:2409.04410 , 2024.\n18\n--- Page 19 ---\n[18] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden,\nand Saining Xie. SIT: Exploring flow and diffusion-based generative models with scalable\ninterpolant transformers. arXiv preprint arXiv:2401.08740 , 2024.\n[19] Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang\nDai, Yujun Shi, Xuan Ju, et al. Token-shuffle: Towards high-resolution image generation with\nautoregressive models. arXiv preprint arXiv:2504.17789 , 2025.\n[20] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar\nquantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505 , 2023.\n[21] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman,\nand Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders.\narXiv preprint arXiv:2412.01827 , 2024.\n[22] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV , 2023.\n[23] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. In The Twelfth International Conference on Learning Representations .\n[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision , 115(3):211–252, 2015.\n[25] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems , 29,\n2016.\n[26] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.\n[27] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.\nAutoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint\narXiv:2406.06525 , 2024.\n[28] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.\nInProceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9,\n2015.\n[29] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang\nZhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autore-\ngressive transformer. arXiv preprint arXiv:2410.10812 , 2024.\n[30] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint\narXiv:2405.09818 , 2024.\n[31] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive\nmodeling: Scalable image generation via next-scale prediction. In NeurIPS , 2024.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n[34] Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural\nnetworks. In International conference on machine learning , pages 1747–1756. PMLR, 2016.\n[35] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS ,\n2017.\n19\n--- Page 20 ---\n[36] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan\nZhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need.\narXiv preprint arXiv:2409.18869 , 2024.\n[37] Pingyu Wu, Kai Zhu, Yu Liu, Liming Zhao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Improved\nvideo vae for latent video diffusion model. arXiv preprint arXiv:2411.06449 , 2024.\n[38] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong\nLin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single trans-\nformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528 ,\n2024.\n[39] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling\nvisual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint\narXiv:2504.08736 , 2025.\n[40] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimiza-\ntion dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423 , 2025.\n[41] Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen,\nYong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing\nGong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats\ndiffusion–tokenizer is key to visual generation. In ICLR , 2024.\n[42] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autore-\ngressive visual generation. arXiv preprint arXiv:2411.00776 , 2024.\n[43] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen.\nAn image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550 ,\n2024.\n[44] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin,\nand Saining Xie. Representation alignment for generation: Training diffusion transformers is\neasier than you think. arXiv preprint arXiv:2410.06940 , 2024.\n[45] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural\nInformation Processing Systems , 32, 2019.\n[46] Chuanxia Zheng and Andrea Vedaldi. Online clustered codebook. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 22798–22807, 2023.\n20",
  "text_length": 55224
}