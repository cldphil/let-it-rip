{
  "id": "http://arxiv.org/abs/2506.04251v1",
  "title": "Language-Guided Multi-Agent Learning in Simulations: A Unified Framework\n  and Evaluation",
  "summary": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.",
  "authors": [
    "Zhengyang Li"
  ],
  "published": "2025-06-01T06:46:49Z",
  "updated": "2025-06-01T06:46:49Z",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.MA"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04251v1",
  "full_text": "--- Page 1 ---\n1 \n \nLanguage -Guided Multi -Agent Learning in Simula tions: A \nUnified Framework and Evaluation  \n \nZhengyang Li , Member, IEEE  \n \n \n Abstract— This paper introduces LLM -MARL, a unified \nframework that incorporates large language models (LL Ms) into \nmulti -agent reinforcement learning (MARL) to enhance \ncoordination, communication, and generalization in simulated \ngame environments. The framework features three modular \ncomponents of Coordinator, Communicator, and Memory, which \ndynamically genera te subgoals, facilitate symbolic inter -agent \nmessaging, and support episodic recall. Training combines PPO \nwith a language -conditioned loss and LLM query gating. LLM -\nMARL is evaluated in Google Research Football, MAgent Battle, \nand StarCraft II. Results sh ow consistent improvements over \nMAPPO and QMIX in win rate, coordination score, and zero -shot \ngeneralization. Ablation studies demonstrate that subgoal \ngeneration and language -based messaging each contribute \nsignificantly to performance gains. Qualitative analysis reveals \nemergent behaviors such as role specialization and \ncommunication -driven tactics. By bridging language modeling \nand policy learning, this work contributes to the design of \nintelligent, cooperative agents in interactive simulations. It offer s \na path forward for leveraging LLMs in multi -agent systems used \nfor training, games, and human -AI collaboration.  \n \nIndex Terms —large language model , multi -agent, simulations, \ngames . \nI. INTRODUCTION  \n Multi -agent reinforcement learning (MARL) has become a \ncornerstone for enabling intelligent cooperation and \ncompetition in complex, dynamic env ironments. From \nautonomous vehicle coordination to real -time strategy games \nand swarm robotics, MARL empowers systems of agents to \nmake decisions that maximize collective or individual rewards \nthrough interaction and learning. Despite numerous \nbreakthrough s, including the development of algorithms like \nQMIX, MAPPO, and MADDPG, MARL systems continue to \nface major obstacles in communication efficiency, task \ngeneralization, strategic coordination, and long -term memory \nmanagement.  \n Traditional MARL approaches rely heavily on structured \nstate representations, end -to-end learned policies, and domain -\nspecific coordination rules, which often fail to scale to novel or \nlanguage -driven tasks. One of the core limitations lies in agen ts' \ninability to process symbolic instructions, reuse high -level \nknowledge, or effectively communicate in naturalistic ways. \nThese constraints result in brittle agents that underperform in \nscenarios demanding adaptive planning, zero -shot \ngeneralization, or  coordination under sparse and delayed \nrewards.  \n Concurrently, large language models (LLMs) such as GPT, \nPaLM, and LLaMA have demonstrated remarkable capabilities \nin language understanding, commonsense reasoning, \ninstruction following, and few -shot adaptatio n. LLMs can parse \ncomplex prompts, generate strategic plans, answer multi -turn queries, and synthesize structured output from ambiguous \ninput —all skills that are critical but often lacking in \nconventional MARL agents.  \n The integration of LLMs with MARL pres ents a \ntransformative opportunity: the symbolic reasoning and \ncommunication capabilities of LLMs can be harnessed to \naugment the data -driven trial -and-error learning of MARL \nagents. This hybridization may resolve several persistent \nchallenges in MARL, incl uding:  \n  \n1) Instruction following: enabling agents to parse human \ncommands or abstract objectives  \n2) Emergent communication: synthesizing interpretable, \ncompositional messages between agents  \n3) High -level planning: generating sub -goals and action \nsequences for temporally extended tasks  \n4) Memory and recall: retrieving past experiences or \nstrategies in context -dependent ways  \n \n In th is paper, we propose LLM -MARL, a unified \nframework that leverages LL Ms to enhance multi -agent \nreinforcement learning. Our core hypothesis is that LLMs can \nact as both a centralized coordinator and a decentralized \ncommunicator within multi -agent systems. Specifically, we \ndesign modular components that incorporate LLMs into the \nMARL learning loop, enabling agents to:  \n \n1) Receive structured sub -tasks derived from natural language \ninstructions  \n2) Communicate and negotiate using shared language tokens  \n3) Access episodic memory and few -shot demonstrations \nstored in language format  \n \n We conduct rigorous evaluations across three diverse \nenvironments —Google Research Football, MAgent, and \nStarCraft II —to test LLM -MARL’s ability to improve task \ncompletion, cooperation metrics, and zero -shot generalization. \nOur findings suggest that integr ating LLMs into MARL \narchitectures not only enhances agent performance, but also \nintroduces scalable abstractions that bridge the gap between \nsymbolic AI and embodied learning.  \n \nThe contributions of this work are threefold:   \n \n1) We propose a novel framework tha t integrates LLMs into \nMARL systems via coordinator, communicator, and \nmemory modules.  \n2) We design training procedures that support dynamic \nprompting, LLM -guided supervision, and RL optimization \nin tandem.  \n3) We empirically demonstrate significant gains in  \n\n--- Page 2 ---\n2 \n \ncooperation, generalization, and language -grounded policy \nlearning across multiple complex environments.  \n \n Through this work, we aim to lay the foundation for the next \ngeneration of intelligent agents that can  learn not only from \nexperience, but also from language, knowledge, and \ncommunication —unlocking new frontiers in reinforcement \nlearning, robotics, and human -AI collaboration.  \n \nII. RELATED WORK   \n Research in multi -agent reinforcement learning (MARL) \nhas grown extensively over the past dec ade, driven by \napplications in autonomous systems, video games, and \ndecentralized robotics. A broad range of learning paradigms \nhave been proposed to address the inherent non -stationarity, \npartial observability, and coordination complexity in multi -\nagent s ettings. Early approaches to MARL include independent \nQ-learning variants, which suffer from stability issues due to \nenvironment non -stationarity. More robust approaches, such as \nCentralized Training with Decentralized Execution (CTDE), \nhave become the dom inant paradigm. Notable algorithms under \nthis umbrella include QMIX, which decomposes the global \nvalue function into monotonic per -agent utilities, and MAPPO, \nwhich applies trust region policy optimization in multi -agent \nscenarios.  \n Recent advancements have  emphasized the role of \ncommunication in MARL. Techniques such as CommNet, \nDIAL, and IC3Net explicitly model communication protocols, \nallowing agents to learn to exchange messages that enhance \ncooperative performance. These works have shown that \nemergent c ommunication can lead to significant gains in \ncooperative tasks; however, the communication is typically \nconstrained to low -dimensional vectors and lacks \ninterpretability or transferability. More recent frameworks like \nSCHEDNet and FOP leverage attention m echanisms to allocate \ncommunication bandwidth dynamically, but they still operate \nwithin narrow message encoding schemes.  \n Another active direction focuses on improving \ngeneralization and transfer in MARL. This includes domain \nrandomization, curriculum lear ning, and population -based \ntraining. Meta -MARL approaches and agent modeling \ntechniques attempt to enable agents to adapt to unseen \nopponents or partners. Nevertheless, these methods often \nrequire retraining or large -scale sampling, limiting scalability \nin real-world applications.  \n In parallel, Large Language Models (LLMs) have \ndemonstrated emergent general intelligence in tasks requiring \ncomplex reasoning, abstraction, and planning. In the \nreinforcement learning (RL) context, works like Decision \nTransformer  have reimagined RL as a sequence modeling \nproblem, while ReAct combines reasoning and acting through \nprompting. Other studies explore the use of LLMs as zero -shot \nplanners or agents, often relying on few -shot in -context \nlearning and prompt engineering. La nguage models have also \nbeen used for task generalization, policy distillation, and \ninstruction grounding in robotics.  \nRecent advancements in integrating large language models into \nagent -based systems have begun to illuminate new directions for decision -making and coordination. For instance, the \nVoyager framework (Xu et al., 2023) introduces an LLM -\npowered agent capable of lifelong learning in Minecraft, \ndemonstrating capabilities such as dynamic skill planning and \nenvironment adaptation. Similarly, AutoRT (Huang et al., \n2023) leverages LLMs for natural language -based robot control \nin real -world environments, showcasing the potential for LLMs \nto translate high -level goals into structured executable \ncommands.  \n In simulated environments, Mind’s Eye (Singh et al ., 2023) \nand InnerMonologue (Huang et al., 2022) highlight LLMs' \ncapacity to generate internal reasoning chains that guide agents \nthrough complex planning and exploration tasks. These \napproaches suggest that LLMs are not only effective at \ninterpreting inst ructions but also at simulating deliberative \nprocesses and hypothesizing future actions.  \n Moreover, the ReAct framework (Yao et al., 2022) and \nReLM (Chiang et al., 2023) have emphasized the dual role of \nLLMs in both reasoning and acting. These studies reinf orce the \nfeasibility of combining natural language reasoning with \naction -oriented environments, which resonates with our use of \nLLMs for subgoal generation and policy conditioning.  \nDespite their promise, existing LLM -agent integrations often \nfocus on singl e-agent paradigms or domain -specific \nconstraints. They lack general frameworks for distributed, \nemergent communication or multi -agent policy learning. In \ncontrast, our work extends these ideas into multi -agent \ncoordination, leveraging language not only for  instruction \ninterpretation but also for inter -agent negotiation and memory -\nbased adaptation. This positions LLM -MARL at the \nintersection of symbolic reasoning, embodied cognition, and \nscalable collective intelligence.  \nOur work contributes to this emerging  intersection by explicitly \nintegrating LLMs into MARL in a modular, reusable way. We \nfocus on three under -addressed aspects in prior work:  \n \n1) LLM -assisted coordination , where sub -goals and \nstrategies are dynamically generated from task descriptions \nand state  summaries.  \n2) Natural language communication , enabling agents to \ninteract via interpretable symbolic messages rather than \nlatent encodings.  \n3) Memory augmentation , where episodic experiences are \nstored and recalled using language representations for few -\nshot ge neralization.  \n \n By grounding our approach in both the MARL and LLM \nliterature, we aim to advance the capabilities of collaborative \nagents through structured language interfaces, bridging gaps \nbetween symbolic and sub -symbolic learning systems.  \nIII. METHOD  \nA. Overview  \n Our LLM -MARL fr amework introduces a hybrid \narchitecture that integrates LLM capabilities into the MARL \ntraining and execution pipeline. It is designed to augment agent \n--- Page 3 ---\n3 \n \nperformance in complex environments by leveraging the \nlinguistic generalization, reasoning, and memory capabilities of \nLLMs. The framework features three key modules:  \n \n• LLM -Coordinator:  Functions as a centralized \nplanner that parses high -level tasks and decomposes \nthem into structured sub -goals for individual agents, \nfacilitating temporal and spatial coordinat ion. For \nexample, in a capture -the-flag environment, the \ncoordinator decomposes a \"defend and retrieve\" \ncommand into roles like \"base defense\" and \"flag \npursuit\" based on agent proximity and game state.  \n• LLM -Communicator:  Serves as a decentralized \ncommunica tion interface, enabling agents to encode, \ndecode, and interpret emergent natural language \nmessages for coordination. Agents exchange symbolic \nmessages such as \"cover me\" or \"focus fire\" generated \nfrom a learned prompt -response loop, allowing for \nscalable communication even in large agent \npopulations.  \n• LLM -Memory:  Acts as a knowledge base to store and \nretrieve episodic experiences, facilitating few -shot \nadaptation and long -horizon planning. For instance, \nthe memory system can recall previously successful \nteam strategies against specific opponents and suggest \nthem in new but similar contexts.  \n \nB. Architecture  \n The LLM -MARL architecture builds on the centralized \ntraining with decentralized execution (CTDE) paradigm by \nembedding LLMs as both central and distributed m odules. Each \nagent is composed of several interconnected components:  \n1) Observation Encoder : A CNN or MLP module that \nencodes raw observations (e.g., images, feature maps) into \na compact latent representation. This embedding is passed \nboth to the policy networ k and to the LLM query interface.  \n2) Policy Network:  A feedforward or recurrent neural \nnetwork (e.g., LSTM, GRU, Transformer) trained with \nPPO that maps the latent observation and auxiliary inputs \ninto action probabilities. The action space may be discrete \nor continuous depending on the environment.  \n3) LLM Query Interface:  A structured prompt builder that \ntransforms current game context —including local \nobservations, team states, and historical trajectories —into \nnatural language queries. These queries are sent to a pre -\ntrained LLM (e.g., GPT -3.5, PaLM) vi a API or embedded \nserver for interpretation.  \n4) Language Adapter Module:  Converts LLM output into \nusable information. This may include:  \n High -level sub -goals: translated into vectorized \nrepresentations for goal -conditioning . \n Natural language messages: passed directly to other \nagents or summarized . \n Semantic cues: injected into the policy network via \nattention or gating mechanisms . \n5) Communication Buffer:  A temporal sequence memory \nthat stores recent messages received from teammates  and LLM outputs. This buffer is accessed recurrently during \npolicy rollout and may be filtered with relevance attention.  \n6) Memory Retriever : An optional module that indexes past \nexperiences based on contextual similarity. For instance, \nwhen encountering an unfamiliar map layout, the retriever \nqueries for previously successful strategies in topologically \nsimilar environments, using LLMs for semantic similarity \nscoring.  \n  \nC. LLM -Guided Subgoal Decomposition   \n A central innovation of LLM -MARL is its ability to \ndynamica lly generate subgoals for agents through natural \nlanguage processing. Traditional MARL frameworks often \nrequire hand -designed reward shaping or role assignments to \nachieve coordinated behavior. In contrast, our framework \nformulates subgoal generation as a language -grounded task \ndelegation problem and offloads the reasoning process to an \nLLM.  \nAt each timestep, the centralized LLM -Coordinator module \nreceives a global state summary 𝑆𝑡, a task instruction , and \noptionally the agents’ histories 𝐻1:𝑁. These i nputs are \nstructured into a templated prompt such as:  \n\"You are the coordinator for a team of 5 agents. The task is: 𝑇. \nBased on the current state: 𝑆𝑡, assign one subgoal for each \nagent.\"  \nThe LLM outputs natural language subgoals, one per agent, \nwhich ar e then parsed into internal representations using a goal \nencoder. These subgoals are conditionally injected into the \nagents' policy networks either as auxiliary conditioning \nvariables or through goal -specific reward reshaping.  \nMathematically, subgoal 𝑔𝑖𝑡 for agent 𝑖 at timestep 𝑡 is given by:  \n \n𝑔𝑖𝑡=𝑃𝑎𝑟𝑠𝑒 (𝐿𝐿𝑀 (𝑃𝑟𝑜𝑚𝑝𝑡 (𝑆𝑡,𝑇,𝐻𝑖𝑡))) \n \nFig. 1: Overview of the LLM -MARL architecture.  The system \nextends centralized training with decentralized execution (CTDE) by \nembedding large language models (LLMs) into multiple agent \nmodules. Each agent includes an observation encoder, a PPO -based \npolic y network, and a query interface that formulates structured \nprompts for the LLM. The language adapter transforms LLM outputs \ninto subgoals, messages, or semantic cues. Communication buffers and \nmemory retrievers allow for context -aware coordination and epi sodic \nstrategy reuse.  \n--- Page 4 ---\n4 \n \n To prevent over -reliance on the LLM and encourage policy \ngeneralization, we incorporate a hybrid learning objective that \ncombines reinforcement learning rewards with a supervised \nloss on subgoal adherence. This encoura ges the policy to align \nwith LLM -generated strategies while retaining autonomy.  \nThis LLM -Guided Subgoal Decomposition mechanism enables \nthe MARL system to handle dynamic role allocation, long -\nhorizon planning, and rapidly changing environments in a \nscalabl e and semantically interpretable way.  \n \nD. Training Paradigm  \n The training paradigm of LLM -MARL integrates \nreinforcement learning with language -based supervision, \nenabling agents to learn grounded behaviors from both \nenvironmental feedback and LLM guidance. It follows the \ncentr alized training with decentralized execution (CTDE) \nframework, with additional auxiliary pathways introduced to \nincorporate LLM -informed knowledge.  \n \nThe overall training process consists of four stages:  \n \nStage 1: Language -Augmented Rollout Collection :  \n Agents begin by interact ing with the environment under \npartial guidance from the LLM -Coordinator. At each timestep, \nthe LLM generates subgoals and optional language feedback \nbased on task instructions and the global state. These are cached \nalongside trajectories to form a languag e-augmented dataset:  \n \n𝐷={(𝑠𝑡,𝑎𝑡,𝑟𝑡,𝑠𝑡+1,𝑔𝑡𝐿𝐿𝑀,𝑚𝑡𝐿𝐿𝑀)}𝑡=1𝑇 \n \nwhere 𝑔𝑡𝐿𝐿𝑀 denotes the subgoal assigned by the LLM and \n𝑚𝑡𝐿𝐿𝑀 represents any intermediate message or strategic hint.  \n \nStage 2: Subgoal -Aligned Policy Learning :  \n Using the da ta collected in Stage 1, we jointly optimize the \npolicy network with two objectives:  \n Standard PPO objective to maximize expected return:  \n \nℒ𝑠𝑢𝑏𝑔𝑜𝑎𝑙 =𝔼𝑡[min (𝑟𝑡(𝜃)⋅𝐴𝑡̂,𝑐𝑙𝑖𝑝 (𝑟𝑡(𝜃),1−𝜀,1+𝜀)⋅𝐴𝑡̂)] \n \n Subgoal alignment loss to encourage policy adherenc e \nto LLM -generated subgoals:  \nℒ𝑠𝑢𝑏𝑔𝑜𝑎𝑙 =𝔼𝑡[𝐶𝐸(𝜋(𝑎𝑡|𝑠𝑡,𝑔𝑡𝐿𝐿𝑀),𝑎𝑡𝐿𝐿𝑀)] \nThe combined objective is:  \n \nℒ𝑡𝑜𝑡𝑎𝑙 =ℒ𝑅𝐿+𝜆𝑔⋅ℒ𝑠𝑢𝑏𝑔𝑜𝑎𝑙  \n \nStage 3: Communication Refinement via Language \nMessages :  \n If the communicator module is active, agents learn to \nencode and decode messages using prompts shaped by their \nlocal observations. The communication buffer is used to \nsupervise message quality via contrastive losses or imitation \nlearning from high -perform ing episodes. This step ensures that \nemergent communication aligns with interpretable language \nconstructs and improves coordination.  \n Stage 4: Gating and Prompt Adaptation . \n To manage the LLM query cost and avoid over -dependence, \na lightweight gating policy 𝜋𝑔𝑎𝑡𝑒  is trained to determine \nwhether to query the LLM at each step. The reward for querying \nis shaped by performance improvement over baseline rollouts \nand penalized by computational cost:  \n \nℒ𝑔𝑎𝑡𝑒 =𝔼𝑡[(𝑅𝑡𝐿𝐿𝑀−𝑅𝑡𝑛𝑜𝐿𝐿𝑀)−𝛼𝐶𝑞𝑢𝑒𝑟𝑦  \n \n In par allel, meta -prompt learning is optionally employed \nwhere agents adapt their prompts over time based on \ndownstream task success.  \n Through this staged approach, the LLM -MARL framework \nachieves stable and interpretable learning dynamics, enabling \nagents to coo rdinate effectively under human -aligned goals \nwhile maintaining robust autonomous decision -making.  \n \nE. Implementation Details  \n We implemented the LLM -MARL framework using \nPyTorch and integrated reinforcement learning components \nthrough the RLlib library. Al l policies were trained using \nProximal Policy Optimization (PPO) with a shared central critic \nunder the CTDE paradigm. For each agent, the policy network \nconsisted of a two -layer MLP with 256 hidden units or a single -\nlayer GRU for recurrent variants. The L LM interaction was \nmediated through the OpenAI GPT -3.5 API with a maximum \ncontext window of 2048 tokens and a temperature of 0.7. All \nprompts were constructed using environment -specific \ntemplates, and previous interactions were stored in a rolling \ncache to  minimize redundant queries.  \n Subgoal vectors derived from LLM outputs were embedded \nvia a 128 -dimensional learned goal encoder. Messages were \nencoded/decoded using a 2 -layer Transformer with 4 attention \nheads. For the LLM gating module, a binary classifier  was \ntrained using a small MLP (2× 64) with sigmoid output, \nsupervised by reward differences with and without LLM \nguidance.  \n Training was conducted over 5 million timesteps per \nenvironment using Adam optimizer with a learning rate of 5e -\n4, γ = 0.99, λ = 0.95 , and a PPO clip ratio of 0.2. All experiments \nused three random seeds. Each policy update used 2048 \nsampled steps per batch and a minibatch size of 256. Entropy \nregularization (0.01) was used to encourage exploration.  \nExperiments were conducted on a singl e NVIDIA RTX 4080 \nGPU with 16GB VRAM.  LLM queries were executed \nasynchronously on a separate node using batching and backoff \nretry logic to avoid API throttling. During evaluation, a prompt \ncache ensured no external API calls.  \n We did not fine -tune the LLM;  instead, we relied on prompt \nengineering and reward shaping to adapt its outputs to task -\nspecific roles. No trainable parameters were introduced inside \nthe LLM module itself, ensuring modular deployment.  \n \n \n \n--- Page 5 ---\n5 \n \nIV. EXPERIMENTS  \n To evaluate the effectiveness of the  LLM -MARL \nframework, we conduct extensive experiments across a diverse \nset of benchmark environments, comparing against strong \nbaselines and analyzing the contribution of each proposed \nmodule. Our objectives are to measure (i) the impact of LLM \nintegration  on coordination and task performance, (ii) the \nquality of emergent communication, and (iii) generalization to \nunseen scenarios.  \n \nA. Experimental Environments  \n We select three representative environments, each \nemphasizing different coordination and complexi ty challenges:  \n Google Research Football (GRF):  A cooperative 2v2 \nand 3v3 sports simulation environment with dense \nspatial dynamics and sparse scoring rewards. Success \nrequires coordinated offense and adaptive defense \nstrategies.  \n MAgent (Battle and Pursuit) : Large -scale multi -\nagent environments with over 20 agents per team. \nAgents must learn swarm behavior, attack coverage, \nand spatial control. The action space is discrete and \npartially observable.  \n StarCraft II Micromanagement Tasks:  Focused on \ntactical unit -level control, this domain introduces \nasymmetric roles, unit -type heterogeneity, and fast -\npaced decision -making under delayed feedback.  \n  B. Baselines  \n We compare LLM -MARL against the following baseline \nmethods:  \n1) MAPPO: A state -of-the-art CTDE algorithm for \ncontinuous and discrete action MARL.  \n2) QMIX: A monotonic value decomposition approach that \nassumes additive utility over agents.  \n3) RMAPPO: A recu rrent variant of MAPPO with partial \nobservability.  \n4) No-LLM (Ablation ): Our framework without any LLM \nmodules (pure PPO + attention).  \n5) No-Comm: Our framework without the communication \nbuffer or LLM -Communicator.  \n6) No-Subgoal: LLM guidance disabled; agents use \nunstructured inputs only.  \n  \nC. Evaluation Metrics  \n We use the following metrics for quantitative evaluation:  \n• Win Rate (%):  Percentage of episodes in which the \nagent team completes the objective.  \n• Coordination Score:  Measures agent co -location, \nsynchronized action divers ity, and temporal alignment.  \n• Language Grounding Accuracy:  Assesses how well \nthe agents align their behavior with the LLM -provided \nmessages or subgoals.  \n• Zero -Shot Generalization:  Performance on unseen \nmap layouts or task variations without additional tr aining.  \n• Sample Efficiency:  Number of environment steps \nrequired to reach 80% of final performance.   \n  \nFig. 2. Win rate comparison across three multi -agent environments (Google Research Football, MAgent, and StarCraft II). LLM -MARL \nconsistently outperforms strong baselines including MAPPO and QMIX. Ablation variants (No -LLM, No -Comm, No -Subgoal) illustrate the \nimportance of each proposed module, especially subgoal decomposition and communication. Results highlight the effectiveness o f LLM \nintegration for improving coordination and generalization.  \n \n\n--- Page 6 ---\n6 \n \nD. Results and Analysis  \n Our results demonstrate the advantages of LLM -MARL \nacross all environments:  \n In GRF , LLM -MARL achieves a win rate of 81.2% in \n3v3 scenarios, outperforming MAPPO (69.4%) and \nQMIX (61.7%). Coordination scores show agents \ndynamically switching between defense and offense \nroles guided by LLM subgoal prompts.  \n In MAgent , LLM -MARL scales to over 40 agents and \nshows better area coverage and group maneuvering. \nThe learned communication  includes messages like \n“split right flank” and “circle back,” interpreted and \nacted upon by teammates.  \n In StarCraft II , agents demonstrate improved tactical \nfocus (e.g., prioritizing healers or high DPS units) and \nshow a +9.2% improvement in win rate over  MAPPO, \nwith higher resilience to fog -of-war and action delays.  \n Ablation studies confirm the utility of each module. \nRemoving the subgoal module leads to up to 13% performance \ndrop. Communication buffer removal degrades coordination \nmetrics. The LLM query gate reduces unnecessary LLM usage \nby 43% with no performance degradation.  \n Qualitative analysis highlights the emergence of \ninterpretable strategy discussions and adaptive role negotiation \namong agents. Zero -shot generalization improves by over 20% \nin out -of-distribution maps, showcasing the strength of \nlanguage -conditioned learning.  \n These results validate the hypothesis that LLMs, when \nproperly integrated, can serve as powerful coordinators and \ncommunicators in MARL systems, yielding significant gains in \nrobustness, interpretability, and strategic depth.  \n Beyond quantitative metrics, we qualitatively analyzed \nagent behavior to understand how LLM integration affected \nlearning dynamics. In GRF, agents guided by LLM -generated \nsubgoals consistently adopted role specialization. For instance, \nwhen the instruction w as “defend and retrieve,” one agent remained near the goal line while the other moved to intercept \nor retrieve the ball, demonstrating task -aware positioning \nwithout hard -coded role assignment. Interestingly, when the \nLLM was temporarily disabled, both age nts defaulted to ball -\nfollowing behavior, resulting in frequent over -clustering and \nfailed coverage.  \n In MAgent’s Battle scenario, LLM -MARL agents exhibited \nstrategic encirclement behaviors and distributed formation -\nbased tactics. Communication messages suc h as “split and \npinch” or “cover zone C2” emerged early in training and \nstabilized after ~5 million steps. These patterns were absent in \nNo-Comm and No -Subgoal baselines, where agents tended to \nconverge toward greedy targeting with minimal spatial spread. \nOne notable emergent behavior was a “feint” maneuver, where \na subset of agents pretended to retreat, triggering opponent \noverextension before ambushing from flanks —a tactic never \nexplicitly trained but facilitated by LLM -mediated \ncoordination.  \n In the StarC raft II micromanagement tasks, LLM guidance \ncontributed to more context -aware unit prioritization. Against \nasymmetric unit mixes, LLM -MARL agents learned to target \nhigh-damage or support units (e.g., Medics, Siege Tanks) early, \nwhereas MAPPO and QMIX showe d less focused targeting \nunder identical training durations. Additionally, the LLM -\nenabled memory module allowed agents to recall past \nsuccessful strategies on similar terrain layouts. For example, in \na repeated “corridor” map variant, the system reused a \npreviously learned “pull -back lure” maneuver without further \ngradient updates.  \n We also examined failure cases. In GRF, LLM -generated \nsubgoals occasionally misaligned with rapidly evolving states, \nespecially during high -speed counterattacks. In these cases,  \nagent hesitation was observed, stemming from outdated \nsubgoal commitments. This highlights a current limitation of \nprompt -to-subgoal latency and suggests the need for finer \ntemporal grounding or recurrent LLM querying. In MAgent, \ncommunication overload wa s another failure mode —when all  \nFig. 3. Comparison of learned attention distributions between a baseline MARL agent (left) and an LLM -MARL agent (right).  The LLM -guided \npolicy exhibits sharper, localized attention, especially around relevant teammates and enemy clusters, indicating more contex t-aware \ncoordination and representation learning.  \n \n\n--- Page 7 ---\n7 \n \nagents issued messages simultaneously, the communication \nbuffer became noisy, causing coordination collapse under \nmessage collisions.  \n Lastly, we visualized agent attention weights from \ncommunication -aware policy heads. Heatm aps revealed that \nagents trained with LLM guidance exhibited sharper, more \nlocalized attention toward allies and enemies within contextual \nranges, while vanilla MARL baselines maintained diffuse, less \ninformative attention distributions. This suggests that  language -\ninformed policies not only benefit from external reasoning but \nalso lead to improved representation learning internally.  \nV. DISCUSSION AND FUTURE WORK \nThe experimental results substantiate the potential of LLMs \nto act as both strategic coordinators and adaptive communicators \nin multi -agent reinforcement learning systems. Through \nmodular integration of LLM -based subgoal decomposition, \nnatural language com munication, and episodic memory recall, \nLLM -MARL significantly improves learning efficiency, \ncooperation metrics, and zero -shot generalization. These \nfindings open several important avenues for future research.  \nScalability and Efficiency:  While LLMs provid e powerful \nreasoning capabilities, querying them during training and \nexecution introduces computational and latency costs. Our \ngating mechanism mitigates this partially, but future work could \nexplore local model distillation, adaptive token truncation, or \nlightweight LLM alternatives to reduce runtime overhead.  \nRobust Prompt Engineering:  As the system partially relies \non templated prompts to extract subgoals and decisions from \nLLMs, its robustness is sensitive to prompt phrasing and context \nquality. One pro mising direction is meta -prompt learning, where \nagents autonomously evolve or adapt prompts during training, \nguided by performance feedback or reward shaping.  \nLearning in Open Worlds:  The current experiments focus \non bounded and rule -driven simulation envi ronments. Applying \nLLM -MARL to open -ended, open -world games (e.g., \nMinecraft, No Man’s Sky) or mixed reality settings could test its \ncapability to support long -horizon planning, human -agent \nteaming, and procedural content adaptation.  \nMulti -modal and Embodi ed Interaction:  Although our \nframework supports natural language input and communication, \nreal-world applications often require integrating vision, sound, \nand physical embodiment. Extending LLM -MARL with vision -\nlanguage models (e.g., Flamingo, GPT -4V) or s ensorimotor \nembeddings could bridge the gap toward general embodied \nagents.  \nTheory and Analysis:  From a theoretical perspective, \nLLM -MARL invites investigation into the convergence \nproperties of language -guided learning, the alignment between \nlanguage prio rs and learned policy gradients, and formal \ncharacterizations of coordination complexity in linguistic multi -\nagent systems.  \nIn summary, LLM -MARL offers a blueprint for combining \nstructured world knowledge with interactive learning. By \nunifying symbolic rea soning and reinforcement -based \nadaptation, this work paves the way toward intelligent, \ncooperative systems capable of operating across dynamic, \nlanguage -rich environments.   VI. CONCLUSION  \nThis paper presents LLM -MARL, a unified framework that \nintegrates large language models into multi -agent reinforcement \nlearning to address longstanding challenges in coordination, \ngeneralization, and emergent communication. By leveraging \nLLMs as dynamic su bgoal generators, language -based \ncommunicators, and episodic memory retrievers, our framework \nenables agents to learn more structured, adaptive, and \ninterpretable behaviors in complex cooperative environments.  \nOur empirical evaluations across three benchma rk \nenvironments —Google Research Football, MAgent, and \nStarCraft II —demonstrate that LLM -MARL significantly \noutperforms conventional MARL baselines. We show that \nagents guided by language -conditioned subgoals and emergent \ncommunication protocols achieve hig her win rates, better \ncoordination scores, and greater sample efficiency. Furthermore, \nLLM -MARL exhibits strong zero -shot generalization \ncapabilities, underscoring the benefit of symbolic knowledge \ntransfer in agent learning.  \nBeyond performance gains, this  work provides critical \ninsights into how symbolic reasoning tools like LLMs can \nenhance sub -symbolic learning systems. By encoding language, \nplanning, and memory into the agent training pipeline, LLM -\nMARL bridges the gap between traditional reinforcement \nlearning and more general intelligent behavior. This represents \na shift from purely reactive control policies to agents capable of \nsemantic abstraction, instruction following, and collaborative \ndialogue.  \n As we continue to explore open -ended multi -agent \nenvironments and human -AI teaming scenarios, the ideas \nintroduced in this work lay the groundwork for new forms of \nagent intelligence. LLM -MARL paves the way for scalable, \nlanguage -aligned, and socially aware multi -agent systems. We \nbelieve this fusion of dee p learning and language understanding \nwill play a key role in future advances in gaming, education, \nrobotics, and human -machine interaction.   \nVII. APPENDIX  \nA. Example Prompts and Template Structure s \n This appendix presents representative prompt templates \nused by the LLM -MARL framework to facilitate subgoal  \ndecomposition and inter -agent communication. These prompts \nare formatted as natural language inputs to the LLM and \nadapted per environment. The responses are parsed to produce \nsymbolic subgoals or messages passed to agents.  \n \nA.1 Subgoal Prompt Template (Go ogle Research Football)  \nPrompt:  \n You are the coordinator for a team of 3 agents. The current \nhigh-level task is: “Defend and counterattack.”  \nState summary: Agent 1 is near the home goal, Agent 2 is \nmidfield, Agent 3 is near the opponent box. Assign a subgoa l to \neach agent.  \nLLM Output:  \n Agent 1: Hold defensive line and intercept passes near goal.  \n Agent 2: Maintain midfield presence and support left flank.  \n Agent 3: Push forward and pressure defenders for turnover.  \nA.2 Communication Prompt Template (MAgent Battl e) \nPrompt:  \n--- Page 8 ---\n8 \n \n You are Agent 7. Your local view shows 3 allies \napproaching from the left and 4 enemies clustering near sector \nC3. \n \n You previously received the message: “C3 cluster forming.”  \n \n What message would you send to teammates?  \n \nLLM Output:  \n “Hold left fla nk. Prepare to collapse on C3 in 5 steps.”  \n \nA.3 Subgoal Recall Prompt (StarCraft II)  \nPrompt:  \n In a previous match on a similar desert map, your team \nsuccessfully defeated a force with Medics and Siege Tanks.  \nGiven the current situation: Zerglings at front, M edics in rear, \nwhat strategy worked before?  \nLLM Output:  \n Initiate a forward lure using Zerglings, then flank with \nMarauders to disable Medics first.  \n \n These examples illustrate how environment -grounded \nprompts lead to semantic responses that improve coordinat ion. \nThe prompt structure is modular and can be templated for other \nenvironments or agent architectures.  \n \nB. Detailed Experimental Results Tables  \n This appendix presents additional numerical results across \nthe three benchmark environments evaluated in the main paper. \nThe results compare LLM -MARL with established baselines \nand ablation variants using key performance metrics.  \n \nTABLE B.1 Win rate (%) across environments  \nMethod  GRF \n(3v3)  MAgent \n(Battle)  StarCraft II  \nLM-MARL  81.2 78.4 83.6 \nMAPPO  69.4 66.1 74.4 \nQMIX  61.7 59.8 68.7 \nRMAPPO  65.3 62.3 70.5 \nNo-LLM  58.5 54.7 66.3 \nNo-Comm  60.2 55.9 69.1 \nNo-Subgoal  68.1 64.2 72.0 \n \nTABLE B.2 Coordination Score (normalized  [0, 1]) \nMethod  GRF  MAgent  StarCraft II  \nLLM -MARL  0.89 0.86 0.91 \nMAPPO  0.73 0.69 0.78 \nNo-Comm  0.55 0.51 0.62 \nNo-Subgoal  0.64 0.61 0.70 \n \nThese results confirm the consistent advantage of integrating \nlanguage -based modules in both coordination and overall task \nsuccess. For reproducibility, all values are averaged over three \nindependent seeds.  \n \nC. Ablation Analysis Methods  \n To isolate the contributions of individual components within \nthe LLM -MARL framework, we conducted ablation experiments by disabling specific modules and comparing \nperformance with the full system.  \n \n1) No-LLM Variant : In this configuration, all LLM -generated \ncontent is removed. Subgoals are replaced with randomly \nsampled predefined roles (e.g., attacker, defender), and no \nnatural language messages are exchanged. This variant \nisolates the effect of language -grounded strate gy \ngeneration.  \n2) No-Comm Variant : This variant retains LLM -generated \nsubgoals but removes inter -agent communication. Agents \noperate based solely on their local observations and \nassigned subgoals. This setting tests the necessity of \ndecentralized communication for coordination.  \n3) No-Subgoal Variant : Here, LLM message exchange \nremains active, but agents receive no explicit subgoal \nconditioning. The baseline policy receives only raw \nobservations and language messages. This tests whether \nsubgoal guid ance improves policy optimization.  \n \nFor each variant, we recorded win rate, coordination score, \nand language grounding accuracy across 10 evaluation episodes \nwith 3 different random seeds. Notably, No -Subgoal systems \nexhibited 8 –13% performance drops in win  rate depending on \nthe environment. No -Comm variants showed sharp decreases in \ncoordination score, particularly in MAgent and GRF.  \n \nThese ablations validate the modular design of LLM -MARL and \nsuggest that both subgoal guidance and language -based \ncommunicat ion independently contribute to enhanced learning \nperformance.  \n \nD. Comprehensive Environment Settings And Parameters  \n This appendix outlines key environment configurations and \nhyperparameters used in training and evaluating LLM -MARL \nacross the three benchm ark domains. These settings ensure \nconsistent comparisons and reproducibility.  \n \n1) Google Research Football (GRF)  \n Scenario: 3v3 full game with sparse scoring rewards  \n Action Space: Discrete (8 directions + pass + shoot)  \n Observation: Stacked position, veloci ty, and \npossession features  \n Agent View: Global (shared during training), partial \n(during execution)  \n Episodes: 1000 steps max, early termination on goal  \n PPO Batch Size: 2048, Discount Factor (γ): 0.99  \n \n2) MAgent (Battle Map)  \n Map Size: 40× 40 grid  \n Number of A gents: 40 per team  \n Action Space: Discrete (move, attack, stay)  \n Observation: Local 9× 9 view with terrain and unit \nfeatures  \n Communication: 1 message per agent per step  \n Episodes: 500 steps max  \n Shared Critic: Yes (CTDE), Optimizer: Adam  \n \n--- Page 9 ---\n9 \n \n3) StarCraft II Micromanagement  \n Tasks: 5 scenarios including 3 Marines vs. 5 \nZerglings, MMM2, and Corridor  \n Unit Types: Heterogeneous (Marines, Medics, \nMarauders)  \n Observation: 20 -dimensional unit -centered local \nfeatures  \n Action Space: Target selection + move commands  \n Reward : Sparse (based on survival + enemy defeat)  \n Frame Skip: 8, Action Repeat: 2, PPO Clip: 0.2  \nThese configurations match established open -source MARL \nbenchmarks and align with prior works using MAPPO, QMIX, \nand population -based training protocols.  \n \nE. Pseud ocode for LLM -Guided Subgoal Decomposition and \nTraining  \n The following pseudocode summarizes the overall \narchitecture and learning cycle of the proposed LLM -MARL \nframework, including subgoal generation via LLM, symbolic \ncommunication, and hybrid optimizatio n: \n \nInitialize MARL environment E  \nInitialize policy networks π_i for each agent i  \nInitialize LLM as a frozen module or API  \nInitialize communication buffers and episodic memory  \n \nFor each episode do:  \n    Retrieve global state S_t  \n    Sample high -level task instruction T  \n    Generate prompt based on S_t and T  \n    Query LLM for subgoals → [g_1, ..., g_N]  \n \n    For each timestep t in episode:  \n        For each agent i:  \n            Get local observation obs_i  \n            Read incoming message from communication \nbuffer  \n            Generate communication prompt  \n            Query LLM for outgoing message  \n            Encode subgoal and message  \n            Compute action a_i ← π_i(obs_i, g_i, m_i)  \n            Environment executes a_i  \n \n    Store full trajectory in replay buffer D  \n \nPerform training loop:  \n    For each minibatch in D:  \n        Compute PPO policy loss L_RL  \n        Compute subgoal alignment loss L_goal  \n        Update policy π_i with total loss L_total = \nL_RL + λ * L_goal  \n \n    Optionally update query gate or prompt adapter  \n \n This pseudocode demonstrates the integration of symbolic \nguidance into decentralized policy learning. It highlights how \nLLM outputs are incorporated as semantic signals throughout \nthe learning and decision process.   \n \n \n \n \n REFERENCES  \n[1] Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H. S., Kohli, \nP., & Whiteson, S. (2 018). Stabilising experience replay for deep multi -\nagent reinforcement learning. International Conference on Machine \nLearning (ICML) , 1146 –1155. Baker, B., Kanitscheider, I., Markov, T., \nWu, Y., Powell, G., McGrew, B., & Mordatch, I. (2020). Emergent tool \nuse from multi -agent autocurricula. arXiv preprint arXiv:1909.07528 . \n[2] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., \nJaderberg, M., ... & Blundell, C. (2018). Value -decomposition networks \nfor cooperative multi -agent learning. Internati onal Conference on \nAutonomous Agents and MultiAgent Systems (AAMAS) , 2085 –2087.  \n[3] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). \nProximal policy optimization algorithms. arXiv preprint \narXiv:1707.06347 . \n[4] Rashid, T., Samvelyan, M., S chroeder, C., Farquhar, G., Foerster, J., & \nWhiteson, S. (2018). QMIX: Monotonic value function factorisation for \ndeep multi -agent reinforcement learning. International Conference on \nMachine Learning (ICML) , 4295 –4304.  \n[5] Lowe, R., Wu, Y., Tamar, A., Harb, J. , Abbeel, P., & Mordatch, I. (2017). \nMulti -agent actor -critic for mixed cooperative -competitive environments. \nAdvances in Neural Information Processing Systems (NeurIPS) , 30. \n[6] Chiang, P. E., Yu, T., Liu, J., Song, Z., & Lin, J. (2023). ReLM: \nReinforcement L earning with Language Models. arXiv preprint \narXiv:2305.13304 . \n[7] Huang, W., Dubois, Y., Gkioxari, G., & Malik, J. (2022). Inner \nMonologue: Embodied Reasoning through Planning with Language \nModels. arXiv preprint arXiv:2207.05608 . \n[8] Huang, W., Xu, D., Chen, T.,  Yao, S., & Fei -Fei, L. (2023). AutoRT: \nEmbodied Multi -Agent Coordination with Large Language Models. arXiv \npreprint arXiv:2304.10592 . \n[9] Singh, P., Ghosh, R., & Riedl, M. (2023). Mind’s Eye: Grounded \nLanguage Understanding for Agents Using LLMs and Mental Si mulation. \narXiv preprint arXiv:2307.10198 . \n[10] Xu, H., Zeng, M., Fang, K., & Fei -Fei, L. (2023). Voyager: An Open -\nEnded Embodied Agent with Large Language Models. arXiv preprint \narXiv:2305.16291 . \n[11] Yao, S., Zhao, J., Wang, D., Yu, D., & Yu, J. (2022). ReAct: Syn ergizing \nReasoning and Acting in Language Models. arXiv preprint \narXiv:2210.03629 . \n[12] Bačić, B., Feng, C., & Li, W. (2024). Jy61 IMU sensor external validity: \nA framework for advanced pedometer algorith m personalisation. ISBS \nProceedings Archive, 42 (1), 60.  \n[13] Bačić, B., Vasile, C., Feng, C., & Ciucă, M. G. (2024). Towards nation -\nwide analytical healthcare infrastructures: A privacy -preserving \naugmented knee rehabilitation case study. arXiv preprint \narXiv:2 412.20733 . \n[14] Liu, Y., Qin, X., Gao, Y., Li, X., & Feng, C. (2025). SETransformer: A \nhybrid attention -based architecture for robust human activity recognition. \narXiv preprint arXiv:2505.19369 . https:/ /arxiv.org/abs/2505.19369  \n[15] Wang, C., Nie, C., & Liu, Y. (2025). Evaluating supervised learning \nmodels for fraud detection: A comparative study of classical and deep \narchitectures on imbalanced transaction data. arXiv preprint \narXiv:2505.22521 . https://arxiv.org/abs/2505.22521  \n[16] Lv, K. (2024). CCi -YOLOv8n: Enhanced fire detection with CARAFE \nand context -guided modules. arXiv preprint arXiv:2411.11011 . \nhttps://arxiv.org/abs/2411.11011  \n[17] Zhang, L., Liang, R., et al. (2025). Avocado price prediction using a \nhybrid deep learning model: TCN -MLP -attention architecture. arXiv \npreprint arXiv:2505.09907 . https://arxiv.org/abs/2505.09907  \n[18] Liu, Y., Qin, X., Gao, Y., Li, X., & Feng, C. (2025). SETransformer: A \nhybrid attention -based architecture for robust human activity recognition. \narXiv preprint arXiv:2505.19369 .",
  "text_length": 45641
}