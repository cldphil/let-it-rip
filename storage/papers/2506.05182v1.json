{
  "id": "http://arxiv.org/abs/2506.05182v1",
  "title": "On the Comprehensibility of Multi-structured Financial Documents using\n  LLMs and Pre-processing Tools",
  "summary": "The proliferation of complex structured data in hybrid sources, such as PDF\ndocuments and web pages, presents unique challenges for current Large Language\nModels (LLMs) and Multi-modal Large Language Models (MLLMs) in providing\naccurate answers. Despite the recent advancements of MLLMs, they still often\nfalter when interpreting intricately structured information, such as nested\ntables and multi-dimensional plots, leading to hallucinations and erroneous\noutputs. This paper explores the capabilities of LLMs and MLLMs in\nunderstanding and answering questions from complex data structures found in PDF\ndocuments by leveraging industrial and open-source tools as part of a\npre-processing pipeline. Our findings indicate that GPT-4o, a popular MLLM,\nachieves an accuracy of 56% on multi-structured documents when fed documents\ndirectly, and that integrating pre-processing tools raises the accuracy of LLMs\nto 61.3% for GPT-4o and 76% for GPT-4, and with lower overall cost. The code is\npublicly available at https://github.com/OGCDS/FinancialQA.",
  "authors": [
    "Shivani Upadhyay",
    "Messiah Ataey",
    "Shariyar Murtuza",
    "Yifan Nie",
    "Jimmy Lin"
  ],
  "published": "2025-06-05T15:52:44Z",
  "updated": "2025-06-05T15:52:44Z",
  "categories": [
    "cs.IR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05182v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05182v1  [cs.IR]  5 Jun 2025On the Comprehensibility of Multi-structured Financial Documents using\nLLMs and Pre-processing Tools\nShivani Upadhyay1Messiah Ataey2Shariyar Murtuza2Yifan Nie2Jimmy Lin1\n1University of Waterloo, Waterloo, Canada\n2Manulife, Toronto, Canada\n{sjupadhyay, jimmylin}@uwaterloo.ca\n{messiah_ataey, syed_shariyar_murtaza, yifan_nie}@manulife.com\nAbstract\nThe proliferation of complex structured data\nin hybrid sources, such as PDF documents\nand web pages, presents unique challenges\nfor current Large Language Models (LLMs)\nand Multi-modal Large Language Models\n(MLLMs) in providing accurate answers. De-\nspite the recent advancements of MLLMs, they\nstill often falter when interpreting intricately\nstructured information, such as nested tables\nand multi-dimensional plots, leading to hallu-\ncinations and erroneous outputs. This paper\nexplores the capabilities of LLMs and MLLMs\nin understanding and answering questions from\ncomplex data structures found in PDF doc-\numents by leveraging industrial and open-\nsource tools as part of a pre-processing pipeline.\nOur findings indicate that GPT- 4o, a popular\nMLLM, achieves an accuracy of 56% on multi-\nstructured documents when fed documents di-\nrectly, and that integrating pre-processing tools\nraises the accuracy of LLMs to 61.3% for GPT-\n4oand 76% for GPT- 4, and with lower over-\nall cost. The code is publicly available at\nhttps://github.com/OGCDS/FinancialQA .\n1 Introduction\nHybrid data sources such as PDF documents and\nweb pages present information in a variety of\nstructures with complex arrangements. The ad-\nvancements in Multi-modal Large Language Mod-\nels (MLLMs), including proprietary models like\nGPT (OpenAI et al., 2024) and Gemini (Team\net al., 2024), as well as open-source models such\nas LlaV A (Liu et al., 2023b,a), LaVIT (Jin et al.,\n2023), and Emu (Dai et al., 2023; Sun et al., 2023),\nhave led to a significant improvement in these\nmodels’ abilities to understand heterogeneous data.\nHowever, when it comes to effectively understand-\ning varied structural information sources and pro-\nviding high-fidelity results, these models still tend\nto hallucinate and generate incorrect results due totheir limited understanding of the input data struc-\nture. For instance, MLLMs can struggle to inter-\npret tables with complex nesting and certain types\nof financial plots (e.g., a plot with more than 2\ndimensions), limiting their usefulness for question-\nanswering (QA) over complex multi-modal data.\nRecent developments in LLMs such as the GPT\nmodel family (OpenAI et al., 2024) and LLaMA\n(Touvron et al., 2023) have shown human-level\neffectiveness in complex QA tasks. One major\ndrawback of such models however is their inabil-\nity to answer questions about data outside of their\ntraining corpus. Retrieval Augmented Generation\n(RAG) (Gao et al., 2023) is a technique that uti-\nlizes LLMs in-context learning abilities by provid-\ning relevant information from external sources as\npart of the prompt, enabling effective QA over such\ninformation. Despite the ability to retrieve and aug-\nment relevant external information, QA over multi-\nstructured document contents such as tables and\ncharts in financial reports still remains a challenge\ndue to their limited reasoning capabilities.\nIn this paper, industrial and open-source tools are\ninvestigated as a way to improve the performance\nof LLMs for QA over complex multi-structured\ncontents found in PDF documents. In particular,\nAzure Document Intelligence is utilized1for the ex-\ntraction of tabular and textual information, pypdf2\nfor the extraction of images, and the ChartVLM\nmodel (Xia et al., 2024) for chart comprehension.\nThis combination of tools and models ensures a\ncomprehensive extraction and understanding of the\ncommon data contents found in PDF documents.\nThese tools are evaluated by comparing them with\nusing the multi-modal GPT- 4omodel directly. Con-\nsidering the vast array of tools, models, and plat-\nforms available for similar purposes, the study pre-\nsented in this paper is not intended to be a com-\n1https://azure.microsoft.com/en-us/products/\nai-services/ai-document-intelligence\n2https://pypi.org/project/pypdf/\n--- Page 2 ---\nprehensive comparison of all possible tools and\ntheir combinations. Instead, the contribution of\nthis paper is to provide insights into the following\nresearch questions:\n(a)Can MLLMs such as GPT- 4operform effec-\ntive QA over multi-structured data contents\nfound in financial documents in the industry\nwhen used directly as input?\n(b)Can pre-processing tools help improve the\ncomprehensibility of LLMs and MLLMs for\nQA over multi-structured documents?\n(c)How can pre-processing tools help save costs\nwhen performing QA over multi-structured\ndocuments compared to popular end-to-end\ncommercial offerings?\nOur results show that: (a) GPT- 4o, when used\nwith document inputs directly, is only able to\nachieve 56% QA accuracy over multi-structured\nfinancial PDF documents; (b) utilizing industrial\nand open-source pre-processing tools improves ac-\ncuracy to 61.3% for GPT- 4oand 76% for GPT- 4\n(both in text-only mode); and (c) the document\ncost per page is only $0.00231 with pre-processing\ntools, which is 74.33% cheaper than the cheapest\nsolutions, such as Anthropic’s Claude 3 Opus.\n2 Background and Related Works\nIn finance, insurance, and other industries, analysts\nrely on various documents such as quarterly and\nannual reports, balance sheets, and financial state-\nments. These documents often include complex\ntables and charts (see Appendix C.1), which are es-\nsential for making informed decisions. Due to the\nlarge volume of such documents, a generative AI\nchat application based on an LLM using retrieval\naugmented generation (RAG) can help analysts find\nanswers to their inquiries faster and with less cogni-\ntive overhead. Figure 1 shows a typical RAG-based\nsystem along with document pre-processing tools\nhighlighted in blue, wherein information present in\ntextual, graphical, and tabular data formats are ex-\ntracted. The extracted contents are then converted\ninto a vector representation using an embedding\nmodel and inserted into an index (e.g., a vector\ndatabase). With such a system, relevant informa-\ntion can then be retrieved from the index at runtime\nbased on similarity to a provided query, which is\nthen augmented with the query in the prompt tem-\nplate to generate relevant responses using an LLM.\nRelevant\nDocuments\nQuery: Core earnings for\nManulife in Q3 2022?\nAnswer: 1,322\nmillions $Vectorize Retrieval\nPromptingDocument Pre-\nprocessorParsed DataFigure 1: Flowchart of a RAG framework utilizing docu-\nment pre-processing tools (shown in the blue box). First,\nmulti-structured content is extracted using the document\npre-processor. Then, the extracted content is converted\nto a vector representation and stored in a vector database,\nwhich can then be used to retrieve relevant content based\non a provided query. The retrieved content is finally aug-\nmented with the query in the LLM prompt for relevant\nin-context generation.\nRetrieval Augmentation with LLMs: RAG has\nproven to enhance LLMs understanding of exter-\nnal data sources and has shown highly accurate\nanswer generation (Lewis et al., 2021; Nakano\net al., 2022; Gao et al., 2023). RAG also reduces\nhallucination as LLMs are provided with relevant\ninformation. However, even when provided with\nrelevant context, LLMs may struggle to interpret\nand comprehend certain multi-structured data for-\nmats such as complex financial tables and plots.\nQA over Tables: Analyzing tabular information\nand answering questions based on them is referred\nto as Table QA (Jin et al., 2022). Table QA datasets\ncan be categorized into two types: those that use\na raw table as direct input for model analysis (Pa-\nsupat and Liang, 2015; Hwang et al., 2019), and\nthose that use a table with additional annotated in-\nformation (Zhu et al., 2021; Nguyen et al., 2023;\nJauhar et al., 2016; Chen et al., 2021).\nQA over Charts: Being a crucial method of in-\nformation representation, Chart QA deals with QA\nabout charts of various types (Hoque et al., 2022).\nChart QA has gained a lot of attention as of late,\nand many datasets have been developed to aid mod-\nels for enhanced chart comprehension (Masry et al.,\n2022; Methani et al., 2020; Kahou et al., 2018).\nQA over Documents: Many works have been\nfocused on benchmarking and evaluating QA tasks\nover various types of documents. For example, Vi-\nsual QA datasets mainly include scanned images of\n--- Page 3 ---\ndocuments (Mathew et al., 2021; Tito et al., 2021)\nfocusing on text-heavy content, (Narayanan et al.,\n2024; Skarlinski et al., 2024; Lála et al., 2023) ana-\nlyzing scientific research and (Dasigi et al., 2021)\nshowcases information retrieval from raw docu-\nments with L ATEX as input. The document pre-\nprocessing tools investigated in this work specifi-\ncally focus on extracting multi-structured content\nfound in complex PDF documents, and thus the\nevaluation dataset used encompasses various data\ntypes such as tables and charts.\n3 Methodology for Pre-processing\nIn this section, the document pre-processor built\nusing industrial and open-source tools is introduced\nwhich transforms a variety of structured informa-\ntion from PDF documents into a simple text for-\nmat. Typically, these documents contain charts and\nnested tables alongside regular text data, which\nmay not be easily interpretable by LLMs. This\ntechnique transforms a variety of structured infor-\nmation into a simple text format, enabling LLMs\nto comprehend such complex data structures more\neasily.\nHybrid Data Sources: Financial and business\ndocuments contain various data types such as raw\ntext, nested tables, and images. Therefore, to gain a\ncomplete understanding of a document, it is neces-\nsary to extract all of these data types in preferably\na standard format. Generally speaking, the raw\ntext contains qualitative information about the top-\nics of a document, and tables and charts convey\nquantitative information.\nExtract images with\npypdfParse PDFs with\nDocument Intelligence\nRead Chart data\nwith ChartVLM\nTextual\nData Tabular\nDataGraphical\nData\nTextual data\nFigure 2: The internal workings of the document pre-\nprocessor. Azure Document Intelligence is used for\nthe extraction of textual, tabular, and graphical infor-\nmation. pypdf is used for the extraction of images, and\nChartVLM is used to convert chart images to their un-\nderlying tabular representation.3.1 Document Pre-processor\nThe pre-processing methodology presented in this\nwork (referred to as a general ‘pre-processor’) han-\ndles the extraction of multi-structured data contents\n(i.e., text, tables, and charts) found in financial PDF\ndocuments, as illustrated in Figure 2.\n3.1.1 Tabular and Textual Data Extraction\nTo accurately extract tabular and textual informa-\ntion from documents, Azure Document Intelli-\ngence is utilized which combines Optical Charac-\nter Recognition (OCR) capabilities and advanced\ndeep learning models for precise data extraction.\nDocument Intelligence extracts both textual and\nstructural information which can be categorized as\neither geometric (e.g., text, tables, and figures) or\na logical role (e.g., section titles, image captions,\npage footers). Along with role-based data extrac-\ntion, it also provides the exact coordinates of each\nextracted object, which could assist in higher-level\ndocument analyses.\nExtracted tables are stored in a standard JSON\nstring format, which contains information for each\ncell in the table including details such as polygon\narea, row/column-span, and a boolean indicator\nthat signifies whether or not the cell is a column\nheader. The column header attribute is particularly\nuseful for capturing key semantic information con-\nveyed by the table. In instances where hierarchical\ncolumn headers are present, the headers are com-\nbined in a way to ensure an accurate mapping of the\nkey-value pairs. This process allows for a compre-\nhensive and precise extraction and representation\nof both simple and complex textual and tabular\ndata structures. Table 6 shows an example of an\nextracted table.\n3.1.2 Graphical Data Extraction\nTo extract graphical images from PDFs, Azure Doc-\nument Intelligence is used to extract the bounding\nbox coordinates of an image on a single page, and\npypdf to then crop and save the image. In the case\nof extracting images that are not relevant to a spe-\ncific task, an optional classifier can be added to eval-\nuate whether an extracted image is of a particular\ntype (e.g., a financial chart) or if it is an incomplete\nimage (e.g., a missing axis of a plot). To obtain\na textual representation of the extracted financial\nchart image, a Pix2Struct (Lee et al., 2023)-based\nChartVLM model is employed that has been fine-\ntuned on the ChartX dataset. ChartVLM presents\nan enhanced understanding of various chart types\n--- Page 4 ---\nand an inherent ability to preserve the aspect ratio\nof the input image, which assists with increased\nrobustness in chart data extraction. The model con-\nverts a chart image into its underlying CSV tabular\nformat which is then converted into the JSON data\nformat as part of the pre-processing pipeline, sim-\nilar to extracted tables. This procedure enables a\ncomprehensive extraction of financial charts from\nthe document. A visualization is shown in Table 7.\nAn attempt to enhance ChartVLM’s interpreta-\ntion abilities through fine-tuning was carried out,\nbut due to the complexities of real-world chart un-\nderstanding and limited data availability, the model\nconsistently overfitted and failed to provide any\nmeaningful performance improvements. Section 6\nprovides details about the challenges associated\nwith chart extraction and interpretation.\n4 Experimental Setup\nThis section discusses the experimental setup em-\nployed to perform QA evaluations.\n4.1 RAG with Document Pre-processing\nIn this section, specific details about the RAG with\ndocument pre-processing framework are discussed.\n4.1.1 Ingestion Pipeline\nThe multi-step ingestion pipeline for the RAG\nframework is shown in Figure 3. Initially, PDF\ndocuments are collected and optionally stored in an\nAzure Blob Storage container3, or locally on disk.\nThese documents are then sent through the docu-\nment pre-processor to extract multi-structured data\ncontents such as text, tables, and charts. The post-\nprocessed contents (text), are subsequently trun-\ncated into chunks of 600 tokens using LangChain’s\nTokenTextSplitter4to avoid excessive data sizes,\nwhere each chunk corresponds to only a single\npage of the document if the number of tokens is\nless than the chunk size. Each chunk is then con-\nverted to a vector representation using OpenAI’s\ntext-embedding-ada model. The resultant embed-\nding, along with its metadata from the document,\nis finally uploaded to an Azure AI Search index.\nThe metadata consists of contextual informa-\ntion related to the indexed chunk and/or document.\nIn the case of a financial document, the metadata\n3https://learn.microsoft.com/en-us/azure/\nstorage/blobs/storage-blobs-overview\n4https://js.langchain.com/v0.1/docs/modules/\ndata_connection/document_transformers/token_\nsplittermight include document-level details such as com-\npany name, year, report quarter, etc., and also in-\ndividual chunk-level details such as page number,\nsection title, bounding-box coordinates, etc. In-\ncorporating metadata to the indexed contents is\nessential as it allows for pre-filtering of the con-\ntents based on either user-provided or automati-\ncally extracted filters. This type of pre-filtering\ncan vastly reduce the volume of indexed contents\nto be searched against based on the query, thus\nsignificantly improving retrieval accuracy.\n4.1.2 LLM Inference\nIn the process of query-based retrieval, the three\nmost relevant chunks from the index are retrieved\naccording to their cosine similarity score against\nthe query.5The retrieved data is then optionally\nprocessed further and augmented with the query\nin the prompt to enable in-context generation with\nan LLM. This approach enables the LLM to gain a\ncomprehensive understanding of the provided con-\ntext and generate accurate responses. The prompt\ntemplate used with the LLM is shown in Figure 4.\n4.1.3 Dataset\nFor industry-relevant evaluations, a private test set\nassembled by financial experts was used consisting\nof 75 question-answer pairs of varying complex-\nity derived from numerous financial documents of\nCanadian companies, where 53 questions are for ta-\nbles and 22 for charts. Questions of a less complex\nnature may involve extracting a specific value from\na table or simply reading values off a chart. Diffi-\ncult questions on the other hand may necessitate the\nmodel to integrate multiple pieces of information\nto arrive at the final result. Appendix A provides\nmore details regarding the evaluation set.\n4.2 Individual Components Evaluation\nIn this experiment, the tabular and chart compo-\nnents of the document pre-processor are evaluated.\nTables and charts are extracted from documents\nand converted to a standard text format using the\ndocument pre-processor, which are then passed\nas input to an LLM, namely GPT-3.5 Turbo and\nGPT- 4, to generate answers based on this processed\ninformation. These experiments do not use a re-\ntrieval mechanism since the relevant table or chart\nis directly provided as input to the LLM. Similarly,\nraw chart images are provided as input to GPT- 4o.\n5A value of k= 3 was empirically chosen as it provided a\ngood trade-off between response accuracy and context size\nin the experiments.\n--- Page 5 ---\nFigure 3: RAG ingestion pipeline used for performing PDF evaluations.\nComprehend the following context and answer the\nquestions in one line:\n{context }\nDo not add extra information on your own.\nFigure 4: Prompt utilized within RAG framework.\n4.2.1 Dataset\nThe following datasets are used for evaluating LLM\ntable and chart comprehension:\n•VQAonBD dataset: Tabular images from the\ndataset (i.e., the validation set of 4535 images)\nare used as input for the evaluations. The dataset\nhas almost 50 questions partitioned into 5 cate-\ngories for each image based on their content and\nformat (Nguyen et al., 2023).\n•ComplexChartQA (CCQA) dataset: A\nmanually-curated chart evaluation dataset con-\nsisting of 50 complex chart images along with\nquestions to assess chart comprehension efficacy.\n5 Results and Discussion\nTo compare with the multi-modal capabilities of\nGPT- 4o, PDF pages were converted into images\nand subsequently provided as input to GPT- 4o.\nSince GPT- 4ohas a maximum input length of 20\nimages, it was necessary to ensure the answer was\ncontained within the limits of the context size. A\nperformance evaluation of the RAG with document\npre-processing framework is also investigated.\nRows numbered with 1* present a comparison of\nthe RAG with document pre-processing framework\nwith GPT-3.5 Turbo, GPT- 4, and GPT- 4o(in text\nmode (“T”)). Rows numbered with 2* present a\ncomparison of the proficiency of GPT- 4oin ana-\nlyzing PDF document pages directly, utilizing bothPre-processing RAG LLM Input\nmodeAcc. Cost\nper call\n1(a) Y Y GPT- 3.5tT 60.0 0.0003\n1(b) Y Y GPT- 4 T 76.0 0.0360\n1(c) Y Y GPT- 4o T 61.3 0.0030\n2(a) - - GPT- 4o I,T 56.0 0.0765\n2(b) Y Y GPT- 4o I 40.9 0.0765\nTable 1: End-to-end comparative analysis between GPT-\n4o and the RAG with pre-processor framework using\nthe private evaluation set. The first section (rows 1*)\ndescribes the performance of (text-only) GPT models\nusing the pre-processor, and the second section (rows\n2*) of GPT-4o using document pages directly as input.\nimage and text (“I, T”) inputs in row 2(a). Row\n2(b) presents the results of a multi-modal RAG\nframework, where raw images are embedded and\nretrieved from an index, as opposed to plain text.\nThe generated results are subject to assessment by\nhuman evaluators.\nAs shown in row 2(a), the direct application\nof GPT- 4oto comprehend multi-structured doc-\numents yields an accuracy of 56%. When the\nRAG with document pre-processing framework\nis used with GPT- 4oin text mode, an improved\naccuracy of 61.3% is achieved. Using GPT-3.5\nTurbo with the framework also yields a higher ac-\ncuracy than GPT- 4o2(a) at 60.0%, while using\nGPT- 4significantly improves performance over\nall models, achieving an accuracy of 76%. This\ndemonstrates the potential of using pre-processing\ntools in enhancing LLM comprehension of com-\nplex multi-structured PDF documents. To gain a\ndeeper understanding of context representations\nand their effectiveness—particularly in the case of\ncomplex tables—we conduct an optimal retrieval\nanalysis comparing various representation methods.\nThis analysis enables us to precisely identify the\n--- Page 6 ---\nlimitations of each representation, independent of\nretrieval errors. For further details, please refer to\nAppendix B.\nLastly, the performance of GPT- 4ousing a fi-\nnancial chart image-based RAG framework is in-\nvestigated in row 2(b). In this setting, the images\nare embedded using a vision-language CLIP model\n(Radford et al., 2021), thus enabling retrieval of im-\nages using plain text and vice versa. Similar to the\ntext-based RAG framework, the top three images\nare retrieved from the index based on their cosine\nsimilarity to the text query. The experiment shows\nthat GPT- 4operforms much worse (40.9%) on chart\nQA for the evaluation set considered, likely due to\nlow retriever recall or the retrieval of conflicting\ninformation leading to an incorrect response.\nCost Effectiveness: An additional advantage of\nusing text-based LLMs is the savings made in API\ncosts. When GPT- 4ois used with both image and\ntext modalities, the average cost per API call can\nbe as high as 0.0765. However, when only the text\nmodality is used with GPT-3.5 Turbo, GPT- 4, and\nGPT- 4o, assuming 600tokens on a typical docu-\nment page, the cost can be reduced to 0.0003 (i.e.,\nreduced by ~200 ×), 0.0360 (i.e., reduced by ~2 ×),\nand 0.0030 (i.e., reduced by ~20 ×), respectively.\nThe pre-processor used in this work is also com-\npared with alternative paid solutions based on the\nunit cost per page in Table 2, where the same as-\nsumption is made as before that a typical document\npage contains 600tokens. In the pre-processor,\nthe cost of calling the Azure Document Intelli-\ngence and Azure OpenAI’s GPT- 4o(in text-mode)\nAPI are considered.6The LlamaParse,7Vertex AI8\nand Anthropic9products are readily available solu-\ntions that provide similar functionality. The vendor\nprices were converted to a cost per page as shown\nin Table 2, where the pre-processor used in this\nwork is shown to be the most cost effective.\nComponent-Level Analysis: Table 3 showcases\nthe performance of tabular data comprehension for\nquestions of varying levels of difficulty from the\nVQAonBD dataset (“VGAonBD categories”) and\nfrom the ComplexChartQA (“CCQA”) dataset. In\nthis experiment, the question-input pair from the\n6https://azure.microsoft.com/en-us/pricing/\ndetails/cognitive-services/openai-service/\n7https://docs.cloud.llamaindex.ai/llamaparse/\nusage_data\n8https://cloud.google.com/vertex-ai/\ngenerative-ai/pricing#modality-based-pricing\n9https://www.anthropic.com/news/claude-3-familySolution Main Model Cost/Page\nLlamaParse GPT-4 o $ 0.03000\nVertex AI Gemini-2.0-flash $ 0.11610\nAnthropic Claude 3 Opus $ 0.00900\nOur solution GPT-4 o $ 0.00231\nTable 2: Cost comparison with alternative solutions.\nModelVQAonBD categoriesCCQA1 2 3 4 5\n1(a) pre-processing + GPT-3.5 t0.64 0.25 0.29 0.41 0.21 0.56\n1(b) pre-processing + GPT- 4 0.77 0.58 0.44 0.57 0.48 0.68\n2 GPT- 4o(Images) 0.59 0.03 0.04 0.45 0.38 0.66\nTable 3: Optimal retrieval evaluation for pre-processor\ncomponents.\ndataset is provided directly to the text-based model\nfor evaluation (row 1*), while for GPT- 4oin image-\nmode, a raw image of the question-input pair is\nused (row 2). For table QA, improvements are\nobserved when using the document pre-processor\nwith GPT-3.5 Turbo and GPT- 4as compared to\nGPT- 4oacross all categories. For chart QA, compa-\nrable effectiveness is observed between both meth-\nods with a slight improvement using the RAG with\ndocument pre-processing framework with GPT- 4.\n6 Discussion\nChallenges with Chart Interpretation. Inter-\npreting charts is challenging due to the variety of\nchart types, each requiring a unique approach. For\nexample, extracting information from bar and line\nplots requires effectively interpreting their axes and\nvalues. Additionally, even within the same chart\ntype, variations in presentation require the model\nto recognize and learn from these differences.\nUpon manually inspecting the chart results, we\nfind that GPT- 4odemonstrates good interpretation\nskills when the charts are well labeled and anno-\ntated with necessary text values for the datapoints,\nlikely because it can use its OCR abilities to un-\nderstand the content of the chart. On the other\nhand, when the charts do not have such explicit tex-\ntual content, GPT- 4ooften struggles to accurately\ninterpret complex chart elements, leading to a po-\ntentially incorrect response. Appendix C.3 shows\nan example of this behavior.\nChartVLM demonstrates the capability to inter-\npret chart images without annotations. However,\nits limited comprehension of complex visualiza-\ntions often results in incomplete or inaccurate in-\nterpretations of the entire plot. Financial docu-\n--- Page 7 ---\nments frequently combine multiple plots within\na single image to enhance interpretability. While\nthis practice is common in real-world scenarios, it\nfalls outside the expected input structure defined\nby ChartVLM’s internal knowledge, leading to po-\ntential misinterpretations. In some cases, the gen-\nerated text focuses on only one of the sub-images\nwithin a composite chart, overlooking or only par-\ntially extracting the remaining components. Ap-\npendix C.4 illustrates an example of this behavior.\nChallenges with Table Interpretation. Inter-\npreting tables becomes increasingly complex when\nhierarchical structures are involved. The extraction\nprocess can be particularly challenging, and the\naccuracy and reliability of the results heavily de-\npend on precise data extraction. Many of the errors\nencountered in the results stem from the loss of\nhierarchical structure during table parsing. When\nhierarchical JSON data is flattened—as is common\nwith many tables in the dataset—important spa-\ntial cues like column alignment and merged cells,\nwhich aid human understanding, are lost. This flat-\ntening obscures relationships such as subtotals and\ngroupings, especially when the original layout is\nnot preserved. In our case, hierarchical column\nheaders are merged into a single string, while hi-\nerarchical row headers are reduced to flat records\nwith empty field values. Crafting post-processing\nheuristics to recover this structure is difficult and\nnot scalable due to the diversity of table formats.\nMoreover, in the RAG setting, enriching table\nchunks with metadata such as section titles, head-\ners, etc. plays a crucial role in resolving ambigui-\nties among similar tables within a document. This\nbecomes especially important when models mis-\ntakenly extract values from incorrect tables due to\nlexically similar or the same row and/or column\nheaders. Incorporating such contextual metadata\naids in both accurate retrieval and generation.\n7 Conclusion\nThis paper investigated three primary research\nquestions: whether LLMs can comprehend com-\nplex multi-structured PDF contents, the use of pre-\nprocessing tools to improve comprehension, and\nhow pre-processing tools can help save costs com-\npared to directly providing document pages to an\nMLLM. The results show that GPT- 4o, a widely-\nused MLLM, answers approximately half of the\nquestions for complex charts and tables. When us-\ning commercially available and open-source toolsfor document pre-processing, the response accu-\nracy is improved for all the text-based LLMs con-\nsidered in the experiments. Furthermore, the cost\nanalysis shows considerable cost savings when us-\ning the RAG with document pre-processing frame-\nwork compared to directly using document page in-\nputs with an MLLM. While such results are promis-\ning, it is clear there is still room for improvement of\ncurrent pre-processing tools and LLMs for compre-\nhending complex multi-structured PDF contents,\nbased on overall accuracy.\nAlthough the source code for the RAG with doc-\nument pre-processing framework presented in this\nwork is available, including any shareable datasets,\nthe framework is ultimately bounded by licenses\nand usage agreements of the proprietary GPT mod-\nels and Azure services.\nReferences\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing , pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vanden-\nhende, Xiaofang Wang, Abhimanyu Dubey, et al.\n2023. Emu: Enhancing image generation models us-\ning photogenic needles in a haystack. arXiv preprint\narXiv:2309.15807 .\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset of\ninformation-seeking questions and answers anchored\nin research papers. arXiv preprint arXiv:2105.03011 .\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997 .\nEnamul Hoque, Parsa Kavehzadeh, and Ahmed Masry.\n2022. Chart question answering: State of the art and\nfuture directions. arXiv preprint arXiv:2205.03966 .\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration on\nWikiSQL with table-aware word contextualization.\narXiv preprint arXiv:1902.01069 .\nSujay Kumar Jauhar, Peter Turney, and Eduard Hovy.\n2016. TabMCQ: A dataset of general knowledge\ntables and multiple-choice questions. arXiv preprint\narXiv:1602.03960 .\n--- Page 8 ---\nNengzheng Jin, Joanna Siebert, Dongfang Li, and\nQingcai Chen. 2022. A survey on table ques-\ntion answering: Recent advances. arXiv preprint\narXiv:2207.05270 .\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao\nTan, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\nXiaoqiang Lei, et al. 2023. Unified language-vision\npretraining with dynamic discrete visual tokenization.\narXiv preprint arXiv:2309.04669 .\nSamira Ebrahimi Kahou, Vincent Michalski, Adam\nAtkinson, Akos Kadar, Adam Trischler, and Yoshua\nBengio. 2018. FigureQA: An annotated fig-\nure dataset for visual reasoning. arXiv preprint\narXiv:1710.07300 .\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,\nFangyu Liu, Julian Eisenschlos, Urvashi Khandel-\nwal, Peter Shaw, Ming-Wei Chang, and Kristina\nToutanova. 2023. Pix2Struct: Screenshot parsing as\npretraining for visual language understanding. arXiv\npreprint arXiv:2210.03347 .\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2021. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401 .\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved baselines with visual instruc-\ntion tuning. arXiv preprint arXiv:2310.03744 .\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning. arXiv preprint\narXiv:2304.08485 .\nJakub Lála, Odhran O’Donoghue, Aleksandar Shtedrit-\nski, Sam Cox, Samuel G. Rodriques, and Andrew D.\nWhite. 2023. PaperQA: Retrieval-augmented gen-\nerative agent for scientific research. arXiv preprint\narXiv:2312.07559 .\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. 2022. ChartQA: A benchmark\nfor question answering about charts with visual and\nlogical reasoning. arXiv preprint arXiv:2203.10244 .\nMinesh Mathew, Dimosthenis Karatzas, and C. V . Jawa-\nhar. 2021. DocVQA: A dataset for vqa on document\nimages. arXiv preprint arXiv:2007.00398 .\nNitesh Methani, Pritha Ganguly, Mitesh M. Khapra,\nand Pratyush Kumar. 2020. PlotQA: Reasoning over\nscientific plots. arXiv preprint arXiv:1909.00997 .\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2022. Web-\nGPT: Browser-assisted question-answering with hu-\nman feedback. arXiv preprint arXiv:2112.09332 .Siddharth Narayanan, James D. Braza, Ryan-Rhys Grif-\nfiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori\nKabeli, Geemi Wellawatte, Sam Cox, Samuel G. Ro-\ndriques, and Andrew D. White. 2024. Aviary: train-\ning language agents on challenging scientific tasks.\narXiv preprent arXiv:2412.21154 .\nPhuc Nguyen, Nam Tuan Ly, Hideaki Takeda, and\nAtsuhiro Takasu. 2023. TabIQA: Table questions\nanswering on business document images. arXiv\npreprint arXiv:2303.14935 .\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\net al. 2024. GPT-4 technical report. arXiv preprint\narXiv:2303.08774 .\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research , pages\n8748–8763. PMLR.\nMichael D. Skarlinski, Sam Cox, Jon M. Laurent,\nJames D. Braza, Michaela Hinks, Michael J. Ham-\nmerling, Manvitha Ponnapati, Samuel G. Rodriques,\nand Andrew D. White. 2024. Language agents\nachieve superhuman synthesis of scientific knowl-\nedge. arXiv preprent arXiv:2409.13740 .\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,\nQiying Yu, Zhengxiong Luo, Yueze Wang, Yongming\nRao, Jingjing Liu, Tiejun Huang, et al. 2023. Gen-\nerative multimodal models are in-context learners.\narXiv preprint arXiv:2312.13286 .\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-\nlican, David Silver, et al. 2024. Gemini: A family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805 .\nRubèn Tito, Dimosthenis Karatzas, and Ernest Valveny.\n2021. Document Collection Visual Question Answer-\ning, page 778–792. Springer International Publish-\ning.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n--- Page 9 ---\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971 .\nRenqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan,\nQi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Bo-\ntian Shi, Junchi Yan, and Yu Qiao. 2024. ChartX\n& ChartVLM: A versatile benchmark and founda-\ntion model for complicated chart reasoning. arXiv\npreprint arXiv:2402.12185 .\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-\nSeng Chua. 2021. TAT-QA: A question answering\nbenchmark on a hybrid of tabular and textual content\nin finance. arXiv preprint arXiv:2105.07624 .\n--- Page 10 ---\nAppendix A Private Evaluation Dataset\nThis section includes details regarding the dataset\nused for performing the evaluations. The ques-\ntion set belongs to 4 different financial documents.\nTable 4 showcases a selection of representative\nqueries extracted from the dataset. For the LLM\nto provide an accurate response to the query, it is\nessential for it to grasp the central concept inherent\nin the query and potentially employ multiple pieces\nof reference information to arrive to the answer.\nLow difficulty questions may involve just reading\noff values from a chart or table, whereas difficult\nquestion may require to perform arithmetic opera-\ntions and/or multiple pieces of information to be\nretrieved as context.\nQuery Difficulty Level # of References\nWhich asset category has the\nhighest percentage of total in-\nvested assets?Low 1\nWhat is the quarterly core\nROE change from 2022 to\n2023 for Manulife?Medium 3\nWhat is the new Q1/23E Core\n/ Underlying YoY in MFC?High 5\nTable 4: Example queries from the private evaluation\ndataset. Each query is assigned a difficulty level based\nloosely on how much \"reasoning\" is required for accu-\nrate QA.\nAppendix B Comparison of Context\nFormats for Table\nIn this section, a comparative study between the uti-\nlization of JSON and Dataframe representation of\ntables is investigated. This evaluation assumes an\n“optimal retrieval” scenario, where the correct input\nis provided directly as input and thus just focuses\non comparing the two representation formats. As\nindicated in Table 5, both methods exhibited com-\nparable performance. This comparison allows us\nto select the best context representation method for\nthe complex hierarchical tables present in the doc-\numents. However, it is noteworthy that for GPT- 4\nand GPT- 4o, the use of JSON demonstrated supe-\nrior efficacy. Consequently, JSON was chosen as\nthe preferred representation for tabular and chart\ninformation.10\n10Given the relatively lower complexity of contextual represen-\ntation in charts, a direct comparison wasn’t needed. Instead,\nwe adopted a unified JSON-based approach for representing\nboth structures.Parser RAG LLM Context type Accuracy\nY - GPT- 3.5t JSON 75.4\nY - GPT- 4 JSON 100.0\nY - GPT- 4o JSON 96.2\nY - GPT- 3.5t Dataframe 84.9\nY - GPT- 4 Dataframe 96.2\nY - GPT- 4o Dataframe 92.4\nTable 5: Direct input analysis with the private evaluation\nset (tables). Here, the relevant extracted information\nis passed directly as context to an LLM. The context\ntype field represents the specific format considered for\nevaluation.\nAppendix C Visualizations\nC.1 Document Example\nThis section showcases an example of a real-world\nfinancial document that is used in this work (shown\nin Figure 5).\nC.2 Document Pre-processor Extractions\nThis section showcases examples of tabular and\ngraphical data extracted using the document pre-\nprocessor (see Table 6 and Table 7).\nC.3 GPT Failure Case\nThis section showcases examples where GPT- 4o\nfails to accurately interpret chart images (see Ta-\nble 8).\nC.4 ChartVLM Failure Case\nThis section showcases examples where ChartVLM\nfails to completely read and interpret chart images\n(see Table 9). Aside from the incorrect value map-\npings, we also observe that the model entirely over-\nlooked the quarter ‘Q2/2024’ bar data.\nAppendix D Cost Calculation for Popular\nCommercial Solutions\nBased on analysis of the datasets, a document page\ncontains approximately 600tokens. The cost per\npage of the proposed pre-processing pipeline and\ncommercial solutions are calculated as follows:\nLlamaParse: LlamaParse charges users by ‘cred-\nits’.11The cost is $1.0per 1000 credits, thus to\nensure a fair comparison, GPT- 4ois chosen as the\nextraction model which costs 30credits per page,\namounting to (1/1000) ∗30 = 0 .03per page.\n11https://docs.cloud.llamaindex.ai/pricing\n--- Page 11 ---\n1Q 2024 Company Fact SheetCompany Overview•Leading international financial services provider with presence in Asia, Canada and U.S. for more than 125 years, 135 years and 160 years, respectively.Manulife provides financial advice, insurance, and wealth and asset management solutions for individuals, groups and institutions1.•A top 10 largest life insurance companies in the world2.•More than 38,000 employeesand thousands of distribution partners servingmore than 35 million customers.•Assets under management and administration (AUMA)3of$1.4 trillion as at March 31, 2024 (Total invested assets and Segregated funds net assets of $410.7 billion and $402.1 billion, respectively).•Business diversification is a key strength which provides resiliency and foundation for growth.Business Diversification (1Q24)Core earnings3Net incomeattributed to shareholders37%21%26%20%-4%42%31%-12%42%-3%AsiaCanadaU.S.Global WAMCorporate &OtherStrategy•Our ambition is to be the most digital, customer-centric global company in our industry•Our strategy is underpinned by five strategic priorities•The goals for our stakeholders areCustomer: Improve NPS4by +37 pointsand delight customersEmployees: Engage our employees -maintain top quartile engagementShareholders: Deliver top quartile returns\nKey Financials Financial Targets1Q24Medium-Term Targets8Core EPS growth520%10%-12%Core ROE616.7%15%+Financial Leverage ratio624.3%25%Core dividend payout ratio643%35%-45%New business CSM growth552%15%CSM balance growth523%8%-10%EPS growth5-38%ROE8.0%Common share dividend payout ratio89%Capital and Liquidity Strength\n9138%\n10$23.7 billion Cash & cash equivalents and marketable securities$242.1 billion Financial Strength Ratings11AgencyAM BestDBRSFitch\nS&PRatingA+AAAA-A1AA-Financial KPIs1Q24Change from 1Q23Net income attributed to shareholders$866$540Core earnings3$1,75416%5Total expenses(incl. General expenses of $1,102) $1,6792%5Expense efficiency ratio645.1%2 ppsAPE sales ($ billions)$1.921%7New business value$66934%7New business CSM$65852%5Global WAM net flows ($ billions)$6.7$2.3Global WAM average AUMA ($ billions)$8809%7Book value per share ($)$23.095%Adjusted book value per share ($)6$33.3911%Shareholder informationMarket capitalization2$60.9 billionDividend yield124.7%1Q24 dividend per common share ($)$0.40Ticker         TSX/NYSE/PSEHKEX\nDividend per common share ($)1,4620132014201520162017201820192020202120222023Note: Figures are in C$ millions unless otherwise noted. 1We operate as Manulife across our offices in Asia, Canada, and Europe, and primarily as John Hancock in the United States. 2 Based on market capitalization data as at March 31, 2024. Source: Bloomberg. 3Core earnings and AUMA are non-\n-\n1Q24 MD&A. 4\n5Percentage changes are stated on a constant exchange rate basis and are non-GAAP ratios. 6Non-GAAP ratios. 7Percentage changes are stated on a constant exchange rate basis. 8\n-\n9Life Insurance Capital Adequacy Test (LICAT) ratio of The Manufacturers Life Insurance Company (MLI) as at March 31, 2024. LICAT rati\nguideline. 10\n11Financial Strength Ratings apply to the main life operating companies of Manulife Financial Corporation including MLI, John Hancock Life Insurance Co. (U.S.A.), John Hancock Life & Health Insurance Co. and John Hancock Life Insurance Co. of New York. DBRS does not rate the U.S. insurance subsidiaries separately. Outlook is Stable\n2024. 12 Dividend per common share paid in the quarter * 4 / Current quarter closing share price.Figure 5: Financial document example.\nVertex AI: Vertex AI’s solution12employs\nGemini-2.0-flash. It can directly process images\nand charges $0.0001935 token based on the im-\nage modality, amounting to $0.0001935 ∗600 =\n$0.1161 per page.\nAnthropic: Anthropic’s solution13leverages\nClaude 3 Opus, which charges $15per 1M tokens,\namounting to ($15 /1,000,000)∗600 = $0 .009\nper page.\nThe proposed solution: Azure Document Intel-\nligence14is utilized to extract multi-structured data\ncontents from documents in the pre-processor. It\ncosts $375 /500,000pages. Regarding the Azure\nAI Search index used to store the extracted con-\ntents, Microsoft provides a free-tier service, thus\n12https://cloud.google.com/vertex-ai/\ngenerative-ai/pricing#token-based-pricing\n13https://cloud.google.com/vertex-ai/\ngenerative-ai/pricing#modality-based-pricing\n14https://azure.microsoft.com/en-us/pricing/\ndetails/ai-document-intelligence/the index hosting cost is $0. There are also nu-\nmerous open-source indexes that can be used in-\nstead. Lastly, the model used for generating vec-\ntor embeddings, Azure OpenAI’s ada-embeddings-\n002, charges $0.00006 page. Azure OpenAI’s\nGPT- 4ocharges $2.5per 1M tokens,15amount-\ning to $0.0015 per page. Therefore, the proposed\npre-processor used in this work costs $0.00075 +\n$0.0015 + $0 .00006 = $0 .00231 .\n15https://azure.microsoft.com/en-us/pricing/\ndetails/cognitive-services/openai-service/\n--- Page 12 ---\n[[{‘Revenues by region;(Dollars in millions);’:\n‘North America’,\n‘Fiscal Years;2013;’: ‘$ 159’,\n‘Fiscal Years;% Change;’: ‘(6%)’,\n‘Fiscal Years;2012;’: ‘$ 130’,\n‘Fiscal Years;2011;’: ‘$ 137’}, ...]]\nTable 6: Visualization for pre-processor table extraction. This is a sampled image from the VQAonBD validation\ndataset.\n--- Page 13 ---\n[{“Quarter \": “ 2Q23 \",\n“ APE Sales: Asia other \": 552,\n“ APE Sales: Japan \": 59,\n“ APE Sales: Hong Kong \": 268,\n“ New business CSM: Asia other \": 167,\n“ New business CSM: Japan \": 14,\n“ New business CSM: Hong Kong \": 142}\nTable 7: Visualization for pre-processor chart extraction. This is a sampled image from the ComplexChartQA\ndataset.\n--- Page 14 ---\nQuestion What is the value of product B value for strategy 5?\nAnswer In the chart, the value of Product B for Strategy 5 is 90.\nQuestion What is the total value for strategy 5?\nAnswer The total value for Strategy 5 is 215.\nTable 8: Failure case for GPT-4o.\n--- Page 15 ---\n[{“Quarter \": “ Q2/2023 \",\n“ Average Deposits \": 316,\n“ Percentage Change \": 250, ...},\n{“Quarter \": “ Q1/2024 \",\n“ Average Deposits \": 338,\n“ Percentage Change \": 271, ...}]\nTable 9: ChartVLM failure case where it overlooks the ‘Q2/2024’ bar data completely due to its limited interpreta-\ntional capabilities.",
  "text_length": 46113
}