{
  "id": "http://arxiv.org/abs/2506.04063v1",
  "title": "Crowd-SFT: Crowdsourcing for LLM Alignment",
  "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation.",
  "authors": [
    "Alex Sotiropoulos",
    "Sulyab Thottungal Valapu",
    "Linus Lei",
    "Jared Coleman",
    "Bhaskar Krishnamachari"
  ],
  "published": "2025-06-04T15:26:38Z",
  "updated": "2025-06-04T15:26:38Z",
  "categories": [
    "cs.HC",
    "cs.DC",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04063v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04063v1  [cs.DC]  4 Jun 2025Crowd-SFT: Crowdsourcing for LLM Alignment\nAlex Sotiropoulos\nViterbi School of Engineering\nUniversity of Southern California\nLos Angeles, USA\nasotirop@usc.eduSulyab Thottungal Valapu\nViterbi School of Engineering\nUniversity of Southern California\nLos Angeles, USA\nthottung@usc.eduLinus Lei\nViterbi School of Engineering\nUniversity of Southern California\nLos Angeles, USA\nlinuslei@usc.edu\nJared Coleman\nSeaver College of Science and Engineering\nLoyola Marymount University\nLos Angeles, USA\njared.coleman@lmu.eduBhaskar Krishnamachari\nViterbi School of Engineering\nUniversity of Southern California\nLos Angeles, USA\nbkrishna@usc.edu\nAbstract —Large Language Models (LLMs) increasingly rely\non Supervised Fine-Tuning (SFT) and Reinforcement Learning\nfrom Human Feedback (RLHF) to align model responses with\nhuman preferences. While RLHF employs a reinforcement learn-\ning approach with a separate reward model, SFT uses human-\ncurated datasets for supervised learning. Both approaches tra-\nditionally depend on small, vetted groups of annotators, making\nthem costly, prone to bias, and limited in scalability. We propose\nan open, crowd-sourced fine-tuning framework that addresses\nthese limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes\nincentive fairness via a point-based reward system correlated\nwith Shapley values and guides model convergence through\niterative model updates. Our multi-model selection framework\ndemonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that\nvalidate our point-based reward mechanism’s close alignment\nwith Shapley values—a well-established method for attributing\nindividual contributions—thereby supporting fair and scalable\nparticipation.\nIndex Terms —SFT, Large Language Models, Crowdsourcing,\nTournament-Based Selection\nI. I NTRODUCTION\nAs Large Language Models (LLMs) become integral to\nvarious applications, aligning their responses with human\npreferences is crucial. Supervised Fine-Tuning (SFT) and\nReinforcement Learning from Human Feedback (RLHF) have\nemerged as the standard approaches for accomplishing this.\nNonetheless, both SFT and RLHF face fundamental limi-\ntations: SFT requires curated, high-quality datasets that are\ntypically expensive to create [1], and both approaches suffer\nfrom limited annotator diversity, introducing potential biases\nand scalability challenges [2], [3]. In this paper, we propose an\nonline SFT framework in which models are iteratively updated\nwith new human feedback. While our focus is on SFT, the\nsame principles could be applied to RLHF by adapting them\nfor reward model training.\nWe propose a framework that leverages a decentralized,\ncrowd-sourced approach to fine-tuning. Instead of relying on\na small set of evaluators, our method allows a broader poolof users to iteratively refine an LLM while tracking individual\ncontributions. At each iteration, users are divided into non-\noverlapping groups, with each group collectively fine-tuning a\nseparate model instance. The best-performing model, selected\nthrough an evaluation function, is used as the base for the\nnext iteration. Although only one model advances, users in all\ngroups receive rewards based on their group’s performance\nrelative to the baseline model, ensuring even non-winning\ngroups are proportionately compensated. Our approach not\nonly democratizes fine-tuning but also introduces a compet-\nitive dynamic that enhances model quality while maintaining\nfairness in rewards.\nOur key contributions1are:\n•We propose a novel iterative fine-tuning framework in\nwhich multiple user groups train competing model in-\nstances. A selection mechanism ensures continual model\nimprovement across iterations, while a point-based sys-\ntem allocates rewards to participants proportionally to\ntheir group’s contribution in each iteration.\n•We empirically validate our framework through simula-\ntions, demonstrating improved model convergence and\naccurate estimation of group-wise contributions.\n–In simulations, our competitive fine-tuning frame-\nwork improved model convergence by as much as\n55% compared to traditional single-model SFT.\n–The proposed point-based reward system success-\nfully estimated individual user contributions, cor-\nrelating with their actual impact as determined by\nShapley values—a robust method from cooperative\ngame theory for fairly attributing contributions to\ncollective outcomes.\n–We also observed that incorporating randomness in\nuser grouping and model evaluation improved both\nconvergence and fairness in reward distribution.\nThe remainder of this paper is organized as follows: Sec-\ntion II reviews related work, Section III introduces a method to\n1Code available at: https://github.com/ANRGUSC/ml-bc-rating-review\n--- Page 2 ---\nempirically measure LLM fine-tuning as movement in vector\nspace, and lays the foundation for our collaborative fine-\ntuning framework. Expanding on this, Section IV details the\nframework and analyzes its performance and scaling effects\nacross various grouping and evaluation methods. Section V\nconcludes the paper, and finally, Section VI outlines future\nresearch directions.\nII. R ELATED WORK\nSFT calibrates pre-trained models by directly training them\non high-quality instruction-response pairs, aligning LLMs with\nhuman preferences. While SFT focuses on demonstrative\nlearning, it is often complemented by RLHF, which optimizes\nmodel behavior using comparative human feedback in a rein-\nforcement learning framework [4]. Despite their effectiveness,\napproaches relying on human feedback face fundamental lim-\nitations in data collection due to small annotator pools, which\nsignificantly restrict alignment methods. Narrow evaluator\ndemographics can embed political and cultural biases, while\ninconsistencies in human judgment—exacerbated by cognitive\nfatigue over extended annotation sessions—further weaken\nreliability [2], [3]. These challenges suggest the need for a\nmore open and inclusive SFT process that leverages broader\nhuman feedback.\nAs the foundation of our approach, we utilize tournament-\nbased selection, where user groups fine-tune separate model\ncopies. The best-performing version becomes the baseline\nfor the next round, driving improvement through competi-\ntion. Tournament-based selection has been widely used to\nbenchmark LLM performance through model comparisons [5],\n[6]. However, our framework extends this paradigm by incor-\nporating contribution-aware tournaments that evaluate model\nquality through pairwise battles and track user impact via\na structured point-based system validated against Shapley\nvalues.\nThough not directly integrated into our approach, Shapley\nvalues serve as a baseline for evaluating our point-based sys-\ntem by fairly attributing individual contributions to collective\noutcomes. Initially developed in cooperative game theory, they\nhave been widely applied in machine learning to quantify the\nimportance of features, data points, and individual participants\nin collaborative settings. In the context of LLM training,\nShapley values have been used to allocate rewards in online\ndata marketplaces [7] and to evaluate contributions in federated\nlearning systems [8]. These prior works primarily focus on\nobjective data sources, such as medical or sensor data; in\ncontrast, our framework extends their application to subjective\nhuman feedback in the SFT setting.\nIII. E XPERIMENT 1: V ECTOR -SPACE MODELING OF LLM\nFINE-TUNING\nWe conceptualize LLM fine-tuning as an iterative process\nin which model updates are guided by user feedback. Our\napproach assumes that there exists some embedding function\nΦthat maps model outputs to an n-dimensional vector space,\nallowing us to quantify their alignment with a predefinedtarget. While the actual fine-tuning process remains opaque,\nthis assumption enables us to analyze the effects of iterative\nrefinement in a structured manner. For the experiments in this\nsection, we implement Φusing a RoBERTa-based model that\nwas trained on the GoEmotions dataset [9]—a curated collec-\ntion of Reddit comments that have been human-annotated for\nemotions. This model maps text to a 28-dimensional vector\nspace representing unique emotions.\nThe process begins with a base open-source Large Language\nModel, M1, which can be fine-tuned using a fine-tuning\ntranscript, F= (f1, f2, . . . , f p). Each interaction firepresents\na user-model exchange, consisting of input-output pairs in\nthe case of SFT, or response rankings in the case of RLHF.\nA fine-tuning function Fthen updates the model based on\nthis feedback, producing a refined version Mi+1. To quantify\nmodel performance, we define an “ideal” model Mideal, which\nis assumed to generate ground-truth outputs for any input\nx. Then, the quality of a given model Mican be expressed\nas the distance between Φ(Mideal(x))andΦ(Mi(x))in the\nembedding space. This distance represents how far the model’s\noutput is from the desired behavior.\nIn our implementation, we define this emotion target by\nselecting the highest-ranking sentence for the given emotion\nfrom the GoEmotions dataset, retrieving its k-nearest neigh-\nbors, and averaging their vectors to create the target centroid.\nThese k-nearest neighbors also serve as the fine-tuning tran-\nscript F. While our experiments use sentiment embeddings,\nthis approach generalizes to any function providing meaningful\nvector representations of LLM outputs. Thus, we can quantify\nfine-tuning progress by measuring how model outputs move\ncloser to the target in embedding space.\nA. Baseline: Single-Model Fine-Tuning\nTo validate this conceptual framework, we apply our vector\nspace approach to traditional single-model fine-tuning, where\nwe feed a singular model a set of fine-tuning samples and\nexamine whether it exhibits measurable convergence toward\na defined target. These results serve as a baseline for Sec-\ntion III-B.\nStarting with a base model, M1, which is fine-tuned for a\nspecific emotion using the k-nearest neighbors as the fine-\ntuning transcript, Fi, we assess how training data volume\ninfluences convergence by progressively increasing the number\nof samples in our fine-tuning set. Given a neutral prompt\n(“Write a Reddit comment”), the fine-tuned model generates\nresponses that are mapped to the emotion space. The Euclidean\ndistance between these response embeddings and the target\ncentroid serves as our primary convergence metric.\nFigure 1 illustrates the fine-tuning process across varying\nsample sizes for the best, median, and worst-performing mod-\nels. These emotion models were ranked based on their outputs’\nmean Euclidean distance to the target centroid when trained\non 100 samples. While there is some initial movement toward\nthe target centroid, the improvement is modest, and after\n66 samples, progress plateaus or even slightly deteriorates.\nThis suggests that while expanding the fine-tuning set can\n--- Page 3 ---\nnudge model outputs in the right direction, they may not be\nsufficient on their own to ensure consistent convergence. These\nlimitations highlight the need for more robust fine-tuning\nstrategies, which we propose in the following subsection.\n33 Samples 66 Samples 100 Samples\nNumber of Fine-Tuning Samples0.00.20.40.60.81.01.2Distance from CenterSingle Model Distance Distribution Comparison\nEmotion Performance\nworst emotion (disappointment)\nmedian emotion (nervousness)best emotion (neutral)\nFig. 1. Baseline single-model fine-tuning experiment. This experiment\ndemonstrates traditional fine-tuning and visualizes model convergence towards\nspecific emotional vector spaces across varying sample sizes. Using the\nGoEmotions dataset of human-annotated Reddit comments, we identified the\n100 nearest neighbors for each emotion’s highest-ranking comment and fine-\ntuned independent models on 33, 66, and 100 samples per emotion. Box\nplots show results from prompting each model 50 times with “Write a Reddit\ncomment” and processing outputs through the GoEmotions classifier. The y-\naxis represents distance to the target emotion centroid (mean of 100 nearest\nneighbors), with shaded bands showing the interquartile range of distances\nfrom these neighbor samples to the centroid. This graph displays the worst,\nmedian, and best-performing emotions based on mean distance to target\ncentroid at 100 samples.\nB. Foundational Framework: Multi-Model Selection Fine-\nTuning\nBuilding on the single-model results, we introduce a com-\npetitive strategy that generates multiple model variants per\niteration, selecting the best performer as the base for the\nnext iteration. This competitive mechanism introduces selec-\ntion pressure, ensuring that improvements accumulate more\neffectively over successive iterations.\nFormally, at iteration i, we generate jfine-tuned model\nvariants, M(j)\ni, each trained on different subsets of feedback\ndataF(j)\ni. For consistency with our single-model experiment,\nwe use the same k-nearest neighbors for each emotion as the\nfine-tuning samples, setting k= 100 . The training samples are\ndistributed evenly across all jmodels in each iteration, with\nthe winning model’s training samples removed from the feed-\nback pool in subsequent rounds. This progressive reduction\nin available training data ensures that each successive model\nlearns from a unique set of samples. Each model produces\na set of test responses using the generic prompt described\nin Section III-A, which are then mapped to the same vector\nspace as before. We then compute the distance between each\nmodel’s response distribution and the target centroid, selecting\nthe model that minimizes this distance as Mi+1.Figure 2 presents the results of our multi-model selec-\ntion approach, evaluated using the best, median, and worst-\nperforming emotions identified in Figure 1. Compared to the\nsingle-model approach fine-tuned on 100 samples, our multi-\nmodel selection method, after three iterations, reduced the\nmean distance to the target centroid by 20.04% for disappoint-\nment , 5.24% for nervousness , and 55.37% for neutral . These\nresults indicate that fine-tuning under a selection-driven pro-\ncess not only improves contribution tracking but also enhances\noverall model quality through competitive refinement.\n1 2 3\nGeneration0.00.20.40.60.81.0Distance from CenterMulti-Model Performance for Best, Median, and\n Worst Single-Model Emotions\nEmotion\nDisappointment\nNervousnessNeutral\nFig. 2. Multi-model competitive fine-tuning performance. This experiment\nimplements a competitive fine-tuning approach using the same GoEmotions\ndataset, 100 nearest neighbor samples per emotion, and classifier methodology\nas Figure 1. The base model is cloned 3 times per iteration, with samples\nevenly split across clones for fine-tuning. After evaluation with the same\ngeneric prompt, the clone with smallest distance to the target emotion centroid\nbecomes the winning model and base for the next iteration. The winning\nmodel’s samples are removed from the training pool, ensuring subsequent\nmodels see unique data. Box plots show outputs from winning models for the\nsame worst, median, and best-performing emotions from Figure 1. Compared\nto the single-model approach using all 100 samples, the competitive approach\nin the third iteration reduced mean distances by 20.04%, 5.24%, and 55.37%,\nrespectively.\nTo provide a comprehensive view of the performance dif-\nferences, Figure 3 extends this comparison across all target\nemotions. The results confirm that multi-model fine-tuning\noutperforms single-model fine-tuning consistently, showcasing\nthat our competitive framework not only improves model\nperformance but also enables the contribution tracking mech-\nanisms outlined in the following section.\nIV. E XPERIMENT 2: C OLLABORATIVE FINE-TUNING\nFRAMEWORK\nHaving established the viability of our multi-model, compet-\nitive fine-tuning approach, we now investigate different strate-\ngies for grouping users and evaluating model performance.\nSpecifically, we aim to determine how different grouping\nmethods impact convergence and whether our contribution-\ntracking mechanism reliably identifies user influence. To do\nso, we experiment with multiple ways of partitioning users\ninto training groups and compare several distance-based eval-\nuation metrics for selecting the best-performing model at each\niteration.\n--- Page 4 ---\nrealization\noptimism\nexcitement\ndisappointment\nlove\nfear\ncaring\nconfusion\nsurprise\nsadness\nneutral\ngrief\ncuriosity\nanger\namusement\ndisapproval\njoy\nembarrassment\ndesire\nremorse\nannoyance\nnervousness\ngratitude\nadmiration\ndisgust\npride\napproval\nrelief\nEmotion0.00.10.20.30.40.50.60.70.8Distance to T arget NeighborhoodSingle-Step vs Multi-Step Distance Comparison in the Final Iteration\nSingle-Step\nMulti-StepFig. 3. This figure compares the performance of single-step fine-tuning at\n100 samples from Figure 1 with multi-step competitive fine-tuning in the\nthird iteration from Figure 2 across all 28 emotions in the GoEmotions\ndataset. Each stacked bar shows the distance to the target emotion centroid,\nwith red representing single-step performance and blue representing multi-\nstep performance. The multi-step approach demonstrates superior performance\nacross all emotions, with particularly notable improvements in emotions like\nlove,gratitude , and admiration .\nWe introduce a point-based system to estimate individual\nuser contributions. To assess the fairness of this mechanism,\nwe compare each user’s accumulated points to their approx-\nimate Shapley value. Since calculating exact Shapley values\nis computationally expensive, we leverage the KernelSHAP\nalgorithm [10], [11], implemented in the SHAP Python library,\nto efficiently approximate these values.\nA. Definitions: Grouping and Evaluation Methods\nAt each iteration, users are divided into groups, each fine-\ntuning a separate model. The model that best aligns with\nan expert-defined target becomes the base model for the\nnext iteration, and all users receive rewards based on their\ngroup’s performance. We test three grouping strategies and\nthree evaluation metrics:\n•Grouping Methods ( Gmethod ):\n1)Random Grouping: Users are randomly assigned to\ngroups with equal probability.\n2)ϵ-greedy: Initially, users are randomly grouped, but\nover time, assignment increasingly favors users with\nhigher accumulated points, inspired by the popular\nϵ−greedy multi-armed bandit algorithm.\n3)Interleaved Grouping: Users are first ranked based\non prior contributions, then divided into high and\nlow-performing groups. Assignments alternate be-\ntween these two groups to create balanced teams.\n•Evaluation Methods ( Emethod ):\n1)L2 Norm (Euclidean Distance)\n2)L1 Norm (Manhattan Distance)\n3)Dot ProductUser 1 User 2 ... Usern\nGmethod\nModel M1 ... Model Mm\nSelect Best Model based on Emethod\nUpdate User Scores\nFig. 4. Iterative Decentralized Fine-Tuning Process : This figure illustrates\nour framework where multiple users contribute to model fine-tuning across\niterative rounds. Users are grouped using Gmethod and assigned to fine-tune\nseparate models ( M1toMm). After each iteration, models are evaluated\nusing Emethod to select the best-performing model. User scores are updated\nbased on their group’s model performance, and the winning model serves as\nthe base for the next iteration, creating a continuous cycle.\nB. Simulation Setup\nWe use the MovieLens dataset2, which contains user pref-\nerence data across ndifferent movie genres, treating each\ngenre as an independent dimension in our vector space.\nFigure 4 shows a high-level overview of the simulation, and\nis structured as follows:\n1)Initialization: A set of users and an initial model M1∈\nRnare randomly placed in the feature space. We define\nan expert target ( E) as the preference vector of a single\nrandomly selected user.\n2)Grouping: Users are divided into groups based on\nGmethod , and each group fine-tunes a model instance.\n3)Model Update: For each group, we compute a weighted\ncentroid Cg(t), where each user’s preference vector is\nweighted by their accumulated point value at round t.\nThe candidate model for group gis updated as:\nMcandidate ,g(t) =Mt+δ(Cg(t)−Mt)\nwhereMtis the base model for round t.\n4)Selection: The candidate model closest to the expert\ntarget—determined by Emethod —becomes base model\nin round t+1. To introduce noise, we arbitrarily assume\nthe expert selects an incorrect model 5% of the time.\n5)Rewarding Contributions: At each round, we rank the\ncandidate models based on their proximity to the expert\n2https://grouplens.org/datasets/movielens/\n--- Page 5 ---\ntarget, using Emethod . Suppose there are mgroups. The\nbest-performing group’s model receives mpoints, the\nsecond-best receives m−1, and so on, down to 1point\nfor the lowest-ranked model. Formally, each user in\ngroup greceives ∆gpoints, where\n∆g=m−argsort (Emethod (Mcandidate ,g(t),E)).\n6)Final Evaluation: At the end of 100 rounds, we com-\npute the estimated Shapley values for each user and\ncompare them to their accumulated points to measure\ncorrelation.\nRound 1\n Round 7\nRound 13\n Round 20How a User Subset Pulls the Model T oward Its Centroid\nAll Users\nExpert Point\nCurrent Model\nChosen Subset\nSubset Centroid\nFig. 5. User Subset Impact on Model Convergence : This figure demon-\nstrates the iterative convergence process, where user preferences are repre-\nsented as points in a multi-dimensional space. In each round, a selected user\nsubset (dark yellow points) computes its weighted centroid (big yellow point),\nwhich pulls the current model (purple circle with black outline) toward the\nexpert target (red X). The arrows show the directional influence of each\nsubset’s centroid on model updates. Over 20 rounds, the model gradually\nconverges toward the expert point as different user subsets contribute to\nfine-tuning, illustrating how decentralized user contributions can guide model\nalignment toward desired outcomes.\nThe intuition behind this approach is that each group’s\nmodel update shifts the model toward the weighted centroid of\nits users’ preferences, mimicking how subgroups of annotators\nmight fine-tune a model based on their distinct biases and\nperspectives. Figure 5 provides a conceptual illustration of\nthis process in action. In the figure, the arrow shows the\ndirectional influence of the computed user subset centroid\non the current model point. Over successive rounds, where\nonly the model closest to the expert-defined target is re-\ntained, the model gradually converges toward the expert point.\nThe deliberate introduction of noise in the selection process\naccounts for inconsistencies that might arise in real-world\nhuman evaluations, ensuring that our method is robust to\noccasional misjudgments. Finally, tracking user contributions\nthrough accumulated rewards allows us to assess whether\nthis competitive structure fairly credits influence to the most\nimpactful participants.C. Results: Performance of Grouping-Evaluation Method\nCombinations\nWe run the simulation 50 times to compute the average\nperformance of each (Gmethod , Emethod )combination, and\ntrack:\n•Convergence Distance: The final Euclidean distance to\nthe expert target.\n•Pearson Correlation: The relationship between user\npoint accumulation and their estimated Shapley values.\n3 4 5 6 711.522.533.544.5Evaluation Function, Grouping Function\ndot_product, epsilon_greedy\ndot_product, interleaved\ndot_product, random\nl1_norm, epsilon_greedy\nl1_norm, interleaved\nl1_norm, random\nl2_norm, epsilon_greedy\nl2_norm, interleaved\nl2_norm, randomAverage Inverse Pearson Correlation vs. Final Distance to Expert Point\nFinal Distance to Expert PointInverse Pearson Correlation\nFig. 6. Evaluation of Method Combinations in Collaborative Model\nTraining : This figure shows simulation results for the framework outlined\nin Section IV-B. Using the MovieLens dataset of user preferences toward\nmovie genres to represent humans in the loop, we randomly sample users by\ntheir preference vectors and select a random expert target. The experiment\ntests combinations of grouping and evaluation methods, rewarding users and\nupdating the model toward each round’s winning subset centroid. Results\nshow average performance across 50 simulation runs with 50 users and 3\ngroups. Each point represents the trade-off between model convergence to the\nexpert target (x-axis) and reward system accuracy compared to Shapley values\n(y-axis). Lower values indicate better performance; error bars show standard\ndeviation.\nWe compute the Pearson correlation between accumulated\nuser scores in the final round and corresponding Shapley\nvalues to assess our point-based method’s accuracy. The\nPearson correlation coefficient measures the strength of the\nlinear relationship between two variables. A coefficient near 1\nindicates our system closely approximates theoretical Shapley\nvalues, rewarding users proportionally to their actual impact.\nLower correlation indicates deviation from ideal tracking,\nsuggesting users are improperly rewarded relative to their\nactual influence.\nWe found that deterministic grouping approaches produce\nlower Pearson correlations, suggesting unfair reward distri-\nbution relative to true user contributions. This likely oc-\ncurs because deterministic strategies limit the redistribution\nof users over time, reinforcing early biases in contribution\nestimates. In contrast, methods incorporating randomness lead\nto higher Pearson correlations, meaning that user impact is\nbetter captured and rewarded more fairly. Additionally, these\nmethods also improve model convergence, indicating struc-\n--- Page 6 ---\ntured randomness aids both model optimization and accurate\nidentification of impactful contributors.\nFigure 6 showcases an initial analysis comparing unique\npairs of (Gmethod , Emethod )based on the inverse Pearson\ncoefficient and final convergence distance for 50 users and\n3 groups. Since lower values are better for both metrics, the\nbest-performing method combinations appear closer to the\norigin. In this configuration, L2 Norm + Random delivers the\nbest convergence distance, while Dot Product + Interleaved\nachieves the best inverse Pearson coefficient. These results\nshow that L2 Norm performs best for convergence distance\nwhile Dot Product does best for inverse Pearson coefficient.\nD. Results: Effects of User and Group Scaling on Method\nPerformance\nThe ability of an open, collaborative system to scale effec-\ntively is crucial for robustness. To investigate this, we system-\natically explore the parameter space by varying the number\nof users ( ∈ {10,25,50,75,100}) and groups ( ∈[2−5]),\nexamining how each (Gmethod , Emethod )pair responds to\nthese scaling challenges. Figures 7 and 8 show the average\nperformance across the 50 simulation runs for Pearson Corre-\nlation and convergence distance, respectively, across the entire\nparameter space.\nFigure 7 shows Pearson correlation declining as user num-\nbers increase, regardless of grouping or evaluation method.\nWhile our naive reward mechanism struggles to approximate\nShapley values as user populations grow, meaningful cor-\nrelation persists, especially in smaller settings where user\ncontributions are easier to estimate. Higher group counts\nyield better Pearson coefficients, yet this improvement remains\nsecondary to the correlation decline caused by increasing user\ncounts.\nIn contrast to the more pronounced decline in Pearson\ncorrelation, the final distance remains relatively stable as the\nnumber of users grows, as seen in Figure 8. The real impact\nin convergence distance stems from differences in evaluation\nmethods—L2 Norm generally achieves lower distances. Dot\nProduct shows poor convergence with fewer users but im-\nproves as user count increases, while L1 and L2 Norm achieve\nbetter convergence with smaller user populations before grad-\nually deteriorating with higher user counts.\nTable I compares which method pairs consistently outper-\nform others across the parameter space. It reveals that Dot\nProduct evaluation (with Random or Interleaved) dominates in\nPearson correlation, achieving the best performance in 85% of\nall user-group configurations. In contrast, we can see that the\nϵ−Greedy + L2 Norm pair minimizes convergence distance,\nwinning in 60% of configurations. These findings reveal a\ntrade-off between reward accuracy and model convergence,\nwith optimal methods depending on system priorities.\nV. C ONCLUSION\nWe introduced a novel framework for decentralized fine-\ntuning of LLMs that leverages a structured, competitive SFT\nprocess. Our approach enables a broader pool of users to\n0.20.40.60.8Pearson Correlation\nEpsilon Greedy\n Interleaved\nDot ProductRandom\n0.20.40.60.8Pearson Correlation\nL1 Norm\n20 40 60 80 100\nNumber of Users0.20.40.60.8Pearson Correlation\n20 40 60 80 100\nNumber of Users\n20 40 60 80 100\nNumber of Users\nL2 NormPearson Correlation vs. Number of Users\nfor Different Group Sizes\nGrouping MethodsEvaluation Methods# of Groups\n2\n3\n4\n5Fig. 7. Scaling Effects on Reward Fairness : This figure shows a holistic\nview of the scaling factors in the experiment outlined in Section IV-B. We\nexamine how increasing the number of users from 10 to 100 (x-axis) and\nvarying group sizes from 2 to 5 (colored lines) affect the accuracy of our\nreward system in estimating user contributions. The y-axis represents Pearson\ncorrelation between accumulated user points and actual Shapley values in the\nfinal round, averaged across 50 simulation runs. Results are shown for all\nnine combinations of grouping methods (columns) and evaluation methods\n(rows). The consistent downward trend across all methods indicates that\nreward accuracy decreases as user populations grow, while higher group\ncounts generally maintain better correlation.\n456Final Distance\nEpsilon Greedy\n Interleaved\nDot ProductRandom\n456Final Distance\nL1 Norm\n20 40 60 80 100\nNumber of Users456Final Distance\n20 40 60 80 100\nNumber of Users\n20 40 60 80 100\nNumber of Users\nL2 NormFinal Distance vs. Number of Users\nfor Different Group Sizes\nGrouping MethodsEvaluation Methods# of Groups\n2\n3\n4\n5\nFig. 8. Scaling Effects on Model Convergence : This figure examines the\nsame scaling factors as Figure 7, but focuses on how scaling impacts model\nconvergence to the expert target. The y-axis represents final distance to the\nexpert point, averaged across 50 simulation runs, with results shown for all\nnine combinations of grouping methods (columns) and evaluation methods\n(rows). Unlike the consistent decline in reward accuracy shown in Figure 7,\nconvergence distance remains relatively stable across user and group scaling.\niteratively refine a model while tracking individual contri-\nbutions. By dividing users into groups, fine-tuning multiple\ncompeting models, and selecting the best-performing variant\nin each iteration, we optimize both model quality and fair\ncontribution assessment.\nThese empirically validated fine-tuning experiments, framed\nas movement through an n-dimensional vector space, demon-\nstrated that model outputs systematically improve when guided\nby a structured selection mechanism, leading to reliable model\nconvergence. We also explored various grouping strategies\nand evaluation metrics, analyzing their effects on convergence\n--- Page 7 ---\nTABLE I\nTOP3METHOD PAIRS ACHIEVING THE BEST CONVERGENCE DISTANCE\nAND PEARSON CORRELATION ,WITH WINNING PERCENTAGES\nAGGREGATED ACROSS ALL PARAMETER CONFIGURATIONS .\nBest Convergence Distance\nGrouping\nFunctionEvaluation\nFunctionWinning Percentage\nϵ-Greedy L2 Norm 60%\nϵ-Greedy L1 Norm 15%\nRandom L2 Norm 10%\nBest Pearson Correlation\nGrouping\nFunctionEvaluation\nFunctionWinning Percentage\nRandom Dot Product 45%\nInterleaved Dot Product 40%\nRandom L2 Norm 10%\naccuracy and the fairness of contribution tracking. Our findings\nsuggest that incorporating randomness into grouping methods\nimproves both model performance and the reliability of user\ncontribution, with L2 Norm evaluation yielding the best con-\nvergence distance and Dot Product providing the most reliable\nestimation of user contributions through Pearson correlation.\nThis framework revealed that competition serves as a con-\ntribution tracking mechanism and as an intrinsic driver of\nimproved performance. Iteratively selecting best-performing\nmodels creates a selection pressure that accelerates alignment\nwith desired outcomes. Overall, our results demonstrate that\na decentralized, competitive SFT approach is feasible and\nbeneficial, enabling more scalable and inclusive LLM fine-\ntuning methods. Leveraging diverse collective intelligence\ncreates models that better reflect human preferences while\nensuring fair rewards for meaningful contributions.\nVI. F UTURE WORK\nWhile this work provides a strong proof of concept, several\ndirections remain for future exploration. A critical next step is\nvalidating the framework in real-world settings with human\nparticipants, such as crowd-sourced annotators or domain\nexperts. This would test the system’s practical feasibility in\nmore diverse and variable environments. Our current approach\nassumes honest and consistent participation, but future work\nshould explore how inconsistent or adversarial behavior affects\nmodel performance and contribution tracking fairness. Future\nresearch could also examine the impact of varying levels\nof evaluator error (currently set at 5% in Section IV-B) on\nsystem performance. Modeling such behaviors and evaluating\nsystem resilience under these conditions will be crucial for\ndeployment in realistic environments.\nAdditionally, the reward system could be extended to sup-\nport more granular or alternative mechanisms. As the number\nof users scales, the current winner-takes-most approach may\nlead to reduced fairness or diminished incentive to contribute.\nExploring modifications to the reward structure may yield\nfairer and more motivating outcomes, particularly in larger\ngroups.ACKNOWLEDGMENT\nSections of this report have been copy-edited with the\nassistance of ChatGPT. We certify that ChatGPT was not\nutilized to produce any technical content and we accept full\nresponsibility for the content of the paper.\nREFERENCES\n[1] Y . Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang,\nX. Jiang, and Q. Liu, “Aligning large language models with human: A\nsurvey,” 2023. [Online]. Available: https://arxiv.org/abs/2307.12966\n[2] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando,\nR. Freedman, T. Korbak, D. Lindner, P. Freire, T. Wang, S. Marks, C.-R.\nSegerie, M. Carroll, A. Peng, P. Christoffersen, M. Damani, S. Slocum,\nU. Anwar, A. Siththaranjan, M. Nadeau, E. J. Michaud, J. Pfau,\nD. Krasheninnikov, X. Chen, L. Langosco, P. Hase, E. Bıyık, A. Dragan,\nD. Krueger, D. Sadigh, and D. Hadfield-Menell, “Open problems\nand fundamental limitations of reinforcement learning from human\nfeedback,” 2023. [Online]. Available: https://arxiv.org/abs/2307.15217\n[3] Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\nD. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath,\nJ. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds,\nD. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda,\nC. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah,\nB. Mann, and J. Kaplan, “Training a Helpful and Harmless Assis-\ntant with Reinforcement Learning from Human Feedback,” Apr. 2022,\narXiv:2204.05862 [cs].\n[4] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,\nJ. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe, “Training language models to\nfollow instructions with human feedback,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02155\n[5] W.-L. Chiang, L. Zheng, Y . Sheng, A. N. Angelopoulos, T. Li, D. Li,\nH. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, and I. Stoica, “Chatbot\narena: An open platform for evaluating llms by human preference,”\n2024. [Online]. Available: https://arxiv.org/abs/2403.04132\n[6] S. Son, J.-M. Oh, H. Jin, C. Jang, J. Jeong, and K. Kim, “Varco arena:\nA tournament approach to reference-free benchmarking large language\nmodels,” 2024. [Online]. Available: https://arxiv.org/abs/2411.01281\n[7] A. Agarwal, M. Dahleh, and T. Sarkar, “A Marketplace for Data: An\nAlgorithmic Solution,” in Proceedings of the 2019 ACM Conference\non Economics and Computation , ser. EC ’19. New York, NY , USA:\nAssociation for Computing Machinery, Jun. 2019, pp. 701–726.\n[8] S. Ma, Y . Cao, and L. Xiong, “Transparent Contribution Evaluation for\nSecure Federated Learning on Blockchain.” IEEE Computer Society,\nApr. 2021, pp. 88–91.\n[9] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade,\nand S. Ravi, “Goemotions: A dataset of fine-grained emotions,” 2020.\n[Online]. Available: https://arxiv.org/abs/2005.00547\n[10] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model\npredictions,” in Proceedings of the 31st International Conference on\nNeural Information Processing Systems , ser. NIPS’17. Red Hook, NY ,\nUSA: Curran Associates Inc., 2017, pp. 4768–4777.\n[11] I. Covert and S.-I. Lee, “Improving kernelshap: Practical shapley\nvalue estimation using linear regression,” in Proceedings of The\n24th International Conference on Artificial Intelligence and Statistics ,\nser. Proceedings of Machine Learning Research, A. Banerjee and\nK. Fukumizu, Eds., vol. 130. PMLR, 13–15 Apr 2021, pp. 3457–3465.\n[Online]. Available: https://proceedings.mlr.press/v130/covert21a.html",
  "text_length": 37478
}