{
  "id": "http://arxiv.org/abs/2506.01158v1",
  "title": "FORT: Forward-Only Regression Training of Normalizing Flows",
  "summary": "Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to neural\ndynamical systems that encompass modern large-scale diffusion and flow matching\nmodels. Despite the scalability of training, the generation of high-quality\nsamples and their corresponding likelihood under the model requires expensive\nnumerical simulation -- inhibiting adoption in numerous scientific applications\nsuch as equilibrium sampling of molecular systems. In this paper, we revisit\nclassical normalizing flows as one-step generative models with exact\nlikelihoods and propose a novel, scalable training objective that does not\nrequire computing the expensive change of variable formula used in conventional\nmaximum likelihood training. We propose Forward-Only Regression Training\n(FORT), a simple $\\ell_2$-regression objective that maps prior samples under\nour flow to specifically chosen targets. We demonstrate that FORT supports a\nwide class of targets, such as optimal transport targets and targets from\npre-trained continuous-time normalizing flows (CNF). We further demonstrate\nthat by using CNF targets, our one-step flows allow for larger-scale training\nthat exceeds the performance and stability of maximum likelihood training,\nwhile unlocking a broader class of architectures that were previously\nchallenging to train. Empirically, we elucidate that our trained flows can\nperform equilibrium conformation sampling in Cartesian coordinates of alanine\ndipeptide, alanine tripeptide, and alanine tetrapeptide.",
  "authors": [
    "Danyal Rehman",
    "Oscar Davis",
    "Jiarui Lu",
    "Jian Tang",
    "Michael Bronstein",
    "Yoshua Bengio",
    "Alexander Tong",
    "Avishek Joey Bose"
  ],
  "published": "2025-06-01T20:32:27Z",
  "updated": "2025-06-01T20:32:27Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01158v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01158v1  [cs.LG]  1 Jun 2025FORT: Forward-Only Regression Training of\nNormalizing Flows\nDanyal Rehman1,2,3∗, Oscar Davis4, Jiarui Lu1,2, Jian Tang1,6,\nMichael Bronstein4,5,Yoshua Bengio1,2,Alexander Tong1,2†, Avishek Joey Bose1,4†\n1Mila – Quebec AI Institute,2Université de Montréal,3Massachusetts Institute of Technology\n4University of Oxford,5AITHYRA,6HEC Montréal\nAbstract\nSimulation-free training frameworks have been at the forefront of the generative\nmodelling revolution in continuous spaces, leading to neural dynamical systems that\nencompass modern large-scale diffusion and flow matching models. Despite the\nscalability of training, the generation of high-quality samples and their correspond-\ning likelihood under the model requires expensive numerical simulation—inhibiting\nadoption in numerous scientific applications such as equilibrium sampling of\nmolecular systems. In this paper, we revisit classical normalizing flows as one-step\ngenerative models with exact likelihoods, and propose a novel, scalable training\nobjective that does not require computing the expensive change of variable formula\nused in conventional maximum likelihood training. We propose FORWARD -ONLY\nREGRESSION TRAINING (FORT ), a simple ℓ2-regression objective that maps prior\nsamples under our flow to specifically chosen targets. We demonstrate that FORT\nsupports a wide class of targets, such as optimal-transport targets, and targets\nfrom large pre-trained continuous-time normalizing flows (CNF). We further\ndemonstrate that by using CNF targets, our one-step flows allow for larger-scale\ntraining that exceeds the performance and stability of maximum likelihood training,\nwhile unlocking a broader class of architectures that were previously challenging to\ntrain. Empirically, we demonstrate that our trained flows can perform equilibrium\nsampling in Cartesian coordinates of alanine dipeptide, tripeptide, and tetrapeptide.\n1 Introduction\nTable 1: Overview of various generative models and their relative\ntrade-offs with respect to the number of inference steps, ability to\nprovide exact likelihoods, and training objective for learning.\nMethod One-step Exact likelihood Regression training\nCNF (MLE) ✗ ✓ ✗\nFlow Matching ✗ ✓ ✓\nShortcut [Frans et al., 2024] ✓ ✗ ✓\nIMM [Zhou et al., 2025] ✓ ✗ ✓\nNF (MLE) ✓ ✓ ✗\nFORT (ours) ✓ ✓ ✓The landscape of modern simulation-\nfree generative models in continuous\ndomains, such as diffusion mod-\nels and flow matching, has led to\nstate-of-the-art generative quality\nacross a spectrum of domains [Betker\net al., 2023, Brooks et al., 2024,\nHuguet et al., 2024, Geffner et al.,\n2025]. Despite the scalability of\nsimulation-free training, generating\nsamples and computing model likelihoods from these model families requires computationally\nexpensive inference—often hundreds of model calls—through the numerical simulation of the\nlearned dynamical system. The search for efficient inference schemes has led to a new wave of\napproaches that seek to learn one-step generative models, either through distillation [Yin et al., 2024,\n∗Correspondence to: danyal.rehman@mila.quebec\n†Equal advising\nPreprint. Under review.\n--- Page 2 ---\nLu and Song, 2024, Sauer et al., 2024, Zhou et al., 2024], shortcut training [Frans et al., 2024],\nor Inductive Moment Matching (IMM) [Zhou et al., 2025] — methods that are able to retain the\nimpressive sample quality of full simulation. However, many highly sensitive applications—for\ninstance, in the natural sciences [Noé et al., 2019, Wirnsberger et al., 2020]—require more than\njust high-fidelity samples: they also necessitate accurate estimation of probabilistic quantities, the\ncomputation of which can be facilitated by having access to cheap and exact model likelihoods.\nConsequently, for one-step generative models to successfully translate to scientific applications, they\nmust additionally provide faithful one-step exact likelihoods for generated samples.\nGiven their capacity to compute exact likelihoods, classical normalizing flows (NF) have remained\nthede facto method for generative modelling in scientific domains [Dinh et al., 2014, 2016,\nRezende and Mohamed, 2015]. For example, in tasks such as equilibrium sampling of molecular\nsystems with Boltzmann generators [Noé et al., 2019], rapid and exact likelihood evaluation is\ncritical both for asymptotically debiasing generated samples, and for refining them via annealed\nimportance sampling [Tan et al., 2025]. Historically, NFs employed in conventional generative\nmodelling domains (such as images) are trained with the maximum likelihood estimation (MLE)\nobjective, which has empirically lagged behind the expressiveness, scalability, and ease of training\nof modern continuous normalizing flows (CNFs) trained with regression-based objectives like flow\nmatching [Peluchetti, 2023, Liu, 2022, Lipman et al., 2023, Albergo and Vanden-Eijnden, 2023].\nA key driver of the gap between classical flows and CNFs can be attributed to the MLE training\nobjective itself, which requires estimating the Jacobian determinant of the inverse flow-map, which\nis often costly to evaluate for expressive flow architectures, thus preventing efficient optimization\nand leading to numerical instability. The tension between MLE training and invertible architectural\nchoices used in flow design has created a plethora of exotic training recipes, even with modernized\nSOTA flows on images with Transformer backbones [Zhai et al., 2024, Kolesnikov et al., 2024],\nthat run counter to the simplicity of training of current one-step generative models—albeit without\nefficient exact likelihood. This raises the natural motivating research question:\nQ.Does there exist a performant training recipe for classical Normalizing Flows beyond MLE?\nPresent work . In this paper, we answer in the affirmative. We investigate how to train an invertible\nneural network to directly match a predefined invertible function. We introduce FORWARD -ONLY RE-\nGRESSION TRAINING (FORT ), a novel regression-based training objective for classical normalizing\nflows that marks a significant departure from the well-established MLE training objective.\nOur key insight is that access to coupled samples from any invertible map is sufficient to train a\ngenerative model with a regression objective. Moreover, with privileged access to such pairings, a\nclassical NF can then be used to directly regress against the target points by pushing forward the\ncorresponding noise points. As a result, we may view FORT as a flow matching objective wherein the\nlearnable flow-map is an exactly invertible architecture. FORT provides similar benefits to NF training\nas flow matching does to continuous NFs. Compared to MLE training of NFs, FORT immediately\nunlocks a key training benefit: to compute the ℓ2-regression objective, we only need to compute the\nNF in the forward direction—removing the need to compute the Jacobian determinant of the inverse\nflow-map during generation. Furthermore, as outlined in table 1, unlike other one-step generative\nmethods, FORT provides faithful access to exact log-likelihoods while being cheaper than CNFs.\nTo train NFs using FORT , we propose a variety of couplings to facilitate simple and efficient\ntraining. We propose endpoint targets that are either: ( 1) outputs of a larger pretrained CNF; or\n(2) the solution to a pre-computed OT map done offline as a pre-processing step. In each case, the\ndesigned targets are the result of already invertible mappings, which simplifies the learning problem\nfor NFs and enhances training stability. Empirically, we deploy FORT flows on learning equilibrium\nsampling for short peptides in alanine di-, tri-, and tetrapeptide, and find even previously discarded\nNF architectures, such as affine coupling [Dinh et al., 2016] or neural spline flows [Durkan et al.,\n2019], can outperform their respective MLE trained counterpart. In particular, we demonstrate that\nin scientific applications where MLE training is unsuccessful, the same model trained using FORT\nprovides higher fidelity proposal samples and their likelihood. Finally, we demonstrate a completely\nnew method of performing Targeted Free Energy Perturbation [Wirnsberger et al., 2020] that avoids\ncostly energy evaluations with FORT that are not possible with MLE training of normalizing flows.\n2\n--- Page 3 ---\n2 Background and Preliminaries\nGenerative models . A generative model can be seen as an (approximate) solution to the distribution\nmatching problem: given two distributions p0andp1, the distributional matching problem seeks to\nfind a push-forward map fθ:Rd→Rdthat transports the initial distribution to the desired endpoint\np1= [fθ]#(p0). Without loss of generality, we set pprior:=p0to be a tractable prior (typically\nstandard normal) and take pdata:=p1the data distribution, from which we have empirical samples.\nWe now turn our attention to solving the generative modelling problem with modelling families\nthat admit exact log-likelihood, logpθ(x), where pθ= [fθ]#(p0), with a particular emphasis on\nnormalizing flows [Dinh et al., 2014, 2016, Rezende and Mohamed, 2015, Papamakarios et al., 2021].\n2.1 Continuous normalizing flows\nLearning the pushforward map, fθ, can be done by converting this problem into the solution to a\nneural dynamical system. For example, in a deterministic dynamical system, the pushforward map\nbecomes a time-dependent sufficiently smooth generator fθ: [0,1]×Rd→Rd,(t, x0)7→xtand\nforms the solution pathway to a (neural) ordinary differential equation (ODE) with initial conditions\nf0(x0) =x0. More precisely, a continuous normalizing flow (CNF) models the problem as the\nfollowing ODEd\ndtft,θ(x) =vt,θ(ft,θ(xt)). Here, vt,θ: [0,1]×Rd→Rdis the time-dependent\nvelocity field associated with the (flow) map that transports particles from p0top1.\nAs a CNF is the solution to a deterministic dynamical system, it is an invertible map, and as a result,\nwe can compute the exact log-likelihood, logpt,θ(xt), using the instantaneous change of variable\nformula for probability densities [Chen et al., 2018]. The overall log-likelihood of a data sample,\nx0, under the model can be computed as follows:\nlogp1,θ(x1) = log p0(x0)−Z0\n1∇ ·vt,θ(xt)dt. (1)\nMaximizing the model log-likelihood in eq. (1) offers one possible method to train CNF’s but\nincurs costly simulation. Instead, modern scalable methods to train CNF’s is to employ flow\nmatching [Lipman et al., 2023, Albergo and Vanden-Eijnden, 2023, Tong et al., 2023, Liu et al.,\n2023], which learns vt,θby regressing against the (conditional) vector field associated with a designed\ntarget conditional flow everywhere in space and time, e.g., constant speed conditional vector fields.\nNumerical simulation . In practice, the simulation of a CNF is conducted using a specific numerical\nintegration scheme that can impact the likelihood estimate’s fidelity in eq. (1). For instance, an Euler\nintegrator tends to overestimate the log-likelihood [Tan et al., 2025], and thus it is often preferable\nto utilize integrators with adaptive step size, such as Dormand–Prince 45 [Hairer et al., 1993]. In\napplications where estimates of the log likelihood suffice, it is possible to employ more efficient\nestimators such as Hutchinson’s trace estimator to get an unbiased—yet higher variance—estimate\nof the divergence. Unfortunately, as we demonstrate in §3.1, such estimators are too high variance\nto be useful for importance sampling even in the simplest settings, and remain too computationally\nexpensive and unreliable in larger scientific applications considered in this work.\nOne-step maps: Shortcut models . One way to discretize an ODE is to rely on the self-\nconsistency property of ODEs, also exploited in consistency models [Song et al., 2023], namely\nthat jumping ∆tin time can be constructed by following the velocity field for two half steps\n(∆t/2). This is the core idea behind shortcut models [Frans et al., 2024] that are trained at\nvarious jumps by conditioning the vector field network on the desired step-size ∆t. Precisely,\nf∗\nshort,t,2∆t(xt) =f∗\nt(xt,∆t)/2 +f∗\nt(x′\nt+∆t,∆t)/2, where x′\nt+∆t=xt+f∗\nt(xt,∆t)∆t. In their\nextreme, shortcut models define a one-step mapping which has been shown to generate high-quality\nimages, but it remains an open question whether these models can reliably estimate likelihoods.\n2.2 Normalizing flows\nThe generative modelling problem can also be tackled using time-agnostic generators. One such\nprominent example are Normalizing Flows (NFs) [Dinh et al., 2016, Rezende and Mohamed,\n2015], which parameterize diffeomorphisms (continuously differentiable bijective functions, with\na continuously differentiable inverse), fθ:Rd→Rd. Those are typically trained using an MLE\nobjective. It is important to highlight that for arbitrary invertible maps fθ, computing the absolute\n3\n--- Page 4 ---\nvalue of the log Jacobian determinant of the flow exerts a prohibitively expensive cost that scales\nO(d3). Consequently, it is popular to build fθusing a composition of Melementary diffeomorphisms,\neach with an easier to compute Jacobian determinant: fθ=fM−1◦ ··· ◦ f0[Papamakarios et al.,\n2021]. Through function composition, simple invertible blocks can lead to flows that are universal\ndensity approximators [Teshima et al., 2020], and the resulting MLE objective for training is simply:\nlogpθ(x1) = log p0(x0)−M−1X\ni=0log det\f\f\f\f∂fi,θ(xi)\n∂xi\f\f\f\f. (2)\nDespite the theoretical expressive power of certain classes of NFs [Teshima et al., 2020, Ishikawa\net al., 2023, Kong and Chaudhuri, 2021, Zhang et al., 2020, Bose et al., 2021], training using the MLE\nobjective does not offer any insight into the ease and practical optimization of fθduring learning.\n3 F ORWARD -ONLY REGRESSION TRAINING\nWe seek to build one-step transport maps that both push forward samples x0∼p0tox1∼p1,\nand also permit exact likelihood evaluation. Such a condition necessitates that this learned map is\na bijective function—i.e. an invertible map—and enables us to compute the likelihood using the\nchange of variable formula. While using an MLE objective is always a feasible solution to learn this\nmap, it is often not a scalable solution for both CNFs and classical NFs. Beyond architectural choices\nand differentiating through a numerical solver, learning flows using MLE is intuitively harder as\nthe process of learning must simultaneously learn the forward mapping, fθ, and the inverse mapping,\nf−1\nθ, without knowledge of pairings (x0, x1)∼π(x0, x1)from a coupling.\nTo appreciate this nuance, consider the set of invertible mappings Iand the subset of flows F ⊂ I ,\nthat solve the generative modelling problem. For instance, there may exist multiple ODEs (possibly\ninfinitely many) that push forward p0top1. It is clear then that the MLE objective allows the choice\nof multiple equivalent solutions f∈ F. However, this is precisely what complicates learning fθ,\nascertain solutions are harder to optimize since there is no prescribed coupling π(x0, x1)for noise\nx0, and data targets x1. That is to say, during MLE optimization of the flow fθ, the coupling πis\nlearned in conjunction with the flow, which can often be a significant challenge to optimize when\nthe pairing between noise and data is suboptimal.\nRegression objectives . In order to depart from the MLE objective, we may simplify the learning\nproblem by first picking a solution f∗∈ F and fixing the coupling π∗(x0, x1)induced under\nthis choice, i.e. p1= [f∗]#(p0). Given privileged access to f∗, we can form a simple regression\nobjective that approximates this in continuous time using our choice of learnable flow:\nL(θ) =Et,x0,x1,xth\n∥ft,θ(xt)−f∗\nt(xt)∥2i\n, (3)\nwhere (x0, x1)∼π∗(x0, x1)andxt∼pt(·|x0, x1)is drawn from a known conditional noising\nkernel such as a Gaussian distribution. We highlight that a key benefit unlocked by using eq. (3)\nis that we only need to evaluate the flow ft,θin the forward direction on samples drawn from the\nfixed coupling π∗. We further note that the regression objective in eq. (3) is more general than just\nflows in I, and, at optimality, the learned function behaves like f∗\nton the support of p0, under mild\nregularity conditions. We formalize this intuition more precisely in the next proposition.\nProposition 1. Suppose that f⋆\ntis invertible for all t, that (f⋆\nt)−1is continuous for all t. Then,\nasL(θ)→0, it holds that ((f⋆\nt)−1◦ft,θ)(x)→xfor almost all (with respect to p0)x.\nThe proof for proposition 1 can be found in §A, and illuminates that solving the original generative\nmodelling problem via MLE can be re-cast as a matching problem to a known invertible function\nf∗. Indeed, many existing generative models already fit into this general regression objective based\non the choice of f∗, such as conditional flow matching (CFM) [Tong et al., 2023], rectified flow [Liu\net al., 2023], and (perfect) shortcut models [Frans et al., 2024]. This proposition also shows why\nthese models work as generative models: they converge in probability to the prespecified map.\n3.1 Warmup: One-step generative models without likelihood\nAs there exists powerful one-step regression-based generative models in image applications,\nit is tempting to ascertain whether they already solve the thesis of this paper by successfully\n4\n--- Page 5 ---\n(a) Non-invertible shortcut.\n (b) Non-invertible IMM.\n (c) IMM with an NF.\n (d) Ground truth.\nFigure 1: Evaluation of IMM and shortcut models with exact likelihood on the synthetic checkerboard experiment.\nDepictions are provided of the 2D histograms after self-normalizing importance sampling is used.\nproviding faithful one-step likelihoods. As a warmup, we investigate the invertibility of current\nstate-of-the-art one-step generative models in shortcut models [Frans et al., 2024] and Inductive\nMoment Matching [Zhou et al., 2024] (see §B for details). Intuitively, both these model classes\nprogressively learn a time-discretized map of the probability path constructed under a typical\ndiffusion or CNF using bootstrap targets and other measures of self-consistency.\nSynthetic experiments . We instantiate both model classes on a simple generative modelling problem,\nwhere the data distribution corresponds to a synthetic checkerboard density psynthin2D. We choose\nthis setup as the target distribution admits analytic log-likelihoods, allowing us to compute the weights\nneeded to perform importance sampling. For example, given a trained model pθ, and the collection\nof importance weights, we aim to compute a Monte–Carlo approximation to any test function ϕ(x)\nof interest under psynthusing self-normalized importance sampling (SNIS) [Liu, 2001] as follows:\nEx∼psynth(x)[ϕ(x)] =Ex∼pθ(x)[ϕ(x) ¯w(x)]≈PK\ni=1w(xi)ϕ(xi)\nPK\ni=1w(xi). (4)\nIn addition, computing importance weights also enables resampling the pool of samples according\nto the collection of normalized importance weights W={¯w(xi)}K\ni=1.\nIn fig. 1, we plot the results of shortcut models and IMM with non-invertible networks and IMM with\nan invertible network, a Neural Spline Flow (NSF) [Durkan et al., 2019]. Given access to correct\nlikelihoods, we can correct generated samples using the self-normalized IS in eq. (4) to produce\nasymptotically exact samples that resemble the ground truth checkerboard. As observed, we find\nthat non-invertible shortcuts are imperfect at learning the target and are unable to be corrected to\npsynthafter resampling. Similarly, for the non-invertible IMM, we observe better initial samples but\nresampling—while better—still presents inaccuracies compared to the ground truth. Finally, when\nIMM is equipped with an invertible backbone, we see samples that almost perfectly match psynth.\nWhile this may initially suggest that IMM with an invertible backbone is ideal, we show that such\nan approach is difficult to learn even in simple problems at the scale of MNIST (see appendix B.2).\nThis puts spotlight on a counter-intuitive question given proposition 1: Why do shortcut models have\nincorrect likelihoods? Shortcut models have incorrect likelihoods for two reasons: ( 1) invertibility\nimplied under proposition 1 only holds at convergence, and ( 2) even if the model has converged, propo-\nsition 1 is only sufficient for accurate generation. Accurate likelihood estimation requires an invertible\nmap and regularity of higher-order gradients, because the likelihood, as given in eq. (2), requires the\ncomputation of the log determinant of the Jacobian. While proposition 1 implies pointwise conver-\ngence of fθtof∗, this does not imply convergence or regularity of the gradients of fθ, and thus short-\ncut models can still achieve high quality generations without the need to provide faithful likelihoods.\nInsufficiency of uniform convergence . While it might seem reasonable to infer that the uniform\nconvergence of fθ→f⋆on a sub-domain D⊆Rd, implies pointwise convergence of gradients\n∇fθ→ ∇f⋆, this is not generally true. For illustrative purposes, consider the following toy example:\nfm(x) =1\nmsin(mx) +xandf⋆(x) =x. Asm→ ∞ ,fmconverges uniformly to f⋆; however,\nthe gradient ∇fm(x) = cos( mx)does not converge. Importantly, this means that while fθwould\nproduce increasingly accurate generations, its likelihoods derived through eq. (2) may not converge\nto those of the base model. This counterexample demonstrates that other methods are necessary\nto ensure correct likelihoods for models with high-quality samples.\n5\n--- Page 6 ---\nAlgorithm 1 FORWARD -ONLY REGRESSION TRAINING\nInput: Prior p0, empirical samples from p1, regularization weight λr, noise scale λn, network fθ\n1:while training do\n2: (x0, x1)∼π(x0, x1) ▷Sample batches of size bi.i.d. from the dataset\n3: x1←x1+λn·ε, with ε∼ N(0, I) ▷Add scaled noise to targets\n4: L(θ)← ∥fθ(x0)−x1∥2\n2+λrR ▷Loss with regularization\n5: θ←Update (θ,∇θL(θ))\n6:return fθ\n3.2 Training normalizing flows using FORT\nWe now outline our FORT framework to train a one-step map for a classical NF. To remedy the\nissue found in shortcut models and IMM in section 3.1, we judiciously choose fθto be an already\nexactly invertible mapping—i.e., a classical NF. Since NFs are one-step maps by construction, eq. (3)\nis instantiated using a simple regression objective follows:\nL(θ) =Ex0,x1h\n∥f0,θ(x0)−f∗\n1(x0)∥2i\n+λrR=Ex0,x1h\n∥ˆx1−x1∥2i\n+λrR, (5)\nwhereRis a regularization strategy and λr∈R+is the strength of regularization. Explicit in eq. (5) is\nthe need to procure one-step targets x1=f∗\n1(x0)from a known invertible mapping f∗\n1. We outline the\nchoice of such functions in §3.3. We also highlight that the one-step targets in eq. (5) differ from the\ntypical flow matching objective where the continuous targets f∗\nt,cfm=∂\n∂pt(xt|x0, x1)(see §A.3 for a\ndetailed discussion). Furthermore, note that training an NF within FORT only requires to evaluate the\nforward direction during training and acts as the closest invertible approximation to the already invert-\nible map f∗\n1. Consequently, for NFs that are universal density approximators [Teshima et al., 2020,\nKong and Chaudhuri, 2021, Zhang et al., 2020], the learning problem includes a feasible solution.\nTraining recipe . We provide the full training pseudocode in algorithm 1. In practice, we find that\nf⋆is often ill-conditioned, with the target distribution often centered around some lower-dimensional\nsubspace of Rdsimilar to prior work [Zhai et al., 2024]. This may cause fθto become numerically\nill-conditioned. To combat this, we use three tricks to maintain numerical stability. Specifically,\nwe regularize the log determinant, add small amounts of Gaussian noise to the target distribution\nsimilar to Hui et al. [2025], Zhai et al. [2024], and, finally, we add weight decay to our optimizer.\n3.3 FORT targets\nTo construct useful one-step targets in FORT , we must find a discretization of a true invertible\nfunction—e.g., an ODE solution—at longer time horizons. More precisely, we seek a discretization\nof an ODE such that each time point t+ ∆twhere the regression objective is evaluated corresponds\nto a true invertible function f∗\nt+∆t. Consequently, if we have access to an invertible map such that\nt+ ∆t= 1, we can directly regress our parametrized function as a one-step map, f0,θ(x0) = ˆx1.\nThis motivates the search and design of other invertible mappings that give us invertibility at longer\ntime horizons, for which we give two examples next.\nOptimal transport targets . Optimal transport in continuous space between two distributions\ndefines a continuous and invertible transformation expressible as the gradient of some convex\nfunction [Villani, 2021, Peyré and Cuturi, 2019]. This allows us to consider the invertible OT plan:\nf∗\not=arg minTZ\nT(x)c(x, T(x))dp0(x)s.t.T#(p0) =p1, (6)\nwhere c:Rd×Rd→Ris the OT cost and T:Rd→Rdis a transport map. We note that this map is\ninteresting as it requires no training; however, exact OT runs in O(n3)time and O(n2)space, which\nmakes it challenging to scale to large datasets. Furthermore, we highlight that this differs from OT-\nCFM [Tong et al., 2023], which uses mini-batches to approximate the OT-plan. Nevertheless, in appli-\ncable settings, full batch OT acts as a one-time offline pre-processing step for training fθusing FORT .\nReflow targets . Another strategy to obtain samples from an invertible map is to use a pretrained\nCNF, also known as reflow [Liu, 2022]. Specifically, we have that:\nf∗\nreflow(x0) =x0+Z1\n0v⋆\nt(xt)dt=x1. (7)\n6\n--- Page 7 ---\nIn other words, the one-step invertible map is obtained from a pre-trained CNF v⋆\nt, from which we\ncollect a dataset of noise-target pairs, effectively forming π∗(x0, x1), which we use during FORT .\nWe now prove that training on reflow targets with FORT reduces the Wasserstein distance to the p1.\nProposition 2. Letpreflow be a pretrained CNF generated by the vector field v∗\nt, real numbers\n(Lt)t∈[0,1]such that v∗\ntisLt-Lipschitz for all t∈[0,1], and a NF fnf\nθtrained using Eq. 5 by\nregressing against f⋆\nreflow(x0), where x0∼ N(0, I). Then, writing pnf\nθ:= Law( fnf\nθ(x0)), we have:\nW2(p1, pθ)≤Kexp\u0012Z1\n0Ltdt\u0013\n+ϵ, K ≥Z1\n0E\u0000\u0002\n∥v∗\nt(xt)−vt,true(xt)∥2\n2\u0003\u00011\n2dt, (8)\nwhere Kis the ℓ2approximation error between the velocity field of the CNF and the ground\ntruth generating field v∗\nt,ϵ2=Ex0,x1h\n∥f⋆\nreflow(x0)−fnf\nθ(x0)∥2\n2i\n.\nThe proof for proposition 2 is provided in §A. Intuitively, the first term captures the approximation\nerror of the pretrained CNF to the actual data distribution p1, and the second term captures the\napproximation gap between the flow trained using FORT to the reflow targets obtained via preflow.\n4 Experiments\nWe evaluate NFs trained using FORT on ALDP, AL3, and AL4, for both equilibrium conformation\nsampling and free energy prediction tasks. Classical Amber force fields are used as the energy\nfunctions both to generate “Ground Truth” MD data, and for self-normalized importance sampling.\n4.1 Molecular conformation sampling\nWe first evaluate FORT on equilibrium conformation sampling tasks for molecular systems. We\ntest three different architectures across three different molecular systems of increasing length in\nalanine dipeptide to alanine tripeptide and alanine tetrapeptide, and compare the performance of\nthe same invertible architecture trained using MLE, and using FORT . We report the following\nmetrics: Effective Sample Size (ESS); the 1-Wasserstein distance on the energy distribution; and\nthe 2-Wasserstein distance on the dihedral angles used in obtaining the Ramachandran plots and\ngenerated samples in fig. 2, fig. 7, fig. 8, and §D.\nNormalizing flow architectures . In our experiments, we use the following NF architectures: the Re-\nalNVP with a residual network parametrization [Dinh et al., 2016], neural spline flows (NSF) [Durkan\net al., 2019], and our implementation of the transformer-based NF in Jet [Kolesnikov et al., 2024].\nMain results . We report our main quantitative results in table 2 and observe that FORT with reflow\ntargets consistently outperforms MLE training of NFs across all architectures on both E-W1and\nT-W2metrics, and slightly underperforms MLE training on ESS. However, this can be justified by\nthe mode collapse that happens in MLE training as illustrated in the Ramachandran plots for alanine\ndipeptide fig. 2, which artificially increases ESS. We further include energy histograms for proposal\nsamples on ALDP for each trained flow, with their corresponding re-weighted proposals using IS\nin fig. 3, and observe that NFs trained using FORT more closely match the true energy distribution.\nWe also illustrate these improvements across metrics when using OT targets over reflow in fig. 5.\nOur results clearly demonstrate that FORT is often a compelling and favourable alternative to MLE\ntraining for all analyzed NF architectures when we have access to high-quality reflow targets.\nTable 2: Quantitative results on alanine dipeptide (ALDP), tripeptide (AL3), and tetrapeptide (AL4).\nDatasets → Dipeptide (ALDP) Tripeptide (AL3) Tetrapeptide (AL4)\nAlgorithm ↓ ESS↑ E -W1↓T-W2↓ ESS↑ E -W1↓T-W2↓ ESS↑ E -W1↓T-W2↓\nNSF (MLE) 0.055 13.797 1.243 0.0237 17.596 1.665 0.016 20.886 3.885\nNSF (FORT) 0.036 0.519 0.958 0.0291 1.051 1.612 0.010 6.277 3.476\nRes–NVP (MLE) <10−4>103>30 <10−4>103>30 <10−4>103>30\nRes–NVP (FORT) 0.032 2.310 0.796 0.025 3.600 1.960 0.013 2.724 4.046\nJet (MLE) <10−4>103>30 <10−4>103>30 <10−4>103>30\nJet (FORT) 0.051 6.349 0.872 <10−4>1033.644 <10−4>103>30\n7\n--- Page 8 ---\n−π−π\n20π\n2π\nϕ−π−π\n20π\n2πψGround Truth MD\n−π−π\n20π\n2π\nϕψNSF (MLE)\n−π−π\n20π\n2π\nϕψNSF (FORT)\n−π−π\n20π\n2π\nϕψRes−NVP (FORT)\n−π−π\n20π\n2π\nϕψJet (FORT)\n4.0\n2.0\n0.0\nFree energy / kBTFigure 2: Ramachandran plots for alanine dipeptide ( left to right : ground truth molecular dynamics data; most\nperformant MLE-trained model (NSF); NSF (FORT); Res–NVP (FORT); and lastly, Jet (FORT)).\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityNSF (MLE)\nTrue data\nProposal\nProposal (reweighted)\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityNSF (FORT)\nTrue data\nProposal\nProposal (reweighted)\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityRes−NVP (FORT)\nTrue data\nProposal\nProposal (reweighted)\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityJet (FORT)\nTrue data\nProposal\nProposal (reweighted)\nFigure 3: Energy distribution of original and re-weighted samples generated with different methods on ALDP.\nTable 3: Ablations on target types and amount of reflow\ntargets on ALDP.\nDatasets → Dipeptide (ALDP)\nAlgorithm ↓ ESS↑ E -W1↓T-W2↓\nNSF (MLE) 0.055 13.80 1.243\nNSF (FORT @ 100k CNF) 0.016 17.39 1.232\nNSF (FORT @ 10.4M CNF) 0.036 0.519 0.958\nNSF (FORT @ OT) 0.003 0.604 2.019\nRes–NVP (MLE) <10−4>103>30\nRes–NVP (FORT @ 100k CNF) 0.009 46.93 1.155\nRes–NVP (FORT @ 10.4M CNF) 0.032 2.310 0.796\nRes–NVP (FORT @ OT) 0.006 0.699 1.969\nJet (MLE) <10−4>103>30\nJet (FORT @ 100k CNF) 0.017 31.42 1.081\nJet (FORT @ 10.4M CNF) 0.051 6.349 0.872\nJet (FORT @ OT) 0.003 2.534 1.913Ablations In table 3, we report FORT using\nOT targets and various amounts of generated re-\nflow targets—a unique advantage of using reflow\nas the invertible map. As observed, each target\nchoice improves over MLE, outside of ESS for\nNSF. Importantly, we find that using more sam-\nples in reflow consistently improves performance\nmetrics for all architectures. In fig. 4, we ablate\nthe impact of regularization and find performance\nimprovements with increasing regularization, up\nto a certain point. Regularizing beyond this guar-\nantees numerical invertibility, but hampers gener-\nation performance. This trade-off occurs between\n10−6≤λr≤10−5for all architectures tested.\n104105106107\nReflow Samples, Nreflow100101102103E-W1\nNSF (MLE)\nNSF (FORT)\nRes-NVP (FORT)\nJet (FORT)\n104105106107\nReflow Samples, Nreflow1.001.251.501.752.002.252.502.75-W2\nNSF (MLE)\nNSF (FORT)\nRes-NVP (FORT)\nJet (FORT)\n107\n106\n105\n104\nRegularization, λr0.800.850.900.951.001.05-W2\nNSF (FORT)\nRes-NVP (FORT)\nJet (FORT)\nFigure 4: Left: Ablations demonstrating performance improvements with an increasing number of reflow\nsamples. Right : Increasing regularization improves T-W2up to a certain point, beyond which numerical\ninvertibility is guaranteed but the regression objective, and subsequently, sample quality, is adversely impacted.\nPerformance on larger peptides . We demonstrate the learned distributions of the two pairs of\ndihedral angles that parameterize alanine tripeptide and tetrapeptide using our best MLE-trained and\nFORT flows in fig. 7. The inability to capture the modes using MLE is elucidated, where multiple\nmodes appear to blend together in both sets of dihedral angles in fig. 7. Conversely, using FORT ,\nmost modes are accurately captured and the general form of the Ramachandran plots conforms well\nto that of the true distribution obtained from MD. The findings observed with alanine tripeptide are\neven more pronounced with alanine tetrapeptide, where certain modes are entirely missed when\n8\n--- Page 9 ---\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityNSF (FORT & OT)\nTrue data\nProposal\nProposal (reweighted)\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityRes−NVP (FORT & OT)\nTrue data\nProposal\nProposal (reweighted)\n50\n 0 50 100\nE(x)0.000.020.040.060.080.100.12Normalized DensityJet (FORT & OT)\nTrue data\nProposal\nProposal (reweighted)Figure 5: Energy distribution of the original and re-weighted samples, as well as the true data, when using\n100,000 OT targets on alanine dipeptide ( left: NSF (FORT); center : Res−NVP (FORT); right : Jet (FORT)).\nMLE-trained flows are used, as seen in fig. 8. With FORT , however, most modes are accurately\ncaptured, and the density distribution is in strong agreement with the ground truth data. These\nfindings clearly demonstrate the utility of a regression-based training objective over conventional\nMLE for applications to equilibrium conformation sampling of peptides. Additional results, with\nrenders of generated molecular samples for alanine tripeptide and tetrapeptide are reported in §D.\nIn fig. 6, we demonstrate that the energy distribution of the re-weighted samples using FORT , which\nyields a more favourable energy distribution over MLE-trained flows. For the tripeptide, the results\nare in strong agreement with the MD data. For the tetrapeptide, the re-weighted samples are superior\nthan their MLE counterparts, but have room for improvement in matching the true energy distribution.\n200\n 150\n 100\n 50\n 0\nE(x)0.000.010.020.030.040.050.060.070.08Normalized DensityAL3 (MLE)\nTrue data\nProposal\nProposal (reweighted)\n200\n 150\n 100\n 50\n 0\nE(x)0.000.010.020.030.040.050.060.070.08Normalized DensityAL3 (FORT)\nTrue data\nProposal\nProposal (reweighted)\n25\n 0 25 50 75 100\nE(x)0.000.020.040.060.080.10Normalized DensityAL4 (MLE)\nTrue data\nProposal\nProposal (reweighted)\n25\n 0 25 50 75 100\nE(x)0.000.020.040.060.080.10Normalized DensityAL4 (FORT)\nTrue data\nProposal\nProposal (reweighted)\nFigure 6: Energy distribution of original and re-weighted samples generated for the most performant MLE and\nFORT models on alanine tripeptide ( leftandcenter left ) and alanine tetrapeptide ( center right andright ).\n−π−π\n20π\n2π\nϕ0ψ0Ground Truth MD\n−π−π\n20π\n2π\nϕ0MLE\n−π−π\n20π\n2π\nϕ0FORT\n0.02.04.0\nFree energy / kBT\n−π−π\n20π\n2π\nϕ1ψ1\n−π−π\n20π\n2π\nϕ1\n−π−π\n20π\n2π\nϕ1\n0.02.04.0\nFree energy / kBT\nFigure 7: Ramachandran plots for the dihedral angles in alanine tripeptide ( left: ground truth, middle : best\nMLE-trained flow, right : best FORT flow). FORT captures most modes, while MLE-trained flows struggle.\n9\n--- Page 10 ---\n−π−π\n20π\n2π\nϕ0ψ0Ground Truth MD\n−π−π\n20π\n2π\nϕ0MLE\n−π−π\n20π\n2π\nϕ0FORT\n0.02.04.0\nFree energy / kBT\n−π−π\n20π\n2π\nϕ1ψ1\n−π−π\n20π\n2π\nϕ1\n−π−π\n20π\n2π\nϕ1\n0.02.04.0\nFree energy / kBT\n−π−π\n20π\n2π\nϕ2ψ2\n−π−π\n20π\n2π\nϕ2\n−π−π\n20π\n2π\nϕ2\n0.02.04.0\nFree energy / kBTFigure 8: Ramachandran plots for the dihedral angles in alanine tetrapeptide ( left: ground truth, middle : best\nMLE-trained flow, right : best FORT flow). FORT captures most modes, while MLE-trained flows struggle.\n4.2 Targeted free energy perturbation\nAccurate calculations of the free energy difference between two metastable states of a physical system\nis both ubiquitous and of profound importance in the natural sciences. One approach to tackling\nthis problem is Free Energy Perturbation (FEP) which exploits Zwanzig’s identity: EA\u0002\ne−β∆U\u0003\n=\ne−β∆F, where ∆F=FB−FAis the Helmholtz free energy difference between two metastable states\nAandB[Zwanzig, 1954]. Targeted Free Energy Perturbation (TFEP) improves over FEP by using\nNFs to learn an invertible map using MLE to increase the distributional overlap between states Aand\nB[Wirnsberger et al., 2020]; however, this can be challenging for several reasons. NFs are difficult to\nlearn, especially when the energy function is expensive to compute, or the states occupy small areas.\nFree energy / kBT\n−π−π\n20π\n2π\nϕ−π−π\n20π\n2πψβplanar\nαR\nMD NSF Res-NVP Jet0.000.050.100.150.200.250.30Free Energy Difference / kBT\nFigure 9: Left: The βplanar andαRconformation states;\nRight : FORT’s ability to learn free energy differences.We propose a new TFEP method that does not\nrequire energy function evaluations during\ntraining. By using FORT , we can train the\nnormalizing flow solely based on samples\nfrom the states AandB. This enables TFEP,\nwhere energy evaluations may be costly—a\nnew possibility that is distinct from NFs\ntrained using MLE. To demonstrate this appli-\ncation of FORT , we train an NF solely from\nsamples from two modes of ALDP (see fig. 9)\nand use OT targets which avoid any energy\nfunction evaluation . We find we can achieve\nhigh-quality free energy estimation in com-\nparison to ground truth Molecular Dynamics (MD) using only samples during training, as illustrated\nin fig. 9. We believe this is a promising direction for future applications of free energy prediction.\n10\n--- Page 11 ---\n5 Related work\nExact likelihood generative models . NFs are generative models with invertible architec-\ntures [Rezende and Mohamed, 2015, Dinh et al., 2016] that produce exact likelihoods for any given\npoints. Common models include RealNVP [Dinh et al., 2016], neural spline flows [Durkan et al.,\n2019], and Glow [Kingma and Dhariwal, 2018]. Jet [Kolesnikov et al., 2024] and TarFlow [Zhai\net al., 2024] are examples of transformer-based normalizing flows. Aside from Jet and Tarflow, NFs\nhave generally underperformed compared to diffusion models and flow matching methods [Ho et al.,\n2020, Lipman et al., 2023, Albergo et al., 2023, Liu, 2022], partly due to the high computational\ncost of evaluating the log-determinants of Jacobians at each training step.\nFew-step generative models . To avoid costly inference, few-step generative models were introduced\nas methods to accelerate the simulation of diffusion and CNFs. Common examples include DDIM\n[Song et al., 2022] and consistency models [Song et al., 2023], which introduced a new training pro-\ncedure that ensured the model’s endpoint prediction remained consistent. Song and Dhariwal [2023],\nLu and Song [2024], Geng et al. [2024] have improved this paradigm. Other lines of work proposed\nrelated but different training objectives, generalizing consistency training [Frans et al., 2024, Zhou\net al., 2025, Kim et al., 2024, Heek et al., 2024]. Beyond diffusion and FM, residual networks [He\net al., 2015] are a class of neural networks that are invertible if the Lipschitz constant of fθis at most\none [Behrmann et al., 2019]. The log-determinant of the Jacobian is then approximated by truncating\na series of traces [Behrmann et al., 2019]—an approximation improved in Chen et al. [2020].\n6 Conclusion\nIn this work, we present FORT , a method for generating high-quality samples alongside exact\nlikelihoods in a single step. Using a base coupling between the dataset samples and the prior,\nprovided by either pre-computed optimal transport or a base CNF, we can train a classical NF using a\nsimple regression objective that avoids computing Jacobians at training time, as opposed to typical\nMLE training. In theory and practice, we have shown that the learned model produces faithful\nsamples, the likelihoods of which empirically allow us to produce state-of-the-art results on several\nmolecular datasets, using importance-sampling resampling. Limitations include the quality of the\nproposal samples, which substantially improve on MLE-trained NFs, but are not on par with state-of-\nthe-art CNFs or variants thereof. Moreover, while producing accurate and high-quality likelihoods,\nthey do not, in theory, match those of the base coupling, which can be a desirable property.\nAcknowledgements\nDR received financial support from the Natural Sciences and Engineering Research Council’s\n(NSERC) Banting Postdoctoral Fellowship under Funding Reference No. 198506. OD is supported\nby both Project CETI and Intel. AJB is partially supported by an NSERC Postdoctoral Fellowship\nand by the EPSRC Turing AI World-Leading Research Fellowship No. EP/X040062/1 and EPSRC\nAI Hub No. EP/Y028872/1. JT acknowledges funding from the Canada CIFAR AI Chair Program\nand the Intel-Mila partnership program. The authors acknowledge funding from UNIQUE, CIFAR,\nNSERC, Intel, and Samsung. The research was enabled in part by computational resources provided\nby the Digital Research Alliance of Canada ( https://alliancecan.ca ), Mila ( https://mila.\nquebec ), and NVIDIA.\n11\n--- Page 12 ---\nReferences\nM. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants.\nInternational Conference on Learning Representations (ICLR) , 2023.\nM. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework\nfor flows and diffusions. arXiv preprint 2303.08797 , 2023.\nJ. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J.-H. Jacobsen. Invertible residual\nnetworks, 2019. URL https://arxiv.org/abs/1811.00995 .\nJ. Benton, G. Deligiannidis, and A. Doucet. Error bounds for flow matching methods. arXiv preprint\narXiv:2305.16860 , 2023.\nJ. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y . Guo,\net al. Improving image generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf , 2(3):8, 2023.\nA. J. Bose, M. Brubaker, and I. Kobyzev. Equivariant finite normalizing flows. arXiv preprint\narXiv:2110.08649 , 2021.\nT. Brooks, B. Peebles, C. Holmes, W. DePue, Y . Guo, L. Jing, D. Schnurr, J. Tay-\nlor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video gener-\nation models as world simulators. 2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators .\nR. T. Q. Chen, Y . Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations.\nNeural Information Processing Systems (NeurIPS) , 2018.\nR. T. Q. Chen, J. Behrmann, D. Duvenaud, and J.-H. Jacobsen. Residual flows for invertible generative\nmodeling, 2020. URL https://arxiv.org/abs/1906.02735 .\nL. Dinh, D. Krueger, and Y . Bengio. Nice: Non-linear independent components estimation. arXiv\npreprint arXiv:1410.8516 , 2014.\nL. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint\narXiv:1605.08803 , 2016.\nC. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural spline flows. Advances in neural\ninformation processing systems , 32, 2019.\nC. Etmann, R. Ke, and C.-B. Schönlieb. iunets: Fully invertible u-nets with learnable up- and\ndownsampling, 2020. URL https://arxiv.org/abs/2005.05220 .\nK. Frans, D. Hafner, S. Levine, and P. Abbeel. One step diffusion via shortcut models. arXiv preprint\narXiv:2410.12557 , 2024.\nT. Geffner, K. Didi, Z. Zhang, D. Reidenbach, Z. Cao, J. Yim, M. Geiger, C. Dallago, E. Kucukbenli,\nA. Vahdat, et al. Proteina: Scaling flow-based protein structure generative models. arXiv preprint\narXiv:2503.00710 , 2025.\nZ. Geng, A. Pokle, W. Luo, J. Lin, and J. Z. Kolter. Consistency models made easy, 2024. URL\nhttps://arxiv.org/abs/2406.14548 .\nD. Ghamari, P. Hauke, R. Covino, and P. Faccioli. Sampling rare conformational transitions with a\nquantum computer. Scientific Reports , 12(1):16336, 2022.\nE. Hairer, S. P. Nørsett, and G. Wanner. Solving Ordinary Differential Equations I: Nonstiff Problems ,\nvolume 8 of Springer Series in Computational Mathematics . Springer-Verlag, 2nd edition, 1993.\nISBN 978-3-540-56670-0.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015. URL\nhttps://arxiv.org/abs/1512.03385 .\nJ. Heek, E. Hoogeboom, and T. Salimans. Multistep consistency models, 2024. URL https:\n//arxiv.org/abs/2403.06807 .\n12\n--- Page 13 ---\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models, 2020. URL https:\n//arxiv.org/abs/2006.11239 .\nG. Huguet, J. Vuckovic, K. Fatras, E. Thibodeau-Laufer, P. Lemos, R. Islam, C.-H. Liu, J. Rector-\nBrooks, T. Akhound-Sadegh, M. Bronstein, et al. Sequence-augmented se (3)-flow matching for\nconditional protein backbone generation. arXiv preprint arXiv:2405.20313 , 2024.\nK.-H. Hui, C. Liu, X. Zeng, C.-W. Fu, and A. Vahdat. Not-so-optimal transport flows for 3d point\ncloud generation. In ICLR , 2025.\nM. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing\nsplines. Communications in Statistics-Simulation and Computation , 18(3):1059–1076, 1989.\nI. Ishikawa, T. Teshima, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama. Universal approximation\nproperty of invertible neural networks. Journal of Machine Learning Research , 24(287):1–68,\n2023.\nD. Kim, C.-H. Lai, W.-H. Liao, N. Murata, Y . Takida, T. Uesaka, Y . He, Y . Mitsufuji, and S. Ermon.\nConsistency trajectory models: Learning probability flow ode trajectory of diffusion, 2024. URL\nhttps://arxiv.org/abs/2310.02279 .\nD. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions, 2018. URL\nhttps://arxiv.org/abs/1807.03039 .\nA. Kolesnikov, A. S. Pinto, and M. Tschannen. Jet: A modern transformer-based normalizing flow.\narXiv preprint arXiv:2412.15129 , 2024.\nZ. Kong and K. Chaudhuri. Universal approximation of residual flows in maximum mean discrepancy.\narXiv preprint arXiv:2103.05793 , 2021.\nY . Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative\nmodeling. International Conference on Learning Representations (ICLR) , 2023.\nJ. S. Liu. Monte Carlo Strategies in Scientific Computing . Springer, 2001.\nQ. Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint\narXiv:2209.14577 , 2022.\nX. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with\nrectified flow. International Conference on Learning Representations (ICLR) , 2023.\nC. Lu and Y . Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv\npreprint arXiv:2410.11081 , 2024.\nF. Noé, S. Olsson, J. Köhler, and H. Wu. Boltzmann generators: Sampling equilibrium states of\nmany-body systems with deep learning. Science , 365(6457):eaaw1147, 2019.\nG. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing\nflows for probabilistic modeling and inference. Journal of Machine Learning Research , 22(57):\n1–64, 2021.\nS. Peluchetti. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589 , 2023.\nG. Peyré and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine\nLearning , 11(5-6):355–607, 2019.\nD. Rezende and S. Mohamed. Variational inference with normalizing flows. In International\nconference on machine learning , pages 1530–1538. PMLR, 2015.\nA. Sauer, D. Lorenz, A. Blattmann, and R. Rombach. Adversarial diffusion distillation. In European\nConference on Computer Vision , pages 87–103. Springer, 2024.\nJ. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.\norg/abs/2010.02502 .\n13\n--- Page 14 ---\nY . Song and P. Dhariwal. Improved techniques for training consistency models, 2023. URL\nhttps://arxiv.org/abs/2310.14189 .\nY . Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency models, 2023. URL https://arxiv.\norg/abs/2303.01469 .\nC. B. Tan, A. J. Bose, C. Lin, L. Klein, M. M. Bronstein, and A. Tong. Scalable equilibrium sampling\nwith sequential boltzmann generators. arXiv preprint arXiv:2502.18462 , 2025.\nT. Teshima, I. Ishikawa, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama. Coupling-based invertible\nneural networks are universal diffeomorphism approximators. Advances in Neural Information\nProcessing Systems , 33:3362–3373, 2020.\nA. Tong, N. Malkin, G. Huguet, Y . Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y . Bengio.\nImproving and generalizing flow-based generative models with minibatch optimal transport. arXiv\npreprint arXiv:2302.00482 , 2023.\nC. Villani. Topics in optimal transportation , volume 58. American Mathematical Soc., 2021.\nP. Wirnsberger, A. J. Ballard, G. Papamakarios, S. Abercrombie, S. Racanière, A. Pritzel,\nD. Jimenez Rezende, and C. Blundell. Targeted free energy estimation via learned mappings. The\nJournal of Chemical Physics , 153(14), 2020.\nT. Yin, M. Gharbi, R. Zhang, E. Shechtman, F. Durand, W. T. Freeman, and T. Park. One-step\ndiffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 6613–6623, 2024.\nS. Zhai, R. Zhang, P. Nakkiran, D. Berthelot, J. Gu, H. Zheng, T. Chen, M. A. Bautista, N. Jaitly, and\nJ. Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329 ,\n2024.\nH. Zhang, X. Gao, J. Unterman, and T. Arodz. Approximation capabilities of neural odes and\ninvertible residual networks. In International Conference on Machine Learning , pages 11086–\n11095. PMLR, 2020.\nL. Zhou, S. Ermon, and J. Song. Inductive moment matching. arXiv preprint arXiv:2503.07565 ,\n2025.\nM. Zhou, H. Zheng, Z. Wang, M. Yin, and H. Huang. Score identity distillation: Exponentially fast\ndistillation of pretrained diffusion models for one-step generation. In Forty-first International\nConference on Machine Learning , 2024.\nR. W. Zwanzig. High-temperature equation of state by a perturbation method. i. nonpolar gases.\nThe Journal of Chemical Physics , 22(8):1420–1426, 08 1954. ISSN 0021-9606. doi: 10.1063/1.\n1740409. URL https://doi.org/10.1063/1.1740409 .\n14\n--- Page 15 ---\nA Proofs\nA.1 Proof of proposition 1\nWe first recall proposition 1 below.\nProposition 1. Suppose that f⋆\ntis invertible for all t, that (f⋆\nt)−1is continuous for all t. Then,\nasL(θ)→0, it holds that ((f⋆\nt)−1◦ft,θ)(x)→xfor almost all (with respect to p0)x.\nTo prove proposition 1, we first prove the following lemma, which is essentially the same as the\nproposition, but it abstracts out the distribution of xt, which depends on x0,x1, and t.\nLemma 1. For functions (fn)n≥1andg, where gis invertible and has a continuous inverse,\nx0∼p0, ifMSE( fn, g):=Ex0∥fn(x0)−g(x0)∥2\n2→0, then limn→∞g−1(fn(x)) = xfor\nalmost all (with respect to p0)x.\nProof. LetYn=∥fn(x0)−g(x0)∥2. We know that limn→∞E[Y2\nn] = 0 (as it corresponds to the\nMSE), which implies that limn→∞Var(Yn) = 0 . Consequently, Yn− →cfor some constant c∈R.\nMoreover, by Jensen’s inequality and the convexity of x7→x2, we find that (E[Yn])2≤E[Y2\nn],\nmeaning that c= 0. This implies that limn→∞∥fn(x)−g(x)∥2\n2= 0almost everywhere, and thus\nthatlimn→∞fn(x) =g(x). Finally, since g−1is continuous, we can apply the function to both sides\nof the limit to find that limn→∞g−1(fn(x)) =x, almost everywhere.\nIt suffices to apply the above lemma to xt∼pt(· |x0, x1)p1(x1|x0)p0(x0).\nA.2 Proof of proposition 2\nWe now prove proposition 2. The proposition reuses the following regularity assumptions, as\nintroduced in Benton et al. [2023], which we recall verbatim below for convenience:\n(Assumption 1) Letvtruebe the true generating velocity field for the CNF with field v∗trained using\nflow matching. Then the true and learned velocity v∗are close in ℓ2and satisfy:R1\n0Et,xt[∥vt,true(xt)−v∗\nt(xt)∥2]dt≤K2.\n(Assumption 2) For each x∈Rdands∈[0,1], there exists unique flows (f∗\ns,t)t∈[s,1]and\n(f(s,t),true)t∈[s,1], starting at f∗\n(s,s)=xandf(s,s),true=xwith velocity fields\nv∗\nt(xt)andvt,true(xt), respectively. Additionally, f∗andftrueare continuously\ndifferentiable in x, sandt.\n(Assumption 3) The velocity field v∗\nt(xt)is differentiable in both xandt, and also for each t∈[0,1]\nthere exists a constant Ltsuch that v∗\nt(xt)isLt-Lipschitz in x.\nProposition 2. Letpreflow be a pretrained CNF generated by the vector field v∗\nt, real numbers\n(Lt)t∈[0,1]such that v∗\ntisLt-Lipschitz for all t∈[0,1], and a NF fnf\nθtrained using Eq. 5 by\nregressing against f⋆\nreflow(x0), where x0∼ N(0, I). Then, writing pnf\nθ:= Law( fnf\nθ(x0)), we have:\nW2(p1, pθ)≤Kexp\u0012Z1\n0Ltdt\u0013\n+ϵ, K ≥Z1\n0E\u0000\u0002\n∥v∗\nt(xt)−vt,true(xt)∥2\n2\u0003\u00011\n2dt, (8)\nwhere Kis the ℓ2approximation error between the velocity field of the CNF and the ground\ntruth generating field v∗\nt,ϵ2=Ex0,x1h\n∥f⋆\nreflow(x0)−fnf\nθ(x0)∥2\n2i\n.\nProof. We begin by first applying the triangle inequality to W2(p1, pθ)and obtain:\nW2(p1, pθ)≤ W 2(p1, preflow) +W2(preflow, pnf\nθ). (9)\nThe first term is an error in Wasserstein-2 distance between the true data distribution and our reflow\ntargets, which is still a CNF. A straightforward application of Theorem 1 in Benton et al. [2023]\n15\n--- Page 16 ---\ngives a bound on this first Wasserstein-2 distance3:\nW2(p1, preflow)≤Kexp\u0012Z1\n0Ltdt\u0013\n. (10)\nTo bound W2(preflow, pθ), recall that the following inequality holds W2(Law(X),Law(Y))≤\nE\u0002\n∥X−Y∥2\n2\u00031\n2, for any two random variables XandY. In our case, these random variables are\np∗\nreflow=Law(f∗\nreflow(x0))andpnf\nθ=Law(fnf\nθ(x0)). This gives:\nW2(preflow, pnf\nθ)≤Ex0,x1h\r\rf∗\nreflow(x0)−fnf\nθ(x0)\r\r2\n2i1\n2. (11)\nCombining eq. (10) and eq. (11) achieves the desired result and completes the proof.\nW2(p1, pθ)≤Kexp\u0012Z1\n0Ltdt\u0013\n+Ex0,x1h\r\rf∗\nreflow(x0)−fnf\nθ(x0)\r\r2\n2i1\n2. (12)\nNote that the bound on W2(preflow, pnf\nθ)is effectively the square-root of the FORT objective and thus\noptimization of the NF using this loss directly minimizes the upper bound to W2(p1, pnf\nθ).\nA.3 FORT in continuous time\nCurrent state-of-the-art CNFs are trained using “flow matching” [Lipman et al., 2023, Albergo and\nVanden-Eijnden, 2023, Liu et al., 2023], which attempts to match the vector field associated with the\nflow to a target vector field that solves for mass transportation everywhere in space and time. Specifi-\ncally, we can cast conditional flow matching (CFM) [Tong et al., 2023] from the perspective of FORT .\nTo see this explicitly, consider a pre-specified probability path, pt(xt), and the following f∗\nt,fm=\n∂\n∂tpt(xt). However, since it is generally computationally challenging to sample from ptdirectly, the\nmarginalization trick is used to derive an equivalent objective with a conditional f∗\nt,cfm. We note that\nFORT requires f∗\nt,cfmto be invertible therefore this assumes regularity on∂\n∂tpt(xt). This is generally\nsatisfied by adding a small amount of noise to the following. We present this simplified form for clarity.\npt(xt) :=Z\npt(xt|x0, x1)dπ(x0, x1), p t(xt|x0, x1) =δ(xt; (1−t)x0+tx1). (13)\nThen setting f∗\nt,cfm=∂\n∂tpt(xt|x0, x1)it is easy to show that:\nL(θ) =Et,x0,x1,xt\"\r\r\r\rvt,θ(xt)−∂\n∂tpt(xt|x0, x1)\r\r\r\r2#\n=Et,xt\"\r\r\r\rvt,θ(xt)−∂\n∂tpt(xt)\r\r\r\r2#\n+C,\n=Et,x0,x1,xth\nλt\r\rft,θ(xt)−f∗\nt,cfm(xt)\r\r2i\n,\nwithCindependent of θ[Lipman et al., 2023], and λtis a loss weighting, which fits within the FORT\nframework in the continuous-time setting with the last equality known as target/end-point prediction.\nB Additional Background\nB.1 Inductive Moment Matching\nIntroduced in Zhou et al. [2025], Inductive Moment Matching (IMM) defines a training procedure for\none-step generative models, based on diffusion/flow matching. Specifically, IMM trains models to\nminimize the difference in distribution between different points in time induced by the model. As a\nresult, this avoids direct optimization for the predicted endpoint, in contrast to conventional diffusion.\n3A sharper bound can be obtained with additional assumptions, as demonstrated in Benton et al. [2023], but\nit is not critically important in our context.\n16\n--- Page 17 ---\n(a) 1-step.\n (b) 2-step.\n (c) 4-step.\n (d) 8-step.\n (e) 16-step.\nFigure 10: Generations of IMM trained with an iUNet with a variable number of steps.\n(a) Using the ResFlow architecture\nproposed in Chen et al. [2020].\n(b) Using the TarFlow architec-\nture [Zhai et al., 2024], m= 4.\n(c) Using the TarFlow architec-\nture [Zhai et al., 2024], m= 16 .\nFigure 11: One-step generation results with a Lipschitz-constrained (ResFlow) model and an invertible model\n(TarFlow) for IMM. The mparameter is the group size in IMM used to approximate the MMD.\nMore precisely, let fθ:Rd×[0,1]2→Rd,(x, s, t )7→fθ(x, s, t )be a function parameterized by θ.\nIMM minimizes the following maximum mean discrepancy (MMD) loss:\nL(θn) =Es,t,x 0,x1\u0002\nw(s, t)MMD2\u0000\npθn−1,(s|r)(xs), pθn,(s|t)(xs)\u0001\u0003\n, (14)\nwhere 0≤r≤r(s, t) :=r≤s≤1, with s, t∼ U(0,1)iid,w≥0is a weighting function, x1is a\nsample from the target distribution, x0∼ N(0, I),xsis some interpolation between x0andx1at\ntimes(typically, using the DDIM interpolation [Song et al., 2022]), the subscript n∈Nof parameter\nθrefers to its training step, and MMD is some MMD function based on a chosen kernel (typically,\nLaplace).4Essentially, the method uses as a target the learned distribution of the previous step at a\nhigher time to train the current distribution at lower times. With a skip parameterization, the higher\ntime distribution is by construction close to the true solution, as pθ(xs|xr)≈p(xs|xr)when\nr≈s, and xsis known. (Or, in other terms, fθ(x, s, r ≈s)≈xwith the skip parameterization.)\nWhen the distributions match (when the loss is zero), MMD2(p1,θ, p1) = 0 , and so the generative\nmodel’s and the target distribution’s respective moments all match.\nThis training procedure allows for variable-step sampling. For chosen timesteps, (ti)n\ni=1, one can\nsample from p1,θby sampling x0∼ N(0, I)and performing the steps:\nxti+1←DDIM( fθ(xti, ti+1, ti), xti, ti, ti+1), (15)\nwhere DDIM is the DDIM interpolant.\nB.2 Inductive Moment Matching negative results\nWe detail in appendix B.1 the Inductive Moment Matching (IMM) framework [Zhou et al., 2025].\nObserving the sampling procedure, which we give in eq. (15), one can make this procedure invertible\nby constraining the Lipschitz constant of the model, or by using an invertible model. For the first\n4Note that we have adapted IMM’s notation to our time notation, with noise at time zero, and clean data at\ntime one.\n17\n--- Page 18 ---\ncase, if we use the “Euler” (skip) parameterization alongside the DDIM interpolation, it is shown that\nthe reparameterized model gθcan be written as:\n∀x, s, t, g θ(x, s, t ) =x−(s−t)fθ(x, s, t ). (16)\nMoreover, 0≤s−t≤1, and so if the Lipschitz constant of fθis strictly less than one, then the\noverall model is invertible, using the argument of residual flows [Behrmann et al., 2019]; so the\nchange of variables formula applies as follows (using the time notation of IMM/diffusion):\nlogpθ\n1(x) = log p0(x0)−X\nilog\u0002\n(ti+1−ti) det( Jfθ(·,ti+1,ti)(xti))\u0003\n, (17)\nThe difficulty of evaluating the log-determinant of the Jacobian remains. Note, however, that we do\nnot need to find the inverse of the function to evaluate the likelihood of generated samples, since we\nknow each (xti)i. The second path (of using an invertible model) is viable only for one-step sampling\nwith no skip parameterization (which, according to Zhou et al. [2025], tends to under-perform,\nempirically), since the sampling procedure then boils down to x1=f(x0,1,0)forx0∼ N(0, I).\nWhile both approaches succeeded in synthetic experiments, they fail to scale to datasets such as\nMNIST, the results of which we include here in fig. 10 and in fig. 11. We have tried iUNet [Etmann\net al., 2020] and TarFlow [Zhai et al., 2024], an invertible UNet and a Transformer-based normalizing\nflow, respectively, for invertible one-step models; and we have tried the ResFlow architecture\nin [Chen et al., 2020] for the Lipschitz-constrained approach. As observed, TarFlow fails to produce\nimages of high quality; iUNets produced significantly better results, albeit still not sufficient,\nespecially for the one-step sampling, which is the only configuration that guarantees invertibility; the\nLipschitz-constrained ResFlow entirely failed to produce satisfactory results, although the loss did\ndiminish during training. In general, an even more important limitation is the difficulty of designing\ninvertible or Lipschitz-constrained models for other data types, for instance, 3D coordinates. Perhaps\nfurther research on the architectural side could allow for higher performance with invertible sampling.\nC Experimental Details\nC.1 Metrics\nThe performance metrics considered across the investigated flows were the effective sample size, ESS,\nWasserstein-1 energy distance, E-W1, and the Wasserstein-2 distance on dihedral angles, T-W2.\nEffective Sample Size (ESS) . We compute the effective sample size (ESS) using Kish’s formula,\nnormalized by the number of samples generated:\nESS\u0000\n{wi}N\ni=1\u0001\n=1\nN\u0010PN\ni=1wi\u00112\nPN\ni=1w2\ni. (18)\nwhere wiis the unnormalized weight of each particle indexed by ioverNparticles. Effective sample\nsize measures the variance of the weights and approximately how many more samples would be\nneeded compared to an unbiased sample. For us, this captures the local quality of the proposal relative\nto the ground truth energy. It does not rely on a ground truth test set; however, it is quite sensitive\nand may be misleading in the case of dropped modes or incomplete coverage, as it only measures\nagreement on the support of the generated distribution.\nWasserstein-1 Energy Distance ( E-W1). The Wasserstein-1 energy distance measures how well\nthe generated distribution matches some ground truth sample (often generated using MD data) by\ncalculating the Wasserstein-1 distance between the energy histograms. Specifically:\nE-W1(x, y) = min\nπZ\nx,y|x−y|dπ(x, y), (19)\nwhere πis a valid coupling of p(x)andp(y). For discrete distributions of equal size, πcan be thought\nof as a permutation matrix. This measures the model’s ability to generate very accurate structures\nas the energy function we use requires extremely accurate bond lengths to obtain reasonable energy\nvalues. When the bond lengths have minor inaccuracies, the energy can blow up extremely quickly.\n18\n--- Page 19 ---\nTorus Wasserstein ( T-W2). The torus Wasserstein distance measures the Wasserstein-2 dis-\ntance on the torus defined by the main torsion angles of the peptide. That is for a peptide of\nlength l, there are 2(l−1)torsion angles defining the dihedrals along the backbone of interest\n((ϕ1, ψ1),(ϕ2, ψ2), . . .(ϕl, ψl)). We define the torus Wasserstein distance over these backbone\nangles as:\nT-W2(p, q)2= min\nπZ\nx,ycT(x, y)2dπ(x, y), (20)\nwhere πis a valid coupling between pandq, and cT(x, y)2is the shortest distance on the torus\ndefined by the dihedral angles:\ncT(x, y)2=2(L−1)X\ni=0[(Dihedrals( x)i−Dihedrals( y)i+π) mod 2 π−π]2. (21)\nThe torus Wasserstein distance measures large scale changes and is quite important for understanding\nmode coverage and overall macro distribution. We find FORT does quite well in this regard.\nC.2 Additional details on experimental setup\nTo accurately compute the previously defined metrics, 250k proposal samples were drawn and\nre-weighted for alanine dipeptide, tripeptide, and tetrapeptide.\nData normalization . We adopt the same data normalization strategy proposed in [Tan et al., 2025],\nin which the center of mass of each atom is first subtracted from the data, followed by scaling using\nthe standard deviation of the training set.\nExponential moving average . We apply an exponential moving average (EMA) on the weights of all\nmodels, with a decay of 0.999, as commonly done in flow-based approaches to improve performance.\nTraining details and hardware . All models were trained on\nNVIDIA L40S 48GB GPUs for 5000 epochs, except those using\nOT targets, which were trained for 2000 epochs. Convergence was\nnoted earlier in the OT experiments, leading to early stopping. The\ntotal training time for all models is summarized in table 4. The time\ntaken to compute the OT map is also provided; since computing the\nOT map is independent of the feature dimension, but only on the\nnumber of data points used, the compute time was relatively consis-\ntent across all datasets. A total of 100k points was used for training\nthe CNF, performing MLE training, and computing the OT map.Table 4: FORT training time (in\nhours) on ALDP, AL3, and AL4.\nModel ALDP AL3 AL4\nOT map 3.6 3.8 3.8\nDiT CNF 27.6 40.7 48.6\nNSF 21.0 23.8 26.8\nRes–NVP 15.7 15.6 15.0\nJet 19.1 19.2 20.1\nReflow targets . Ablations were done to investigate the influence of synthetic data quantity on all\nmetrics. For all benchmarking performed against MLE training, the largest amount of synthetic data\nwas used. For ALDP, AL3, and AL4, this constituted 10.4M, 10.4M, and 10M samples, respectively.\nDeterminant regularization . During FORT , it was initially ob-\nserved that as proposal sample quality improved, the re-weighted sam-\nples progressively deteriorated across all metrics due to the models\nbecoming numerically non-invertible. This was partially addressed\nby adding regularization to the loss in the form of a log determi-\nnant penalty. Sweeps were conducted using multiple regularization\nweights ranging between 10−7and10−4to prevent hampering sam-\nple performance. The amount of regularization added was a function\nof the flow and dataset. The final weights are summarized in table 5.Table 5: Regularization weights\nused across datasets and flows.\nModel ALDP AL3 AL4\nNSF 10−610−510−5\nRes–NVP 10−510−510−6\nJet 10−510−610−5\nTarget noise . To discourage numerical non-invertibility of the trained flows, Guassian noise was\nalso introduced to the target samples. Experiments were conducted with noise magnitudes of 0.01,\n0.05, 0.1, and 0.25, with a final value of 0.05 being selected for use across models and datasets.\nFORT implementation details . A summary of all trained model configurations is provided\nin table 6. To maintain a fair comparison, the configurations reported below were unchanged for\nMLE training and FORT . Adam was used as the optimizer with a learning rate of 5×10−4and\na weight decay of 0.01. We also included a varying cosine schedule with warmup in line with the\napproach suggested in [Tan et al., 2025].\n19\n--- Page 20 ---\nTable 6: Model configurations for the DiT CNF, NSF, Res–NVP, and Jet across all datasets (ALDP, AL3, AL4).\nA dash (–) indicates the parameter is not applicable to the respective model.\nModel hidden features transforms layers blocks per layer conditioning dim. heads dropout # parameters (M)\nDiT CNF 768 – 6 – 128 12 0.1 46.3\nNSF 256 24 – 5 – – – 76.8\nRes–NVP 512 – 8 6 – – 0.1 80.6\nJet 432 – 4 12 128 12 0.1 77.6\nQuality of CNF targets . To maximize the likelihood that models trained with FORT have the\npotential to outperform MLE, securing high-quality targets is essential. In line with this pursuit,\na CNF with a diffusion transformer backbone was used. In fig. 12, the true data and the CNF\nproposal are shown, where it can be seen that the learned energy distributions across all three\npeptides are nearly perfect. Re-weighted samples are not included as obtaining likelihoods from\nthe CNF requires estimating the trace of the divergence, which is often an expensive operation with\na large time and memory cost. Although many unbiased approaches for approximating the likelihood\nexist [Hutchinson, 1989], these methods are typically unusable for Boltzmann generators due to\ntheir variance, which can introduce bias into the weights needed for importance sampling.\n50\n 0 50 100\nE(x)0.000.020.040.060.08Normalized DensityDiT CNF @ ALDP\nTrue data\nProposal\n200\n 150\n 100\n 50\n 0\nE(x)0.000.010.020.030.040.050.06Normalized DensityDiT CNF @ AL3\nTrue data\nProposal\n0 50 100\nE(x)0.000.010.020.030.040.05Normalized DensityDiT CNF @ AL4\nTrue data\nProposal\nFigure 12: True energy distribution and learned proposal using the DiT-based CNF.∗The re-weighted proposal\nis not present because it was too computationally expensive to compute for a sufficient number of points.\nD Additional Results\nD.1 Generated samples of peptide conformations\nSamples of generated peptides . Below we provide sample conformations of alanine dipeptide,\nalanine tripeptide, and alanine tetrapeptide generated using both MLE training and FORT.\nFigure 13: Generated conformations of alanine dipeptide across various flow-based methods ( left: NSF w/ MLE;\ncenter left : NSF w/ FORT; center right : Res–NVP w/ FORT; right : Jet w/ FORT.\nFigure 14: Generated samples of various peptides using NSF w/ FORT (left: ALDP; center : AL3; right : AL4).\n20\n--- Page 21 ---\nD.2 Targeted free energy perturbation\nGenerating regression targets . Using the available MD data, two conformations of alanine dipeptide\nwere selected: βplanar andαR[Ghamari et al., 2022]. The (ϕ, ψ)ranges for the βplanar conformation\nwere chosen as (−2.5,−2.2)and(2.3,2.6), and for the αRconformation as (−1.45,−1.2)and\n(−0.7,−0.4), respectively. The dataset was then truncated to 82,024 source-target conformation\npairs, which were used to compute the OT pairing and generate an invertible map. These pairs were\nsubsequently trained using FORT , with the same model configurations and settings outlined in table 6.\n21",
  "text_length": 69273
}