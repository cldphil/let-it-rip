{
  "id": "http://arxiv.org/abs/2505.24826v1",
  "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text",
  "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.",
  "authors": [
    "Li yunhan",
    "Wu gengshen"
  ],
  "published": "2025-05-30T17:30:18Z",
  "updated": "2025-05-30T17:30:18Z",
  "categories": [
    "cs.CL",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24826v1"
}