{
  "id": "http://arxiv.org/abs/2505.24826v1",
  "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated\n  Legal Text",
  "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.",
  "authors": [
    "Li yunhan",
    "Wu gengshen"
  ],
  "published": "2025-05-30T17:30:18Z",
  "updated": "2025-05-30T17:30:18Z",
  "categories": [
    "cs.CL",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24826v1",
  "full_text": "arXiv:2505.24826v1 [cs.CL] 30 May 2025LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text Li Yunhan1,2, Wu Gengshen1 1City University of Macau 2Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences Correspondence: gswu@cityu.edu.mo Abstract As large language models (LLMs) are increas- ingly used in legal applications, current evalu- ation benchmarks tend to focus mainly on fac- tual accuracy while largely neglecting impor- tant linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and ter- minology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework. Our analysis identifies three key findings: First, model quality levels off at 14 billion param- eters, with only a marginal improvement of 2.7% noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as in- dicated by statistical significance thresholds above 0.016. Third, reasoning models consis- tently outperform base architectures. A signifi- cant outcome of our research is the release of a ranking list and Pareto analysis, which high- light the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation pro- tocols for legal LLMs but also uncovers fun- damental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/ LegalEval-Q. 1 Introduction The rapid advancement of LLMs has significantly transformed natural language processing, with their widespread deployments across specialized do- mains such as finance (Lu et al., 2023; Xie et al., 2024; Zhang et al., 2023), healthcare (Dai et al., 2023; Fei et al., 2023; Guha et al., 2023), and legal practice (Dada et al., 2024; Wang et al., 2023a; Li et al., 2023). Current evaluation benchmarks primarily assess three key capabilities: domain-specific knowledgemastery (Lu et al., 2023; Dai et al., 2023; Dada et al., 2024), extended context processing (Kam- radt, 2023; Zhao et al., 2024), and reasoning ca- pability (Hendrycks et al., 2021). However, these benchmarks systematically neglect an essential di- mension—textual quality. This oversight leads to a critical paradox: LLMs may generate factually correct outputs that nevertheless exhibit suboptimal linguistic characteristics including awkward phras- ing, stylistic inconsistencies, or impaired readabil- ity. Establishing robust textual quality assessment methodologies therefore represents a pressing need for both academic research and industrial applica- tions, particularly in specialized domains. In the legal domain, two unresolved challenges persist. First, existing frameworks lack standard- ized quantitative metrics for evaluating the nuanced aspects of legal text quality. Second, the relation- ship between model parameters—such as scale and architectural configurations—and resultant textual quality remains poorly understood. These gaps hin- der model selection and optimization processes in legal applications. Our study presents three significant contribu- tions: 1) We have developed a specialized bench- mark for evaluating the quality of legal texts. This benchmark introduces a multidimensional assess- ment model that measures critical quality attributes, including clarity, coherence, and terminological precision. 2) We have constructed a comprehensive validation set that includes legal questions from various subdomains. This set is designed to system- atically assess the quality of responses generated by models across different legal contexts. 3) Through rigorous empirical analysis, we demonstrate how key characteristics of models—particularly their size and fine-tuning strategies—affect the quality of the text outputs. Our framework not only provides practitioners with actionable insights for selecting LLMs in legal applications but also establishes stan- dardized evaluation protocols for future research in 1 domain-specific text quality assessment. 2 Related Work 2.1 Traditional Natural Language Generation Evaluation With the proliferation of LLMs across diverse do- mains, establishing rigorous evaluation method- ologies has become critical. Current benchmarks predominantly focus on task-oriented or reasoning- based assessments through verifiable instructions. The IFEval framework (Zhou et al., 2023) exem- plifies this approach by quantifying instruction- following accuracy via objective metrics. For com- plex reasoning, the BIG-Bench Hard suite (Suz- gun et al., 2023) evaluates multi-step problem- solving, while the MATH dataset (Hendrycks et al., 2021) provides a standardized testbed for mathe- matical deduction. For domain-specific challenges, benchmarks adopt specialized designs: GPTA (Rein et al., 2024), which curates expert-level questions in STEM fields, and MuSR (Sprague et al., 2024)targets soft reasoning in narrative contexts. Knowledge-intensive evaluations have evolved from MMLU (Hendrycks et al., 2020) to its advanced variant MMLU-Pro (Wang et al., 2024), incorporating adversarial noise to better probe rea- soning robustness. For non-English contexts, C- Eval (Huang et al., 2023) simultaneously measures Chinese linguistic competence and cross-domain knowledge application. While these methods excel at quantifying task completion, such as solution accuracy and instruction adherence, structured rea- soning, they reveal fundamental limitations when applied to open-domain text quality assessment. 2.2 Early Exploration of Text Quality Assessment Current evaluation of generated text remains pre- dominantly reliant on similarity-based metrics (Ka- sai et al., 2021), which can be categorized into two paradigms: (1) lexical overlap-based methods such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) that compute n-gram matches between gen- erated and reference texts, and (2) contextualized embedding-based approaches (Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019) that measure semantic similarity using deep language models. Despite their prevalent use in more than 60% of recent natural language generation(NLG) studies (Kasai et al., 2021), these metrics exhibit fundamen- tal limitations. First, they fail to capture essentialquality dimensions including content coherence (Reiter and Belz, 2009) and syntactic correctness (Stent et al., 2005). Second, as NLG models ad- vance, surface-level feature matching becomes in- creasingly inadequate for discriminating system capabilities (Gehrmann et al., 2023). Most prob- lematically, this similarity-based paradigm intro- duces circular dependency on reference texts, in- herently constraining the assessment of creative or open-ended generation where optimal outputs may diverge from provided references. 2.3 LLM-driven Text Quality Evaluators To advance fine-grained evaluation in NLG, re- searchers have developed both specialized and uni- fied assessment frameworks. Dimension-specific evaluators now target critical quality aspects such as summarization consistency (Kryscinski and Mc- Cann, 2021; Wang et al., 2020) and dialogue coher- ence (Dziri et al., 2019; Huang et al., 2020), while unified approaches employ diverse methodologies including adaptive prompting (Yuan et al., 2021), model ensembles (Mehri and Eskenazi, 2020), and dynamic metric integration (Scialom et al., 2021) to generate comprehensive quality assessments. While these methods demonstrate versatility, their lack of explainability remains a significant limitation, often reducing complex evaluations to simplistic summative metrics (Li et al., 2025). In response, the field has witnessed three key developments in transparent evaluation architec- tures: (1) rationale-enhanced systems like G-EV AL (Liu et al., 2023) and UNIEV AL (Li et al., 2025) that combine scoring with interpretable reason- ing chains; (2) reference-free paradigms includ- ing GPTScore’s probability-based quantification (Fu et al., 2023) and PandaLM’s efficient compar- ative assessment (Wang et al., 2023b); and (3) ro- bust evaluation frameworks such as JudgeLM (Zhu et al., 2023) designed for adversarial testing sce- narios. These innovations, while significantly im- proving evaluation transparency, have inadvertently created new challenges regarding computational ef- ficiency and scoring flexibility. The current landscape reveals two critical limi- tations: prohibitive computational demands from LLM-dependent architectures (Liu et al., 2023; Fu et al., 2023), and inflexible scoring mecha- nisms that either oversimplify quality distinctions or depend on rigid weighting schemes (Li et al., 2025). Our proposed solution introduces a novel decoupled evaluation framework that strategically 2 combines a fine-tuned quality assessor with a regression-based scoring model, achieving both computational efficiency and nuanced quality anal- ysis while maintaining interpretability. 3 Method High-quality, large-scale datasets are crucial for effectively fine-tuning LLMs to act as evaluative judges (Zhu et al., 2023). However, the concurrent datasets, such as the one by PandaLM (Wang et al., 2023b), present limitations in terms of diversity and the granularity of judgment criteria. In order to achieve a more refined assessment of text qual- ity, this study has chosen as a research area the domain of law, which will have a greater impact on the actual use of the model in practical model deployment. 3.1 Data Generation Our main purpose in generating data is to create a small-scale but high-quality dataset in order to maximize the evaluation capability of the model. In this study, the data is divided into the following five parts: query, answer, comment, conclusion and score. Query: In this study, a \"query\" refers to the in- put legal issue presented to the evaluated model, with all queries drawn from Chinese legal domains including criminal law, civil code, and general statutes. We sampled 10,000 queries from an inte- grated collection comprising the DISC-Law-SFT- Pair dataset (Yue et al., 2024) for general legal mat- ters, the Criminal-Law-Dataset (Yuan, 2024) for criminal cases, and our proprietary dataset specif- ically focused on the Civil Code of the People’s Republic of China. Answer: An \"answer\" refers to the output gener- ated by an evaluated model in response to an input query. Unlike the training dataset where answers are pre-generated by selected models, the evalua- tion dataset contains no pre-defined answers—all responses are dynamically generated by the evalu- ated models during testing. As detailed in Table 1, we selected models span- ning five orders of magnitude in parameter size (26M to 685B). The Doubao-1.5-Pro-32k*and Error-Input**cases serve as special instances for ar- chitecture diversity and noise control respectively. Comment: A \"comment\" represents a meta- evaluation of answer quality, manifesting as textual feedback that critically analyzes response content.Table 1: Models for Training Data Generation and Their Parameter Sizes Model Name Parameter Size MiniMind2-small 26M Qwen2.5-0.5B-GPTQ-Int4 0.49B (Int4) Qwen2.5-0.5B 0.49B Qwen2.5-1.5B 1.5B Qwen2.5-3B 3.1B Qwen2.5-14B 14.7B Doubao-1.5-Pro-32k Unpublished* Deepseek-R1 671B Deepseek-V3-0324 685B Error-Input None** *Unpublished: Official parameter count not disclosed. Esti- mated active parameters: 20B (MoE). **None: Randomly select a random answer from another query and fill it in to simulate the situation where the answer is irrelevant or the relevance of the answer is not high. Sources: (Jing, 2025), (Qwen et al., 2025), (Bytedance, 2025), (DeepSeek-AI et al., 2025b) As illustrated in Figure 1, our comment framework systematically evaluates answers through five key dimensions: (1) content quality (including topical relevance and professional accuracy), (2) structure and organization, (3) linguistic expression, (4) ad- ditional considerations, and (5) identified deficien- cies. Figure 1: Comment framework and example These comments serve two intrinsically linked functions: (1) as qualitative assessments that diag- nose answer deficiencies through domain-specific critique (e.g., the example in Figure 1 demonstrates 3 Figure 2: Probability Density of Score topical deviation analysis), and (2) as the founda- tional training corpus for developing the comment model’s reasoning module. While adhering to this evaluative framework that ensures methodological consistency, the comments preserve necessary lin- guistic variability to capture the full spectrum of potential answer quality issues, thereby enabling the model to learn nuanced judgment capabilities. Conclusion: A \"conclusion\" in our dataset syn- thesizes the comprehensive evaluation of an answer into a summative statement that provides global quality assessment. This component serves three critical functions: (1) it consolidates the multi- dimensional analysis from the comment into an overarching judgment (e.g., \"While demonstrating professional terminology usage, the answer sub- stantially deviates from the core legal issues\"); (2) it trains the evaluation model’s scoring module by supplying holistic quality references rather than di- rect score mappings; and (3) it completes the evalu- ation framework’s logical flow from query analysis to final assessment. The conclusion’s global per- spective ensures the model considers all quality di- mensions simultaneously when generating scores, while its non-prescriptive nature maintains flexibil- ity in the final scoring decision. Score: The \"Score\" serves as the final quantita- tive output of this study, measuring the text qualitylevel of a model’s responses. It is derived from three key inputs: (1) the original answer text, which provides direct semantic and structural details; (2) the Comment, offering a fine-grained analysis of strengths and weaknesses; and (3) the Conclusion, synthesizing these observations into a holistic as- sessment. We calculate scores on a 0100% scale, ensuring a standardized and interpretable metric. Figure 2 illustrates the score distributions across all models in the training set, along with trends in average performance and variance. All components of the evaluation frame- work—Comments, Conclusions, and Scores—are developed through a structured pipeline combining AI-assisted drafting and human expert review. Do- main specialists validate the consistency, fairness, and domain-relevance of each data point, ensuring the final dataset meets rigorous quality standards for model training and evaluation. 3.2 Model Structure and Training Our modeling framework integrates two synergistic components to achieve comprehensive text quality evaluation shown in Figure 3. The first compo- nent is a fine-tuned Qwen2.5-7B language model specialized for generating structured quality com- ments and synthesizing conclusions through chain- of-thought reasoning. This comment model op- erationalizes the evaluation framework described 4 earlier, producing both detailed analytical feedback (Comments) and summative judgments (Conclu- sions) that adhere to established quality dimen- sions. The second component is a multi-source regres- sion model designed to process three complemen- tary information sources: the original answer text providing Detailed Content, the Comment offer- ing Partial Analysis, and the Conclusion deliver- ing Overall Evaluation. This regression architec- ture employs hierarchical feature extraction (via layer-wise CLS token aggregation) and gated fu- sion mechanisms to dynamically integrate informa- tion across different semantic levels. By extract- ing and weighting CLS representations from multi- ple depth layers, the model captures both granular textual features and high-level quality indicators, enabling precise score calibration against human- verified benchmarks. The training process follows a phased ap- proach. Initially, the comment model under- goes domain-adaptive fine-tuning using expert- annotated comment-conclusion pairs to ensure eval- uation consistency. Subsequently, the regression model is optimized through multi-task learning that combines score prediction with feature reconstruc- tion objectives. This dual-component architecture ensures methodological alignment with our evalu- ation framework while maintaining the flexibility needed for cross-domain quality assessment tasks. 4 Experiments In the experimental part, we used a validation set with the same data source distribution as the train- ing set but strictly non-overlapping content. In order to evaluate the text quality performance of more models more quickly, the number of valida- tion sets content is only 60. 4.1 Results Analysis Table 2 reveals a clear dichotomy in model performance based on scale. For low-scoring models below 70 points (MiniMind2-small and Qwen2.5-0.5B variants), we observe substan- tial cross-set variance, with MM2 showing a 5.57-point performance drop (17.66 →23.23) and Qwen0.5B-Int4 demonstrating 6.89-point degrada- tion (38.92 →45.81). These models exhibit stan- dard deviations exceeding 7 points (versus <3 points for high-scoring models), attributable to training data bias (85In contrast, larger models display remarkable stability, exemplified by Qwen14B’s <3-point vari- ance (82.89 →80.05) and top-performing Ds-V3’s minimal fluctuation (94.36 →94.09). The high- score group maintains an average cross-set error of just 1.2 points (versus 12.7 for low-score mod- els), confirming a strong correlation between model scale and evaluation stability. Table 2: Cross-Set Performance Evaluation of Language Models Model Training Validation MiniMind2-small 17.66 23.23 Qwen0.5B-Int4 38.92 45.81 Qwen0.5B 56.93 55.31 Qwen1.5B 61.76 68.08 Qwen3B 74.39 76.83 Qwen14B 82.89 80.05 Doubao-1.5 89.41 89.59 Deepseek-R1 92.32 93.76 Deepseek-V3 94.36 94.09 Abbreviations: Qwen (Qwen2.5 series), Doubao-1.5 (Doubao-1.5-Pro-32k), Deepseek-v3(Deepseek-v3-0324) 4.2 Leaderboard During the experiment, we used the evaluator pro- posed in this paper to test a large number of models. We found that different models not only had dif- ferent scores, but also had significant differences in stability. Therefore, we developed an indicator that simultaneously measures performance (aver- age score) and stability (standard deviation) - the adjusted CV index. LetDbe the evaluation dataset consisting of N test samples. For a given model f, we define its performance score sion the i-th sample according to the established evaluation protocol. The model’s empirical mean performance µand standard devia- tionσare then given by: µ=E[si] =1 NNX i=1si (1) σ=p E[(si−µ)2] =vuut1 NNX i=1(si−µ)2(2) where: - µ∈R+represents the model’s average capability across all test samples - σ∈R+quan- tifies the dispersion of individual scores about the mean 5 Figure 3: End-to-end data flow of the evaluation framework, showing how raw inputs are processed through comment generation, conclusion synthesis, and final scoring modules. The Coefficient of Variation (CV) normalizes the dispersion metric relative to mean performance: CV=σ µ∈[0,+∞) (3) This dimensionless quantity characterizes the intrinsic stability of a model’s predictions, with: - CV≪1indicating high consistency - CV≫1 suggesting erratic behavior To jointly quantify performance and stability, we define the adjusted score: AdjScore =µ 1 +CV=µ2 µ+σ∈(0, µ)(4) Table 3 presents a comprehensive stability- adjusted evaluation of model performance using the proposed AdjScore metric. Our analysis reveals significant variations not only in mean performance but also in prediction stability across models. The top-tier models demonstrate exceptional consistency: Deepseek-v3-0324 achieves the high- est AdjScore (93.16) by combining near-perfect mean performance (94.09) with minimal fluctua- tion (Std=0.93). Comparable mean-scoring mod- els like Qwen-QWQ-32B (94.06) show slightly reduced AdjScore (93.01) due to marginally higher variance (Std=1.06). Notably, the metric effectively captures stability trade-offs in mid-tier models. While ERNIE_4.5 exhibits lower mean performance (89.07) than V olces-Pro (89.59), its superior stability (Std=4.99vs 5.73) results in a higher AdjScore (84.34 vs 84.21). This aligns with our metric’s design prin- ciple in Eq. (4), where the non-linear µ2/(µ+σ) formulation penalizes high-variance predictions. The bottom-tier models reveal two failure pat- terns: (1) consistently low performance (Qwen2.5- Math-15B: Mean=13.58) and (2) unstable mid- range performers (Qwen2.5-1.5B: Mean=68.08 but Std=11.15). This bifurcation suggests our AdjS- core provides distinct signals for both capability limitations and reliability concerns. 5 Discussion 5.1 Model Capacity Analysis Systematic evaluation across seven model scales (0.5B to 72B parameters) reveals a logarithmic re- lationship between parameter count and capabil- ity metrics (Figure 4). The 72B variant demon- strates merely a 2.7% performance gain over its 7B counterpart (p=0.032, paired t-test), with over- lapping 95% confidence intervals confirming per- formance saturation beyond 7B parameters. This scaling behavior exhibits marked task dependence: while mathematical reasoning (MATH benchmark) plateaus at 7B parameters, text quality continues improving until 14B, suggesting distinct scaling thresholds for different capability domains. The emergent nonlinear scaling patterns warrant deeper examination. The MATH benchmark, quan- tifying intrinsic reasoning capacity, and our text quality metric, assessing expressive fidelity, collec- 6 Table 3: Model Performance Benchmark Model Mean Std AdjScore High Tier (AdjScore more than 90) Deepseek-v3-0324 94.09 0.93 93.16 Qwen-QWQ-32B 94.06 1.06 93.01 Deepseek-R1 93.76 1.23 92.54 Qwen3-14B 93.25 1.17 92.09 Mid Tier (AdjScore more than 70) ERNIE_4.5 89.07 4.99 84.34 V olces-Pro 89.59 5.73 84.21 Deepseek-v3(old) 87.27 7.08 80.72 qwen2.5_32B 80.18 8.05 72.87 Qwen2.5-7B 79.60 7.60 72.66 Kimi-128k 75.67 6.94 69.32 ChatGPT-4o 74.56 7.75 67.54 Low Tier (AdjScore more than 50) ChatGPT-3.5 66.11 7.74 59.18 Qwen2.5-1.5B 68.08 11.15 58.49 Qwen2.5-0.5B 55.31 12.43 45.16 MiniMind2 23.23 12.55 15.08 Qwen2.5-Math-15B 13.58 12.99 6.94 tively demonstrate premature diminishing returns in Qwen2.5 series models. We posit this stems from dual constraints: fundamentally, the linguistic quality ceiling of training data limits expressive potential, while architectural bottlenecks restrict reasoning scalability. This finding carries signifi- cant industrial implications—in cost-constrained deployments, brute-force parameter scaling proves suboptimal compared to architectural efficiency im- provements. To facilitate informed model selection, we con- duct Pareto analysis on 42 open-source mod- els spanning four architectural families (Qwen3, Qwen2.5, Deepseek-distill, and Llama3), as visu- Figure 4: Scaling Laws of Model Capabilities: Text Quality (blue) vs Mathematical Reasoning (red) across 0.5B-72B models..alized in Figure 5. Three critical insights emerge. First, Qwen3-series models consistently dominate the Pareto frontier, delivering 12-18% superior text quality at comparable parameter counts. Second, reasoning-optimized variants (red markers) exhibit substantially better parameter efficiency than stan- dard dense architectures. Third, the 7B-14B param- eter range emerges as the optimal cost-performance sweet spot, aligning perfectly with our scaling law observations. These results provide concrete guid- ance: practitioners should (1) balance capability ceilings against throughput costs, (2) prioritize ar- chitecturally efficient designs over pure parameter scaling, and (3) focus deployment efforts within the identified optimal parameter range. 5.2 Engineering Trade-offs When using LLM models in engineering, various characteristics of the model will affect the actual cost. In the following section, we will respectively explore the impact of these factors on the text qual- ity of the model from three perspectives: model quantization, the maximum number of Tokens of the model, and whether it is an reasoning model, and draw a scatter plot of the cost/text quality of the open-source model. Explore and search for the open-source model with the best cost performance. 5.2.1 Quantization Model quantization reduces numerical precision from FP16/BF16 to Int8 or lower formats. Us- ing Qwen2.5-7B-Instruct as a case study, Table 4 demonstrates the performance impact across three precision formats. The non-normal score distri- butions necessitated Wilcoxon signed-rank testing (Table 5), revealing: Table 4: Wilcoxon signed-rank Test with Quantization Model TQ MMLU C-Eval IEVAL BF16 79.6 70.5 77.2 53.1 Int8 78.76 69.1 76.7 52.9 Int4 77.56 67.8 75.2 49.4 All models are different Qwen2.5-7B-Instruct quantiza- tion version; It has been verified that the test scores of the model are not normally distributed. Therefore, the Wilcoxon signed-rank Test is adopted, and the ex- perimental results are shown in Table 5. All comparisons show non-significant (NS) dif- ferences at the adjusted alpha-level. However, the 7 Figure 5: Parameter-Performance Pareto Frontier of Open-Source LLMs: Comparative Analysis of Qwen3, Qwen2.5, and Baseline Architectures Table 5: Wilcoxon signed-rank Test with Quantization Model Model P Value BF16 Int8 0.601 BF16 Int4 0.085 Int8 Int4 0.200 Corrected alpha=0.0167 decreasing p-value trend (0.601 -> 0.085) suggests potential performance degradation with extreme quantization (e.g., 2-bit), warranting further inves- tigation. 5.2.2 The maximum Number of Tokens of the model Following the release of standard model versions, many providers subsequently develop variants with extended context windows through various archi- tectural modifications. These different context length versions typically exhibit varying API call costs and latency characteristics. This section in- vestigates the potential impact of context length on text generation quality, using the Kimi model (Team et al., 2025) as a case study with three con- text window variants (8k, 32k, and 128k). The experimental results are presented in Figure 6. The data reveals an inverse relationship between context length and text quality scores, with the 128k variant showing a 1.58-point decrease com-Table 6: Wilcoxon signed-rank Test with Quantization Model Mean Std Median AdjCV Kimi_8k 76.53 8.55 75.81 68.84 Kimi_32k 77.25 7.92 75.89 70.06 Kimi_128k 75.67 6.94 74.95 69.32 pared to the 32k version. However, this marginal quality reduction coincides with improved out- put stability, as evidenced by the reduced stan- dard deviation (6.94 vs 7.92). Statistical analysis via Wilcoxon signed-rank tests (Table 7) confirms these observations: Table 7: Wilcoxon signed-rank Test with Context Length Model Model P Value Kimi_8k Int8 0.502 kimi_32k Int4 0.365 kimi_128k Int4 0.230 Corrected alpha=0.0167 All pairwise comparisons yield non-significant results (P-values > 0.016), indicating that context length variations within this range have statistically negligible effects on text quality. Consequently, model selection should prioritize economic factors (API costs) and task requirements (necessary con- 8 text size) rather than anticipated quality differences between context length variants. 5.2.3 Reasoning Models & Base Models The reasoning model is a solution that has gained widespread adoption in many industrial scenarios recently. Meanwhile, the general reasoning model also has stronger performance (DeepSeek-AI et al., 2025a). The purpose of this section is to discuss whether the performance of the reasoning model in terms of text quality has also increased like its reasoning performance. A total of four control groups were selected in this section. Among them, two were reasoning models distilled from Qwen’s mathematical model, one was obtained distilled from Qwen’s instruction model, and the other was the R1 model released by deepseek. The result are shown in Figure 6. Figure 6: Parameter-Performance Pareto Frontier of Open-Source LLMs: Comparative Analysis of Qwen3, Qwen2.5, and Baseline Architectures It can be seen that a very obvious trend - in all comparison groups, the performance of the reason- ing model will be stronger than that of the base model. 5.2.4 Output Text Length The purpose of this part of the research is to analyze whether there is a rule that the longer the output text content, the higher the text quality. We drew a scatter plot as Figure 7 of the average length of the output text and the mean value of the text quality. In the figure, the distribution of the scattered points shows no significant correlation, that is, there is no linear relationship. The part in the up- per left corner of this figure indicates that the text quality is high and the output text is small, while the lower right corner indicates that the content is very long but the quality is significantly lower. It is worth noting that the four models in the Pareto Frontier are chatgpt_3.5_turbo, qwen2.5_14B, qwen_max(api), and deepseek_v4_0324 respec- tively. Although qwen_qwq_32B achieves very Figure 7: Parameter-Performance Pareto Frontier of Open-Source LLMs: Comparative Analysis of Qwen3, Qwen2.5, and Baseline Architectures good performance, the content it outputs is also the longest. Meanwhile, qwen_math_1.5B has output content that is both long and of poor quality. 5.2.5 Price of APIs In API-based deployment scenarios, token pric- ing emerges as the primary cost determinant. Fig- ure 8 illustrates the relationship between API call costs and text quality scores across evaluated mod- els. The visualization reveals ChatGPT-4o as a dis- tinct outlier due to its premium pricing tier, while the Pareto frontier comprises three cost-efficient options: Qwen-Turbo, V olces-Pro, and Deepseek- v3-0324. It should be noted that API pricing ex- hibits temporal variability influenced by promo- tional campaigns, distribution channels, and model updates, requiring periodic reevaluation of these economic tradeoffs. Figure 8: Parameter-Performance Pareto Frontier of Open-Source LLMs: Comparative Analysis of Qwen3, Qwen2.5, and Baseline Architectures This analysis demonstrates how our text qual- ity metric integrates with dynamic pricing factors to inform industrial deployment decisions. The framework enables practitioners to identify opti- mal model choices along the cost-quality frontier, 9 balancing performance requirements with budget constraints. For instance, while Qwen-Turbo de- livers superior cost efficiency, Deepseek-v3-0324 may warrant its price premium for mission-critical applications demanding higher quality guarantees. Such data-driven selection proves particularly valu- able when onboarding new models or renegotiating API service agreements. 6 Limitations & Future Directions While our novel NLG evaluation framework ad- vances text quality assessment, three key limita- tions merit discussion. The current legal-domain specialization introduces potential bias for medi- cal or technical applications, necessitating multi- domain corpora in future work. Secondly, the Sig- moid activation’s 96-point ceiling limits differenti- ation between elite models (Qwen3 vs. Deepseek), addressable through linear activation adoption or high-end score annotations. Finally, annotation granularity constraints for top-tier outputs could be mitigated via refined rating scales paired with expert validation. These limitations conversely chart clear research trajectories: cross-domain generalization through diversified training data, architectural modifica- tions for extended dynamic range, and precision enhancements via hierarchical evaluation protocols. Such improvements would particularly benefit ap- plications requiring nuanced quality discrimination, such as academic writing or professional documen- tation review systems. 7 Conclusion This study establishes a novel quantitative frame- work for evaluating text quality in LLM outputs through regression analysis, addressing the crit- ical gap in standardized assessment metrics for domain-specific applications. Our systematic eval- uation of 49 language models across diverse ar- chitectures yields three principal findings: First, text quality demonstrates unique scaling charac- teristics distinct from mathematical reasoning ca- pabilities, exhibiting diminishing returns beyond the 14B parameter threshold. Second, engineer- ing optimizations—including quantization strate- gies and context window extensions—produce sta- tistically insignificant effects on text quality (p > 0.0167), enabling cost-efficient deployment con- figurations. Third, reasoning-optimized models consistently surpass their base counterparts in qual-ity metrics despite architectural parity, highlighting the importance of specialized training methodolo- gies. The developed multidimensional assessment model, rigorously validated across legal subdo- mains, offers practitioners actionable guidance for model selection. Our Pareto frontier analysis iden- tifies optimal quality-cost tradeoffs, with Qwen3 series models emerging as current leaders in param- eter efficiency. These findings fundamentally chal- lenge conventional scaling paradigms by revealing inherent limitations in contemporary training data quality—constraints that cannot be circumvented through parameter scaling alone. Future research should prioritize: (1) framework generalization through multidisciplinary dataset in- tegration, (2) dynamic activation function devel- opment to overcome existing scoring limitations, and (3) industry-standard benchmark establishment for text quality assessment. This work not only advances the methodological foundations of spe- cialized neuro-linguistic programming evaluation but also delivers immediate practical value for de- ploying AI systems in legal and other professional domains. References Bytedance. 2025. doubao_1.5_pro. official website. Accessed: 2024-05-22. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI confer- ence on artificial intelligence, volume 32. Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover’s similarity: Automatic evalu- ation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics, pages 2748–2760. Amin Dada, Marie Bauer, Amanda Butler Contreras, Os- man Alperen Kora¸ s, Constantin Marc Seibold, Kaleb E Smith, and Jens Kleesiek. 2024. Clue: A clinical language understanding evaluation for llms. arXiv e-prints, pages arXiv–2404. Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. 2023. Laiw: a chinese legal large language models benchmark. arXiv preprint arXiv:2310.05620. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, 10 Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi- hong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. Deepseek-r1: Incentivizing reasoning capa- bility in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx- uan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithful- ness assessment in abstractive summarization. arXiv preprint arXiv:2005.03754. Nouha Dziri, Ehsan Kamalloo, Kory W Mathewson, and Osmar Zaiane. 2019. Evaluating coherence in dialogue systems using entailment. arXiv preprint arXiv:1904.03371. Z Fei, X Shen, D Zhu, F Zhou, Z Han, S Zhang, K Chen, Z Shen, and J Lawbench Ge. 2023. Benchmarking le- gal knowledge of large language models. arxiv 2023. arXiv preprint arXiv:2309.16289. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Sebastian Gehrmann, Elizabeth Clark, and Thibault Sel- lam. 2023. Repairing the cracked foundation: A survey of obstacles in evaluation practices for gener- ated text. Journal of Artificial Intelligence Research, 77:103–166. Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zam- brano, and 1 others. 2023. Legalbench: A collab- oratively built benchmark for measuring legal reason- ing in large language models. Advances in Neural Information Processing Systems, 36:44123–44279. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under- standing. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceed- ings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, and Xiaodan Liang. 2020. Grade: Automatic graph-enhanced coherence metric for evaluating open-domain dialogue systems. arXiv preprint arXiv:2010.03994.Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, and 1 oth- ers. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Ad- vances in Neural Information Processing Systems, 36:62991–63010. Yaogong Jing. 2025. Minimind. GitHub repository. Accessed: 2024-05-22. Greg Kamradt. 2023. Needle in a haystack - pressure testing llms. Website. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R Fab- bri, Yejin Choi, and Noah A Smith. 2021. Bidimen- sional leaderboards: Generate and evaluate language hand in hand. arXiv preprint arXiv:2112.04139. Wojciech Kryscinski and Bryan McCann. 2021. Evalu- ating the factual consistency of abstractive text sum- marization. US Patent App. 16/750,598. Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. 2023. Huatuo-26m, a large- scale chinese medical qa dataset. arXiv preprint arXiv:2305.01526. Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chen- chang Hu, Hualiang Wang, and Xiaomeng Li. 2025. Unieval: Unified holistic evaluation for unified multi- modal understanding and generation. arXiv preprint arXiv:2505.10483. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human align- ment. arXiv preprint arXiv:2303.16634. Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. 2023. Bbt-fin: Compre- hensive construction of chinese financial domain pre-trained language model, corpus and benchmark. arXiv preprint arXiv:2302.09432. Shikib Mehri and Maxine Eskenazi. 2020. Usr: An unsupervised and reference free evaluation metric for dialog generation. arXiv preprint arXiv:2005.00456. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311–318. Qwen,:, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin 11 Yang, Jiaxi Yang, Jingren Zhou, and 25 oth- ers. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jack- son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju- lian Michael, and Samuel R. Bowman. 2024. GPQA: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evalu- ating natural language generation systems. Computa- tional Linguistics, 35(4):529–558. Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Sta- iano, and Alex Wang. 2021. Questeval: Summariza- tion asks for fact-based evaluation. arXiv preprint arXiv:2103.12693. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. Preprint, arXiv:2310.16049. Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In International confer- ence on intelligent text processing and computational linguistics, pages 341–351. Springer. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se- bastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. InFindings of the Association for Computational Lin- guistics: ACL 2023, pages 13003–13051, Toronto, Canada. Association for Computational Linguistics. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, and 75 others. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. arXiv preprint arXiv:2004.04228. Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, and 1 others. 2023a. Cmb: A comprehensive medical benchmark in chinese. arXiv preprint arXiv:2308.08833. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, and 1 others. 2023b. Pandalm: An automatic evaluation benchmark forllm instruction tuning optimization. arXiv preprint arXiv:2306.05087. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. MMLU-pro: A more robust and challenging multi-task language under- standing benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, and 1 others. 2024. Finben: A holistic financial benchmark for large lan- guage models. Advances in Neural Information Pro- cessing Systems, 37:95716–95743. Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and Xiaodan Liang. 2021. Towards quantifiable dialogue coherence evaluation. arXiv preprint arXiv:2106.00507. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text gener- ation. Advances in neural information processing systems, 34:27263–27277. Zhipeng Yuan. 2024. Crimina_law_dataset. GitHub repository. Accessed: 2024-05-22;. Shengbin Yue, Shujun Liu, Yuxuan Zhou, Chenchen Shen, Siyuan Wang, Yao Xiao, Bingxuan Li, Yun Song, Xiaoyu Shen, Wei Chen, and 1 others. 2024. Lawllm: Intelligent legal system with legal reason- ing and verifiable retrieval. In International Con- ference on Database Systems for Advanced Applica- tions, pages 304–321. Springer. Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, and 1 others. 2023. Fineval: A chi- nese financial domain knowledge evaluation bench- mark for large language models. arXiv preprint arXiv:2308.09975. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675. Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: scaling language models to 128k context through multi-agent collaboration. arXiv preprint arXiv:2402.11550. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized em- beddings and earth mover distance. arXiv preprint arXiv:1909.02622. 12 Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 3: System Demonstra- tions), Bangkok, Thailand. Association for Computa- tional Linguistics. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Preprint, arXiv:2311.07911. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631. A Complete experimental ranking This section presents the complete 49 models par- ticipating in the evaluation, including their average scores, Std, and AdjCV values in table 8. B A Complete Set of Data Examples This section presents a complete set of data exam- ples, including five parts: query, answer, comment, conclusion and score. The details is shown in Fig- ure 9. Figure 9: A complete set of data examplesCA Very Low-scoring Negative Example: 0 This section selects the responses of the data gener- ated by different models for the same query as in Appendix B to visually reflect the data differences represented by different scores of different models. The details is shown in Figure 10. Figure 10: A very low-scoring negative example The answers to this part are from the MiniMind2- small model. The translation of the Answer part is: \"The crime of illegally crossing national (border) boundaries, also known as evading bribery, refers to the act of using state public power without per- mission in certain circumstances, and using state public power without permission in other countries or regions, resulting in the loss of original property or the failure of public disclosure, and ultimately putting citizens or civil organizations under law- lessness.\" The different sentencing standards for the crime of illegally crossing national (border) boundaries and the legal logic behind them are as follows: 1. ** Evading bribes abroad **: Evad- ing statutory bribes abroad is usually obtained by 13 Table 8: Model Performance Comparison Model Mean Std AdjScore Deepseek_v3_0324 94.09 0.93 93.16 Qwen_qwq_32B 94.06 1.06 93.01 Deepseek_r1 93.76 1.23 92.54 Qwen3-14b 93.25 1.17 92.09 ERNIE_X1 92.95 1.12 91.85 Qwen3-235b-a22b 93.38 1.65 91.76 Qwen3-32b 93.04 1.48 91.58 Qwen3-30b-a3b 92.87 1.62 91.28 Qwen3-1.7b 92.66 2.06 90.65 Qwen3-4b 92.61 2.60 90.08 Qwen-plus 89.78 5.77 84.36 ERNIE_45 89.07 4.99 84.34 V olces_pro 89.59 5.73 84.21 Deepseek_v3 87.27 7.08 80.72 Qwen-max 84.65 5.72 79.29 Qwen_ds_distill_32B 85.51 7.74 78.42 Qwen_72B 82.29 6.73 76.07 Qwen-turbo 84.59 10.04 75.62 V olces_lite 84.84 12.14 74.22 Qwen_14B 80.05 7.01 73.60 Qwen_32B 80.18 8.05 72.87 Qwen_7B 79.60 7.60 72.66 Qwen_farui 79.12 7.71 72.10 Qwen_7B_int8 78.76 7.84 71.64 Qwen_ds_distill_14B 81.99 12.98 70.79 Qwen_7B_int4 77.56 7.55 70.68 Qwen_3B 76.83 7.15 70.29 Kimi_32k 77.24 7.92 70.06 Kimi_128k 75.67 6.94 69.32 Kimi 76.53 8.55 68.84 Llama4_maverick-17B 75.77 8.91 67.80 Chatgpt_4o 74.56 7.75 67.54 ERNIE_40 74.07 7.72 67.08 Qwen3-0.6b 78.59 14.49 66.36 Llama4_scout_17B 75.32 10.59 66.03 ERNIE_speed 70.10 9.11 62.04 Llama33_70B 69.80 9.62 61.35 Chatgpt_35_turbo 66.11 7.74 59.18 Qwen_15B 68.08 11.15 58.49 Llama32_3B 57.38 10.72 48.35 Qwen_05B_int8 55.68 11.64 46.05 Qwen_05B 55.31 12.43 45.16 Qwen_ds_distill_7B 53.43 18.53 39.67 Qwen_05B_int4 45.81 14.06 35.05 Llama32_1B 43.44 11.84 34.13 Qwen_math_7B 35.84 17.41 24.13 MiniMind2_small 23.23 12.55 15.08 Qwen_ds_distill_15B 21.69 11.73 14.08 Qwen_math_15B 13.58 12.99 6.94 Note: AdjScore is calculated as Mean minus Std. 14 means of hiring, inspecting prosecutors or lawyers, etc. If arrested, the court will sentence the person to death, that is, to evade legal bribery abroad, and impose imprisonment of not more than three years, criminal detention, public surveillance or a fine. 2. ** Overseas Evasion Method **: The overseas evasion method means that under specific circum- stances, the court will make a judgment on the per- son’s behavior based on the above situation. When the person is arrested, the court will determine that their behavior is related to the case and transfer the ill-gotten gains in accordance with legal proce- dures. If arrested, the court will determine that his or her actions are related to the case and sentence him or her to death. 3. ** Foreign Evasion Law **: Foreign evasion law means that under specific cir- cumstances, the court will order the country where the act of evasion occurs to disclose its power. If one evades the law abroad, the court will rule that when arrested, the court will sentence the person to death. 4. ** Punishment **: Foreign evasion law, that is, in specific circumstances, when the court sentences an arrest, the court will sentence the person to death. If arrested, the court will deter- mine that their actions are related to the case and sentence them to death. 5. ** Change of Stolen Money **: The overseas evasion law, that is, in specific circumstances, when the court rules that someone is arrested, The court will rule that the ill-gotten gains change, change, change, change, change, change, change, change, change, change, change, change, change, change, change Change, change, change, change, change, change, change, change, change Change, change, change, change, change, change, change, change, change Change, change, change, change, change, change, change, change, change, change, change, change, change, change, change.\" This Comment means that the quality of this re- ply is extremely poor. It has no relevance to the topic at all. The definition of the crime of illegally crossing national (border) boundaries is incorrect. The accuracy of professional knowledge is seri- ously insufficient. The depth of the answer and the breadth of association are both zero. The logic of the text is chaotic. There is no clarity, orderliness or sense of hierarchy in terms of structure and or- ganization. There are a large number of redundant and meaningless repetitions in language expression, the grammar is chaotic, and the style adaptation is completely wrong. Overall, there are serious errors and it completely fails to meet the requirements foranswering this question. DA Positive Example With A High Score: 96 This section presents an example of a high-score reply based on Deepseek-v3-0324. The details is shown in Figure 11. This Comment means that the quality of this reply is high, closely related to the question, the professional knowledge is accurate, the answer is in-depth and the association is extensive, and the text is logically strong. The structure and organiza- tion are clear, and the sense of logic and hierarchy is excellent. The language expression is standard- ized, with no grammatical or spelling errors, low redundancy, smooth and natural, and well-matched in style. Overall, it has almost no flaws and per- forms outstandingly in terms of comprehensive and in-depth content, precise citation of legal provi- sions and judicial interpretations, rigorous logic, and discussion of disputes in combination with ju- dicial practice, thus having its own shining points. 15 Figure 11: A positive example with a high score: 96 16",
  "text_length": 52728
}