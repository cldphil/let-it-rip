{
  "id": "http://arxiv.org/abs/2506.01187v1",
  "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation",
  "summary": "Grounded text generation models often produce content that deviates from\ntheir source material, requiring user verification to ensure accuracy. Existing\nattribution methods associate entire sentences with source documents, which can\nbe overwhelming for users seeking to fact-check specific claims. In contrast,\nexisting sub-sentence attribution methods may be more precise but fail to align\nwith users' interests. In light of these limitations, we introduce Localized\nAttribution Queries (LAQuer), a new task that localizes selected spans of\ngenerated output to their corresponding source spans, allowing fine-grained and\nuser-directed attribution. We compare two approaches for the LAQuer task,\nincluding prompting large language models (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling framework that extends existing\nattributed text generation methods to LAQuer. We evaluate this framework across\ntwo grounded text generation tasks: Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). Our findings show that LAQuer methods\nsignificantly reduce the length of the attributed text. Our contributions\ninclude: (1) proposing the LAQuer task to enhance attribution usability, (2)\nsuggesting a modeling framework and benchmarking multiple baselines, and (3)\nproposing a new evaluation setting to promote future research on localized\nattribution in content-grounded generation.",
  "authors": [
    "Eran Hirsch",
    "Aviv Slobodkin",
    "David Wan",
    "Elias Stengel-Eskin",
    "Mohit Bansal",
    "Ido Dagan"
  ],
  "published": "2025-06-01T21:46:23Z",
  "updated": "2025-06-01T21:46:23Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01187v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01187v1  [cs.CL]  1 Jun 2025LAQuer: Localized Attribution Queries in Content-grounded Generation\nEran Hirsch1Aviv Slobodkin1David Wan2\nElias Stengel-Eskin2Mohit Bansal2Ido Dagan1\n1Bar-Ilan University2UNC Chapel Hill\n{hirsch.eran, lovodkin93}@gmail.com\n{davidwan, esteng, mbansal}@cs.unc.edu dagan@cs.biu.ac.il\nAbstract\nGrounded text generation models often pro-\nduce content that deviates from their source\nmaterial, requiring user verification to ensure\naccuracy. Existing attribution methods asso-\nciate entire sentences with source documents,\nwhich can be overwhelming for users seeking\nto fact-check specific claims. In contrast, exist-\ning sub-sentence attribution methods may be\nmore precise but fail to align with users’ inter-\nests. In light of these limitations, we introduce\nLocalized Attribution Quer ies (LAQuer), a\nnew task that localizes selected spans of gener-\nated output to their corresponding source spans,\nallowing fine-grained and user-directed attribu-\ntion. We compare two approaches for the LA-\nQuer task, including prompting large language\nmodels (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling\nframework that extends existing attributed text\ngeneration methods to LAQuer. We evaluate\nthis framework across two grounded text gen-\neration tasks: Multi-document Summarization\n(MDS) and Long-form Question Answering\n(LFQA). Our findings show that LAQuer meth-\nods significantly reduce the length of the at-\ntributed text. Our contributions include: (1)\nproposing the LAQuer task to enhance attribu-\ntion usability, (2) suggesting a modeling frame-\nwork and benchmarking multiple baselines, and\n(3) proposing a new evaluation setting to pro-\nmote future research on localized attribution in\ncontent-grounded generation.1\n“ChatGPT can make mistakes. Check important\ninformation. ” — ChatGPT interface\n1 Introduction\nGrounded text generation aims to produce content\nbased on specific sources, whether retrieved—such\nas in retrieval-augmented generation (RAG) (Lewis\net al., 2020; Ram et al., 2023)—or user-provided.\n1https://github.com/eranhirs/LAQuer\n     [nbcnews.com]  Organic food companies want labeling to provoke \nsafety concerns that drive consumers  toward their ‘natural’ products. … \nThey have a right to know what is in their food . Think about the words that \ngo onto food products now that have nothing to do with safety, … \nYes, GMO food should be labeled to provide \ntransparency and inform consumers. They  deserve to  \nknow what they are eating , and mandatory labeling could \nalleviate confusion and distrust around genetically \nmodified foods. … Retrieved docs: Q: Should GMO food be labeled? \n     [statnews.com]  The conversation around genetic engineering and food \nis undermined by a lack of information that breeds  confusion and distrust . \nConsumers feel misled. … Not labeling products made with GMOs only \nstoked the concern it was intended to minimize. Perversely, the only \nproducts that bear transparent GMO labels are those that do not contain … \nLAQuer:   They deserve to know what they are eating 1\n2Figure 1: Top: example RAG scenario. Bottom : our\nLocalized Attribution Queries (LAQuer), where the at-\ntribution is constructed per user query, highlighted in\nyellow. Existing sentence-level attribution methods, un-\nderlined in green, can often be disorienting and lengthy.\nYet, model outputs frequently diverge from these\nsources, resulting in factual inaccuracies, or ‘hal-\nlucinations’ (Mishra et al., 2024). To address this,\nusers often need to manually review retrieved doc-\numents to ensure the accuracy of generated claims.\nThis in turn has driven a growing interest in at-\ntributed text generation (Thoppilan et al., 2022;\nMenick et al., 2022; Bohnet et al., 2023), which\nincorporates supporting evidence or citations into\nthe output, thereby enhancing model reliability and\nhelping mitigate potential factuality errors.\nWhile attributed text generation enhances trans-\nparency by providing citations, its effectiveness\ndepends on how easily users can interpret these\nattributions, as shown in Fig. 1. Most existing\nattribution methods associate each generated sen-\ntence with its corresponding attributions (Gao et al.,\n2023b; Slobodkin et al., 2024). For example, the\noutput sentence underlined green is attributed to\nmany spans in the source document, also under-\n--- Page 2 ---\nlined green. Yet, in practice, users often seek to\nfact-check specific details rather than an entire sen-\ntence (e.g., the highlighted fact in Fig. 1). As sen-\ntences typically contain multiple facts (Min et al.,\n2023), sentence-level attribution requires readers\nto examine both the full sentence and its sources\nbefore assessing factual accuracy of a single fact.\nFor instance, in Fig. 1, the highlighted fact is at-\ntributed by the first source, while another within\nthe same sentence is linked to the second source.\nAs a result, users must review the entire sentence\nand all cited sources to verify this single fact.\nIn this work, we introduce a more precise at-\ntributed generation task, which we call Localized\nAttribution Quer ies (LAQuer), that links specific\nspans in generated text to their corresponding\nsource spans. Each query consists of pre-selected\noutput spans, or ‘highlights’ (e.g., the highlighted\nspan in the top part of Fig. 1), while the response\nidentifies the relevant source spans (e.g., the high-\nlighted spans in the bottom part of Fig. 1). Since\nqueries can vary from single words to full sen-\ntences, this approach generalizes existing attribu-\ntion methods while enabling targeted attribution.\nWe model the LAQuer setting as a framework\nconsisting of two processing stages, illustrated in\nFig. 2. First, a source-grounded generation sys-\ntem produces text expected to be supported by\nidentified source texts. Some generation methods\nmay include attribution metadata, mapping output\nsegments to supporting source spans. For exam-\nple, in Fig. 1, a sentence-level attribution method\ncan attribute the second sentence to the texts un-\nderlined green. In our experiments (Section 5),\nwe benchmark LAQuer using three generation ap-\nproaches: one without attribution and two contem-\nporary attributed-generation methods. In the sec-\nond stage, users request localized attribution by\nhighlighting spans that correspond to a fact of in-\nterest. The LAQuer task then identifies the exact\nsupporting source spans for the given highlight.\nThis second stage is composed of two steps: (A)\ndecontextualization of the user’s query, and (B)\nquery-focused attribution. The decontextualization\nstep converts the highlighted fact to a stand-alone\ndecontextualized statement, for which source attri-\nbution can be more easily sought in an unambigu-\nous matter. For example, “ They ” in Fig. 1 refers to\n“consumers ”. In such cases, attributions should\naccount for the decontextualized meaning, e.g.,\nthat “ They ” is correctly attributed to “ consumers .”\nThe query-focused attribution step searches for thesupporting source spans for the decontextualized\nstatement. For the query-focused attribution, we\ncompare two approaches: one that prompts a large\nlanguage model (LLM) to produce the alignment\nand another that uses the internal representations\nof the model to align phrases (Phukan et al., 2024).\nIf attribution metadata from the generation step is\navailable, it is leveraged to narrow the search space.\nFor example, instead of scanning the entire source\ndocument in the figure, our approach can focus on\nthe spans underlined green.\nFor evaluation, to simulate user interaction in\nthis process, our methodology involves decompos-\ning the generated output into atomic facts using\nLLMs (Min et al., 2023), which are subsequently\naligned with output spans. The LAQuer task can\nthen be applied to any type of generation, unlike\nprevious work which focuses on datasets annotated\nwith sub-sentence alignments (Phukan et al., 2024;\nQi et al., 2024; Cohen-Wang et al., 2024). Our ex-\nperimental setup includes two grounded generation\ntasks, Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). A key\nfinding is that LAQuer methods can significantly\nreduce the length of the attributed text. Overall,\nLAQuer remains a challenging task, particularly\nin attributing decontextualized facts. In total, our\ncontribution in this work is enumerated as follows:\n1.We propose Localized Attribution Queries\n(LAQuer) as a task to improve the accessi-\nbility of attributions for users.\n2.We introduce a novel modeling framework for\nthe LAQuer setting and benchmark various\nbaselines. We demonstrate their potential to\nenable targeted attribution while maintaining\naccuracy.\n3.We establish a new evaluation setting that en-\ncourages future research on localized attribu-\ntion in content-grounded generation.\n2 Background\nHallucinations produced by LLMs have attracted\nincreasing interest in generating attributed text.\nThe task of attributed text generation requires mod-\nels to generate summaries or answers that cite spe-\ncific evidence for their claims (Gao et al., 2023b;\nThoppilan et al., 2022; Menick et al., 2022; Bohnet\net al., 2023). When considering the granularity of\n--- Page 3 ---\nLAQuer method \nConsumers  deserve to know  what they are eating. Source-grounded \nGeneration [statnews.com] : … Source \ntexts \n[nbcnews.com]  Organic \nfood companies want \nlabeling to provoke \nsafety concerns that \ndrive consumers  … \nThey have a right to  \nknow what is in their  \nfood .Attributed source spans Generated output \nYes, GMO food should be labeled to provide \ntransparency and inform consumers. They \ndeserve to know what they are eating, and \nmandatory labeling could alleviate confusion and \ndistrust around genetically modified foods.  … \nYes, GMO food \nshould be labeled to \nprovide transparency \nand inform \nconsumers. … They  \ndeserve to know what  \nthey are eating , and \nmandatory labeling \ncould alleviate … (optional) \nAttribution \nmetadata \nHighlighted output spans Q: Should GMO food be labeled? \nDecontextualize (2) (3) (1)\n(4)(Stage 1) Generation \n(Stage 2) LAQuer \n(B)\nYes, GMO food should be labeled to provide \ntransparency and inform consumers .(A) \nDecontextualized \nfact \nAdditional \noutput spans Query-focused \nAttribution Figure 2: Overview of our LAQuer framework. The top section illustrates the generation of an output based on\nidentified source texts, either provided as input or retrieved. The bottom section represents the LAQuer task, where\noutput spans are attributed back to their source texts, enabling users to verify the provenance of individual pieces of\ninformation. The inputs to our proposed LAQuer approach are labeled (1) to (4). In Step (A), the highlighted spans\nare transformed into a decontextualized fact along with its corresponding output spans. In Step (B), the user’s query\nis attributed to relevant source texts, enabling precise fact verification.\nthe attribution, there are two key factors: the gran-\nularity of the summary or answer (i.e., the output)\nand the granularity of the source text (i.e., the in-\nput). The standard level of output granularity is\nsentence-level (Gao et al., 2023b; Slobodkin et al.,\n2024). Some work focuses on sub-sentence attri-\nbution, based on the internal representations of a\nmodel (Phukan et al., 2024; Qi et al., 2024; Ding\net al., 2024) or manipulation to the input (Cohen-\nWang et al., 2024). Similarly, input granularity\ncan vary between pointing to the entire response\n(Thoppilan et al., 2022), documents (Gao et al.,\n2023b), snippets (Menick et al., 2022), paragraphs\nor sentences (Buchmann et al., 2024), and spans\n(Schuster et al., 2024; Phukan et al., 2024; Qi et al.,\n2024; Ding et al., 2024; Cohen-Wang et al., 2024).\nThe above methods provide fixed pre-\ndetermined attributions, that often do not\ncorrespond most effectively to the specific scope\nof output information for which attribution is\nsought. Some systems provide attributions for\nlonger output spans, requiring the user to examine\nirrelevant source segments (Gao et al., 2023b;\nSlobodkin et al., 2024), while others provide\nonly partial attributions for narrow output spans,\nrequiring the user to look around the attributed\nsource spans for complete supporting information(Phukan et al., 2024; Qi et al., 2024; Ding\net al., 2024). Our work is the first to explore\nuser-initiated attribution queries across variable\nscales, introducing a novel evaluation methodology\nto assess their effectiveness.\n3 Localized Attribution Queries\nThe LAQuer task assumes as input a generation o\ngrounded in source documents D. For instance, in\nFig. 1, the answer to the question “Should GMO\nfood be labeled?” is generated based on two source\ndocuments. A key aspect of this task is the inclu-\nsion of ‘highlights’, which are specific parts of\nthe generated output that are marked by the user.\nThese highlights indicate a fact that the user wants\nto verify or examine within the source. The user\nconveys the fact of interest by selecting the spans\nin the output that best express it. For example, in\nthe figure the highlighted span is: “They deserve\nto know what they are eating. ” Importantly, the\nuser may not care about other claims made in the\nsame sentence, such as “labeling could alleviate\nconfusion and distrust. ” Formally, we are given a\nset of highlighted output spans o1, . . . , o nwhere\neach span may range in length from a single word\nto the entire generated output. The goal of the LA-\nQuer task is to provide the highlighted source spans\n--- Page 4 ---\ns1, . . . , s mthat support the fact expressed in these\nhighlights.\nWithin this setting, we aim that our LAQuer task\ndefinition would capture the following desiderata:\n1) User-initiated Attribution Queries. Most at-\ntribution methods provide ‘fixed’, pre-determined\nattributions, meaning that attribution is generated\nalongside the output, only allowing users to explore\nit afterward (Gao et al., 2023b; Slobodkin et al.,\n2024; Phukan et al., 2024). However, we point\nout that users are often interested in checking the\nattribution only for a limited subset of facts within\nthe generated output, and it is not possible to pre-\ndict a user’s specific interests in advance. LAQuer\nrequires developing methods that can dynamically\nprovide attribution for any arbitrary fact of interest,\nwhich the user highlights in the output.\n2) Source and Output Localization. Slobod-\nkin et al. (2024) introduce the Locally Attributable\nText Generation task, where the goal is to provide\nthe user with concise source spans necessary to\nverify a complete output sentence; in other words,\nthe goal is to provide localized , precise input spans.\nIn this work, we also consider the localization for\nthe other side of the attribution, which is the output\nlocalization. Instead of complete output sentences,\nwe work with output spans. Formally, the con-\ncatenation of the source spans should contain only\nthe necessary content to support the information\nconveyed by the output spans.\n3) Output Decontextualization. Given a contex-\ntualized claim cextracted from some text r, Choi\net al. (2021) define a decontextualized claim m\nas one that uniquely specify entities, events, and\nother context such that the claim cis now inter-\npretable. In our setting, it is likely that the high-\nlights provided by the user are contextualized. For\nexample, the output spans in Fig. 1 mention “They, ”\nwhich refers to the consumers mentioned in the\nprevious sentence. However, the user did not high-\nlight “consumers, ” because it is redundant and\ncan be inferred from “They. ” Accordingly, source\nspans should correspond to a decontextualized ver-\nsion of the output. For example, in Fig. 1, the\nsource from nbcnews.com must explicitly include\n“consumers” to avoid ambiguity. Only including\n“they” in the source spans would be problematic,\nas it lacks a clear referent and could lead to mis-\ninterpretation or false attributions. Formally, we\ndenote the decontextualized meaning of the out-put spans in the context of the complete output as\nI(o1, . . . , o n|o). The source spans should express\nthe decontextualized meaning of the output spans,\nconcat (s1, . . . , s m)|=I(o1, . . . , o n|o).\n4 LAQuer Modeling Framework\nThe LAQuer setting, as defined above, inherently\ninvolves two processing stages, illustrated in Fig. 2.\nIn the first stage, a source-grounded generation sys-\ntem generates a user-requested text, such as a sum-\nmary or a long-form answer to a question, based\non provided documents. This system may also\ninclude attribution metadata, mapping output seg-\nments to supporting source segments. For example,\nin Fig. 1, the generation system could output the\nsentence-level attribution underlined green. In our\nexperiments (Section 5), we evaluate LAQuer using\nthree generation methods: one without attributions\nand two recent attributed-generation approaches.\nIn the second stage, users who read the generated\ntext can request localized attribution for specific\nfacts by highlighting relevant spans. The LAQuer\ntask then identifies the exact supporting source\nspans for the highlighted facts. Specifically, during\nstage 2, the LAQuer input consists of the follow-\ning: (1) the source documents, based on which the\noutput text was generated; (2) the generated output\ntext; (3) the attribution metadata (if available); (4)\nthe output spans highlighted by the user, which are\nassumed to correspond to a particular fact in the\noutput text, for which attribution is sought.\nGiven these inputs, our proposed LAQuer\nmethod first performs a decontextualization step\n(A), which converts the input highlights into a co-\nherent standalone sentence. Next, in the attribution\nstep (B), we search for the supporting source spans\nthat provide evidence for the decontextualized state-\nment, where we explore two alternative methods\nfor this step (prompt- and internals-based). This\nstep leverages the attribution metadata from the\ngeneration step, if available, while also incorporat-\ning the extended highlights. These two steps are\ndescribed in detail below.\n4.1 Generating a Decontextualized Output\nStatement\nAs described in Section 3, a user’s query consists of\ncontextualized spans extracted from the output that\ndepend on the surrounding text for full comprehen-\nsion (e.g., the word “consumers” in Fig. 1). Step\n(A) of our method reformulates the selected spans\n--- Page 5 ---\nHighlighted output sentence Decontexutalized Fact\nThe Los Angeles County Fire Department responded to multiple 911calls\naround 4:30 p.m. at Penn Park, where the tree had toppled, trapping up to 20\npeople beneath its branches.The 911 calls were made around 4:30.\nThe confirmation hearings for Supreme Court nominee Brett Kavanaugh . . .Key\nissues included his views on presidential power , abortion rights, and potential\nconflicts of interest regarding the Russia investigation.Key issues included Brett Kavanaugh’s\nviews on presidential power.\nTable 1: Examples illustrating our decontextualization step, drawn from Gunjal and Durrett (2024). Initially,\nLAQuer highlights ( bold ) are reformulated into decontextualized facts ( →). These facts are subsequently aligned\nwith revised highlights ( ←, underlined ), to allow sentence-level attribution to incorporate additional context when\nneeded. For example, in the second row, the mention of “Brett Kavanaugh” originates from a separate sentence,\nrequiring the inclusion of additional source text to ensure accurate attribution.\ninto a self-contained decontextualized sentence, for\nwhich source attribution can be more easily sought\nin an unambiguous manner. We use the approach\nfrom Gunjal and Durrett (2024), as exemplified in\nTable 1.\nThis process may incorporate in the decontextu-\nalized statement additional phrases from the gen-\nerated output text, beyond the user’s initial high-\nlights. For example, replacing the ambiguous “they”\npronoun with the explicit “consumers” mention in\nFig. 2, highlighted orange. Consequently, the ob-\ntained decontextualized statement includes all the\ninformation for which attribution should be identi-\nfied within the source texts. If the query remains\ncontextualized, this key information may be omit-\nted, resulting in inaccurate attribution. By includ-\ning the additional output span, the attribution used\nfor the first sentence would be included, ensuring\ncomprehensive coverage of the relevant content.\nFor more details, see Appendix E.\n4.2 Query-focused Attribution\nStep (B) of our LAQuer method attributes the de-\ncontextualized sentence to the source texts, ensur-\ning factual consistency while minimizing the re-\ntrieval of irrelevant spans. The effectiveness of\nthis step depends on the generation method, par-\nticularly whether attribution metadata is available.\nSentence-level attribution approaches, which pro-\nvide fixed links between source and output spans,\nsignificantly reduce the search space, facilitating\nthe localization of supporting evidence. In con-\ntrast, for non-attributed generation, the system must\nsearch the entire source document, increasing com-\nputational complexity.\nFor this step, we explore two approaches: one\nuses an LLM prompt while the other leverages the\nmodel’s internal representations to identify align-ments based on hidden state similarities (Phukan\net al., 2024).\nLLM-based Prompt Alignments. Leveraging\nthe strong few-shot learning and reasoning capabil-\nities of LLMs, we prompt an LLM to output the\naligned spans. The prompt is listed in Fig. 4. At-\ntributed source spans are separated by a semicolon\n(;). If a span does not match the source text, we\napply a fuzzy search.2If the fuzzy search fails, we\nretry the prompt up to five times. If that also fails,\nwe fall back to the original attribution provided by\nthe attribution metadata, if available. Otherwise,\nwe use all the source documents for attribution.\nLLM-based Internals Alignments. Another\nstrategy for achieving granular attribution is to\ncompute the cosine similarity between the contex-\ntual hidden state representations of the source to-\nkens and the output tokens (Dou and Neubig, 2021;\nPhukan et al., 2024). Phukan et al. (2024) has been\nshown to surpass GPT-4-based prompting meth-\nods in terms of accuracy, but was only evaluated\nin paragraph-level citations. In this work, we in-\nvestigate its usefulness in LAQuer settings.3Com-\npared to the previous LLM prompt-based approach,\nthis method requires direct access to the model’s\nweights, necessitating the use of open models.4\n5 Experimental Setup\nWe evaluate the efficacy of our proposed framework\nin Section 4 by benchmarking multiple baseline\nmethods for each stage in the process. We design\nan experimental setup that assesses both the qual-\nity of generated outputs and the accuracy of their\n2https://github.com/google/diff-match-patch\n3We re-implemented Phukan et al. (2024), as no source\ncode was available.\n4For more details on both approaches, see Appendix A.\n--- Page 6 ---\nOutput sentence Example decomposed fact\nExposing students to texts from different religions can be beneficial for their\nlearning, as it helps them understand the development and advancement of\nsocieties, promoting understanding , respect, and fellowship.Exposing students to texts from different\nreligions promotes understanding.\nGuns are rarely used in self-defense, are frequently stolen and used by criminals,\nand their presence makes conflicts more likely to become violent ; armed\ncivilians are unlikely to stop crimes and may make situations more deadly.The presence of gun make conflict more\nlikely to become violent.\nTable 2: Example synthesized LAQuer inputs, simulating a user highlighting the output. First, output sentences are\ndecomposed into atomic facts ( →). Then, these facts are aligned back to highlights ( ←), denoted in bold .\nFigure 3: Distribution of span types based on syntactic\ncomplexity.\nattributions. Our evaluation consists of automatic\nassessments on two key content-grounded genera-\ntion tasks: Multi-Document Summarization (MDS)\nand Long-Form Question Answering (LFQA).\nThis section provides the foundation for bench-\nmarking LAQuer and examining its effectiveness\nin reducing cognitive load while preserving factual\nconsistency. We first introduce the datasets used\nin our experiments and describe the methodology\nfor synthesizing attribution queries to simulate user\nfact-checking behavior (Section 5.1). Then, we\ndescribe our evaluation framework (Section 5.3),\nwhich measures the quality of the attribution under\ncontextualized and decontextualized conditions.\n5.1 Datasets\nOur benchmark includes both a multi-document\nsummarization setting (MDS) and a long-form QA\nsetting setting (LFQA). Both are content-grounded\nsettings such that source documents are used to\ngenerate an output. Specifically, we use SPARK\n(Ernst et al., 2024) for MDS,5and the RAG-based\ndataset curated by Liu et al. (2023) for LFQA.6\nSynthesizing LAQuer Highlights for a Given\nOutput. The source documents are used to gener-\nate outputs with attribution metadata, as described\n5SPARK is a subset of Multi-News (Fabbri et al., 2019)\n6Statistics and more details are provided in Appendix C.in Section 5.2. Given the outputs generated, we\nsynthesize LAQuer inputs by simulating the user’s\nprocess of highlighting relevant spans.\nOur approach for generating highlights involves\nfirst decomposing each output sentence into atomic\nfacts and then aligning these facts with the output\nsentence, exemplified in Table 2. To ensure our\ndecomposition method closely mimics how users\nselect contextualized spans, we adopt the contextu-\nalized decomposition approach from FActScore,\nwhich was specifically designed to break down\nlong-form generations into atomic facts (Min et al.,\n2023). We use GPT-4o (OpenAI, 2024) for the de-\ncomposition. In order to align the generated output\nfacts with the output, we use a naive lexical-based\nalgorithm, described in Appendix E.\nOur process for synthesizing facts results in an\naverage of 2.6 facts per sentence. For each instance,\nwe sample ten facts extracted from the entire output.\nWe report the distribution of facts according to their\nsyntactic complexity as a measure of how diverse\nthe generated facts are. Specifically, we use the\nfollowing categories:\n1.Phrase : A span consisting of syntactic con-\nstituents without a complete clause structure\n(i.e., no finite verb or predicate). Example\nspans include: “Kavanaugh past writings” ,\n“A technical glitch” .\n2.Simple Clause : Contains at least one fi-\nnite verb. Example spans: “Judge Brett Ka-\nvanaugh faced intense scrutiny” ,“His previ-\nous escape occurred in 2005” .\n3.Complex Sentence : Contains at least one em-\nbedded or subordinate clause and explicit dis-\ncourse connectives. Example: “Peach trees\nshould be planted while they are dormant” .\nAs illustrated in Fig. 3, the majority of extracted\nfacts are simple clauses (roughly two-thirds), fol-\n--- Page 7 ---\nlowed by phrases (about one-third), with complex\nsentences making up only a small proportion.7\n5.2 Generation Baselines\nFollowing our suggested framework in Section 4,\nwe benchmark three baseline methods for the first\ngeneration stage, from methods that provide no\nattribution to those that provide fine-grained attri-\nbution. Full details for the following methods are\nprovided in Appendix B.\nVanilla. We include a naive baseline that gener-\nates text without attribution, as this represents a\ncommon approach in many real-world applications\nwhere attribution is not explicitly modeled. Evalu-\nating this baseline allows us to measure the extent\nto which LAQuer methods can provide correct at-\ntribution on the entire source documents.\nALCE. Gao et al. (2023b) is a prominent attribu-\ntion method that prompts the LLM to add citations\nat the end of each output sentence, in the form of\nsquare bracket, such as “. . . [1].” This method pro-\nvides a fairly coarse-grained attribution, as citations\npoint to an entire source document.\nAttr. First. Slobodkin et al. (2024) divide the\ngeneration process into multiple explicit steps, al-\nlowing the attribution to be traced back to source\nspans. The first step, content selection, involves\nhighlighting relevant source spans. The generation\nis then constrained to these selected spans, allow-\ning the output to be tied back to the source. Unlike\nALCE, which attributes at the document level, this\napproach attributes source spans, significantly re-\nducing the costs associated with LAQuer while\nincreasing the number of tokens required for gener-\nating the initial output. We analyze this trade-off\nin Section 6.3.\n5.3 Evaluation\nOur evaluation is comprised of different metrics\nfor the quality of the output, following standard\npractices of each task, as well as the quality of the\ncitations, adapted to the LAQuer setting. The pur-\npose of measuring output quality is to show that\noverall methods that support localized attribution\ndo not hurt output quality with respect to relevance.\nWe incorporate both automated and human evalua-\ntions into our methodology.\n7Categorization was performed using the SpaCy NLP\ntoolkit: https://spacy.io/Automatic evaluation To evaluate the quality\nof the output, we follow Slobodkin et al. (2024)\nand calculate Rouge-L (Lin, 2004) and BertScore\n(Zhang* et al., 2020), which were also used in their\nstudy. Additionally, we calculate METEOR (Baner-\njee and Lavie, 2005) and BLEURT20 (Sellam et al.,\n2020). Rouge-L and METEOR utilize n-gram\ncomparisons, while BertScore and BLEURT20\nare based on language models. All these metrics\ncompare the generated output to a reference out-\nput. Lastly, we include a fluency metric based on\nMAUVE (Pillutla et al., 2021), which compares the\ndistribution of the output to that of the reference\ntexts.\nTo evaluate LAQuer citations, we sample ten\nfacts from the facts extracted from the output, as\ndescribed in Section 5.1. We then calculate Au-\ntoAIS (Gao et al., 2023a), which is an entailment\nmetric commonly used for evaluating attribution.\nThe metric outputs binary classification of whether\nan attributed source text supports an output fact,\nwhich is then averaged across all output facts to cal-\nculate the final score. Following Gao et al. (2023a),\nwe make the distinction between evaluating entail-\nment with contextualized facts and decontextual-\nized facts. We source contextualized facts from the\nprocess described in Section 5.1, and decontextual-\nized facts from the process described in Section 4.2.\nAdditionally, we measure the attributed text\nlength in content words8to confirm that our method\nsignificantly reduces unnecessary reading. Lastly,\nwe report the percent of non-attributed facts.9\n6 Results and Analyses\n6.1 Main Results\nThe output quality metrics are reported in Table 3.\nATTR. FIRST outperforms other methods in terms\nofROUGE -L,METEOR and MAUVE , while\nthe Vanilla generation outperforms in terms of\nBLEURT-20 . This suggests that ATTR. FIRST\nhas more lexical overlap with the reference outputs,\nwhile the Vanilla generation is more semantically\nsimilar. In general, both methods achieve similar\noutput quality results, with ALCE lagging behind.\nThe citations quality metrics are reported in Ta-\nble 4. We make the following observations.\nLAQuer methods significantly and attractively\nreduce the length of the attributed text. Across\n8Excluding stop-words https://nltk.org\n9More details are provided in Appendix C.2.\n--- Page 8 ---\nMethod R-L ↑ METEOR ↑BERTSCORE↑BLEURT-20 ↑MAUVE ↑MDSVANILLA 19.2±0.6 28.3 86.4 ±0.2 43.0±0.7 59.8\nALCE 19.4 ±0.6 27.3 86.1 ±0.2 38.2±0.8 63.7\nATTR. FIRST 21.1±0.7 29.7 86.6 ±0.2 41.1±0.9 84.9LFQAVANILLA 37.2±3.2 45.6 90.7±0.6 60.5±1.7 81.5\nALCE 34.4 ±2.7 44.3 90.1 ±0.5 56.8±1.7 90.6\nATTR. FIRST 38.2±2.7 46.1 90.6±0.6 58.5±1.8 96.7\nTable 3: Generated text quality results, averages include standard error of the mean.MDSMethod A UTOAIS C ON.↑AUTOAIS D ECON .↑ LENGTH ↓ NONATT. (%)↓\nVANILLA 82.2±1.6 84.5±2.0 1681.6 ±205 .5 0.0\nLLM Prompt 62.5 ±2.0 49.7±2.5 32.0±1.8 0.0\nLLM Internals 18.0 ±1.7 13.1±1.5 28.1±0.9 0.0\nALCE 67.4 ±2.3 74.8±2.3 979.1 ±117 .8 5.2±0.8\nLLM Prompt 55.8 ±2.2 44.3±2.4 41.6±3.4 5.2±0.8\nLLM Internals 15.5 ±1.6 10.2±1.5 29.9±8.2 8.2±1.2\nATTR. FIRST 80.3±2.2 58.0±2.8 33.0±2.4 0.4±0.2\nLLM Prompt 71.5±2.3 42.4±2.4 14.6±0.5 0.4±0.2\nLLM Internals 28.6 ±2.4 13.2±1.7 12.2±0.4 21.4±0.9LFQAVANILLA 69.5±4.6 71.0±4.5 4636.8 ±488 .3 0.0\nLLM Prompt 65.1 ±3.8 65.4±4.3 38.1±2.6 0.0\nLLM Internals 19.0 ±2.8 18.0±2.4 24.9±1.5 0.0\nALCE 50.8 ±4.8 55.6±5.1 2346.0 ±300 .2 13.8±4.2\nLLM Prompt 56.8 ±4.0 52.8±3.9 42.0±10.9 13.8±4.2\nLLM Internals 13.0 ±2.4 12.8±2.4 26.6±1.5 17.1±3.0\nATTR. FIRST 88.0±3.4 83.9±3.3 43.3±2.4 0.0\nLLM Prompt 83.0±3.1 69.6±4.3 17.3±0.8 0.0\nLLM Internals 46.6 ±4.0 37.8±4.4 14.3±0.7 7.0±1.7\nTable 4: LAQuer citation results, averages include standard error of the mean. We separately calculate AutoAIS for\ncontextualizd (Con.) and decontextualized (Decon.) output facts.\n indicates LAQuer methods and yellow indicates\nthe best LAQuer method. Non Attributed measures the percentage of facts without attribution.\nMDSMethod AIS D ECON .↑\nVANILLA 91.5±2.3\nLLM Prompt 39.0±4.2\nATTR. FIRST 54.3±4.4\nLLM Prompt 31.6 ±3.8LFQAVANILLA 90.9±5.1\nLLM Prompt 59.5±7.7\nATTR. FIRST 53.6±8.6\nLLM Prompt 50.2 ±9.2\nTable 5: LAQuer human evaluation of citation results,\naverages include standard error of the mean.\n indicates\nLAQuer methods.\nall methods, LAQuer reduces attribution length by\ntwo orders of magnitude for Vanilla and ALCE, and\nby an average of 59% for ATTR. FIRST . For exam-\nple, in the Vanilla setting, which does not rely on a\nparticular generation method but does not provide\nany attribution, LAQuer attribution can direct the\nuser to correct highly localized supporting spans innearly two thirds of the cases.\nThe LLM prompt is the best-performing LA-\nQuer method. In all generation methods, we find\nthat the LLM prompt performs the best in terms of\nAutoAIS, significantly surpassing the LLM inter-\nnals method. This is true for both MDS and LFQA\nsettings. The LLM internals method has low per-\nformance across all generation methods. The best\nresults for the LLM internals are achieved when the\nsource is localized with ATTR. FIRST , suggesting\nthat it struggles with localization of document-level\ntexts. In addition, when the LLM internals method\nis applied on top of source-localized attribution\nmethods, ALCE and ATTR. FIRST , we observe an\nincrease in non-attributed output words.\nLAQuer methods can leverage localized attribu-\ntions provided by ATTR. FIRST .Even without\napplying LAQuer, ATTR. F IRST provides very\nconcise sentence-level attribution, averaging only\n36 characters. This means that the localized sup-\n--- Page 9 ---\nport for the LAQuer fact needs to be identified\nonly within a quite short span. Consequently, the\nstrong performance of ATTR. FIRST carries over\nto LAQuer. In the contextualized setting, ATTR.\nFIRST is the top-performing LAQuer method, indi-\ncating that LAQuer methods can leverage initially\nlocalized attributions provided by the generation\nmethod itself. However, in the decontextualized\nsetting, ATTR. FIRST yields notably low AutoAIS\nscores, as low as 58 for MDS, and 53.6 for LFQA\nin our manual evaluation, described in Section 6.2.\nThese low scores limit the effectiveness of LA-\nQuer methods, since the necessary evidence for the\ndecontextualized facts is absent from the original\nATTR. FIRST provided spans. We hypothesize that\nthis degredation stems from ATTR. FIRST failure\nto decontextualize its attributions. This suggests\nthat when generating attributions for localized out-\nput segments, it is crucial to first decontextualize\nthese output spans, and accordingly to make sure\nto support also the decontextualizing information\nwithin the source attributions.\n6.2 Human Analysis\nTo further assess our findings, we report a small-\nscale human annotation conducted by the authors\nusing our most promising methods. We annotated\n20 examples per task, each for the Vanilla and\nATTR. FIRST methods, both with and without LA-\nQuer, resulting in 80 examples per task (160 in to-\ntal). For each example, we calculate AIS (Rashkin\net al., 2023) at the decontextualized fact-level. For\nAIS, similar to the AutoAIS metric, the annotator\nis asked to make a binary classification of whether\nan output fact is supported by the attributed source\ntexts; we then average classifications across all out-\nput facts to calculate the final score.\nOur results are reported in Table 5. In accor-\ndance with our main results in Table 4, we find\nthat LAQuer methods struggle with decontextual-\nized facts. From this analysis, we observe that the\nmodel often omits the document’s broader theme.\nFor example, in Table 11, the LLM prompt method\ncorrectly attributes multiple “issues”, yet it fails to\nattribute “Supreme Court”.\n6.3 Cost Analysis\nWe provide the average size of prompts in Table 7.\nOn one hand, we find that LAQuer prompts in\nATTR. FIRST are an order of magnitude smaller\nthan in Vanilla generation. On the other hand,\nATTR. F IRST generation is costly, inducing anincrease of 90% in prompt length compared to\nVanilla generation, as reported by Slobodkin et al.\n(2024). These results suggest that increased com-\nputational cost during generation can lead to more\nefficient LAQuer methods.\n6.4 Estimate for LAQuer Localization\nTo better understand the potential benefits of LA-\nQuer, we estimate the average amount of text\nrequired to support an output fact or sentence.\nWe compare this across different levels of source\ngranularity, including source spans, source sen-\ntences, and entire source documents. For this anal-\nysis, we utilize the SPARK dataset (Ernst et al.,\n2024), which is used in our study and contains\nfine-grained, human-annotated attribution.\nSource granularity Output facts Output sentences\nSpans 128.0 231.4\nSentence 278.5 485.1\nDocument 4679.5 7226.6\nTable 6: Analysis of attribution lengths (measured in\ncharacters) with varying granularities, based on the\nSPARK dataset (Ernst et al., 2024).\nOur analysis, summarized in Table 6, presents\nthe average number of characters to read under\ndifferent attribution granularities. LAQuer operates\nat both the source and output fact levels, requiring\nan average of 128 characters to read. In contrast,\nATTR. FIRST attributes at the output sentence level\nwith source spans, resulting in an average of 278.5\ncharacters. This finding highlights the benefits of\nlocalizing attribution per output fact, reducing the\ntext users need to read by 54%.\n7 Conclusion\nIn this work, we introduce a novel motivation for\npost-hoc attributed text generation, enabling users\nto create localized attribution queries, LAQuer. We\nintroduce a challenging benchmark, which sub-\nsumes existing attribution methods by considering\nboth the generation and post-hoc steps. Our results\nshow that LAQuer methods significantly reduce\nattribution length, but LAQuer attribution remains\na challenging task for decontextualized facts. In ad-\ndition, our methods are associated with a high cost\nof LLM calls, suggesting future research should fo-\ncus on creating more efficient frameworks. Lastly,\nthere is a performance gap between different gen-\neration methods.\n--- Page 10 ---\nLimitations\nAddressing attribution queries increases computa-\ntional cost on top of fixed sentence-level or token-\nlevel attribution. In Section 6.3, we discuss the\ntrade-off between computational cost during gener-\nation and that during attribution.\nWhile our work is focused on content-grounded\ngeneration, LAQuer could be applied to outputs\ngenerated by the model’s parametric knowledge,\nby retrieving the documents after the generation\nrather than before. We leave such exploration for\nfuture work.\nAutoAIS is used as a key metric for evaluating\nattribution quality, which is an LLM-based auto-\nmated metric. We conducted a small-scale human\nanalysis to support these results in Section 6.2, find-\ning similar trends.\nEthical Considerations\nThe ability to attribute outputs of LLMs to specific\nsources is crucial for transparency, accountability,\nand trust in AI-generated content. Our work con-\ntributes to this goal by simplifying the attribution\nprocess for users and making it more localized.\nHowever, errors in attribution can mislead users\ninto assuming a stronger or weaker connection be-\ntween the generated content and its source than\nwhat actually exists.\nWe utilized AI-assisted writing tools during the\npreparation of this paper to improve clarity and\ncoherence. However, all content was carefully re-\nviewed and edited by the authors to ensure accu-\nracy.\nAcknowledgements\nWe would like to thank our reviewers for their\nconstructive suggestions and comments. This\nwork was supported by the Israel Science Founda-\ntion (grant no. 2827/21), NSF-CAREER Award\n1846185, and NSF-AI Engage Institute DRL-\n2112635.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha-\nroni, Daniel Andor, Livio Baldini Soares, Massimil-\niano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,\nJonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma,\nJianmo Ni, Lierni Sestorain Saralegui, Tal Schus-\nter, William W. Cohen, Michael Collins, Dipanjan\nDas, Donald Metzler, Slav Petrov, and Kellie Webster.\n2023. Attributed question answering: Evaluation\nand modeling for attributed large language models.\nPreprint , arXiv:2212.08037.\nJan Buchmann, Xiao Liu, and Iryna Gurevych. 2024.\nAttribute or abstain: Large language models as long\ndocument assistants. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 8113–8140, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm,\nTom Kwiatkowski, Dipanjan Das, and Michael\nCollins. 2021. Decontextualization: Making sen-\ntences stand-alone. Transactions of the Association\nfor Computational Linguistics , 9:447–461.\nBenjamin Cohen-Wang, Harshay Shah, Kristian\nGeorgiev, and Aleksander Madry. 2024. Contextcite:\nAttributing model generation to context. In The\nThirty-eighth Annual Conference on Neural Infor-\nmation Processing Systems .\nQiang Ding, Lvzhou Luo, Yixuan Cao, and Ping Luo.\n2024. Attention with dependency parsing aug-\nmentation for fine-grained attribution. Preprint ,\narXiv:2412.11404.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment\nby fine-tuning embeddings on parallel corpora. In\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (EACL) .\nOri Ernst, Ori Shapira, Aviv Slobodkin, Sharon Adar,\nMohit Bansal, Jacob Goldberger, Ran Levy, and Ido\nDagan. 2024. The power of summary-source align-\nments. In Findings of the Association for Compu-\ntational Linguistics: ACL 2024 , pages 6527–6548,\nBangkok, Thailand. Association for Computational\nLinguistics.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 1074–1084, Florence, Italy. Asso-\nciation for Computational Linguistics.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\nKelvin Guu. 2023a. RARR: Researching and revis-\ning what language models say, using language mod-\nels. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 16477–16508, Toronto, Canada.\nAssociation for Computational Linguistics.\n--- Page 11 ---\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023b. Enabling large language models to generate\ntext with citations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing , pages 6465–6488, Singapore. Associa-\ntion for Computational Linguistics.\nAnisha Gunjal and Greg Durrett. 2024. Molecular facts:\nDesiderata for decontextualization in LLM fact veri-\nfication. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2024 , pages 3751–3768,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 3905–3920, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems , volume 33, pages 9459–\n9474. Curran Associates, Inc.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nNelson Liu, Tianyi Zhang, and Percy Liang. 2023. Eval-\nuating verifiability in generative search engines. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023 , pages 7001–7025, Singapore.\nAssociation for Computational Linguistics.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022. Teaching language models to support answers\nwith verified quotes. ArXiv , abs/2203.11147.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\nHannaneh Hajishirzi. 2024. Fine-grained halluci-\nnation detection and editing for language models.\nArXiv , abs/2401.06855.OpenAI. 2024. Gpt-4o system card. Preprint ,\narXiv:2410.21276.\nAnirudh Phukan, Shwetha Somasundaram, Apoorv Sax-\nena, Koustava Goswami, and Balaji Vasan Srinivasan.\n2024. Peering into the mind of language models: An\napproach for attribution in contextual question an-\nswering. In Findings of the Association for Compu-\ntational Linguistics: ACL 2024 , pages 11481–11495,\nBangkok, Thailand. Association for Computational\nLinguistics.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In NeurIPS .\nJirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna\nBisazza. 2024. Model internals-based answer attribu-\ntion for trustworthy retrieval-augmented generation.\nInProceedings of the 2024 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n6037–6053, Miami, Florida, USA. Association for\nComputational Linguistics.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics , 11:1316–1331.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2023. Measuring attribution in natural lan-\nguage generation models. Computational Linguistics ,\n49(4):777–840.\nTal Schuster, Adam Lelkes, Haitian Sun, Jai Gupta,\nJonathan Berant, William Cohen, and Donald Met-\nzler. 2024. SEMQA: Semi-extractive multi-source\nquestion answering. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers) , pages\n1363–1381, Mexico City, Mexico. Association for\nComputational Linguistics.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation. In Proceedings of ACL .\nAviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster,\nand Ido Dagan. 2024. Attribute first, then generate:\nLocally-attributable grounded text generation. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 3309–3344, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam M. Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,\nYu Du, Yaguang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping\n--- Page 12 ---\nMethod Input Length Output LengthMDSVANILLA 25674.7 ±396 .7 227.7 ±6.6\nALCE 22239.6 ±279 .5 214.5 ±6.8\nATTR. FIRST 2843.0 ±7.3 89.3±2.0LFQAVANILLA 58299.8 ±1031 .1 232.4 ±8.0\nALCE 45104.4 ±826 .6 200.9 ±8.1\nATTR. FIRST 3025.4 ±7.7 107.2 ±3.4\nTable 7: Average number of characters in the LLM\nprompt LAQuer method, including standard error of the\nmean.\nHuang, Maxim Krikun, Dmitry Lepikhin, James\nQin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen,\nAdam Roberts, Maarten Bosma, Yanqi Zhou, Chung-\nChing Chang, I. A. Krivokon, Willard James Rusch,\nMarc Pickett, Kathleen S. Meier-Hellstern, Mered-\nith Ringel Morris, Tulsee Doshi, Renelito Delos San-\ntos, Toju Duke, Johnny Hartz Søraker, Ben Zeven-\nbergen, Vinodkumar Prabhakaran, Mark Díaz, Ben\nHutchinson, Kristen Olson, Alejandra Molina, Erin\nHoffman-John, Josh Lee, Lora Aroyo, Ravi Rajaku-\nmar, Alena Butryna, Matthew Lamm, V . O. Kuzmina,\nJoseph Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Mar-\nian Rogers Croak, Ed H. Chi, and Quoc Le. 2022.\nLamda: Language models for dialog applications.\nArXiv , abs/2201.08239.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations .\nA LAQuer Methods Details\nIn this section, we provide a full description of the\nLAQuer methods used, described in Section 4.\nA.1 LLM Prompt\nThe prompt is provided in Fig. 4. The average size\nof prompts is reported in Table 7. We use GPT-4o\n(OpenAI, 2024). In our experiments, we include\nthree in-context examples sourced from the dev\nsplit of the corresponding datasets. We manually\noptimized the prompt instructions and few-shot\nexamples based on iterations on the development\nset.\nA.2 LLM Internals\nOur LLM-based internals method is based on the\nmethod by Phukan et al. (2024). Since this method\nrequires access to the weights of the model, we\nrunLLAMA -3.1-8 B-INSTRUCT on a single A100-\n80GB GPU for approximately 8 hours. More run-\nning time details are available in Table 8.\nWe now provide a short description of this work,\nand the adaptation we made to support the LAQuerYou are provided with an output sentence and\nthe source texts from which it was generated.\nYou need to identify spans in the source\nfrom which the output sentence was generated.\nCopy verbatim the attributing source spans,\nand use a semicolon (;) as a delimiter\nbetween each consecutive span. The output\nsentence should be fully supported by the\nconcatenation of the attributed source spans.\nIMPORTANT: Each span must be verbatim copied\nfrom the corresponding sources. Do not make\nany changes or paraphrases to the source spans.\nIf necessary, you may copy multiple spans\nfrom the same or source, but avoid adding\nun-necessary spans and keep each span as short\nas possible.\nInput:\nSource 1: Voters in 11 states will pick their\ngovernors tonight Republicans appear on track\nto increase their numbers by at least one,\nand with the potential to extend their hold\nto more than two-thirds of the nation’s top\nstate offices\n...\nOutput: There is a race for the governor’s\nmansion in 11 states today.\nAttribution: Voters in 11 states will pick\ntheir governors tonight\nFigure 4: Example prompt for LLM-based post-hoc\nalignment. The instructions are depicted in green, input\nto the model in black, and model’s output in red. This\nexample is one of three few-shot examples. The source\ntexts of the few-shot examples are adapated based on\nthe generation method: Vanilla includes all documents,\nALCE includes relevant documents, and ATTR. FIRST\nincludes relevant source spans.\nsetting. The method proposed by Phukan et al.\n(2024) is based on the idea that LLMs have inher-\nent awareness of the document parts they use while\ngenerating answers. They claim that it is likely\ncaptured by the hidden states of the LLM. Accord-\ningly, their method includes creating a prompt that\nconcatenates the query q, the documents D, and the\noutput o, and then feeds this to a LLM in a single\nforward pass. This creates the hidden representa-\ntions of the text.\nFormally, the prompt is denoted P, such that\nP=q+D+o, where ‘+’ denotes concatena-\ntion. Also, the hidden layer representation of token\nti∈Pfor layer lof the model is denoted hl\ni. The\nattribution process is then composed of two sub-\ntasks:\nSub-Task 1: Identification of extractive answer\ntokens An important claim made in their paper\n--- Page 13 ---\nMethod Avg. time (sec.)MDSVANILLA 0.6±0.0\nALCE 4.2 ±0.2\nATTR. FIRST 8.6±0.6LFQAVANILLA 0.7±0.0\nALCE 20.1 ±2.1\nATTR. FIRST 54.1±4.0\nTable 8: Average time of the LLM internals LAQuer\nmethod, including standard error of the mean.\nis that not all tokens should be attributed, because\nsome tokens are ‘glue’ tokens created by the LLM.\nThis task involves identifying extractive tokens,\nwhich are tokens that originate from the source\ndocuments, usually verbatim.\nFormally, a token oi∈ois an extractive token\nif there exists a token dj∈Dsuch that the co-\nsine similarity between hl\niandhl\njis greater than a\nthreshold θ.\nIn our work, we use the threshold θ= 0.7and\nlayer l= 5, which achieves the highest F1 scores\nbased on their paper. In addition, as formalized in\nSection 3, we only look at output spans o1, . . . , o n\nprovided as input, and not the entire output o.\nSub-Task 2: Attribution of extractive answer\nspan S Given an output span Swith tokens\no1, . . . , o m⊆o, compute the average hidden layer\nrepresentation hSfor each token oi∈Sas:\nhs=1\nnnX\ni=1hl\ni\nNext, hsis used to identify anchor tokens in D.\nAnchor tokens, denoted DT, are the tokens most\nsimilar to the output span S. This is calculated\nfor each document token dj∈Das the cosine\nsimilarity between hSandhl\nj. For each anchor\ntoken da∈DT, a window of tokens around dais\nexplored, up to a length L. For each window, an\naverage representation is calculated and the highest\nranked window is considered the attribution for S.\nIn our work, we use L= 30 .\nB Generation Methods Details\nIn this section, we provide a full description of the\ngeneration methods used, described in Section 4.\nAs as a pre-processing step, we first decontex-\ntualize the output spans. We use the decontextual-\nization prompt from MolecularFacts (Gunjal and\nDurrett, 2024), which takes the concatenated out-\nput spans as input, together with the entire outputas context, and outputs decontextualized facts. We\nused the original MolecularFacts prompt and ran it\nwith GPT-4o. The resultant decontextualized fact\nis then mapped back to the output, as described in\nAppendix E.\nB.1 ALCE\nGao et al. (2023b) introduced the idea of allowing\nLLMs to generate citations together with the output.\nWe use the same prompt as the original paper with\ntwo few-shot examples and T= 0.5, following\nSlobodkin et al. (2024).\nB.2 Attr. First\nSlobodkin et al. (2024) decompose the generation\nprocess into multiple explicit steps, allowing for\nprecise attribution tracing. The first step, content se-\nlection, involves highlighting relevant source spans.\nThe second step, sentence planning, consists of\nclustering spans for each sentence, followed by\nsentence generation based on the clustered infor-\nmation. Each new sentence is generated with condi-\ntioning on the previously generated sentences. We\nadopt the same prompt and few-shot demonstration\nexamples as used in the original paper. Among\nthe multiple variants of ATTR. FIRST , we select\nATTR. FIRST CoT, which the paper identifies as the\nbest-performing variant.\nC Experimental Setup Details\nC.1 Datasets\nOur benchmark includes both a multi-document\nsummarization setting (MDS), as well as a long-\nform QA setting (LFQA). Both are content-\ngrounded settings such that the source texts are\nused to generate an ouput. Specifically, we use\nSPARK (Ernst et al., 2024) for MDS, and the RAG-\nbased dataset curated by Liu et al. (2023) for LFQA.\nWe used the same split of the datasets created by\nSlobodkin et al. (2024). The datasets sizes are\nprovided in Table 9. Both datasets are in English.\nThe licenses for the datasets are following: Ernst\net al. (2024) CC BY-SA 4.0, Liu et al. (2023) MIT\nlicense.\nC.1.1 Synthesizing Attribution Queries\nFollowing Section 5.1, we provide more details\nabout the decomposition of an output text to output\nfacts. We first split the output into sentences.10\n10using spaCy https://spacy.io/\n--- Page 14 ---\nTask Dataset Dev Test\nMDS SPARK (E RNST ET AL ., 2024) 45 65\nLFQA E VALUATING (LIU ET AL ., 2023) 44 45\nTable 9: Datasets sizes used in our benchmark for devel-\nopment and evaluation.\nFor each output sentence, we then run a prompt de-\ncomposing the output into atomic facts. FActScore\n(Min et al., 2023) is an LLM-based method used\nto breakdown a sentence into atomic facts. It is a\nprompt comprised of instructions and multiple few-\nshot examples. We used the original FActScore\nprompt and run it with GPT-4o. The resultant fact\nis then mapped back to the output, as described in\nAppendix E.\nC.2 Evaluation\nFor calculating AutoAIS, we use the model\nGOOGLE /T5_XXL_TRUE _NLI_MIXTURE (Hon-\novich et al., 2022), which is trained on NLI datasets\nand has been used in previous work to analyze attri-\nbution (Gao et al., 2023a; Slobodkin et al., 2024). It\ncorrelates well with AIS scores (Gao et al., 2023a).\nD Attribution Metadata Details\nIllustrated in Fig. 2, we suggest that some gen-\neration methods can provide attribution metadata.\nIn this section, we discuss the attribution meta-\ndata provided by the ATTR. FIRST method. Each\nsentence-level localized attribution is composed of\none or more records, each consisting of the fol-\nlowing information: output sentence idx, source\nfile ID, and a list of source character offsets. For\nexample, ‘<0, doc_1.txt, [[17367, 17562]]>‘. In\ncomparison to non-localized attribution, such as\nthe ALCE method, this requires one additional col-\numn for offsets. We analyze the average storage\nrequired for saving sentence-level attribution per\noutput. Our analyses show that it requires 2Kb on\naverage per attributed output, totalling in a fairly\nsmall increase of 700 bytes per attributed output.\nE Alignment of Facts to Spans\nThroughout our work, we extracted facts from the\noutput text and later needed to map them back to\ntheir corresponding spans. In this section, we de-\nscribe the algorithm used to align extracted facts\nwith the original output text.\nThe first application of this alignment process is\nin our evaluation methodology, where we decom-pose each output sentence into atomic facts using\nan LLM, as detailed in Section 5.1. For instance,\nconsider the sentence “Exposing students to texts\nfrom different religions promotes understanding”\nfrom Table 2. To simulate a user’s highlight, we\nneed to align these atomic facts with spans in the\noutput, providing the necessary spans for the LA-\nQuer method. In this example, the aligned highlight\nwould be “exposing students to texts from different\nreligions . . . promoting understanding.”\nThe second application is in our proposed\nmethod, where we decontextualize queries. For\nexample, in Table 1, we need to align the fact “The\n911 calls were made around 4:30” with the output\ntext “The . . . 911 calls around 4:30 p.m.” This\nalignment is crucial to ensure proper attribution,\nsuch as correctly highlighting the word “911.”\nTo achieve this alignment, we implement a naive\nlexical alignment algorithm. This approach is ex-\npected to perform well since each output fact is\nextracted from a single output sentence, and the\ngenerated fact does not contain any paraphrases.\nFormally, given an output oand a fact fex-\npressed by o, we wish to find spans o1, . . . , o n⊆o\nsuch that f|=concat (o1, . . . , o n).\nAlignment algorithm\n1.Tokenization & Lemmatization: We first\nsplit the output ointo words o1, . . . , o n, and\nthe fact finto words f1, . . . , f m. Each word\nis lemmatized.11\n2.Edit Script Calculation: We compute the\nedit script12between the output words and the\nfact words. The edit script represents the min-\nimal set of operations (insertions, deletions,\nand substitutions) required to transform one\nsequence into the other. Each word in the\noutput is assigned an edit operation.\n3.Word Alignment Based on Edit Operations:\nAny output word oiclassified as unchanged is\nconsidered aligned to the corresponding fact\nword fj.\nThe advantage of using an edit script is that it\nconsiders the order in which the words appeared.\nHowever, sometimes the fact transposes informa-\ntion from the output sentence. For example, in\n11using spaCy https://spacy.io/\n12Using Levenshtein distance https://nltk.org/\n--- Page 15 ---\nthe second row of Table 2, the fact mentions “pub-\nlic school” after the mention of “the First Amend-\nment”, but in the output sentence the order is re-\nversed. The algorithm will then not be able to align\n“public school”. To support such transpositions, we\ngenerate a new fact f′with non-aligned words from\nf. We then run this algorithm recursively with f′.\nOverall, in 88% of the examples we are able to\nalign all content words,13and in 99% we are able\nto align all content words but one.\n13Excluding stop-words https://nltk.org\n--- Page 16 ---\nExample\nOutput sentence The confirmation hearings for Brett Kavanaugh were marked by controversy over the withholding\nof documents, with Democrats repeatedly complain ingthatRepublicans and the White House were\nkeep ingimportantrecords from the public and the committee.\nLLM Prompt Such theatrics have characterized Kavanaugh’s hearings, in which Democrats have repeatedly\ncomplained that Republicans have with held documents from thecommitteeand thepublic\nthat shed important light onKavanaugh’s past. . . . Democrats have repeatedly complained that\ntheWhite House iswithholdingtens ofthousands ofdocuments relevant tothenomination and\nwants many more that have been provided released to the public.\nLLM Internals Such theatrics have characterized Kavanaugh’s hearings, in which Democrats have repeatedly\ncomplained that Republicans have with held documents from thecommitteeand thepublic\nthat shed important light onKavanaugh’s past.\nTable 10: Example MDS result. Top: one example output sentence from the ATTR. FIRST baseline with synthesized\nLAQuer highlights. Bottom: the predicted attributions, with correct attribution in bold .\nExample\nOutput sentence The upcomingSupreme Court term ispoised toaddress severalcon-\ntentious issues that could significantly impact American society and\npolitics.\nLLM prompt After a year in which liberals scored impressive, high-profile Supreme\nCourt victories, conservatives could beinlineforwins onsome ofthis\nterm’s most contentious issues, as the justices consider cases that could\ngut public sector labor unions and roll back affirmative action at state\nuniversities. . . . Apotentialbody blow tolabor Public-employee unions\nand politicians of both parties are keenly focused on a California dispute\nabout whether states can compel government employees to pay union\ndues. . . . Higher ed affirmative action back in the crosshairs . . . The\nmean ingof\"one person, onevote’ . . . Testing when abortion clinic\nregulations go too far . . . Thedeath penalty isshapinguptobeabig\nissuefortheSupreme Court asitbegins anew term\nLLM internals However, asthecourt’s new term kicks offMon day, uncertainty sur-\nrounds severalother politically potentcases thatcould wind uponthe\ncourt’s agenda. Litigationover state efforts to limit abortion by regulating\nclinics and doctors is making its way to the high court. Lois Lerner should\nhave been gone shortly after the scandal first unraveled.\nTable 11: Example MDS result. Top: one example output sentence from the Vanilla baseline with synthesized\nLAQuer highlights. Bottom: the predicted attribution, with correct attribution in bold .",
  "text_length": 64998
}