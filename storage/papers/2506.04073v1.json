{
  "id": "http://arxiv.org/abs/2506.04073v1",
  "title": "A Statistics-Driven Differentiable Approach for Sound Texture Synthesis\n  and Analysis",
  "summary": "In this work, we introduce TexStat, a novel loss function specifically\ndesigned for the analysis and synthesis of texture sounds characterized by\nstochastic structure and perceptual stationarity. Drawing inspiration from the\nstatistical and perceptual framework of McDermott and Simoncelli, TexStat\nidentifies similarities between signals belonging to the same texture category\nwithout relying on temporal structure. We also propose using TexStat as a\nvalidation metric alongside Frechet Audio Distances (FAD) to evaluate texture\nsound synthesis models. In addition to TexStat, we present TexEnv, an\nefficient, lightweight and differentiable texture sound synthesizer that\ngenerates audio by imposing amplitude envelopes on filtered noise. We further\nintegrate these components into TexDSP, a DDSP-inspired generative model\ntailored for texture sounds. Through extensive experiments across various\ntexture sound types, we demonstrate that TexStat is perceptually meaningful,\ntime-invariant, and robust to noise, features that make it effective both as a\nloss function for generative tasks and as a validation metric. All tools and\ncode are provided as open-source contributions and our PyTorch implementations\nare efficient, differentiable, and highly configurable, enabling its use in\nboth generative tasks and as a perceptually grounded evaluation metric.",
  "authors": [
    "Esteban Gutiérrez",
    "Frederic Font",
    "Xavier Serra",
    "Lonce Wyse"
  ],
  "published": "2025-06-04T15:39:16Z",
  "updated": "2025-06-04T15:39:16Z",
  "categories": [
    "cs.SD",
    "eess.AS"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04073v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04073v1  [cs.SD]  4 Jun 2025Proceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nA STATISTICS-DRIVEN DIFFERENTIABLE APPROACH FOR SOUND TEXTURE\nSYNTHESIS AND ANALYSIS\nEsteban Gutiérrez\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona, Spain\nesteban.gutierrezc@upf.eduFrederic Font\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona, Spain\nfrederic.font@upf.edu\nXavier Serra\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona, Spain\nxavier.serra@upf.eduLonce Wyse\nMusic Technology Group\nUniversitat Pompeu Fabra\nBarcelona, Spain\nlonce.wyse@upf.edu\nABSTRACT\nIn this work, we introduce TexStat , a novel loss function specif-\nically designed for the analysis and synthesis of texture sounds\ncharacterized by stochastic structure and perceptual stationarity.\nDrawing inspiration from the statistical and perceptual framework\nof McDermott and Simoncelli, TexStat identifies similarities\nbetween signals belonging to the same texture category without\nrelying on temporal structure. We also propose using TexStat\nas a validation metric alongside Frechet Audio Distances (FAD) to\nevaluate texture sound synthesis models. In addition to TexStat ,\nwe present TexEnv , an efficient, lightweight and differentiable\ntexture sound synthesizer that generates audio by imposing ampli-\ntude envelopes on filtered noise. We further integrate these com-\nponents into TexDSP , a DDSP-inspired generative model tailored\nfor texture sounds. Through extensive experiments across various\ntexture sound types, we demonstrate that TexStat is perceptu-\nally meaningful, time-invariant, and robust to noise, features that\nmake it effective both as a loss function for generative tasks and as\na validation metric. All tools and code are provided as open-source\ncontributions and our PyTorch implementations are efficient, dif-\nferentiable, and highly configurable, enabling its use in both gen-\nerative tasks and as a perceptually grounded evaluation metric.\n1. INTRODUCTION\nDefining audio textures is a complex problem that has been ex-\nplored by various authors. The concept originated as an analogy to\nvisual textures. In [1], Julesz, one of the pioneers in this field, pro-\nposed the so-called \"Julesz conjecture,\" suggesting that humans\ncannot distinguish between visual textures with similar second-\norder statistics. This hypothesis was later disproven in [2], but\nwhose statistical perspective remains influential in texture analysis\n(see [3]) and synthesis (see [4], [5], and [6]). This perspective is\nalso foundational for this work.\nRegarding the auditory domain, an early definition of audio\ntextures was introduced in [7]. The authors described them as pat-\nCopyright: © 2025 Esteban Gutiérrez. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution 4.0 International License, which\npermits unrestricted use, distribution, adaptation, and reproduction in any medium,\nprovided the original author and source are credited.terns of basic sound elements, called atoms, occurring in struc-\ntured but potentially random arrangements. These high-level pat-\nterns must remain stable over time while being perceivable within\na short time span. Rosenthal et al. [8] later expanded this idea,\ndefining audio textures as a two-level structure: atoms forming\nthe core elements and probability-based transitions governing their\norganization. Recent research has refined these ideas, but the bal-\nance between short-term unpredictability and long-term stability\nremains a common thread. In this matter, Wyse et al. [9] points out\nthat sound texture’s complexity and unpredictability at one level is\ncombined with the sense of eternal sameness at another.\nMany approaches have been taken regarding the synthesis and\nre-synthesis of texture sounds. These approaches can be classified\nin various ways. For example, Sharma et al. [10] separates them\ninto three categories: granular synthesis, which corresponds to\nthe concatenation of already existing sounds, as in the pioneering\nworks [11], [12], and [13]; model-based synthesis, which relies on\neither improved time-frequency representation models, as in [14],\nphysical models, as in [15], and/or physiological/statistical mod-\nels, as in the foundational series of articles [4], [5], and [6]; and\nfinally, deep learning-based models, encompassing various con-\ntemporary machine learning techniques, as in [16], [9], and [17].\nThe sound resynthesis task involves recreating a given sound\nthrough analysis and synthesis. While sound reconstruction can be\nviewed as an exact replication of the original sound, in the case of\ntexture-based resynthesis, it is often sufficient to generate a sound\nthat is perceptually similar to the original, and thus, the resulting\nsound may differ significantly in its detailed time structure from\nthe original one. Moreover, Caracalla et al. [16] state that typ-\nically the texture sound resynthesis goal is to \"create sound that\nis different from the original while still being recognizable as the\nsame kind of texture.\" In the context of deep learning, this goal can\nbe approached in various ways. For instance, models with inherent\nstochasticity naturally generate a \"similar enough\" sound, drawing\nfrom the set of sounds they can produce, which is influenced by\ntheir biases. Conversely, even in the absence of stochasticity, if the\nmodel’s loss function is perceptually grounded, it will inevitably\nfocus solely on that instead of exact replication.\nAn example of a model that performs resynthesis by accu-\nrately matching the spectrum of a texture sound is NoiseBandNet\n[17], a Differentiable Digital Signal Processing (DDSP)-based ar-\nDAFx.1\n--- Page 2 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nchitecture that indirectly follows this approach by using a multi-\nscale spectrogram loss applied to small time scales (starting at ap-\nproximately 0.7ms) and small sound windows (around 180ms).\nOn the side of perceptual/features-based resynthesis using deep\nlearning, [16] employed a loss function that compares the Gram\nmatrices of two sets of features computed from pretrained Convo-\nlutional Neural Networks (CNNs). Another example comes from\n[9] and its use of Generative Adversarial Networks (GANs), where\nthe objective of the loss function is to train the model to generate\nsounds that can deceive a classifier, thereby biasing the training in\na more nuanced manner than direct signal comparison.\nIn the context of texture sound analysis and resynthesis, this\npaper has three main goals. First, we introduce TexStat , a loss\nfunction grounded in human perception of texture sounds. This\nfunction builds upon the pioneering work of McDermott and Si-\nmoncelli [4], [5], [6], and is made available as an efficient, open-\nsource implementation in PyTorch. Second, we present TexEnv ,\na simple and efficient noise-based texture sound synthesizer devel-\noped in synergy with TexStat . Finally, to evaluate both TexStat\nandTexEnv , we introduce TexDSP , a DDSP-based architecture\ntrained across multiple scenarios, demonstrating the effectiveness\nof our proposed components.\nIn Section 2, we formally define the loss function TexStat\ntogether with its potential variations. In Section 3, we briefly in-\ntroduce the TexEnv synthesizer, highlighting both its capabilities\nand its limitations. In Section 4, we present the TexDSP archi-\ntecture as a showcase for the tools introduced earlier. In Section\n5, a series of experiments demonstrating the tools introduced in\nthis paper are presented. Finally, in Section 6, we summarize the\ntools and results discussed in this work and outline some future\ndirections for research.\n2.TEXSTAT : A LOSS FUNCTION SPECIFICALLY\nTAILORED FOR TEXTURE SOUNDS\nIn this section, we discuss some desirable properties of a texture\nsound loss function and then introduce TexStat , a loss function\nspecifically designed for texture sounds that fulfills, to some ex-\ntent, the properties outlined here.\n2.1. What Should a Texture Loss Be?\nIn the context of deep learning, solving a task—whatever the task\nmay be—is framed as an optimization problem. Such an opti-\nmization problem typically involves minimizing a loss function,\nwith the expectation that doing so will lead to solving the proposed\ntask. In this scenario, the loss function must be able to represent,\nto some extent, the task to be solved.\nThe texture sound generation task can be described as a pro-\ncess in which a sound is created so that it is recognized as belong-\ning to a particular type of texture. However, it does not need to\nbe—and in fact, it is preferable that it is not—identical to the in-\nput or dataset examples. Additionally, depending on the context, it\nmay also be desirable to generate non-repetitive sound over time,\nwith the sound generation process allowing for some degree of\ntemporal control.\nThis task description, together with our current understanding\nof texture sounds, leads us to the following desirable properties for\na loss function specifically tailored for texture sounds.\nOverall Structure Focus: Texture sounds resemble filtered noise\nover short periods but are stable and recognizable over time. Thus,a loss function should prioritize long-term characteristics, like\nrhythm, pitch variations, or granular shifts, rather than specific and\ndetailed temporal structure.\nStochastic Focus: Following [1] and [8], texture sounds arise\nfrom stochastic processes in time and timbre. A suitable loss func-\ntion should capture these statistical properties to recognize simi-\nlarity between sounds corresponding to the same type of texture.\nPerceptual Foundation: The loss function must capture percep-\ntual similarity, ensuring that two sounds considered perceptually\nsimilar are treated as such. This involves leveraging psychoacous-\ntic principles to align with human auditory perception.\nTime Invariance: Texture sounds can be chopped or rearranged\nwith minimal perceptual impact. The loss function should thus ex-\nhibit time invariance, allowing rearrangement without significantly\naltering the sound’s characteristics.\nNoise Stability: Texture sounds can tolerate noise, so the loss\nfunction should be robust to subtle noise variations, preserving\ncore texture features even with low to mid level disturbances.\nFlexibility: A highly restrictive loss function risks overfitting,\nleading to audio too similar to the training data. It should encour-\nage creative variation, generating novel yet perceptually consistent\nsounds that retain defining textural characteristics.\n2.2.TexStat Formal Definition\nAccording to Julesz’s conjecture and McDermott and Simon-\ncelli’s synthesis approach, the nature of textures can be understood\nthrough direct comparison of their statistics. McDermott and Si-\nmoncelli’s synthesis approach [4], [5], and [6] involves the impo-\nsition of a set of statistics precomputed for a given texture sound\nusing numerical methods, suggesting the possibility of using that\nexact set of statistics as a feature vector for comparison.\nIn this work, we introduce TexStat , a loss function that op-\nerates by directly comparing a slight variation of a superset of the\nsummary statistics used by McDermott and Simoncelli.\nIn the following, we formally introduce the necessary tools,\npreprocessing steps, and sets of summary statistics related to the\ncomputation of TexStat . For further details on summary statis-\ntics’ perceptual importance, we refer to [5] and [6].\nPreliminary Tools: At the core of TexStat there are a series of\nsubband decompositions. Following the original work of McDer-\nmott and Simoncelli, we consider the following: a Cochlear Filter-\nbank, which is an Equivalent Rectangular Bandwidth (ERB) filter-\nbank [18], [19] F={fj}NF\nj=1made up of NFfilters; and a Modu-\nlation Filterbank, which is a Logarithmic Filterbank G={gk}NG\nk=1\nmade up of NGfilters.\nPreprocessing: Given a signal s, the Cochlear filterbank Fis\nused to compute a subband decomposition of it by sj=s∗fj,\nj= 1, . . . , N F. Then, the amplitude envelope of each one of\nthese subbands is computed using the Hilbert Transform ej=\n|sj+iH(sj)|, j= 1, . . . , N F. Finally, the Modulation Filter-\nbankGis used to compute a subband decomposition of each one\nof the envelopes ej, that is, mj,k=gk(ej), j= 1, . . . , N F, k=\n1, . . . , N G.\nStatistics sets: The first set of statistics is comprised of the first L\nnormalized moments of the signals ej, that is, S1,l,j=Ml(ej)for\nj= 1, . . . , N Fandl= 1, . . . , L , where M1(X) = E(X) =µ,\nM2(X) =V(X)/µ2=σ2/µ2and\nMl(X) =E(X−µ)l\nσl, l= 3, . . . , L.\nDAFx.2\n--- Page 3 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nNote that the vectors S1,l∈RNFcorrespond to different mo-\nments and hence they are prone to have values that vary in orders\nof magnitude. In order to fix this issue, the first set of statistics cor-\nrespond to a weighted concatenation of the vectors corresponding\nto different moments\nS1(s) =concat (α1S1,1, . . . , α NS1,L)∈RL·NF.\nThe second set of statistics corresponds to the Pearson corre-\nlation between different amplitude envelopes ej, that is,\nS2(s) =vech(corr(e1, . . . , e NF))∈RTNF−1,\nwhere vech (·)corresponds to the half-vectorization operator,\ncorr(·)to the correlation matrix and Tnto the n-th triangular num-\nber.\nThe third set of statistics corresponds roughly to the proportion\nof energy in each modulation band, that is,\nS3,j=\u0010\nV(mj,1)1/2, . . . , V(mj,NG)1/2\u0011\nV(ej)1/2∈RNG\nS3(s) = concat (S3,1, . . . , S 3,NF) ∈RNG·NF.\nThe fourth set of statistics corresponds to the Pearson corre-\nlations between modulation subbands corresponding to the same\namplitude envelope, that is,\nS4,j= vech(corr(mj,1, . . . , m j,NG))∈RTNG−1\nS4(s) = concat (S4,1, . . . , S 4,NF)∈RNF·TNG−1.\nFinally, the fifth set of statistics corresponds to the Pearson\ncorrelation between modulation subbands corresponding to the\nsame band but different amplitude envelopes, that is,\nS5,k= vech(corr(m1,k, . . . , m NF,k))∈RTNF−1\nS5(s) = concat (S5,1, . . . , S 5,NG)∈RNG·TNF−1.\nNow that we have the five sets of statistics well-defined, we\ncan finally define the TexStat loss function.\nDefinition 1. ( TexStat loss function) TheTexStat loss func-\ntion is defined as\nLα,β(x, y) =5X\nj=1βj·MSE(Sj(x), Sj(y)),\nwhere α∈RLandβ∈R5are parameters, x, y are a pair of\nsignals, and S1(x), S2(y), . . . , S 5(x), S5(y)are the summary\nstatistics vectors defined above.\nBefore continuing, it is important to mention that although the\nTexStat loss function is strongly based on the work of McDer-\nmott and Simoncelli, several changes and additions were made to\nmake it more suitable for machine learning tasks. Most of these\nchanges involve adaptations that ensure control over the number\nof statistics and their weights during training. The number of\nstatistics is important, as it would be counterproductive to com-\npute more statistics than the size of the window used. Furthermore,\nweighting the statistics provides a way to balance their contribu-\ntion to the loss. The main changes and their impact are outlined in\nTable 1.McDermott and\nSimoncelli’s\nSummary StatisticsTexStat Summary\nStatistics\nFilterbanks Fixed type, frequency\nrange, and size of\nfilterbanks for their\nexperiments.Variable type, frequency\nrange, and size of\nfilterbanks for\ncontrollability.\nStatistical\nMomentsFixed number of\nstatistical moments.Variable number of\nstatistical moments.\nUseful for compensating\nsmall filterbanks.\nModulation\nBand\nCorrelationsOnly some\ncorrelations are\ncomputed.Multiple correlations\nwere avoided in the\noriginal work for\ncomputational efficiency;\nhowever, with modern\nGPU computations, this\ndoesn’t make a significant\ndifference.\nCompressive\nNon-linearityApplies one to the\namplitude envelopes\nfollowing past\nauditory models.Removes compressive\nnon-linearity for gradient\nstability.\nWeights Summary statistics\nare not weighted.Variable weights to\ncontrol the importance of\ncertain sets of statistics\nduring training and to\navoid overflow (especially\ninS1).\nTable 1: Summary statistics comparison between McDermott and\nSimoncelli’s work, and those used by TexStat .\n2.3.TexStat Properties\nAs discussed in Subsection 2.1, there are several desirable proper-\nties that a texture sound loss function should have, and we argue\nthat, when used correctly, TexStat addresses them all.\nFirst, TexStat can be utilized on arbitrarily large frames of au-\ndio, and in any case, all summary statistics are directly influenced\nby the entire signal, which we argue implies a focus on the overall\nstructure. Moreover, since it is built on a statistical and perceptual\nmodel, we also argue that its focus is on the stochastic properties\nof the signal, and that it has a strong foundation in perception.\nOverall, one could argue that the operations involved in the com-\nputation of summary statistics are quite stable with respect to the\naddition of low-amplitude white noise. This will be empirically\ndemonstrated in Subsection 5.1. Moreover, if s∈C(R)is a con-\ntinuous infinite signal whose summary statistics exist (for exam-\nple, if it belongs to the Schwarz class s∈ S(R)), there are no\noperations in the process of computing S1(s), . . . , S 5(s)that can\nbe affected by time shifting, i.e., ˆs(t) =s(t−t0). This is, of\ncourse, not the case for discrete finite signals, where time shifting,\nˆs[t] =s[t−t0mod len(s)], might introduce a click sound at the\nbeginning, which would affect the spectrum and, consequently, the\nsubband decomposition. However, this is not a significant issue for\nnoisy signals, as adding a click will not strongly impact their spec-\ntrum. This will also be further explored in Subsection 5.1.\nDAFx.3\n--- Page 4 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nFinally, given a choice of parameters that does not generate an\nexcessive number of summary statistics in relation to the samples\nused in the original signal, there are generally many signals that,\nwhen compared using TexStat , will result in low loss values.\nFor example, all chopped and reordered versions of the same signal\nwill typically yield similar summary statistics, and hence would be\nclose in the sense of the TexStat distance. We claim that this\nsuggests flexibility for the TexStat loss function.\n2.4. Capabilities and Limitations\nTheTexStat loss function is based on the direct comparison\nof summary statistics, meaning that two sounds whose summary\nstatistics are similar will not be recognized as different by this loss\nfunction. In their original work, McDermott and Simoncelli im-\nposed the summary statistics of a series of sounds onto white noise\nand found that, although they successfully synthesized a range of\nsounds with good results, this process was unable to generate con-\nvincing sounds for certain types of timbres. This was because the\nsummary statistics were not sufficient to fully characterize those\ntypes of sounds. Regarding the sounds that couldn’t be fully cap-\ntured by the summary statistics, McDermott and Simoncelli stated\nthat \"they fall into three general classes: those involving pitch (e.g.,\nrailroad crossing, wind chimes, music, speech, bells), rhythm (e.g.,\ntapping, music, drumming), and reverberation (e.g., drumbeats,\nfirecrackers).\" These capabilities and limitations are naturally in-\nherited by the TexStat loss function, and both effective and inef-\nfective examples are shown in Subsection 5.5 to make transparent\nthis loss’ limitations.\n2.5. Usage as a Loss and/or Evaluation Metric\nTheTexStat loss function introduced here is differentiable,\nmaking it suitable for use as a loss function in machine learning\napplications. Moreover, together with this article we release an\nefficient and open-source PyTorch implementation.\nThe number of computations required to run both TexStat\nand its gradients are relatively large compared to other loss func-\ntions, such as the Multi-Scale Spectral (MSS) Loss function. How-\never, this can be mitigated by adjusting the frame size, as summary\nstatistics are intended to be used on large audio frames. For exam-\nple, most models evaluated in Section 5.5 used a frame size cor-\nresponding to approximately 1.5seconds of audio at a framerate\nof44100 Hz. In contrast, architectures like NoiseBandNets [17]\nuse the MSS on frames of up to 0.18seconds. Additionally, the in-\ncreased computational cost also comes with higher memory usage,\nespecially when increasing NFandNG, which can significantly\nimpact memory requirements.\nGiven the timbre limitations of the summary statistics dis-\ncussed earlier, we believe that, in order to fully guide the learning\nprocess of a general generative model, additional losses should be\nintroduced to exert full control over different types of sounds, such\nas pitched or rhythmic sounds. In such cases, TexStat can be re-\ngarded as a regularizer that aids in guiding the timbre learning of\ntexture sounds.\nIn some instances, TexStat may not be suitable as a loss\nfunction for the reasons mentioned earlier. In these cases, one can\nuse other losses to guide the learning process and, if appropriate\nfor the task, employ either TexStat as an evaluation metric. To\nfacilitate this, we propose a fixed set of parameters, including fil-\nterbank types, filterbank sizes, and values for αandβ, which havebeen proven useful for texture sound synthesis in the past and that\nwere tested in this article’s experiments. These parameters ensure\ncomparability, and we also provide a list of precomputed values\nfor interpretability. All of this can be found in the repository for\ntheTexStat loss function. Moreover, since the TexStat loss\nfunction is essentially a direct comparison of summary statistics,\none could view this set of statistics as a fully interpretable feature\nvector. This feature vector can thus be used as the basis for other\nevaluation metrics, such as Frechet Audio Distance (FAD) [20].\nOur repository comes with a simple to use implementation of\nthis method that uses a subset of the summary statistics here pro-\nposed and extensive experimentation to prove this concept can be\nfound in Subsections 5.3 and 5.5.\n3.TEXENV : A DIFFERENTIABLE SIGNAL PROCESSOR\nTAILORED FOR TEXTURE SOUNDS\nThe foundations of TexStat are based on the idea that summary\nstatistics of amplitude envelopes derived from a low-size filterbank\nsubband decomposition are sufficient to compare certain types of\ntexture sounds. Implicit in this concept is the fact that directly im-\nposing amplitude envelopes on a subband decomposition of white\nnoise is one method of resynthesizing texture sounds while pre-\nserving their summary statistics. In the context of this work, two\nquestions arise: How can we efficiently and differentiably create\namplitude envelopes from a sequence of parameters? How can we\nefficiently and differentiably impose amplitude envelopes?\nCreating amplitude envelopes from scratch can be done in\nmultiple ways. For this synthesizer, we chose to use the Inverse\nFast Fourier Transform (IFFT) because it is differentiable and can\nbe computed efficiently to generate cyclic functions. For the im-\nposition process, we fixed precomputed white noise, decomposed\nit using a filterbank, and then normalized the amplitude envelope\nof each subband. This procedure generates an object we call the\nseed, which serves as a \"source of deterministic randomness\" and\ncan be used to impose amplitude envelopes via simple multiplica-\ntion. Algorithm 1 outlines this synthesis method.\nAlgorithm 1 TheTexEnv Synth\nInput: LetFbe a filterbank of size NF,(s1, . . . , s NF)aseed\ngenerated from F,p1, . . . , p NF∈CNPa set of complex vector\nparameters and Nthe length of the signal to be generated.\nOutput: A texture sound synthesized y∈RN.\n1:For each j= 1, . . . , N Fconstruct a spectrum as\nAj=concat (pj,0, . . . , p j,NP−1),0,(pj,NP−1, . . . , pj,1)),\nwhere 0is a null vector of size N−2NP+ 1.\n2:For each j= 1, . . . , N Fconstruct a real signal using the In-\nverse Discrete Fourier Transform (IDFT)\naj=IDFT (Aj).\n3:Impose the signals a1, . . . , a NFas amplitude envelopes on the\nseed and sum up to generate the final signal\ny=NFX\nj=1sj⊙aj.\n4:return The synthesized signal y.\nDAFx.4\n--- Page 5 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\n4.TEXDSP : A DDSP-BASED ARCHITECTURE\nTAILORED FOR TEXTURE SOUNDS\nIn this section we introduce TexDSP , a relatively simple texture\nsound generative model based on DDSP [21] that showcases the\ncapabilities of TexStat andTexEnv .\nThe original DDSP model requires an encoder, decoder, sig-\nnal processor, and loss function, with each component designed\nto generate small frames of pitched sounds based solely on pitch\nand loudness features. However, in the context of texture sound\ngeneration, the task is quite different, necessitating several mod-\nifications. The changes we propose ensure that the model’s ob-\njective is to learn high-level statistical patterns, rather than aiming\nfor perfect (frequency domain) reconstruction, as is the case with\nthe original DDSP architecture. The final architecture is shown in\nFigure 1, and the changes made are briefly outlined here.\n4.1. Encoder and Decoder\nAs in the original DDSP architecture, the encoding process in-\nvolves both feature extraction and a more complex transformation\nof these features. In this case, we used different pairs of features\nthat can be more informative for texture sounds than pitch and\nloudness. These features include spectral mean (frequency cen-\ntroid), spectral standard deviation, energy in each band of a sub-\nband decomposition, and onset rate. To increase the complexity of\nthe feature extraction process, we followed the original approach:\nfirst, we applied a Multi-Layer Perceptron (MLP) to each feature\nFj=MLP(fj|εenc\nj), and then concatenated the results with the\noutput of a Gated Recurrent Unit (GRU) applied to the same fea-\ntures Z=GRU(F1, F2|φ), yielding the latent representation\nL=concat (F1, F2, Z). The decoding process involves trans-\nforming the latent representation Lthrough another MLP and an\nadditional layer to obtain a polar representation of the parameters\nused by the signal processor, i.e.,\nρ=σ(A·MLP(L|εdec))\nθ= 2πσ(B·MLP(L|εdec))\nwhere A, B are real-valued matrices to be learned, and σ(·)de-\nnotes the sigmoid function applied component-wise.\n4.2. Signal Processor and Loss Function\nThe original DDSP model used a Spectral Modeling Synthesis\n(SMS) [22] synthesizer, which is well-suited for pitched sounds,\nand a Multi-Scale Spectrogram (MSS) Loss, which is effective for\nperfect reconstructions. Since the goal of this architecture is to test\nthe capabilities of TexStat , we opted for our signal processor,\nas it was designed to work synergistically with our loss function.\nBothTexEnv andTexStat are built around a filterbank, and to\noptimize synergy, this filterbank is shared between the two.\n5. EXPERIMENTS AND RESULTS\nIn this section, we briefly explain a series of experiments con-\nducted to provide proof of concept for the models proposed in\nthis work. For these experiments, we hand-curated MicroTex1,\na dataset made from a selection of texture sounds from the follow-\ning sources: the BOReilly dataset, containing textures made using\n1MicroTex HuggingFace repository: cordutie/MicroTex.analog synthesizers; Freesound [23], which contains environmen-\ntal sounds; and synthetically generated data using Syntex [24]. All\nexperiments can be found in their respective repositories and are\nfully replicable2.\n5.1.TexStat Properties Tests\nTwo desirable properties proposed for a loss function tailored to\ntexture sounds are stability under time shifting and robustness to\nadded noise. To test these properties in the TexStat loss func-\ntion, we computed the loss between the sounds in the Freesound\nclass of MicroTex and their corresponding transformations us-\ning various parameters. We focused on the Freesound class of\nMicroTex , as it contains the most representative examples of en-\nvironmental texture sounds and includes recordings that are long\nand dynamic enough to allow meaningful time shifting and noise\naddition. The other two classes were excluded, as their sounds are\neither too short or too silent, making such transformations imprac-\ntical without introducing substantial alterations. The experiment\nwas also run with the MSS loss for comparison and some of the\nresults can be found in Table 2.\nTexStat MSS\nTransformation 10% 30% 50% 10% 30% 50%\nTime-Shift0.04\n±0.030.04\n±0.030.04\n±0.036.09\n±1.226.27\n±1.386.29\n±1.41\nNoise-Add2.08\n±1.992.51\n±2.212.65\n±2.2711.79\n±4.9116.84\n±5.9219.57\n±6.26\nTable 2: Loss measurements ( µ±σ) between sounds in the\nFreesound class of MicroTex and their corresponding time-\nshifted and noise-added transformations. Time shift is expressed\nas a percentage of the total signal duration, and noise percentage\nare defined by their maximum amplitude relative to the original\nsignal. All measurements were computed over one-second seg-\nments for each of the sounds mentioned above. For reference, all\nsatisfactory models trained using TexStat converged to loss val-\nues below 3, whereas evaluations using MSS typically yield rea-\nsonable values below 10.\nThe results demonstrate that TexStat exhibits strong stabil-\nity under both time shifting and noise addition, incurring a consis-\ntent penalty for time shifts and a sublinear increase in penalty as\nnoise levels increases.\n5.2.TexStat Benchmarks\nTo benchmark the computational requirements of TexStat , we\nevaluated its computation time, gradient descent time, and GPU\nmemory usage. These measurements were conducted multiple\ntimes, recording the time taken for loss computation and optimiza-\ntion while tracking memory allocation. The results are presented\nin Table 3, along with the values for other typical losses.\nThe results show that, as expected, the TexStat loss function\nis slower than other less specific losses, but it uses a similar amount\nof memory.\n2Find access to all repositories, experiments and sound examples in this\narticle’s webpage: cordutie.github.io/ddsp_textures/.\nDAFx.5\n--- Page 6 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nTarget Texture\nSoundFeature\nExtractorEncoder DecoderSynthesizerOutput Texture\nSound\nSeed\n loss function\nFigure 1: TexDSP architecture. Prechosen features are computed and are used to run the model. The encoder adds complexity and\nentangles this features into the latent representation L= (F1, F2, Z). The decoder transforms this representation into a set of complex\nparameters that are used to run the TexEnv synthesizer. Finally, the output signal is compared to the original one using the TexStat loss\nfunction.\nLoss Forward pass\ntime (ms)Backward pass\ntime (ms)Memory usage\n(mb)\nTexStat 93.5±0.4 154 .6±0.4 0 .84±2.5\nMSS 3.9±0.3 8 .5±0.3 0 .85±2.6\nMSE 0.2±0.3 0 .2±0.1 1 .7±5.0\nMAE 0.1±0.0 0 .2±0.1 0 .8±2.5\nTable 3: Measurements regarding computation time, gradient\ncomputation time, and memory usage ( µ±σ) in batches of 32sig-\nnals of size 65536 (around 1.5s at a sample rate of 44100 Hz). The\nlosses studied were TexStat , Multi-Scale Spectrogram (MSS),\nMean Squared Error (MSE), and Mean Absolute Error (MAE).\nAll measurements were done using CUDA on an RTX 4090 GPU.\n5.3.TexStat as an Evaluation Metric\nIn order to test TexStat summary statistics as a powerful rep-\nresentation that can be used in metrics like FAD, we conducted\nthe following experiment. First, all data in the three selections of\ntheMicroTex dataset were segmentated and both their summary\nstatistics and VGGish [25] embeddings were computed. Then,\na downstream classifier (MLP with hidden layers 128, 64) was\ntrained in both cases. A summary of the results can be found in\nTable 4.\nModel Selection Accuracy Precision Recall F1\nTexStat BOReilly 0.94 0 .94 0 .94 0 .94\nVGGish BOReilly 0.71 0 .73 0 .71 0 .71\nTexStat Freesound 0.99 0 .99 0 .99 0 .99\nVGGish Freesound 0.98 0 .99 0 .98 0 .98\nTexStat Syntex 1.0 1 .0 1 .0 1 .0\nVGGish Syntex 0.95 0 .95 0 .95 0 .94\nTable 4: Classification performance of equivalent models trained\non both our proposed feature vector and VGGish embeddings.\nThe results indicate that in the context of texture sounds, sum-\nmary statistics are strictly more informative than general-purpose\nembeddings such as VGGish.5.4. Texture resynthesis using TexEnv\nExtensive exploration using the TexEnv synthesizer in resynthe-\nsis tasks employing a signal processing-based parameter extractor\nwas conducted to better understand its limitations and overall be-\nhavior. A summary of sound examples can be found on this arti-\ncle’s webpage. Some of our key findings were as follows: water-\nlike sounds such as flowing water, rain, and continuous bubbling\ndo not benefit from larger parameter sets, but do benefit from larger\nfilterbanks. In contrast, crackling sounds like fireworks or bonfires\nbenefit from larger parameter sets, but not as much from larger fil-\nterbanks. These insights were crucial in determining the optimal\nparameters for the models trained later.\n5.5.TexDSP Trained Models\nTo showcase the capabilities of TexStat , we trained a set of\nTexDSP models using different parameters, with TexStat as\nthe sole loss function to guide the learning process. The details\nand results for some of these models are presented below.\nTraining Details: A curated set of sounds representing different\nclasses of texture sounds from Freesound was used for each model.\nEach model employed different parameters tailored to the specific\ntexture type. These parameters were chosen based on the resyn-\nthesis exploration discussed in Subsection 5.4. The number of lay-\ners in the MLPs for both the encoder and decoder was limited to a\nmaximum of 3, with the number of parameters capped at 512. This\nconfiguration ensured that the resulting models, even when com-\nbined with the seed used for the filterbank, remained under 25MB\nand could, if necessary, be ported to a real-time environment. The\nTexStat αandβparameters were set to the default values pro-\nposed in our repository, and all models used the same optimizer,\ntraining for up to 1500 epochs with early stopping enabled. Addi-\ntionally, for each TexDSP model trained, a corresponding Noise-\nBandNet model was also trained using default parameters for com-\nparison.\nValidation Method: To evaluate model performance, we resyn-\nthesized a subset of the dataset, excluded from training, for both\ntheTexStat and NoiseBandNet models. We then segmented the\noriginal and resynthesized signals, and measured Fréchet Audio\nDistance (FAD) using both VGGish embeddings and our custom\nsummary statistics, along with frame-level TexStat and MSS\nlosses. For the latter, we report the mean and standard deviation\nacross all segments. Results are presented in Table 5.\nDAFx.6\n--- Page 7 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\nFAD Loss metrics\nTexture\nSoundVGGish\nTexDSPVGGish\nNoiseBandNetOurs\nTexDSPOurs\nNoiseBandNetTexStat\nTexDSPTexStat\nNoiseBandNetMSS\nTexDSPMSS\nNoiseBandNet\nBubbles 35.20 21.37 1.86 1.15 1.2±0.3 0.7±0.1 6.6±0.3 4.7±0.1\nFire 11.86 2.53 6.14 1.52 2.8±2.1 1.7±1.0 9.6±1.3 4.5±0.2\nKeyboard 13.02 9.70 16 .64 277.12 5.7±2.0 20.0±7.7 9.1±0.7 13.8±0.6\nRain 9.09 11.31 0.98 6.19 0.5±0.2 2.4±2.0 9.0±0.2 9.1±0.4\nRiver 43.66 49.85 0.80 1.75 0.5±0.1 0.6±0.1 6.0±0.6 6.7±0.3\nShards 4.64 1.36 3 .79 7.58 1.0±0.2 1.1±0.3 7.9±0.2 8.8±0.2\nWaterfall 18.23 25.88 0.53 1.06 0.3±0.0 0.4±0.0 5.0±0.0 6.3±0.0\nWind 9.66 31.35 1.95 8.48 0.8±0.5 1.1±0.7 5.6±0.1 5.8±0.2\nTable 5: Validation metrics for both a TexDSP and a NoiseBandNet model trained on different textures. FAD metrics (lower is better) use\nVGGish and our proposed feature vector. Additionally, TexStat and MSS loss metrics are reported with means and standard deviations.\nIn all metrics smaller is better and the best performer is highlighted in bold.(1)Energy bands were imposed post-resynthesis.(2)A loudness\ntracker was added post-resynthesis.\nResults: The results highlight three key observations. First, per-\nformance varied across models, reflecting patterns observed in\nMcDermott and Simoncelli’s work and aligning with the limita-\ntions discussed in Subsection 2.4. Second, although some models\nperformed adequately, their scores remained lower than those of\nthe reconstruction focused model NoiseBandNet. This outcome\nis expected, as our approach prioritizes the disentanglement of\nhigher-level sound structures over precise reconstruction, an aspect\nfavored by the evaluation metrics used. The latter being said, sur-\nprisingly some TexDSP models managed to beat its counterpart\neven in these metrics. Third, the metrics derived from our mod-\nels appear to align more closely with our own perception of sound\nquality. However, to support this claim more robustly, a subjective\nevaluation would be necessary—an analysis that was beyond the\nscope of this work.\nAdditional Comments: A widely recognized application of the\noriginal DDSP model was timbre transfer [21], where features\nsuch as pitch and loudness from an unexpected input sound are\nused to drive a model trained on a different instrument’s timbre.\nFor instance, applying the features of a voice recording to a violin-\ntrained model will generate output that sounds like a violin play-\ning the same melody. This effect is primarily due to the strong\ninductive bias of the model, which is trained exclusively on vio-\nlin sounds and thus can only synthesize violin-like audio. Since\nthe model operates on extracted features rather than raw audio, it\nnaturally generates (\"transfer\") the timbre it was trained to pitch\nand loudness content extracted from the source sound, though this\neffect diminishes when pitch is less relevant.\nIn the models developed in this article, the same principle ap-\nplies, though with less clear-cut results. These models retain the\nDDSP architecture’s bias but are designed for a wider range of\nsounds. For example, a fire sound can be processed by a water-\ntrained model, resulting in a form of timbre transfer. However,\nunlike the pitched case, the results are harder to interpret be-\ncause pitch and loudness are more meaningful for musical sounds.\nFor texture-based sounds, meaningful timbre transfer occurs only\nwhen the input and output share a key feature from the training\ndata. While spectral centroid and rate might seem like viable fea-\ntures, they lack pitch’s distinctiveness in musical contexts. Many\nexamples of this effect are available on the article’s webpage.6. CONCLUSIONS\nThis paper introduced a novel framework for advancing the analy-\nsis and synthesis of texture sounds through deep learning. Central\nto our contribution is TexStat , a loss function grounded in au-\nditory perception and statistical modeling. By explicitly encoding\nkey properties such as time invariance, perceptual robustness, and\nlong-term structural focus, TexStat provides a formulation that\nis better aligned with the inherent nature of texture sounds than\nconventional alternatives.\nIn addition to TexStat , we presented two complementary\ntools: TexEnv , a signal processor that efficiently generates tex-\nture audio via amplitude envelope imposition, and TexDSP , a\nDDSP-based model that demonstrates the effective integration of\nour proposed loss into a synthesizer framework.\nOur experiments validated the theoretical motivations and\npractical utility of TexStat . Specifically, when used as a fea-\nture vector, the summary statistics derived from TexStat outper-\nformed general-purpose embeddings like VGGish in classification\ntasks. Moreover, TexStat exhibited improved stability against\ntransformations such as time shifting and noise addition, provid-\ning a perceptually coherent metric for evaluating resynthesis tim-\nbre similarity.\nWe also demonstrated the successful application of TexStat\nas a loss function in training the TexDSP model, where it guided\nthe learning process to generate indefinitely long sequences of con-\ntrollable texture sounds. Although the synthesized textures dif-\nfered from the input sounds, they maintained the essential percep-\ntual qualities that define their type. Despite its strengths, we ac-\nknowledge limitations in handling pitched or rhythmically struc-\ntured content, suggesting that TexStat is most effective when\ncombined with other losses as a regularization component or used\nas an evaluation metric.\nFuture work should extend TexStat to hybrid tasks by incor-\nporating additional loss terms for pitched and/or rhythmic sounds,\nand explore its applications in generative modeling, texture trans-\nformation, and neural sound design. Overall, this framework rep-\nresents a promising step toward achieving better perceptual align-\nment in machine listening and synthesis tasks.\nDAFx.7\n--- Page 8 ---\nProceedings of the 28thInternational Conference on Digital Audio Effects (DAFx25), Ancona, Italy, 2 - 5 September 2025\n7. ACKNOWLEDGMENTS\nThis work has been supported by the project \"IA y Música: Cá-\ntedra en Inteligencia Artificial y Música (TSI-100929-2023-1)\",\nfunded by the \"Secretaría de Estado de Digitalización e Inteligen-\ncia Artificial and the Unión Europea-Next Generation EU\".\n8. REFERENCES\n[1] Bela Julesz, “Visual pattern discrimination,” IRE Transac-\ntions on Information Theory , vol. 8, no. 2, pp. 84–92, Feb.\n1962.\n[2] Terry Caelli and Bela Julesz, “On perceptual analyzers un-\nderlying visual texture discrimination: Part i,” Biological\nCybernetics , vol. 28, no. 3, pp. 167–175, Sep. 1978.\n[3] Anne Humeau-Heurtier, “Texture feature extraction meth-\nods: A survey,” IEEE Access , vol. 7, pp. 8975–9000, 2019.\n[4] Josh H. McDermott, Andrew J. Oxenham, and Eero P. Si-\nmoncelli, “Sound texture synthesis via filter statistics,” in\n2009 IEEE Workshop on Applications of Signal Processing\nto Audio and Acoustics , New Paltz, NY , October 2009.\n[5] Josh H. McDermott and Eero P. Simoncelli, “Sound texture\nperception via statistics of the auditory periphery: evidence\nfrom sound synthesis,” Neuron , vol. 71, no. 5, pp. 926–940,\n2011.\n[6] Josh H. McDermott, Michael Schemitsch, and Eero P. Si-\nmoncelli, “Summary statistics in auditory perception,” Na-\nture Neuroscience , vol. 16, no. 4, pp. 493–498, 2013.\n[7] Nicholas Saint-Arnaud, “Classification of sound textures,”\nM.S. thesis, Massachusetts Institute of Technology, Cam-\nbridge, MA, Sep. 1995.\n[8] D.F. Rosenthal, H.G. Okuno, H. Okuno, and D. Rosenthal,\nComputational Auditory Scene Analysis: Proceedings of the\nIjcai-95 Workshop , CRC Press, 1st edition, 1998.\n[9] Lonce Wyse, “An audio texture lutherie,” Annual Art Jour-\nnal, vol. 43, no. 54, 2022.\n[10] Garima Sharma, Karthikeyan Umapathy, and Sridhar Krish-\nnan, “Trends in audio texture analysis, synthesis, and appli-\ncations,” J. Audio Eng. Soc. , vol. 70, no. 3, pp. 108–127,\nMarch 2022.\n[11] Nicolas Saint-Arnaud and Kris Popat, “Analysis and synthe-\nsis of sound textures,” in Readings in Computational Audi-\ntory Scene Analysis , pp. 125–131. 1995.\n[12] S. Dubnov, Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and\nM. Werman, “Synthesizing sound textures through wavelet\ntree learning,” IEEE Computer Graphics and Applications ,\nvol. 22, no. 4, pp. 38–48, 2002.\n[13] Diemo Schwarz, Data-Driven Concatenative Sound Synthe-\nsis, Ph.D. thesis, Université Paris 6 – Pierre et Marie Curie,\nParis, France, 2004.\n[14] Agostino Di Scipio, “The synthesis of environmental sound\ntextures by iterated nonlinear functions, and its ecological\nrelevance to perceptual modeling,” Journal of New Music\nResearch , vol. 31, no. 2, pp. 109–117, 2002.\n[15] James F. O’Brien, Chen Shen, and Christine M. Gatchalian,\n“Synthesizing sounds from rigid-body simulations,” in ACM\nSIGGRAPH/Eurographics Symposium on Computer Anima-\ntion, 2002, pp. 175–181.[16] Hugo Caracalla and Axel Roebel, “Sound texture synthe-\nsis using convolutional neural networks,” in Proceedings of\nthe 22nd International Conference on Digital Audio Effects\n(DAFx-19) , Birmingham, UK, September 2019.\n[17] Adrián Barahona-Ríos and Tom Collins, “Noisebandnet:\nControllable time-varying neural synthesis of sound effects\nusing filterbanks,” IEEE/ACM Trans. Audio, Speech and\nLang. Proc. , vol. 32, pp. 1573–1585, Feb. 2024.\n[18] Brian C. Moore and Brian R. Glasberg, “Suggested formu-\nlae for calculating auditory-filter bandwidths and excitation\npatterns,” The Journal of the Acoustical Society of America ,\nvol. 74, no. 3, pp. 750–753, 1983.\n[19] Roy Patterson, Ian Nimmo-Smith, John Holdsworth, and Pe-\nter Rice, “An efficient auditory filterbank based on the gam-\nmatone function,” in Speech-Group Meeting of the Institute\nof Acoustics on Auditory Modelling , RSRE, Malvern, Dec.\n1987, Institute of Acoustics, Meeting held on December 14–\n15, 1987.\n[20] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi, “Frechet audio distance: A reference-free\nmetric for evaluating music enhancement algorithms,” in IN-\nTERSPEECH 2019 , Graz, Austria, September 2019, ISCA.\n[21] Jesse Engel, Lalit Hantrakul, Chenjie Gu, and Adam Roberts,\n“Ddsp: Differentiable digital signal processing,” in Interna-\ntional Conference on Learning Representations , 2020.\n[22] Xavier Serra and Julius Smith, “Spectral modeling synthesis:\nA sound analysis/synthesis system based on a deterministic\nplus stochastic decomposition,” Computer Music Journal ,\nvol. 14, no. 4, pp. 12–24, 1990.\n[23] Frederic Font, Gerard Roma, and Xavier Serra, “Freesound\ntechnical demo,” in Proceedings of the 21st ACM Inter-\nnational Conference on Multimedia , New York, NY , USA,\n2013, MM ’13, p. 411–412, Association for Computing Ma-\nchinery.\n[24] Lonce Wyse and Prashanth Thattai Ravikumar, “Syntex:\nparametric audio texture datasets for conditional training of\ninstrumental interfaces.,” in Proceedings of the International\nConference on New Interfaces for Musical Expression , The\nUniversity of Auckland, New Zealand, jun 2022.\n[25] Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis,\nJort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj\nPlakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm\nSlaney, Ron J. Weiss, and Kevin Wilson, “Cnn architectures\nfor large-scale audio classification,” in 2017 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) , 2017, pp. 131–135.\nDAFx.8",
  "text_length": 46531
}