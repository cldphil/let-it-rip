{
  "id": "http://arxiv.org/abs/2506.05343v1",
  "title": "ContentV: Efficient Training of Video Generation Models with Limited\n  Compute",
  "summary": "Recent advances in video generation demand increasingly efficient training\nrecipes to mitigate escalating computational costs. In this report, we present\nContentV, an 8B-parameter text-to-video model that achieves state-of-the-art\nperformance (85.14 on VBench) after training on 256 x 64GB Neural Processing\nUnits (NPUs) for merely four weeks. ContentV generates diverse, high-quality\nvideos across multiple resolutions and durations from text prompts, enabled by\nthree key innovations: (1) A minimalist architecture that maximizes reuse of\npre-trained image generation models for video generation; (2) A systematic\nmulti-stage training strategy leveraging flow matching for enhanced efficiency;\nand (3) A cost-effective reinforcement learning with human feedback framework\nthat improves generation quality without requiring additional human\nannotations. All the code and models are available at:\nhttps://contentv.github.io.",
  "authors": [
    "Wenfeng Lin",
    "Renjie Chen",
    "Boyuan Liu",
    "Shiyue Yan",
    "Ruoyu Feng",
    "Jiangchuan Wei",
    "Yichen Zhang",
    "Yimeng Zhou",
    "Chao Feng",
    "Jiao Ran",
    "Qi Wu",
    "Zuotao Liu",
    "Mingyu Guo"
  ],
  "published": "2025-06-05T17:59:54Z",
  "updated": "2025-06-05T17:59:54Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05343v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05343v1  [cs.CV]  5 Jun 2025ContentV: Efficient Training of Video Generation\nModels with Limited Compute\nWenfeng Lin, Renjie Chen, Boyuan Liu, Shiyue Yan, Ruoyu Feng, Jiangchuan Wei\nYichen Zhang, Yimeng Zhou, Chao Feng, Jiao Ran, Qi Wu, Zuotao Liu, Mingyu Guo∗\nByteDance Douyin Content Group\nFigure 1: Video Generation Samples. ContentV can generate diverse, creative, and high-quality\nmulti-resolution videos based on textual prompts.\nAbstract\nRecent advances in video generation demand increasingly efficient training recipes\nto mitigate escalating computational costs. In this report, we present ContentV ,\nan 8B-parameter text-to-video model that achieves state-of-the-art performance\n(85.14 on VBench) after training on 256 ×64 GB Neural Processing Units (NPUs)\nfor merely four weeks. ContentV generates diverse, high-quality videos across\nmultiple resolutions and durations from text prompts, enabled by three key inno-\nvations: (1) A minimalist architecture that maximizes reuse of pre-trained image\ngeneration models for video generation; (2) A systematic multi-stage training\nstrategy leveraging flow matching for enhanced efficiency; and (3) A cost-effective\nreinforcement learning with human feedback framework that improves generation\nquality without requiring additional human annotations. All the code and models\nare available at: https://contentv.github.io .\n∗Project Leader\n--- Page 2 ---\n1 Introduction\nVideo generation has long been considered an extremely challenging task, primarily due to the\ndifficulties in modeling complex temporal dynamics and spatial details. With the release of Sora [ 6]\nby OpenAI, latent diffusion models based on the diffusion transformer (DiT) architecture have\ndemonstrated significant potential in this field, spurring a series of open-source efforts such as\nOpen-Sora-Plan [ 26], CogVideoX [ 52], HunyuanVideo [ 21], Step-Video-T2V [ 29], and Wan2.1 [ 46].\nHowever, current video generation models typically require massive computational resources for\ntraining. Moreover, the long-sequence nature of videos leads to exceptionally high GPU memory\ndemands, necessitating the use of advanced GPUs, which severely hinders progress in this domain.\nFor example, MovieGen [ 36] employed 6,144 H100 GPUs to train a 30B model on O(100 M)\nvideos and O(1 B) images. Additionally, we observe that existing methods predominantly adopt\na two-stage training paradigm of \"image-first, video-later.\" This raises a critical question: Can we\ndirectly leverage well-established image generation models from the open-source community (e.g.,\nStable Diffusion 3 [ 12], FLUX [ 24]) and adapt them to rapidly acquire video generation capabilities?\nTo address this question and the associated challenges, this technical report presents ContentV ,\nthe first open-source video generation model based on Stable Diffusion 3.5 Large (SD3.5L) that is\ntrained entirely on NPUs. We systematically share our data curation pipeline, model architecture\nadaptation, training infrastructure, as well as large-scale pre-training and post-training recipes. Our\nfindings reveal that latent diffusion models exhibit remarkable adaptability to variational autoencoder\n(V AE) substitution. Specifically, by simply replacing the V AE in SD3.5L with a 3D-V AE and\nincorporating 3D positional encoding, the original image model can rapidly acquire video generation\ncapabilities. Through large-scale pre-training at multiple resolutions and durations, the model learns\nto generate diverse and creative videos conditioned on prompts. Further enhancements via supervised\nfine-tuning (SFT) and reinforcement learning with human feedback (RLHF) significantly improve its\ninstruction-following ability and visual quality.\nFor training infrastructure, we designed a distributed training framework leveraging 64GB-memory\nNPUs (AI accelerators as GPU alternatives). By decoupling feature extraction and model training\ninto separate clusters and integrating asynchronous data pipelines with 3D parallelism strategies,\nwe achieved efficient training for 480P resolution, 24 FPS, 5-second videos. After one month of\ntraining on 256 NPUs, ContentV scored 85.14 on the VBench [ 19], outperforming CogVideoX-\n5B [52] and HunyuanVideo-13B [ 21], while also maintaining a slight edge over Wan2.1-14B [ 46] in\nhuman evaluations. This work establishes a new technical pathway for video generation research in\nresource-limited environments.\n2 Related Work\nIn the past few years, diffusion models [ 17,11] have demonstrated exceptional capabilities in image\ngeneration [ 41,39,33,12,24]. With the rapid advancement and widespread adoption of these image\ngeneration technologies, attention has increasingly shifted toward extending image into the temporal\ndomain: video composed of multiple continuous frames.\nEarly text-to-video generative models [ 44,18,14,15,5,48] primarily focused on adapting pre-\ntrained text-to-image architectures for video generation by incorporating learnable temporal layers.\nThese approaches typically employed hierarchical cascaded pipelines comprising distinct stages for\nkeyframe generation, temporal upsampling, and spatial upsampling. However, several limitations\nhindered their effectiveness. Some methods [ 44,18] performed video generation directly in pixel\nspace, which proved computationally expensive and difficult to optimize. Furthermore, even when\nutilizing variational autoencoders to map video in pixel space to latent space, these approaches often\nfailed to leverage the temporal redundancy inherent in video data, resulting in inefficient optimization\nand suboptimal performance. Additionally, the cascaded structure necessitated complex generation\nworkflows and precluded end-to-end optimization.\nThe debut of Sora [ 7] in early 2024 represented a pivotal advancement in this field. Leveraging\na 3D-V AE architecture with full attention mechanisms [ 32], Sora demonstrated how scaling com-\nputational resources and training data could yield substantial improvements. Specifically, Sora\nachieved remarkable spatial-temporal consistency, generated extended video sequences with coherent\nnarratives, and facilitated seamless video continuation. Furthermore, Sora’s capacity to simulate\n2\n--- Page 3 ---\nphysical interactions suggested its potential as a world model capable of encoding physical dynamics,\nthereby catalyzing intensified research interest in high-fidelity video synthesis methodologies.\nSora’s impressive performance and substantial potential have sparked a research surge in academia\nand prompted increased resource investment from industry, driving rapid advancements in the field.\nOpen-source models such as Open-Sora [ 57], Open-Sora-Plan [ 26], Mochi-1 [ 45], CogVideoX [ 52],\nHunyuanVideo [ 21] and Wan2.1 [ 46] have provided a solid foundation for academic research, while\ncommercial closed-source models like Kling [23], Hailuo [30], Vidu [43], PixelVerse [35], Runway\nGen-3 [ 40], Pika [ 34], and Veo [ 13] have established more user-friendly interfaces, facilitating\nwidespread adoption and dissemination of video generation technology among the general public.\n3 Data Curation Pipeline\nIn modern model training, data quality and diversity take precedence over quantity. Especially for\ngeneration tasks, low-quality data can deteriorate the generation results of the model. In this section,\nwe describe the pipelines for large-scale high-quality data collection, and processing, as well as\nvarious processors and criteria for data filtering.\n3.1 Processing and Filtering\nWe collected raw data from a diverse range of sources, including publicly available academic\ndatasets [ 42,9,49] and online CC0-licensed websites, such as Mixkit and Pexels. These datasets\ncover a broad spectrum of scenes, motions, and visual styles, which are crucial for training a video\ngeneration model with strong generalization capabilities.\nGiven the diverse sources of our raw data, a substantial portion of the videos are of low quality, which\nsignificantly degrades the performance of video generation. To address this, we use a comprehensive\ncontent-aware data filtering pipeline that operates on intrinsic video dimensions to enhance the overall\nquality of the dataset.\nVideo Segmentation. The duration of the original videos ranges from a few seconds to several hours.\nTraining requires splitting long videos into second-level video clips. Meanwhile, in order to avoid\nscene change in a video clip, we use PySceneDetect [ 1] to distinguish scene changes, cut long videos\ninto continuous shots, and then split them into short video clips ranging from 3 to 6 seconds based on\nthe duration. To ensure clear temporal boundaries, we further remove transitions with gradual scene\nchanges using temporal pattern analysis.\nVideo Deduplication. We propose a hierarchical deduplication approach to address data redundancy\nand imbalance in video datasets. Specifically, we use an internal embedding model for video feature\nextraction. For the original long videos, we perform k-means clustering to group semantically similar\ncontent, and apply adaptive thresholding where clusters with higher density undergo more aggressive\ndeduplication using stricter similarity thresholds. Video clips derived from the same source video\noften exhibit substantial content and scene overlap. To mitigate redundancy, we compute pairwise\nfeature similarities and discard highly similar clips.\nSubtitle and Watermark Detection. We utilize PaddleOCR [ 2] to locate and crop subtitles, as well\nas to remove video clips with excessive text. Furthermore, we also use an internal detection model to\ndetect and remove sensitive information such as watermarks, logos, and black borders,.\nBlur Detection. We detect blurriness by computing the variance of the Laplacian operator [ 31],\nwhich serves as a quantitative measure of frame sharpness. Lower variance values correspond to\nincreased blurriness, typically resulting from camera motion or focus inaccuracies.\nMotion Dynamics. The dynamics of the main object in the video mainly come from two aspects:\none is the movement of the object itself, and the other is the simultaneous movement of the foreground\nand background brought about by the movement of the camera lens. We take the average dynamic\nscore of the foreground and the background as the overall dynamic score. Specifically, The dense\noptical flow is obtained by using GMFlow [ 51]. The approximate background optical flow is obtained\nby fitting the perspective transformation, and then the foreground optical flow is rectified. So as to\nseparate the foreground and background and calculate the dynamic scores respectively.\n3\n--- Page 4 ---\nAesthetic Score. We use an improved aesthetic score predictor based on the CLIP model trained on\nLAION [42] to predict the aesthetic scores of key frames in each clip and calculate their average.\nAs shown in Figure 2, using our data curation pipeline, we construct a high-quality visual dataset at a\nscale of O(100 M) images, and O(10 M) video clips, the duration range from 2 to 8 seconds.\nFigure 2: Overall of data filtering pipeline. For the filtered pre-training data, data from different\ntraining stages will be filtered out based on duration and resolution.\n3.2 Post-training Data Selection\nAfter passing through the data filtering pipeline, a large number of low-quality and repetitive videos\nin the original data were filtered out, and a pre-training dataset of tens of millions of video clips\nwas constructed through video slicing. However, due to the large quantity of pre-trained video data,\nthe variation range of the quality of pre-trained data is very wide. Furthermore, we applied stricter\nfiltering criteria on the pre-training data, obtaining a high-quality subset of one million. In particular,\nwe emphasize the following refined selection criteria:\n•Aesthetic Score. The aesthetic model trained on laion is mainly for images and has a strong\npreference for high color contrast. And unlike a single image, the frames extracted from the\nvideo will have an inter-frame smoothing effect due to the need for temporal continuity. We\ntrained an aesthetic model of video based on the VideoCLIP [ 47], which can consider both\nthe visual quality of the picture and the degree of text alignment in the video simultaneously.\n•Motion Dynamics. At this stage, we emphasize more on the motion dynamics of the main\nsubject of the video. We calculate the dynamic score in a way similar to the foreground\nand background in the pre-training stage respectively. However, we give a higher weight to\nthe foreground dynamic score and take the weighted dynamic score of the foreground and\nbackground as the total dynamic score.\nFor the pre-training data, we use these two stricter criteria to score and rank the clips, and select the\nclips that are in the top 10% in both criteria. A total of one million clips were obtained.\n3.3 Captioning\nAs demonstrated by DALL-E 3 [ 4], the precision and comprehensiveness of captions are essential\nfor enhancing the prompt-following ability and output quality of generative models. We extract\nvideo frames at 1 fps and generate dense captions using the Qwen 2.5 VL [ 3] multimodal model.\nWe found that larger models yield more accurate captions while mitigating hallucination effects.\nHowever, generating dense captions for tens of millions of videos with the largest 72B model incurs\nprohibitively high computational costs. Therefore, we employ the 7B model for pre-training data,\nreserving the 72B model exclusively for the higher-quality subsets used in post-training.\n4 Architecture\nIn this section, we present the architecture of ContentV . Unlike previous approaches [ 52,46,21,\n37,29] that prioritize novel structural designs, our methodology emphasizes the efficient adapta-\n4\n--- Page 5 ---\ntion of open-source text-to-image models, achieving video generation capability through minimal\narchitectural modifications.\n4.1 Overview\nN x MM-DiT Blocks\nCausal 3D VAE\nEncoder\nText \nEncoder\nA curious monkey sits \natop a weathered \nstone, surrounded by \nlush greenery.\n......\n...\n......3D Patchify...\nNoise3D positional \nembedding\nTime-step t...\n...\n...\nFigure 3: Architecture of ContentV . Simply replace the V AE in SD3.5L with a causal 3D-V AE and\nincorporate 3D position embedding to unlock the model’s video generation capabilities.\nAs shown in Figure 3, the text-to-video diffusion model based on the SD3.5L [ 12] architecture also\nincludes three components: variational autoencoder, text encoder, and DiT. The most significant\narchitectural change occurred in the variational autoencoder, where 3D-V AE replaced the 2D-V AE in\nimage generation. This modification enables the model to handle images and videos by compressing\nthem into a unified latent representation. The 3D-V AE first compresses the input visual data, which\nis then processed through a 3D patchification operation. Subsequently, these patchified features\nare flattened into a one-dimensional sequence, enabling the DiT model to handle both image and\nvideo generation tasks simultaneously. As shown in the Figure 1, the number of attention layers and\ndimensions of ContentV are consistent with those of SD3.5L.\nTable 1: Architecture configuration of ContentV .\nPatch Size #Layers Attention Heads Head Dimension FFN Dimension\n1x2x2 38 38 64 9728\n4.2 3D Variational Auto-encoder\nPrevious works [ 26,52,46,21] use a 3D-V AE to compress pixel-space images and videos into a\nunified latent space. It is common to adopt temporal causal 3D convolution [ 53], ensuring the frame\nonly depends on previous frames. Specifically, for a video of shape (T+ 1)×3×H×W, the\n3D-V AE compresses it into latent space with shape (T\nct+ 1)×C×(H\ncs)×(W\ncs), where csis spatial\ndownsample factor and ctis temporal downsample factor. Follow previous works, we use ct= 4,\ncs= 8, and C= 16 in our implementation. This compression significantly reduces the number of\ntokens for long-duration videos, allowing us to train videos at the original resolution and high frame\nrate. For the open-source version, we directly used the V AE of Wan2.1 [ 46] as the encoder for images\nand videos.\n4.3 3D Diffusion Transformer\nThe DiT of SD3.5L is designed for image input and needs to be modified to adapt to video input.\nSince the latent feature will be flattened into a one-dimensional sequence, the position information\nneeds to be distinguished through position embedding. Furthermore, for video input, the sequence\nlength becomes significantly longer (~116k), and QK-Norm plays an important role in stabilization\ntraining.\n5\n--- Page 6 ---\n0 100 200 300 400\nTraining Step0.00.51.01.52.0LossAbsolute Position Embedding\nRotary Position EmbeddingFigure 4: RoPE v.s. APE3D Position Embedding. In SD3.5L [ 12],Absolute Posi-\ntion Embedding (APE) is used to capture spatial relationships.\nWhen extending the DiT of SD3.5L from image input to video\ninput, we add additional time position embeddings to appropri-\nately represent sequential time information. Recently, Rotary\nPosition Embedding (RoPE) uses rotation matrix for position\nencoding, which enables the relative position information be-\ntween tokens at different positions to be well learned, improves\nthe model’s adaptability to long context, and has become the\nmainstream method for large language model and text-to-video\ndiffusion models. We also attempted to replace APE in SD3.5L\nwith RoPE. The loss curve is shown in the Figure 4. The model\nonly needs to be trained with a small number of steps (~500)\nto quickly adapt to the new position encoding. And after longer\ntraining, RoPE did not show a significant improvement in its score on VBench compared to APE.\nConsidering the marginal gap between RoPE and APE, we still retained APE in text-to-video setting.\nQ-K Normalization. In large language model, training large-scale transformers can occasionally\nresult in loss spikes, which may lead to model corruption. Training DiT on long sequences also\nfaces the same challenges, especially when training with mixed precision. Half-precision can\nlead to gradient explosion in long sequences, and the growth of grad norm causes the loss to not\nconverge. Single-precision training using float32 can avoid this problem, but the training time\nincreases significantly. The query-key normalization [ 10] used in SD3.5L can avoid this problem.\nSpecifically, we apply RMSNorm [ 54] to each query-key feature prior to attention computation,\nensuring smoother and more reliable training dynamics.\n5 Training Strategy\n5.1 Flow Matching\nTraining Objective. TheContentV is trained using Flow Matching [ 27], an algorithm that enables\nefficient sampling through straight probability paths in continuous time. Given paired data samples\nx1∼pdataand noise samples x0∼ N(0,I), we define the interpolated sample at time t∈[0,1]as\nxt= (1−t)x0+tx1. The model is trained to predict the velocity vtarget= dxt/dt=x1−x0,\nwhich guides the xttowards x1. The model parameters are optimized by minimizing the mean\nsquared error between the predicted velocity vθand the ground truth velocity vtarget, expressed as the\nloss function:\nLFM=Et,x0,x1\u0002\n∥vθ(xt, t)−vtarget(xt,x0,x1)∥2\n2\u0003\n(1)\nDuring the sampling, a gaussian noise x0∼ N (0,I)will be drawn at first. The first-order euler\nordinary differential equation solver is used to compute x1by the velocity prediction. This process\nultimately generates the final sample x1.\nFlow Shift. Thetin Equation (1)is initially sampled from uniform distribution. However, Esser\net al. [12] demonstrated that directly applying the same noise schedule from low-resolution to high-\nresolution image generation leads to insufficient noise addition and inadequate image degradation.\nThey proposed using logit-normal and flow shift to replace the uniform distribution. As shown in\nFigure 5, during training, this timestep sampling method is equivalent to using different weights\nfor the timestep, while during inference, this sampling method is an adaptive step size adjustment,\nthat is, using small-step updates with more steps in the high-noise stage and small-step updates with\nfewer large-step updates in the low-noise stage. In the experiment, we found that using a larger flow\nshift during training can bring a faster convergence speed, but has a smaller impact on the model’s\ngeneration ability. Using a larger flow shift in the sampling can enhance the generation effect. As\nshown in the Figure 6, when the flow shift is 1, it is almost impossible to generate distinguishable\nresults. When the flow shift increases, the picture quality is significantly improved. Therefore, we\nused a flow shift of 1 during training and a flow shift of 17 during sampling.\n6\n--- Page 7 ---\n0.0 0.2 0.4 0.6 0.8 1.0\nTime-stepDistributionShift = 1.0\nShift = 5.0\nShift = 9.0\nShift = 13.0\nShift = 17.0(a) Distribution of timesteps with various\nshift.\n0 10 20 30 40 50\nSampling step0.00.20.40.60.81.0Time-step\nShift = 1.0\nShift = 5.0\nShift = 9.0\nShift = 13.0\nShift = 17.0 (b) Sampling timestep under varying shift.\nFigure 5: Illustration of Flow shift.\nFigure 6: Sampling results under varying classifier-free guidance scales and flow shift.\nShift = 1 Shift = 5 Shift = 9 Shift = 13 Shift = 17\nCFG = 3\nCFG = 6\nCFG = 9\nCFG = 12\n5.2 Video V AE Adaptation\nTo extend the DiT of SD3.5L from image to the video generation, a critical first step involves replacing\nthe original 2D-V AE, which is trained exclusively on images, with a 3D-V AE capable of processing\nboth images and videos. The adaptation process begins by retraining the DiT model on images to\nreestablish its fundamental image generation capabilities. As demonstrated in Figure 7, the transition\nto 3D V AE requires only 800 training steps for the DiT to successfully adapt to the new latent space\nrepresentation while maintaining its image synthesis performance.\nWe quantitatively evaluated this adaptation process through Fréchet Inception Distance (FID) [ 16]\nmeasurements across 10K generated images. The FID of the SD3.5L at a resolution of 512 is 12.8.\nWhen we directly replace the V AE, all the generated images are meaningless, and the FID is 294.3,\nwhich is very large. However, after only 200 steps of training, the FID rapidly dropped to 27.8 and\nwas able to generate distinguishable images. After training for 1600 steps, FID further decreased to\n13.05, which was basically close to the generation ability before switching V AE. These quantitative\nresults align precisely with qualitative observations from the generated samples, confirming the\neffectiveness and efficiency of our adaptation approach.\n7\n--- Page 8 ---\nStep 0 Step 100 Step 200 Step 400 Step 800\nFigure 7: Generated images during 3D-V AE adaptation. The rapid recovery of image generation\ncapability demonstrates that DiT of SD3.5L can quickly adapt to the new 3D-V AE.\n5.3 DiT Pre-Training\nImage Video Joint Training. Our training recipe incorporates images in controlled proportions for\njoint video-image optimization. This strategy serves two key purposes: (1) mitigating the scarcity of\nhigh-quality video datasets by leveraging the abundant visual knowledge available in static images,\nand (2) maintaining the model’s image comprehension capabilities despite the domain shift between\nvideo and image distributions. Contrary to conventional approaches, our analysis reveals a nuanced\ntrade-off: while image data enhances generation quality through improved spatial understanding, it\nsimultaneously compromises temporal dynamics. This phenomenon stems from the fundamental\ntension between learning spatial semantics (critical for images) and temporal coherence (essential\nfor videos). To resolve this conflict, we implement a phased training recipe. The initial phase\nfocuses exclusively on video data to establish robust temporal representations. Subsequently, we\nintroduce a joint training period where both modalities are processed in balanced proportions. This\nstaged approach enables effective cross-modal knowledge transfer while minimizing the risk of\nrepresentation bias toward either domain.\nMultiple Aspect Ratios and Durations Bucketization. After the data filtering process, the videos\nhave different aspect ratios and durations. To effectively utilize the data, we categorize the training\ndata into buckets based on duration and aspect ratio. Specifically, we have preset seven common\nvideo aspect buckets: {16 : 9 ,3 : 2,4 : 3,1 : 1,3 : 4,2 : 3,9 : 16}. Temporal duration is\ndiscretized through uniform partitioning with 1-second intervals, creating a comprehensive set of\nduration buckets. To address the memory constraints imposed by variable sequence lengths across\nbuckets, we implement a dynamic batch sizing mechanism. Each bucket is assigned an empirically\ndetermined maximum batch size that optimizes memory utilization while preventing out-of-memory\nconditions. All data are allocated to the nearest bucket and truncated to form a batch. During training,\neach rank randomly pre-fetches batch data from a bucket. This random selection ensures the model is\ntrained on varying data sizes at each step, which helps maintain model generalization by avoiding the\nlimitations of training on a single size. The combined bucketization approach effectively balances\ncomputational efficiency with representational diversity in the training process.\nProgressive Video Training. Directly training on high-resolution, long-duration video sequences\noften leads to difficulties in model convergence and suboptimal results. Therefore, progressive\ncurriculum learning has become a widely adopted strategy for training text-to-video models. In\n8\n--- Page 9 ---\nContentV , we designed a progressive training strategy, starting from the text-to-image weights and\nfollowing the training method of increasing the duration first and then the resolution\n•Stage1: low resolution, short duration stage. The model establishes the basic mapping\nbetween text and visual content, ensuring consistency and coherence in short-term actions.\n•Stage2: low resolution, long duration stage. The model learns more complex temporal\ndynamics and scene changes, ensuring temporal and spatial consistency over a longer\nduration.\n•Stage3: high resolution, long duration training. The model enhances video resolution\nand detail quality while maintaining temporal coherence and managing complex temporal\ndynamics.\nThe detailed training configuration and hyperparameter specifications are presented in Table 2. We\nfound that the model primarily acquires dynamic visual representations during stage 2 of the training\nprocess. Consequently, we allocated the majority of computational resources to this critical phase,\noptimizing both training efficiency and model performance.\nTable 2: Training configuration across different stages.\nStage Max ShapeTotal Batch SizeLearning Rate Training Steps\nImage Video\nV AE Adaption 1×288×288 4096 - 1×e−45k\nStage 1 29×288×288 4096 2048 1×e−440k\nStage 2 125×288×288 4096 512 1×e−460k\nStage 3 125×432×768 2048 256 5×e−430k\nSFT 125×432×768 - 256 1×e−55k\n6 Post-Training\nAfter pretraining on large-scale data, the diffusion model demonstrates the capability to generate\ndiverse and creative videos. However, several critical challenges remain in the generated videos,\nincluding text-video misalignment, deviations from human aesthetic preferences. To enhance both the\nvisual fidelity and motion dynamics of generated videos, we employ post-training on the pretrained\nmodel. Specifically, we conduct SFT on a high-quality subset of the pretrain data, followed by RLHF\nto further refine visual quality.\n6.1 Supervised Fine-Tuning\nFor the data in the pre-training stage, we use a loose filtering threshold, which enables us to retain\na large amount of training data, but reduces the data quality in the pre-training stage. On the other\nhand, due to the need for multi-duration training, the duration of the video will be truncated, resulting\nin inconsistency between the actual training video and the caption. These problems have led to the\nlow quality of the videos generated by the pre-trained model and poor alignment with the prompts.\nTherefore, in the SFT stage, we use a high-quality subset of the pre-training data and more consistent\ncaptions to improve the performance of the model.\nData. As mentioned in the Section 3.2, we use stricter filtering criteria in the two dimensions\nof aesthetic quality and motion dynamics to filter out a high-quality subset of 1 million from the\npre-training data. Furthermore, we pre-crop and truncate the video to ensure that the training video is\ncompletely aligned with the caption. At the same time, each video is marked with both short and\ndetailed captions, and one of the captions is randomly selected during SFT.\nImplementation Details. The SFT stage uses a smaller learning rate (10% of pre-training learning\nrate) with gradient clipping to avoid catastrophic forgetting. To maintain generative diversity, we\nregularize the training with KL-divergence penalties against the original pre-trained model’s outputs.\nThis supervised fine-tuning stage typically requires only 5% of the computational cost of pre-training\nwhile yielding significant improvements in video quality.\n9\n--- Page 10 ---\n6.2 Reinforcement Learning from Human Feedback\nRLHF for diffusion model aims to optimize a conditional distribution pθ(x0|c)such that the reward\nmodel r(c, x0)defined on it is maximized, while regularizing the KL-divergence from a reference\nmodel pref. More specifically, RLHF optimizes a model pθto maximize the following objective:\nmax\npθEc∼Dc,x0∼pθ(x0|c)[r(c, x0)]−βDKL[pθ(x0|c)∥pref(x0|c)] (2)\nwhere the hyperparameter βcontrols KL-regularization strength. Inspired by Wu et al. [50], supervises\nthe final output of diffusion model and back-propagates through the iterative sampling process to the\ninput noise. Instead of training at equal intervals, we randomly sample ksteps carrying gradient from\nthe timesteps of infer. Considering that the model for video generation is large and the sequence is\nlong while the memory of the AI accelerators is not large, we ignored the KL constraint for the sake\nof computational efficiency.\nData. For the RLHF training, we constructed a high-quality prompts set through a rigorous multi-\nstage process. First, we randomly selected 80,000 prompts from our SFT dataset, ensuring balanced\ncoverage across different content categories and prompt styles. The SFT dataset itself was carefully\ncurated from diverse sources including: (1) professionally created content descriptions, (2) user-\ngenerated prompts from creative platforms, and (3) synthetic prompts generated by language models\nwith human verification.\nReward Models. Initially, we employed VideoAlign [ 28], a reward model based on the visual\nlanguage model (VLM), to evaluate generated videos across three key dimensions: visual quality\n(VQ), motion quality (MQ), and text alignment (TA). During training, we observed that while the\nreward scores improved steadily, the qualitative performance of the generated videos showed no\nsignificant enhancement. Further analysis revealed that the VLM-based reward model introduced\nan unintended bias: instead of holistically optimizing video quality, the model disproportionately\nprioritized literal prompt adherence, leading to suboptimal generations. We choose MPS [ 55], a\nCLIP-based model, as our visual quality reward model. It can effectively enhance the visual quality\nof the video.\nImplementation Details. The process of generating initial videos x1from noise through diffusion\nmodels proves computationally intensive, particularly for extended video sequences. A complete\niterative generation cycle typically requires tens of minutes to execute, while the subsequent decoding\nfrom latent space to pixel space demands substantial video memory resources. To address these\nefficiency challenges during the RLHF stage, we implemented two key optimizations: (1) reducing\nthe generated video length from 125 frames to 29 frames, which dramatically decreases generation\ntime; and (2) employing a selective decoding strategy that leverages the causal properties of V AE\narchitecture. Specifically, we decode only the first frame and utilize MPS to enhance the percep-\ntual quality of generated images while maintaining computational efficiency. These modifications\ncollectively improve the training pipeline’s throughput without compromising output quality.\n7 Infrastructure\nThis section presents the infrastructure designed to enable efficient and scalable training of the text-\nto-video model. Our training is based on a cluster consisting of 256 NPUs. To address the substantial\nmemory and computational demands of large-scale video generation, we implement a series of\noptimization strategies. These include: (1) architectural separation between the feature encoder\nand training modules to reduce memory overhead; (2) advanced memory optimization techniques\nto maximize hardware utilization; and (3) a 3D parallelism strategy that combines data, param,\nand sequence parallelism. Through the synergistic application of these methods, we achieve stable\ntraining of an 8-billion parameters video generation model within the 64 GB memory constraints.\n7.1 Hardware\nOur training setup employs a cluster consisting of 32 nodes, each node equipped with 8 NPUs\nand the torch_npu 2.1.0 deep learning framework. To optimize distributed training performance,\nwe leverage the Collective Communication Library (HCCL), a high-performance communication\nlibrary that enables efficient multi-node synchronization with high bandwidth and low latency.\n10\n--- Page 11 ---\nThe interconnect architecture delivers exceptional bandwidth capabilities: each node supports a\none-to-one communication bandwidth of 56 GB/s and an aggregate one-to-many bandwidth of\n392 GB/s. The NPU feature 64 GB of High Bandwidth Memory (HBM) per card and deliver a\ncomputational throughput of 320 TFLOPS when operating with BF16/FP16 mixed-precision. This\nhardware configuration provides the necessary computational density and communication bandwidth\nfor large-scale distributed training workloads.\n7.2 Asynchronous Encode Server\nHDFS/\nDISKVAE\nEncoderVAE\nEncoderVAE\nNode\nText\nEncoderText\nNodeDiT\nNodeDiT\nNodeDiT\nNodeDiT\nNode\nNetwork IO Network IO\nEncode Train Data\nFigure 8: Asynchronous Encode Server. The training of the model and feature extraction are placed\non different nodes, and the transmission of data and features is completed through asynchronous\nnetwork requests.\nSince the text-to-video diffusion model performs denoising in latent space, it requires both V AE\nand text encoder to extract latent features and text embeddings respectively. Although this inference\nstage involves no gradient computation, the T5-XXL [ 38] text encoder and V AE encoder still\ndemand significant memory resources. In SD3.5L [ 12], feature extraction is performed offline, which\ninevitably consumes substantial storage space. To address these resource constraints, we propose a\ndecoupled architecture that separates computing resources for inference and training. Specifically,\nthe V AE and text encoder operate on dedicated inference clusters, continuously generating processed\ninput data (including image/video latent featuers and corresponding text embeddings) for DiT training.\nThe training cluster then retrieves these pre-computed features via network transmission and focuses\nexclusively on the training process.\nBatch Buffer. For each rank, we employ a dedicated process to manage a data buffer. During the\ninitial phase, while the buffer has not reached its full capacity, the process iteratively fetches new\nbatch data from the server and stores it locally. This design enables training nodes to directly access\nthe required features from their local storage, eliminating the need for frequent remote data retrieval\nduring training. To ensure synchronization across all ranks, we utilize the training step count as a\nunified identifier, which guarantees consistent coordination between data loading and model training\nprocesses.\n7.3 Memory Optimization\nAttention optimization. As the sequence length grows, the computational overhead of attention\nbecomes the primary bottleneck in training. We leverage npu_fusion_attention to accelerate\nattention calculations, achieving significant improvements in computational efficiency. Addition-\nally, this optimization substantially reduces memory consumption, enabling the training of longer\nsequences within constrained hardware resources.\nGradient Checkpointing. Gradient checkpointing is a memory optimization technique that trades\ncomputational overhead for reduced memory requirements. The method consists of three key steps:\n(1) selectively designating specific layers or blocks for recomputation, (2) releasing intermediate\nactivations during the forward pass, and (3) recalculating these activations as needed during the\n11\n--- Page 12 ---\nbackward pass. This approach significantly decreases memory consumption during training, enabling\nthe processing of larger models or batch sizes within limited memory constraints.\n7.4 Parallelism Strategies\nThe substantial model size and the exceptionally long sequence length (exceeding 116K tokens\nfor the longest sequence) necessitates the implementation of advanced parallelism strategies to\nensure computationally efficient training. To address this challenge, we adopt 3D parallelism, a\nscalable approach that combines data parallelism, tensor (parameter) parallelism, and sequence\nparallelism. This strategy enables efficient distribution of computational workloads across all three\ncritical dimensions, including data batches, model parameters, and long sequences, thereby optimizing\nmemory usage and computational throughput.\nSequence Parallelism(SP). Sequence Parallelism is a technique that partitions input tensors along\nthe sequence dimension, enabling independent processing of layers such as LayerNorm to eliminate\nredundant computations and reduce memory overhead [ 22,25,20]. This approach also facilitates\nefficient handling of variable-length sequences by supporting dynamic padding for non-conforming\ninputs. Our implementation leverages Ulysses [ 20], a framework that shards input samples across\nthe sequence parallel group at the beginning of the training loop. During attention computation,\nUlysses employs all-to-all communication to distribute query, key, and value tensors such that\neach worker processes the full sequence length but only a subset of attention heads. Following\nparallel computation, a second all-to-all operation reassembles the outputs, reconstructing both the\nattention heads and the original sequence shards. To ensure robustness, our implementation includes\nengineering optimizations for input sequences that do not inherently meet SP requirements, such as\nautomatic padding to maintain computational efficiency.\nFully Sharded Data Parallelism. Zhao et al. [56] introduces Fully Sharded Data Parallelism\n(FSDP), a memory-efficient distributed training approach that partitions model parameters, gradients,\nand optimizer states across data-parallel ranks. Unlike Distributed Data Parallelism (DDP), which\nrelies on all-reduce for synchronization, FSDP employs all-gather for parameter retrieval and reduce-\nscatter for gradient aggregation. These operations are strategically overlapped with forward and\nbackward computations, reducing communication overhead and improving training efficiency. In our\nimplementation, we leverage the HYBRID_SHARD strategy, which combines intra-group parameter\nsharding ( FULL_SHARD ) with inter-group parameter replication. This hybrid approach effectively\nimplements hierarchical data parallelism: while FULL_SHARD optimizes memory usage within each\nshard group, cross-group replication enhances computational throughput. By localizing all-gather\nand reduce-scatter operations within shard groups, this method significantly reduces inter-rank\ncommunication costs compared to global synchronization schemes.\n8 Performance\n8.1 Qualitative Visualizations\nFor intuitive comparisons, we conduct qualitative assessments and present sampled results in Fig-\nure 9. The evaluation includes open-source models CogVideoX-5B [ 52], HunyuanVideo-13B [ 21],\nWan-14B [ 46], as well as closed-source products, including MINIMAX Hailuo [ 30], Vidu [ 43],\nKling1.6 [ 23], and Jimeng PixelDance 2.0 [ 8]. The Figure 9 shows that, compared with other open-\nsource models, ContentV can achieve better visual quality and be comparable to some closed-source\nmodels. This is because RLHF aligns with human preferences, significantly enhancing the visual\nquality of the video. We present the visualization results of the Pre-train, SFT, and RLHF stages\nrespectively in Figure 10. The SFT stage enhances the semantic alignment between the video and\nthe prompt, while RLHF improves the visual quality and details, particularly in terms of human\npreference. More video samples are shown on the project page.\n8.2 Quantitative Analysis\nFor quantitative analysis, we evaluate ContentV against the SOTA text-to-video models on\nVBench [ 19], a benchmark for comprehensively assessing the quality of video generation from\n16 dimensions. Vbench provides two types of prompts: One is short and simple prompts, with an\n12\n--- Page 13 ---\nCogVideoX-5B\nHunyuanVideo-13B\nWan2.1-14B\nMINIMAX Hailuo\nVidu\nKling 1.6\nJimeng PixelDance 2.0 Pro\nContentV -8B\nFigure 9: Qualitative comparison between ContentV and other T2V models. Prompt: A lively group\nof friends, diverse in appearance and style, gather in a cozy, warmly lit living room, filled with\nlaughter and camaraderie. They sit on a plush, colorful rug, surrounded by soft cushions and a\nlow wooden table adorned with snacks and drinks. Each friend, dressed in casual, vibrant attire,\nexpresses agreement through enthusiastic hand gestures, such as thumbs up, high-fives, and fist\nbumps. Their faces light up with genuine smiles and nods, reflecting a shared understanding and\nmutual support. The room’s ambiance, with its soft lighting and eclectic decor, enhances the sense of\nwarmth and friendship.\n13\n--- Page 14 ---\n(a)A serene individual, dressed in a flowing white gown, sits gracefully in a sunlit room adorned with lush\ngreen plants and soft, billowing curtains. Their fingers delicately pluck the strings of a golden harp, producing\nethereal melodies that fill the air. The camera captures close-ups of hands, showcasing the intricate movements\nand the harp’s ornate details. Sunlight filters through the window, casting a warm glow on serene face, eyes\nclosed in deep concentration. The scene transitions to a wider shot, revealing the tranquil ambiance of the room,\nwith the gentle sway of the curtains and the soft rustle of leaves enhancing the peaceful atmosphere.\n(b)A playful panda stands confidently on a surfboard, riding gentle waves in the ocean during a breathtaking\nsunset. The sky is ablaze with hues of orange, pink, and purple, casting a warm glow on the water. The panda,\nwith its black and white fur glistening in the golden light, balances effortlessly, its eyes wide with excitement. The\nsurfboard, painted in vibrant colors, cuts through the shimmering waves, leaving a trail of sparkling droplets. In\nthe background, the sun dips below the horizon, creating a serene and magical atmosphere, as the panda enjoys\nits unique adventure amidst the tranquil sea.\n(c)A suited astronaut, with the red dust of Mars clinging to boots, reaches out to shake hands with an alien,\ntheir skin a shimmering blue, under the pink-tinged sky of the fourth planet. In the background, a sleek silver\nrocket, stands tall, its engines powered down, as the two representatives of different worlds exchange a historic\ngreeting amidst the desolate beauty of the Martian landscape.\nFigure 10: Qualitative comparison of ContentV at different stages.\n14\n--- Page 15 ---\naverage of 7.6 words. Another type is long and detailed prompts enhanced by chatGPT, with an\naverage of 100.5 words. We evaluate both types of prompts simultaneously to comprehensively\nmeasure the capabilities of ContentV .\nAs illustrated in Table 3, ContentV demonstrated outstanding performance in multiple evaluation\ndimensions and outperformed numerous open-source and commercial models in the overall score.\nAmong all open-source models, ContentV is only inferior to Wan2.1 [ 46]. However, compared\nwith Wan2.1 with 14B parameters, ContentV achieved comparable performance by using only 8B\n(~57% of 14B) parameters. Notably, ContentV achieved a higher VBench overall score in long\nprompts, mainly because detailed prompts brought a significant improvement of 3.51 in the semantic\nscore. This indicates that ContentV has the ability to understand complex prompts and enhance the\nalignment of text-video.\nTable 3: Comparison with leading T2V models on VBench[ 19] in descending order of overall score.\n†denotes models with non-publicly available weights. The results of other models come from official\nVBench leaderboard.\nModels OverallQuality\nScoreSemantic\nScoreHuman\nActionSceneDynamic\nDegreeMultiple\nObjectsAppear.\nStyle\nVidu-Q1† 87.41 87.28 87.94 99.60 67.41 91.85 93.89 22.37\nWan2.1-14B 86.22 86.67 84.44 99.20 61.24 94.26 86.59 21.59\nContentV (Long) 85.14 86.64 79.12 96.80 57.38 83.05 71.41 23.02\nGoku† 84.85 85.60 81.87 97.60 57.08 76.11 79.48 23.08\nOpen-Sora 2.0 84.34 85.40 80.12 95.40 52.71 71.39 77.72 22.98\nSora† 84.28 85.51 79.35 98.20 56.95 79.91 70.85 24.76\nCausVid† 84.27 85.65 78.75 99.80 56.58 92.69 72.15 24.27\nContentV (Short) 84.11 86.23 75.61 89.60 44.02 79.26 74.58 21.21\nLuma† 83.61 83.47 84.17 96.40 58.98 44.26 82.63 24.66\nEasyAnimate 5.1 83.42 85.03 77.01 95.60 54.31 57.15 66.85 23.06\nHailuo-Video-01† 83.41 84.85 77.65 92.40 50.68 64.91 76.04 20.06\nKling 1.6† 83.40 85.00 76.99 96.20 55.57 62.22 63.99 20.75\nHunyuanVideo 83.24 85.09 75.82 94.40 53.88 70.83 68.55 19.80\nGen-3† 82.32 84.11 75.17 96.40 54.57 60.14 53.64 24.31\nCogVideoX-5B 81.61 82.75 77.04 99.40 53.20 70.97 62.11 24.91\nPika-1.0† 80.69 82.92 71.77 86.20 49.83 47.50 43.08 22.26\nVideoCrafter-2.0 80.44 82.20 73.42 95.00 55.29 42.50 40.66 25.13\nAnimateDiff-V2 80.27 82.90 69.75 92.60 50.19 40.83 36.88 22.42\nOpenSora 1.2 79.23 80.71 73.30 85.80 42.47 47.22 58.41 23.89\nShow-1 78.93 80.42 72.98 95.60 47.03 44.44 45.47 23.06\nMira 71.87 78.78 44.21 63.80 16.34 60.33 12.52 21.89\nFurthermore, to ensure the robust evaluation, we use the end-to-end visual language model based\nevaluator VideoAlign[ 28] to complement the VBench evaluation criteria. VideoAlign provides\nfine-grained analysis across three dimensions of video generation: visual quality (VQ) quantifies\nframe-level clarity and structural fidelity; motion quality (MQ) evaluates temporal coherence and\nphysical plausibility of movements; and text alignment (TA) measures semantic consistency between\nthe generated video and its textual prompt. We use both Vbench[ 19] and VideoAlign[ 28] to measure\nthe impact of the Pre-train, SFT, RLHF stages on the generated videos.\nTable 4: Improvements of ContentV across stages on the experimental model. The blue represents\nwin rates against the pretrain model.\nStageVbench [19] VideoAlign [28]\nQuality Semantic Overall VQ MQ TA Overall\nPretrain 85.11 78.29 83.74 -0.2160 0.2494 0.8737 0.9071\nSFT 85.24 79.73 84.13-0.2080\n50.26%0.2443\n46.40%1.0145\n61.31%1.0508\n56.77%\nRLHF 85.85 80.02 84.680.3034\n89.38%0.4339\n65.96%1.1599\n76.48%1.8972\n85.62%\n15\n--- Page 16 ---\nThe results in Table 4 demonstrate progressive performance improvements across successive training\nstages, with each phase contributing distinct enhancements to different evaluation metrics. During the\nSFT stage, the model shows a significant improvement in TA performance, achieving a 61.31% win\nrate from 0.8737 to 1.0145. This improvement is attributed to the utilization of filtered video samples\nfrom the pre-training dataset accompanied by more precise and comprehensive captions. Notably,\nwhile TA shows substantial gains, VQ and MQ metrics remain stable, maintaining performance levels\ncomparable to the baseline. This observation suggests that the SFT process primarily optimizes\ntext-video alignment through improved caption quality rather than affecting the visual quality. The\nRLHF stage produces more comprehensive improvements across all evaluation dimensions. Most\nremarkably, this stage yields an 89.38% win rate enhancement in VQ (from -0.2160 to 0.3034) and an\n85.62% win rate improvement in overall score (from 0.9071 to 1.8972). These substantial gains can\nbe attributed to RLHF’s optimization of the complete diffusion process, which systematically aligns\nthe denoised x1output with human preference patterns through iterative feedback mechanisms.\n8.3 User Study\nDespite the use of multi-dimensional evaluation schemes in Vbench and VideoAlign, there is still a\ngap between model-based evaluation and human evaluation. Therefore, we conducted a user study\nbetween ContentV and three other mainstream open-source models, namely CogvideoX-5B [ 52],\nHunyuanVideo-13B [ 21], and WanX2.1-14B [ 46] . Specifically, our evaluation examines four critical\ndimensions of video generation quality:\n•Perceptual Quality : Assesses fundamental video characteristics including temporal consis-\ntency, frame-to-frame stability, motion fluidity, and overall aesthetic coherence.\n•Instruction Following : Accuracy in matching textual prompts for elements, quantities, and\nfeatures\n•Physical Simulation : Realism in motion, lighting, and physical interactions\n•Visual Quality : Quantifies production-grade quality metrics including resolution fidelity,\ncolor accuracy, noise levels, and artifact presence.\nEvaluation Metrics. We use the Good-Same-Bad (GSB) ratio to quantitatively represent the differ-\nences in human preferences between the evaluation model, namely ContentV , and the comparison\nmodel. Given a prompt and videos generated respectively by the evaluation model and the comparison\nmodel, human annotators need to make a choice based on their personal preferences: the evaluation\nmodel is better (\" Good\"), the two are about the same (\" Same\"), and the comparison model is better\n(\"Bad\"). The GSB ratio is calculated as:\nGSB Ratio =G+S\nB+S(3)\nA ratio greater than 1 indicates superior performance of the evaluated model, while a ratio below 1\nsuggests the comparison baseline performs better. For instance, when assessing 10 sample videos\nwith results of 7 better, 2 equivalent, and 1 worse cases, the resulting 7:2:1 ratio yields a GSB value\nof 3.0, clearly indicating performance superiority.\nWe invite five human annotators to comprehensively consider and select the preferred videos. The\ncomparative GSB results are presented in Figure 11. Consistent with the model-based evaluation,\nContentV demonstrates superior performance over both CogVideoX and HunyuanVideo across\noverall evaluation dimensions. An interesting observation emerges from the comparison between\nmetric-based and human evaluations: while ContentV showed lower scores than Wan2.1 on VBench,\nit achieved better human preference ratings. This discrepancy suggests that the VBench evaluation\ncriteria may emphasize different aspects of video quality compared to human perception. We\nhypothesize that human evaluators place greater emphasis on holistic factors such as natural motion,\ntemporal coherence, and aesthetic quality, which may not be fully captured by automated metric\nsystems.\n9 Conclusion\nWe present ContentV , the first video generation model fully trained on NPUs, achieving SOTA\nperformance both on VBench and user studies. By initializing from Stable Diffuison 3.5 Large with\n16\n--- Page 17 ---\n0 20 40 60 80 100\nGood RatesCogVideox-5BHunyuanVideo-13BWan2.1-14B\n58.4 6.8 34.760.5 5.8 33.751.6 10.0 38.4\nContentVGood Same BadFigure 11: User studies of ContentV against open-source SOTA T2V models. Compared to\nCogVideoX, HunyuanVideo, and Wan2.1, ContentV achieves a GSB ratio of 1.57, 1.68, and 1.30,\nrespectively.\nminimal architectural changes, implementing a novel phased joint progressive training strategy for\nsimultaneous image-video optimization, and RLHF adaptation for video generation, our framework\nproduces high-quality, diverse videos across varying resolutions and durations while converging in\njust one month. Remarkably, ContentV matches the performance of current leading open-source\nmodels with only half their parameters, demonstrating unprecedented training efficiency without\ncompromising output fidelity. This work not only advances the frontier of scalable video synthesis\nbut also establishes a new paradigm for large-scale generative model training on alternative hardware\narchitectures.\nAcknowledgements\nWe sincerely appreciate our collaborators at ByteDance for their support. They contributed to this\nwork but were not listed as authors. Tao Jiang, Xiong Ke, Xiaotong Shi, Xinjie Huang, Xukai Jiang,\nJunfu Wang, Zirui Guo, Shuhan Yao, Zhenyu Huang, Hanyu Li, Ziyu Shi, Siqi Wang, Yan Qiu, Yaling\nMou, Qingyi Wang, Fengxuan Zhao.\n17\n--- Page 18 ---\nReferences\n[1]Breakthrough AI. Pyscenedetect: Video scene cut detection tool. https://www.scenedetect.\ncom/ , 2023. Version 0.6.2.\n[2]PaddlePaddle Authors. Paddleocr, awesome multilingual ocr toolkits based on paddlepaddle.\nhttps://github.com/PaddlePaddle/PaddleOCR , 2020.\n[3]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,\n2025.\n[4]James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,\nJuntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions.\nComputer Science. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023.\n[5]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 22563–22575, 2023.\n[6]Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,\nJoe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators.\nOpenAI Blog , 1:8, 2024.\n[7]Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe\nTaylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024.\nURL https://openai. com/research/video-generation-models-as-world-simulators , 3:1, 2024.\n[8] ByteDance. Jimeng. https://jimeng.jianying.com/ , 2024.\n[9]Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao,\nByung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m:\nCaptioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 13320–13331, 2024.\n[10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin\nGilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.\nScaling vision transformers to 22 billion parameters. In International Conference on Machine\nLearning , pages 7480–7512. PMLR, 2023.\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems , 34:8780–8794, 2021.\n[12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transform-\ners for high-resolution image synthesis. In Forty-first International Conference on Machine\nLearning , 2024.\n[13] Google. Veo-2. https://deepmind.google/technologies/veo/veo-2/ , 2024.\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh\nAgrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image\ndiffusion models without specific tuning. In The Twelfth International Conference on Learning\nRepresentations , 2024.\n[15] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,\nand José Lezama. Photorealistic video generation with diffusion models. arXiv preprint\narXiv:2312.06662 , 2023.\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems , 30, 2017.\n18\n--- Page 19 ---\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems , 33:6840–6851, 2020.\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303 , 2022.\n[19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang,\nTianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark\nsuite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 21807–21818, 2024.\n[20] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song,\nSamyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling\ntraining of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509 , 2023.\n[21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin\nLi, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603 , 2024.\n[22] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Ander-\nsch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large\ntransformer models. Proceedings of Machine Learning and Systems , 5:341–353, 2023.\n[23] Kuaishou. Kling. https://klingai.kuaishou.com/ , 2024.\n[24] Black Forest Labs. Flux. https://blackforestlabs.ai/, 2024.\n[25] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence paral-\nlelism: Long sequence training from system perspective. arXiv preprint arXiv:2105.13120 ,\n2021.\n[26] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang\nYe, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation\nmodel. arXiv preprint arXiv:2412.00131 , 2024.\n[27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow\nmatching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.\n[28] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin\nWang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback.\narXiv preprint arXiv:2501.13918 , 2025.\n[29] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi\nWan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The\npractice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248 ,\n2025.\n[30] Minimax. Hailuo. https://hailuoai.com/video , 2024.\n[31] José Luis Pech-Pacheco, Gabriel Cristóbal, Jesús Chamorro-Martinez, and Joaquín Fernández-\nValdivia. Diatom autofocusing in brightfield microscopy: a comparative study. In Proceedings\n15th International Conference on Pattern Recognition. ICPR-2000 , volume 3, pages 314–317.\nIEEE, 2000.\n[32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings\nof the IEEE/CVF international conference on computer vision , pages 4195–4205, 2023.\n[33] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville.\nWürstchen: An efficient architecture for large-scale text-to-image diffusion models. In The\nTwelfth International Conference on Learning Representations , 2024.\n[34] Pika. Pika. https://pikartai.com/ , 2024.\n[35] Pixelverse. Pixelverse. https://www.pixelverse.xyz/ , 2025.\n19\n--- Page 20 ---\n[36] A Polyak, A Zohar, A Brown, A Tjandra, A Sinha, A Lee, A Vyas, B Shi, CY Ma,\nCY Chuang, et al. Movie gen: A cast of media foundation models, 2025. URL https://arxiv.\norg/abs/2410.13720 , page 51, 2024.\n[37] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv\nVyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang\nWang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Ja-\ngadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu,\nMitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan\nSumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,\nSean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani,\nTao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao\nLuo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert\nPumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena\nKerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blan-\nchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire,\nKarthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel\nMoritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan\nPetrovic, , and Yuming Du. Movie gen: A cast of media foundation models. arXiv preprint\narXiv:2410.13720 , 2024.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research , 21(140):1–67, 2020.\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 10684–10695, 2022.\n[40] Runway. Gen-3. https://runwayml.com/ , 2024.\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nneural information processing systems , 35:36479–36494, 2022.\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. Advances in\nneural information processing systems , 35:25278–25294, 2022.\n[43] ShengShu-AI. Vidu. https://www.vidu.com/ , 2024.\n[44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\ntext-video data. In The Eleventh International Conference on Learning Representations , 2022.\n[45] Genmo Team. Mochi 1. https://github.com/genmoai/models , 2024.\n[46] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao,\nJianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative\nmodels. arXiv preprint arXiv:2503.20314 , 2025.\n[47] Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, and Lianwen Jin. Videoclip-\nxl: Advancing long description understanding for video clip models, 2024. URL https:\n//arxiv.org/abs/2410.00741 .\n[48] Yanhui Wang, Jianmin Bao, Wenming Weng, Ruoyu Feng, Dacheng Yin, Tao Yang, Jingxu\nZhang, Qi Dai, Zhiyuan Zhao, Chunyu Wang, et al. Microcinema: A divide-and-conquer\napproach for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 8414–8424, 2024.\n20\n--- Page 21 ---\n[49] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen,\nXinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal\nunderstanding and generation. arXiv preprint arXiv:2307.06942 , 2023.\n[50] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song,\nYu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models.\nInEuropean Conference on Computer Vision , pages 108–124. Springer, 2024.\n[51] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning\noptical flow via global matching. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 8121–8130, 2022.\n[52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming\nYang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion\nmodels with an expert transformer. arXiv preprint arXiv:2408.06072 , 2024.\n[53] Lijun Yu, José Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen,\nYong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats\ndiffusion–tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 , 2023.\n[54] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural\nInformation Processing Systems , 32, 2019.\n[55] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan\nWang. Learning multi-dimensional human preference for text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n8018–8027, 2024.\n[56] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully\nsharded data parallel. arXiv preprint arXiv:2304.11277 , 2023.\n[57] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun\nZhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all.\narXiv preprint arXiv:2412.20404 , 2024.\n21",
  "text_length": 66273
}