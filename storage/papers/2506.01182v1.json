{
  "id": "http://arxiv.org/abs/2506.01182v1",
  "title": "Humanoid World Models: Open World Foundation Models for Humanoid\n  Robotics",
  "summary": "Humanoid robots have the potential to perform complex tasks in human centered\nenvironments but require robust predictive models to reason about the outcomes\nof their actions. We introduce Humanoid World Models (HWM) a family of\nlightweight open source video based models that forecast future egocentric\nobservations conditioned on actions. We train two types of generative models\nMasked Transformers and FlowMatching on 100 hours of humanoid demonstrations.\nAdditionally we explore architectural variants with different attention\nmechanisms and parameter sharing strategies. Our parameter sharing techniques\nreduce model size by 33 to 53 with minimal impact on performance or visual\nfidelity. HWM is designed to be trained and deployed in practical academic and\nsmall lab settings such as 1 to 2 GPUs.",
  "authors": [
    "Muhammad Qasim Ali",
    "Aditya Sridhar",
    "Shahbuland Matiana",
    "Alex Wong",
    "Mohammad Al-Sharman"
  ],
  "published": "2025-06-01T21:33:36Z",
  "updated": "2025-06-01T21:33:36Z",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01182v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01182v1  [cs.RO]  1 Jun 2025Humanoid World Models\n : Open World Foundation Models for Humanoid\nRobotics\nQasim Ali* 1Aditya Sridhar* 1Shahbuland Matiana1Alex Wong1Mohammad Al-Sharman1\nAbstract\nHumanoid robots have the potential to perform\ncomplex tasks in human-centered environments\nbut require robust predictive models to reason\nabout the outcomes of their actions. We intro-\nduce Humanoid World Models (HWM), a family\nof lightweight, open-source video-based models\nthat forecast future egocentric observations con-\nditioned on actions. We train two types of gener-\native models—Masked Transformers and Flow-\nMatching—on 100 hours of humanoid demon-\nstrations. Additionally, we explore architectural\nvariants with different attention mechanisms and\nparameter-sharing strategies. Our parameter-\nsharing techniques reduce model size by 33–53%\nwith minimal impact on performance or visual\nfidelity. HWM is designed to be trained and de-\nployed in practical academic and small-lab set-\ntings, such as 1–2 GPUs.\n1. Introduction\nAutonomous humanoid robots have the potential to trans-\nform both industry and daily life by automating tasks that\nare dull, dangerous, or physically demanding (Goswami &\nVadakkepat, 2019). Their human-like morphology allows\nthem to operate in spaces built for people, interact naturally\nwith humans, and easily learn from teleoperated demon-\nstrations. Realizing this promise, however, requires more\nthan hardware: these agents must be equipped with sophisti-\ncated models of reasoning to navigate the complexity and\nunpredictability of real-world environments.\nWhile large multimodal models (Bommasani et al., 2022;\nZhang et al., 2024; Kim et al., 2024; Black et al., 2024)\nshow some promise in generalization and reasoning, they\noften lack the accuracy, reliability, and robustness needed\nfor embodied AI in open-world settings (Tong et al., 2024;\n*Equal contribution1University of Waterloo, Waterloo, Canada.\nCorrespondence to: Qasim Ali <m45ali@uwaterloo.ca >, Aditya\nSridhar <a27sridh@uwaterloo.ca >.Pfeifer & Iida, 2004). As a result, they struggle to meet\nthe demands of embodied agents that must act safely and\neffectively in dynamic, unstructured environments (Pfeifer\n& Iida, 2004; Li et al., 2024; Duan et al., 2024).\nFigure 1. Overview of Humanoid World Model.\nOne avenue for improving humanoid intelligence and con-\ntrol is through World Models —predictive models trained\nto forecast future outcomes based on past observations and\nactions (Ha & Schmidhuber, 2018). In our setting, these\nfunction as action-conditioned video generators: they pre-\ndict future visual states as sequences of frames, enabling\nagents to simulate the consequences of candidate actions\n(Du et al., 2023). By simulating the outcomes of actions\nbefore trying them in the real world, world models support\nlong-horizon planning, counterfactual reasoning, and data-\nefficient policy learning through synthetic rollouts (Yang\net al., 2024; 2023).\nDespite recent progress in video generation, most models\nare built for entertainment applications (Liu et al., 2024;\nPolyak et al., 2024), emphasizing visual appeal over phys-\nical, ego-centric plausibility. Many world models remain\nclosed-source, require large-scale compute for training or\ninference, or are not designed for humanoid robots (Yang\net al., 2023; NVIDIA et al., 2025; Zhu et al., 2024). As a\nresult, there is a clear gap: few open-source models are both\nphysically grounded for humanoid robots and lightweight\nenough to train or run inference on modest academic hard-\nware (e.g., 2–3 GPUs). To address this, we ask: Can we\nbuild a physically plausible, humanoid-specific world model\nthat runs on just two GPUs?\nWe introduce Humanoid World Models (HWM), an open-\nsource lightweight world model for humanoid robotics on\n100 hours of humanoid video demonstrations. We inves-\n1\n--- Page 2 ---\nHumanoid World Models\ntigate two distinct video generation paradigms: Masked\nTransformers and Flow-Matching models. Drawing from\nrecent advances in image and video generation, we explore\nfour architectural variations within each framework. These\nvariations are defined along two axes: (1) joint vs. cross-\nattention mechanisms, and (2) shared vs. separate parame-\nters across token streams. This design space is motivated\nby successful architecture strategies from the much more\nexplored text-to-image models.\nOur experiments show that, given our dataset and com-\npute constraints, Masked Transformers consistently outper-\nformed Flow-Matching models—even when the latter used\nmore parameters and were trained for longer. Across both\nfamilies of models, we observed that the different archi-\ntectural variants performed comparably in most scenarios.\nHowever, key trends emerged: within the Masked Trans-\nformer framework, the joint attention variant achieved the\nbest overall performance. In contrast, for Flow-Matching\nmodels, split attention proved most effective.\nImportantly, we found that parameter-sharing strategies\nyielded near-identical performance to their non-shared coun-\nterparts while reducing parameter counts by 33–53%, sig-\nnificantly lowering computational requirements. These\nresults suggest that architectural and efficiency trade-\noffs—particularly attention design and parameter shar-\ning—can be leveraged to build lightweight, performant\nworld models without compromising quality.\nRelated Works\n1.1. Humanoid Robots\nHumanoid robots (Humanoids) are actuated, bipedal, and\nbimanual robots designed to anthropomorphically resem-\nble the human body structure (Goswami & Vadakkepat,\n2019). Their morphology is well-suited for operating in\nhuman-centered environments, enabling effective interac-\ntion and physical compatibility with everyday settings. This\nembodiment facilitates deployment across a wide range of\nreal-world domains, including households (Imtiaz & Khan,\n2024), manufacturing facilities (Hirose & Ogawa, 2007),\nelderly care homes (Imtiaz & Khan, 2024), and clinical or\nmedical environments (Goswami & Vadakkepat, 2019). Be-\nyond physical compatibility with environments, the human-\nlike form of humanoids allows for more natural interac-\ntion with people and the ability to imitate human behaviors\n(Vianello et al., 2021). Furthermore, collecting demonstra-\ntion data is relatively straightforward for these platforms,\nas they can directly mimic human motions and tasks (Zhao\net al., 2023). Our world models are trained specifically for\nhumanoids, but can easily be extended to other embodi-\nments.1.2. World Models\nA core challenge in robotics is enabling agents to per-\nceive their environment, reason over possible actions, and\nplan goal-directed behavior—especially in novel or un-\nstructured settings. Foundation models (Bommasani et al.,\n2022) attempt to generalize across tasks through large-scale,\nmultimodal training. Recent approaches include Vision-\nLanguage-Action (VLA) models that predict actions from\nvideo and language inputs (Black et al., 2024; Kim et al.,\n2024), and large language or vision-language models used\nas high-level planners (Liang et al., 2023; Li et al., 2024;\nWang et al., 2024). However, these methods often strug-\ngle with spatial reasoning (Tong et al., 2024), continuous\nsensorimotor processing, and require complex prompting\nstrategies (Li et al., 2024).\nIn contrast, video generation models offer a more grounded\nalternative for embodied agents. Video captures fine-grained\nphysical and temporal structure that language alone cannot\nexpress (Yang et al., 2024). When trained to predict future\nvideo frames given past observations and actions, these\nmodels serve as World Models —internal simulators that\nforecast the outcomes of potential action sequences (Ha &\nSchmidhuber, 2018; Du et al., 2023). This enables agents\nto plan through imagination, reason counterfactually, and\ngenerate synthetic experience for policy training.\nIn this work, we develop a humanoid-specific world model\nimplemented as a video generator: it predicts future egocen-\ntric video frames from past video and action sequences. This\ngenerative modeling approach allows us to simulate plau-\nsible futures, supporting both planning and data-efficient\nlearning in complex environments.\n1.3. Video Generation Models\nEarly video generation relied on GANs (Goodfellow et al.,\n2014), but training instability limited their use. Recent\nmethods improve both quality and efficiency by training\ngenerative models like diffusion models (Sohl-Dickstein\net al., 2015; Rombach et al., 2022) or masked transform-\ners (Vaswani et al., 2023) in the compressed latent spaces\nof Variational Autoencoders (V AEs) (Harvey et al., 2022).\nDiscrete latent approaches like vector quantized V AEs (VQ-\nV AEs) (van den Oord et al., 2018) enable token-based video\ngeneration using masked or autoregressive transformers\n(Ramesh et al., 2021; Chang et al., 2022b). Flow Match-\ning (Lipman et al., 2023) offers an alternative to diffusion\nwith simpler training and faster sampling while preserving\nsample quality.\nMasked Video Generation : Transformer-based approaches\nhave been widely used for image and video generation.\nThese methods operate in the finite and quantized spaces\nof VQ-V AEs. MaskGIT (Chang et al., 2022b) introduced\n2\n--- Page 3 ---\nHumanoid World Models\na masked, bidirectional transformer for image generation\nfrom VQ-V AE latents, along with a non-autoregressive de-\ncoding scheme that significantly reduced sampling time\ncompared to diffusion and autoregressive methods. Masked\ntoken prediction offers two key advantages over autoregres-\nsive approaches (Ramesh et al., 2021; Wu et al., 2024):\n(1) bidirectional context across space and time improves\nrepresentation learning, and (2) tokens can be decoded in\nparallel, greatly accelerating inference. MAGVIT (Yu et al.,\n2023) extended this approach to video using spatio-temporal\nVQ-V AEs, while MAGVIT2 (Yu et al., 2024) introduced a\nstronger tokenizer that outperformed diffusion-based base-\nlines with fewer sampling steps. Open-MAGVIT2 (Luo\net al., 2024) provides an open-source implementation. We\nexplore similar non-autoregressive masked video transform-\ners for building humanoid-specific world models.\nDiffusion and Flow-Matching : Video diffusion models\n(Ho et al., 2022) extend diffusion processes to sequences of\nimages, modeling both spatial and temporal dynamics. How-\never, the added temporal dimension significantly increases\ncomputational cost. Recent advances in spatiotemporal\nV AEs (Xing et al., 2024; Bar-Tal et al., 2024) mitigate this\nby compressing video into low-dimensional latent spaces,\nmaking training and inference more tractable.\nLarge-scale text-to-video diffusion models such as Sora (Liu\net al., 2024), CogVideoX (Yang et al., 2025), and MovieGen\n(Polyak et al., 2024) achieve impressive visual quality but\nare designed for entertainment and lack support for con-\nditioning on past video—a key requirement for physically\ngrounded, ego-centric prediction.\nSeveral recent works explore video generation for robotic\nagents, but most are not designed for humanoid platforms\nand are not open source. UniSim (Yang et al., 2023) trains a\ntext-conditioned video diffusion model for zero-shot policy\ntransfer, but it relies on an outdated U-Net backbone and is\nnot open source. IraSim (Zhu et al., 2024) uses a factorized\nspatial-temporal transformer within a diffusion framework,\nbut targets robot arms and lacks temporal compression, rely-\ning instead on frame-wise image V AEs. Navigation World\nModels (Bar et al., 2025) are limited to low-DoF mobile\nrobots and generate individual future frames rather than\ncontinuous video sequences.\nNVIDIA’s Cosmos (NVIDIA et al., 2025) is a notable excep-\ntion—an open-source, high-fidelity video-to-video model.\nHowever, it is not designed for humanoid embodiments and\nis prohibitively resource-intensive. Its smallest variant (7B\nparameters) requires 8 NVIDIA H100 GPUs for training and\nover 40GB of VRAM for inference. On our compute setup\nof 2 NVIDIA A6000s, generating 121 frames video took\nover an hour, making finetuning or deployment impractical.\nWe explore training a lightweight flow-matching model inthe continuous latent space of Cosmos’s V AE (NVIDIA\net al., 2025).\n1.4. Video Transformer Architectures\nFor the Masked Video Model, we follow prior work in us-\ning factorized spatio-temporal attention (Bruce et al., 2024;\nXiang et al., 2024) to reduce computational overhead. This\napproach separates spatial and temporal attention to scale\nmore efficiently with video length.\nIn the broader diffusion and flow-matching video generation\nmodels literature, architectural trends have shifted from\nconvolutional U-Nets (Ho et al., 2020) to transformer-based\ndesigns for improved scalability (Peebles & Xie, 2023). In\nimage generation, joint attention blocks—where image and\ncontext tokens are processed together—are now standard,\nas seen in Stable Diffusion 3 (SD3) (Esser et al., 2024).\nSubsequent work has shown that complexity can be further\nreduced through parameter sharing across token streams\n(fal.ai Blog, 2024; Chen et al., 2025).\nIn contrast, most video diffusion models still avoid joint\nattention due to its high memory cost over long video se-\nquences. Instead, they typically use a two-stage attention\nscheme: self-attention over video tokens, followed by cross-\nattention with context (Polyak et al., 2024; NVIDIA et al.,\n2025). Leading models such as Cosmos (NVIDIA et al.,\n2025) follow this structure, while more complex designs\nlike mixture-of-experts (Kong et al., 2025) or pyramidal\ntransformers (Jin et al., 2024) further increase system com-\nplexity.\nIn our work, we revisit joint attention for video genera-\ntion. Joint attention over spatiotemporal video tokens is\nnow feasible thanks to two key advances: (1) highly com-\npressive V AEs that reduce spatial resolution (e.g. factor\nof 16) and temporal resolution (e.g. factor of 8), and (2)\nefficient parameter-sharing techniques from recent image\ngeneration models (fal.ai Blog, 2024; Chen et al., 2025).\n2. Methodology\nWe develop two humanoid-specific world models based on\ndistinct generative paradigms: Masked Humanoid World\nModel (Masked-HWM) and Flow Humanoid World Model\n(Flow-HWM). Masked-HWM employs masked video mod-\neling in a discrete latent space (via VQ-V AE), while Flow-\nHWM uses flow matching in a continuous latent space. We\ndetail the video generation frameworks and architectures in\n2.2 and 2.3 respectively, but describe the transformer block\ndesign in detail in 2.4.\n3\n--- Page 4 ---\nHumanoid World Models\n2.1. Formulation\nThe goal is to predict plausible future video frames given\na sequence of past video frames and associated actions.\nFormally, the model predicts a sequence of ffuture RGB\nframes vf∈Rf×3×H×W, conditioned on ppast frames\nvp∈Rp×3×H×W,ppast actions ap∈Rz, and ffuture\nactions af∈Rz. Action vectors include joint angles, veloc-\nities, and gripper states of the humanoid.\nFollowing prior works (Rombach et al., 2022; Chang et al.,\n2022a), we train our generative models in a V AE’s com-\npressed latent space: vpandvfare encoded into latent\nrepresentations LpandLf. Masked-HWM uses VQ-V AE\nto quantize latents into tokens from a finite vocabulary of\nsizes, while Flow-HWM uses a continuous V AE.\n2.2. Masked Video Modelling\nWe train the Masked-HWM variant using the Masked Video\nModelling (MVM) paradigm (Chang et al., 2022a; Yu et al.,\n2023). After passing passing vp, vfthrough a VQ-V AE\nto yield Lp,Lf, we concatenate the past and future latent\ntokensL= [Lp;Lf]along the temporal dimension.\nDuring training, Masked-HWM receives corrupted and\nmasked versions of the latent sequence as input. Following\nCopilot-4D (Zhang et al., 2023), we add noise to the latents\nby corrupting Lwith random token replacements at a rate\nuniformly sampled from U(0, ρmax), where ρmaxdenotes the\nmaximum corruption rate. Next, we apply masking to the fu-\nture latents Lfusing a per-frame thresholding strategy. For\neach frame, we sample a value r∼ U(0,1)and compute a\nmasking threshold γ(r)using a predefined scheduling func-\ntion. Then, for each token in the future sequence Lf, we\nsample a probability from U(0,1)and mask the token if it\nfalls below the frame’s threshold.\nThe model is trained to reconstruct the original tokens at\nthese masked positions. Let Mdenote the binary mask indi-\ncating which tokens have been masked within corrupted Lf.\nThe training objective is to minimize the cross-entropy loss\nbetween the predicted tokens ˆLfat the masked locations M\nand the true tokens at those same locations, as follows:\nL=−EM\"X\niMilogp(ˆLi|Lf)#\nwhere ˆLiis the model’s prediction and Mi∈0,1indicates\nwhether location iwas masked. This loss encourages the\nmodel to accurately reconstruct corrupted or hidden tokens\nin the future sequence.\nAt inference time, we begin by masking all tokens in the\nfuture latent sequence Lf. Generation proceeds latent frame\nby latent frame: after predicting one frame’s tokens, theresult is fed back to help condition the next. Within each\nframe, tokens are predicted in parallel over Krefinement\nsteps. At each step, a random subset of tokens is re-masked\nand re-predicted, allowing the model to iteratively improve\nits guesses. This parallel decoding strategy significantly\naccelerates generation compared to traditional autoregres-\nsive methods, which decode tokens sequentially. For more\ndetails on the sampling procedure, we refer readers to\nMaskGiT (Chang et al., 2022a).\n2.2.1. A RCHITECTURE\nTokenization The input video frames are encoded using a\nVQ-V AE performing spatiotemporal compression, yielding\ndiscrete tokens. Each latent pixel is treated as a token and\nprojected into h-dimensions. Action sequences ( ap,af) are\nindependently embedded into the same h-dimensional space\nusing a multi-layer perceptron (MLP), ensuring compatibil-\nity with the video tokens.\nTransformer : The resulting four token streams—past video\n(vp), future video ( vf), past actions ( ap), and future actions\n(af)—are fed into a stack of dtransformer blocks. After\nprocessing, the video tokens are linearly projected to a dis-\ntribution over the VQ-V AE codebook of size s, representing\nthe predicted token identities. Details of the transformer\nblock variants are provided in Section 2.4.\n2.3. Flow Matching for Video Generation\nWe train the Flow-HWM variant using the Flow Matching\n(FM) framework (Lipman et al., 2023; Albergo et al., 2023;\nLiu et al., 2022). The framework formulates video gen-\neration as a continuous transformation of samples from a\nsimple prior distribution (Gaussian noise) into data samples\ndrawn from the target distribution. As opposed to learning a\nreversed stochastic process like in traditional diffusion mod-\nels (Song et al., 2021), FM directly learns a time-dependent\nvelocity field that drives this transformation.\nLetX1denote a video sample in the latent space, and let\nX0∼ N (0,I)represent a random sample from the Gaus-\nsian prior. We train the model by sampling an intermediate\ntimet∈[0,1]and construct a point along the trajectory Xt\nusing linear interpolation:\nXt=tX1+ (1−(1−σmin)t)X0, (1)\nwhere σminis a small positive constant ensuring non-zero\nsupport at t= 1. The ground-truth velocity of the transfor-\nmation path is then given by the time derivative:\nVt=dXt\ndt=X1−(1−σmin)X0. (2)\nOur model, parameterized by θ, predicts the instantaneous\n4\n--- Page 5 ---\nHumanoid World Models\nvelocity field uθ(Xt,P, t)conditioned on the past video\nframes vp, past actions ap, future actions afPand time\nt. The training objective of the model is to minimize the\nexpected mean squared error between the predicted and\nground-truth velocity:\nEt,X0,X1,ap,af,vp=h\n∥uθ(Xt, ap, af, vp, t)−Vt∥2i\n.\n(3)\nWe adopt classifier-free guidance (Ho & Salimans, 2022)\nto improve conditional generation by enabling the model to\nbetter balance conditioning signals from actions and past\ncontext during training and inference. During inference,\ngeneration proceeds by integrating the learned velocity field\nfrom t= 0tot= 1, starting from pure Gaussian noise and\nemploying the first-order Euler ODE solver.\n2.3.1. A RCHITECTURE\nTokenization We tokenize the compressed latent video\nframes LpandLfby dividing them into plw×plwspa-\ntial and pttemporal segments per token. Each token is\nprojected to hchannels via a convolutional layer. Action\nsequences apandafare embedded using an MLP into the\nsame h-dimensional space. The timestep tis encoded using\nsinusoidal embeddings following DDPM (Ho et al., 2020).\nTransformer After tokenization, each of the four streams\nof tokens ( vf,vp,af,ap) are kept separate and processed by\ndtransformer blocks sequentially. In the final layer, we\napply time modulation as described in DDPM, followed by\na linear projection of the future tokens vffromhdimensions\nback to llatent dimensions. The resulting tokens are then\nreshaped into the original video’s spatiotemporal format\nto be decoded back to pixel space the V AE. We detail the\ntransformer block design in 2.4.\n2.4. Transformer Block Design\nWe evaluate several architectural variants of the transformer\nblock used in both Masked-HWM and Flow-HWM, focus-\ning on three key design dimensions: (1) attention structure\n(joint vs. split attention), (2) parameter sharing across token\nstreams, and (3) token stream grouping (modality-based vs.\nfully separate).\nBase Block : We start by describing the Base Transformer\nBlock, which is augmented to create other block designs. It\nprocesses four token streams—past video ( vp), future video\n(vf), past actions ( ap), and future actions ( af)—each with\nits own set of parameters but a shared joint attention layer.\nThe Base Block design for the Masked-HWM is illustrated\nin Figure 2. Following prior work in non-autoregressive\nvideo generation methods (Bruce et al., 2024; Xiang et al.,\n2024), we adopt a factorized or separate spatial and tempo-\nFigure 2. Architecture of a single transformer block in Masked-\nHWM (Base Block variant). Video and action streams are pro-\ncessed independently, with video streams also receiving Spatial\nAttention. All streams interact via joint Temporal Attention. RoPE\nis applied per attention type (2D for spatial and 1D for temporal).\nEach stream uses distinct MLP weights in the feedforward stage.\nral attention layers. Compared to full spatiotemporal atten-\ntion, factorized attention reduces the computational cost and\nscales more efficiently with video length. During temporal\nattention, all tokens from various streams [ap, af,X]jointly\nattend to one another along the temporal dimension. Spatial\nattention is applied seperately only to the video tokens. Ro-\ntary Position Embeddings (RoPE)(Su et al., 2023) are used\nduring spatial and temporal attention.\nThe Base Block for the Flow-HWM variant, as illustrated in\nFigure 3, is inspired by Stable Diffusion 3 (SD3)(Esser et al.,\n2024). This design processes the four token streams— vp,\nvf,ap, and af—with separate parameters and enables in-\nteraction through a joint attention operation. Within each\nblock, each stream is first modulated by the timestep using\nlearned scale α0and shift β0parameters(Peebles & Xie,\n2023). Subsequently, stream-specific queries, keys, and val-\nues are computed using separate WQKV projections. We\nadd positional encodings to the queries and keys of each\ntoken steam. Specifically, we add 3D Rotary Position Em-\nbeddings (RoPE) to the video tokens, as done in Cosmos\n5\n--- Page 6 ---\nHumanoid World Models\nFigure 3. Architecture of a single transformer block in Flow-HWM\n(Base Block variant). Each token stream (past/future video and\nactions) uses separate weights for timestep modulation, QKV pro-\njection, and feedforward MLPs. Joint Attention integrates all\nstreams. RoPE is applied by modality: 3D for video tokens, 1D\nfor action tokens.\n(NVIDIA et al., 2025), and apply 1D RoPE across time for\nthe action tokens. Past and future tokens are concatenated\nprior to adding positional encoding.\nA joint multi-stream attention operation is then applied\nacross all tokens, followed by another timestep-dependent\nrescaling using γ0. In the feedforward stage, tokens are\nagain modulated with the timestep embedding using new\nparameters α1andβ1, passed through a stream-specific\nMLP, and finally rescaled using γ1. Residual connections\nare added during the attention and the feedforward stages.\nParameter Sharing : Recent advances in efficient diffusion\ntransformer design (Chen et al., 2025; fal.ai Blog, 2024)\nhave showcased that the benefits of joint attention can be\ngarnered with far fewer parameters using shared attention.\nWe evaluate two parameter-sharing strategies. These strate-\ngies selectively share key transformer components across\ntoken streams, including timestep modulation parameters ( α,\nβ,γ), QKV projection weights ( WQKV ), and feedforward\nMLPs.In the Full Sharing variant, the modulation scalars, QKV\nprojections, and MLPs are shared across all four token\nstreams ( vp,vf,ap,af). This maximally reduces parameter\ncount and compute overhead. In the Modality Sharing vari-\nant, parameters are shared within each modality, i.e., video\nstreams ( vp,vf) share weights, and action streams ( ap,af)\nshare weights. This strikes a balance between model com-\npactness and representational flexibility.\nIn our implementation, we retain the original Base Block\nconfiguration (with fully separate parameters) for the first\nfour layers and apply parameter sharing in the remaining\nl−4transformer layers, following practices from (fal.ai\nBlog, 2024). This hybrid scheme allows early layers to learn\nmodality-specific representations, while deeper layers focus\non cross-modal reasoning in a more compact parameter\nregime.\nSplit Attention. While joint attention across all token\nstreams enables rich cross-modal interactions, it becomes\nincreasingly expensive with longer video sequences and\nhigher token counts. To address this, we implement a two-\nstage Split Attention mechanism, which has been widely\nadopted in recent large-scale video generation models\n(NVIDIA et al., 2025; Bar et al., 2025; Polyak et al., 2024)\nfor its computational efficiency.\nIn this variant, each stream—future video ( vf), past video\n(vp), past actions ( ap), and future actions ( af)—first un-\ndergoes independent self-attention within its own sequence.\nThis allows each modality and temporal context to process\nintra-stream dependencies with minimal overhead.\nFollowing self-attention, we apply a cross-attention opera-\ntion in which the future video tokens vfserve as queries, and\nthe keys and values are drawn from the remaining streams\n(vp,ap,af). This structure enables the model to selectively\ncondition future video generation on past observations and\nintended actions, while avoiding the full cost of global at-\ntention. As in the joint attention variant, both self- and\ncross-attention layers are modulated using time-dependent\nscaling and shifting, with parameters ( α,β,γ) learned per\nstream.\n3. Results\nDataset We train our models on the 1xGPT dataset (1X\nTechnologies, 2024), which contains 100 hours of egocen-\ntric video captured from the Humanoid EVE Android exe-\ncuting various tasks. Video frames are recorded at 30 Hz,\nwith each frame paired with a corresponding action vector\na∈R25representing movement velocities, hand closure\nstates, and pitch-yaw-roll angles of the joints (wrist, knee,\nelbow, shoulder, neck, and hip). We train both models to\ngenerate f= 8 future frames conditioned on p= 9 past\nframes at H= 256 ×W= 256 resolution.\n6\n--- Page 7 ---\nHumanoid World Models\nTable 1. Performance of Masked-HWM variants. Base Block yields the best FID; Split Attention gives the highest PSNR. Parameter\nsharing improves efficiency with minimal quality loss.\nMETRIC SPLIT ATTENTION BASE BLOCK MODALITY SHARING FULL SHARING\nMODEL SIZE(BILLION ) 0.220 0.321 0.237 0.195\nPEAK GPU M EMORY (GB) 2.22 2.63 2.30 2.12\nSAMPLES PER SECOND 2.09 2.27 2.25 2.36\nFID 15.31 10.13 11.67 14.21\nPSNR ( DB) 29.37 29.02 28.97 28.66\nEvaluation We evaluate our models using Fr ´echet Incep-\ntion Distance (FID) (Heusel et al., 2018) and PSNR (Hor ´e\n& Ziou, 2010), computed over 21,000 generated frames\nfrom a held-out validation set. To isolate the impact of the\ngenerative model from the V AE’s reconstruction quality,\nall ground truth frames are passed through the same V AE\nencoder-decoder used during training. In addition to image\nquality, we report model parameter count, video latent de-\ncoding speed (measured in samples per second), and peak\nGPU memory usage during inference.\n3.1. Masked Video Modelling\nExperimental Setup We tokenize all video frames using\nthe NVIDIA Cosmos DV8x8x8 tokenizer, which applies\n8×compression in both spatial and temporal dimensions,\nreducing 256×256RGB frames to 32×32latent grids.\nEach training sample consists of 2 fully unmasked past\nlatents and 1 partially masked future latent. We use a cosine\nmasking schedule from MaskGIT and inject Copilot-4D\nstyle noise with a uniform corruption rate sampled from\nU(0, ρmax), where ρmax= 0.2.\nModels are trained for 60,000 steps using the AdamW opti-\nmizer, with a learning rate linearly decayed from 3e−5after\n100 warmup steps. We use 24 transformer layers, 8 heads,\n512-dimensional tokens, and an MLP hidden size of 2048.\nWe apply standard normal initialization ( µ= 0,σ= 0.02)\nfor all weights, except Xavier initialization for the mask\ntoken and output projection. Training is performed on a\nsingle NVIDIA A6000 with batch size 16. During inference,\nwe use K= 2decoding iterations.\n3.1.1. Q UALITATIVE RESULTS\nSample videos from the Base Block variant are shown in\nFigure 4. The model learns both structural elements of the\nscene, such as furniture and small objects that the robot\nis manipulating, and overall textures. Larger parts of the\nrobot’s appendage, including arms and wheels, are generally\nmodeled accurately. However, precision elements, such as\nfingers, are often slightly blurry and entangled. Further, the\nvisual quality of generated images is robust to lighting, as\nshown in the 3rd (lighter) and 4th (darker) sequences.\nFigure 4. Four sample videos from the Base Block Variant of\nMasked-HWM. Top row: generated frames; bottom row: ground\ntruth.\n3.1.2. Q UANTITATIVE RESULTS\nTable 1 reports the performance of different Masked-HWM\nvariants. The Base Block achieved the best FID score\n(10.13), indicating the strongest visual quality among all\nconfigurations. The Split Attention variant obtained the\nhighest PSNR (29.37 dB), but slightly underperformed in\nFID, suggesting better pixel-level fidelity at the cost of less\nrealistic global structure.\nBoth parameter-sharing variants— Modality Sharing and\nFull Sharing —reduced model size and memory while main-\ntaining competitive FID and PSNR. Modality Sharing\nmatched Base Block quality with 26% fewer parameters,\nshowing that intra-modality sharing suffices. Full Sharing ,\nthough slightly lower in quality, had the smallest footprint\nand fastest inference (2.36 samples/sec), making it ideal for\nefficiency-focused settings.\n3.2. Flow Matching\nExperimental Setup.\nWe use the Cosmos Continuous 8x16x16 tokenizer that spa-\n7\n--- Page 8 ---\nHumanoid World Models\nTable 2. Performance of Flow-HWM variants. Full Sharing achieves the best FID, memory usage, and speed even when compared to the\nBase Variant; Split Attention yields the highest PSNR.\nMETRIC SPLIT ATTENTION BASE BLOCK MODALITY SHARING FULL SHARING\nMODEL SIZE(BILLION ) 0.944 1.36 0.886 0.648\nPEAK GPU M EMORY (GB) 4.37 5.94 4.41 3.25\nSAMPLES PER SECOND 1.11 1.69 1.89 1.91\nFID 111.12 111.59 112.75 110.73\nPSNR ( DB) 20.50 20.42 20.50 20.43\ntially compresses frames by a factor of 16 (from 256×256\nto16×16), and performs 8×temporal compression. Using\na more spatially compressive V AE relative to Masked-HWM\nallows for joint attention while using much larger models\nrequired for flow-matching networks. Models are trained\nwithd= 17 transformer layers and h= 1172 -dimensional\ntokens using the AdamW optimizer, a learning rate of 1e−4,\ncosine learning rate scheduler, and batch size 128. Train-\ning runs for 150,000 steps across 2 NVIDIA A6000 GPUs.\nWe use patch sizes plw= 2, pt= 1. We initialize final\nlinear layers with Xavier initialization, as zero-initialization\n(as in prior works (Peebles & Xie, 2023)) led to instability.\nNo learning rate warmup is used, as it degraded conver-\ngence. During inference, we apply 50 denoising steps with\na classifier-free guidance scale of 3.0.\n3.2.1. Q UALITATIVE RESULTS\nFigure 5. Sample videos from the Base Variant of Flow-HWM.\nTop row: generated frames; bottom row: ground truth.\nAs shown in Figure 5, the generated videos successfully cap-\nture overall scene structure (walls, floors, and doors). How-\never, the outputs exhibit noticeable blurriness and artifacting\nlike spotted patches. Visual quality tends to degrade in later\nframes, with the model struggling to preserve straight edges\nand rounded shapes. While the arms are often rendered withhigh fidelity, the model defaults to a canonical arm appear-\nance and fails to represent unusual or out-of-distribution\narm configurations (as seen in the top video strip).\n3.2.2. Q UANTITATIVE RESULTS\nTable 2 summarizes Flow-HWM performance. Full Shar-\ningoffered the best trade-off, outperforming the Base\nBlock in all metrics while halving the parameter count. It\nachieved the lowest FID (110.73), fastest inference (1.91\nsamples/sec), and minimal memory use (3.25 GB). Modal-\nity Sharing delivered similar quality with notable efficiency\ngains, while Split Attention yielded the highest PSNR (20.50\ndB) but required more memory and was slower. Overall, pa-\nrameter sharing improves efficiency without compromising\nquality.\nOverall, none of the Flow-HWM variants outperformed the\nMasked-HWM models in either visual quality or sampling\nspeed, suggesting that masked video modeling is a more\neffective generative paradigm for our dataset and compute\nconstraints. The results consistently show that parameter\nsharing—particularly in the Full Sharing variant—provides\nsubstantial efficiency gains with minimal loss in visual fi-\ndelity, very benefitial in resource-constrained settings.\n4. Conclusion\nHumanoid World Models demonstrate that it is possible to\nbuild physically plausible, efficient predictive models tai-\nlored for humanoid robotics using modest computational\nresources. Through effective parameter-sharing strategies\nand compressive V AEs, HWM enables open-world reason-\ning for embodied agents in compute-constrained settings.\nImpact Statement\nThis work advances video-based world models for hu-\nmanoid robots, aiming to make predictive simulation more\ncomputationally accessible. While our models support\nprogress in embodied AI, they are not intended for unsafe\nor unethical deployment.\n8\n--- Page 9 ---\nHumanoid World Models\nReferences\n1X Technologies. 1X World Model Challenge, June 2024.\nAlbergo, M. S., Boffi, N. M., and Vanden-Eijnden, E.\nStochastic interpolants: A unifying framework for flows\nand diffusions, 2023. URL https://arxiv.org/\nabs/2303.08797 .\nBar, A., Zhou, G., Tran, D., Darrell, T., and LeCun, Y . Nav-\nigation world models, 2025. URL https://arxiv.\norg/abs/2412.03572 .\nBar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss,\nR., Zada, S., Ephrat, A., Hur, J., Liu, G., Raj, A.,\nLi, Y ., Rubinstein, M., Michaeli, T., Wang, O., Sun,\nD., Dekel, T., and Mosseri, I. Lumiere: A space-\ntime diffusion model for video generation, 2024. URL\nhttps://arxiv.org/abs/2401.12945 .\nBlack, K., Brown, N., Driess, D., Esmail, A., Equi, M.,\nFinn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B.,\nJakubczak, S., Jones, T., Ke, L., Levine, S., Li-Bell, A.,\nMothukuri, M., Nair, S., Pertsch, K., Shi, L. X., Tanner,\nJ., Vuong, Q., Walling, A., Wang, H., and Zhilinsky, U.\npi0: A vision-language-action flow model for general\nrobot control. arXiv preprint arXiv:2410.24164 , 2024.\nURL https://arxiv.org/abs/2410.24164 .\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\nlut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\nD., Castellon, R., Chatterji, N., Chen, A., Creel, K.,\nDavis, J. Q., Demszky, D., Donahue, C., Doumbouya,\nM., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh,\nK., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel,\nK., Goodman, N., Grossman, S., Guha, N., Hashimoto,\nT., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu,\nK., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P.,\nKaramcheti, S., Keeling, G., Khani, F., Khattab, O., Koh,\nP. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A.,\nLadhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li,\nX. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchan-\ndani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan,\nA., Narayanan, D., Newman, B., Nie, A., Niebles, J. C.,\nNilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadim-\nitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C.,\nRaghunathan, A., Reich, R., Ren, H., Rong, F., Roohani,\nY ., Ruiz, C., Ryan, J., R ´e, C., Sadigh, D., Sagawa, S.,\nSanthanam, K., Shih, A., Srinivasan, K., Tamkin, A.,\nTaori, R., Thomas, A. W., Tram `er, F., Wang, R. E., Wang,\nW., Wu, B., Wu, J., Wu, Y ., Xie, S. M., Yasunaga, M.,\nYou, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X.,\nZhang, Y ., Zheng, L., Zhou, K., and Liang, P. On the\nopportunities and risks of foundation models, 2022. URL\nhttps://arxiv.org/abs/2108.07258 .Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J.,\nShi, Y ., Hughes, E., Lai, M., Mavalankar, A., Steiger-\nwald, R., Apps, C., et al. Genie: Generative interactive\nenvironments. In Forty-first International Conference on\nMachine Learning , 2024.\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pp. 11315–11325, 2022a.\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T.\nMaskgit: Masked generative image transformer, 2022b.\nURL https://arxiv.org/abs/2202.04200 .\nChen, C., Qian, R., Hu, W., Fu, T.-J., Tong, J., Wang, X., Li,\nL., Zhang, B., Schwing, A., Liu, W., and Yang, Y . Dit-air:\nRevisiting the efficiency of diffusion model architecture\ndesign in text to image generation, 2025. URL https:\n//arxiv.org/abs/2503.10618 .\nDu, Y ., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter,\nB., Sermanet, P., Yu, T., Abbeel, P., Tenenbaum, J. B.,\nKaelbling, L., Zeng, A., and Tompson, J. Video language\nplanning, 2023. URL https://arxiv.org/abs/\n2310.10625 .\nDuan, J., Pumacay, W., Kumar, N., Wang, Y . R., Tian, S.,\nYuan, W., Krishna, R., Fox, D., Mandlekar, A., and Guo,\nY . Aha: A vision-language-model for detecting and rea-\nsoning over failures in robotic manipulation, 2024. URL\nhttps://arxiv.org/abs/2410.00371 .\nEsser, P., Kulal, S., Blattmann, A., Entezari, R., M ¨uller,\nJ., Saini, H., Levi, Y ., Lorenz, D., Sauer, A., Boesel,\nF., Podell, D., Dockhorn, T., English, Z., Lacey, K.,\nGoodwin, A., Marek, Y ., and Rombach, R. Scaling recti-\nfied flow transformers for high-resolution image synthe-\nsis, 2024. URL https://arxiv.org/abs/2403.\n03206 .\nfal.ai Blog. Auraflow: Generate high-fidelity 3d assets\nwith diffusion models. https://blog.fal.ai/\nauraflow/ , Apr 2024. Accessed April 19, 2025.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY . Generative adversarial networks, 2014. URL https:\n//arxiv.org/abs/1406.2661 .\nGoswami, A. and Vadakkepat, P. (eds.). Hu-\nmanoid Robotics: A Reference . Springer Dor-\ndrecht, 2019. ISBN 978-94-007-6046-2. doi:\n10.1007/978-94-007-6046-2. URL https:\n//link.springer.com/referencework/\n10.1007/978-94-007-6046-2 .\n9\n--- Page 10 ---\nHumanoid World Models\nHa, D. and Schmidhuber, J. World mod-\nels. CoRR , abs/1803.10122, 2018. URL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1803.html#abs-1803-10122 .\nHarvey, W., Naderiparizi, S., and Wood, F. Condi-\ntional image generation by conditioning variational auto-\nencoders, 2022. URL https://arxiv.org/abs/\n2102.12037 .\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium, 2018. URL\nhttps://arxiv.org/abs/1706.08500 .\nHirose, M. and Ogawa, K. Honda humanoid robots develop-\nment. Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences , 365\n(1850):11–19, 2007. doi: 10.1098/rsta.2006.1917. URL\nhttps://royalsocietypublishing.org/\ndoi/10.1098/rsta.2006.1917 .\nHo, J. and Salimans, T. Classifier-free diffusion guid-\nance, 2022. URL https://arxiv.org/abs/\n2207.12598 .\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion prob-\nabilistic models, 2020. URL https://arxiv.org/\nabs/2006.11239 .\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,\nand Fleet, D. J. Video diffusion models, 2022. URL\nhttps://arxiv.org/abs/2204.03458 .\nHor´e, A. and Ziou, D. Image quality metrics: Psnr vs.\nssim. In 2010 20th International Conference on Pattern\nRecognition , pp. 2366–2369, 2010. doi: 10.1109/ICPR.\n2010.579.\nImtiaz, R. and Khan, A. Perceptions of humanoid robots\nin caregiving: A study of skilled nursing home and long\nterm care administrators, 2024. URL https://arxiv.\norg/abs/2401.02105 .\nJin, Y ., Sun, Z., Li, N., Xu, K., Xu, K., Jiang, H., Zhuang, N.,\nHuang, Q., Song, Y ., Mu, Y ., and Lin, Z. Pyramidal flow\nmatching for efficient video generative modeling, 2024.\nURL https://arxiv.org/abs/2410.05954 .\nKim, M., Pertsch, K., Karamcheti, S., Xiao, T., Balakr-\nishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G.,\nSanketi, P., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake,\nR., Sadigh, D., Levine, S., Liang, P., and Finn, C. Open-\nvla: An open-source vision-language-action model. arXiv\npreprint arXiv:2406.09246 , 2024.\nKong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J.,\nXiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q.,Yuan, J., Long, Y ., Wang, A., Wang, A., Li, C., Huang,\nD., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J.,\nXue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang,\nW., Yu, W., Deng, X., Li, Y ., Chen, Y ., Cui, Y ., Peng, Y .,\nYu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y ., Lu, Q.,\nLiu, S., Zhou, D., Wang, H., Yang, Y ., Wang, D., Liu, Y .,\nJiang, J., and Zhong, C. Hunyuanvideo: A systematic\nframework for large video generative models, 2025. URL\nhttps://arxiv.org/abs/2412.03603 .\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000) , pp. 1207–1216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLi, W., Yu, Z., She, Q., Yu, Z., Lan, Y ., Zhu, C., Hu, R., and\nXu, K. Llm-enhanced scene graph learning for household\nrearrangement, 2024. URL https://arxiv.org/\nabs/2408.12093 .\nLiang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter,\nB., Florence, P., and Zeng, A. Code as policies: Lan-\nguage model programs for embodied control, 2023. URL\nhttps://arxiv.org/abs/2209.07753 .\nLipman, Y ., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and\nLe, M. Flow matching for generative modeling, 2023.\nURL https://arxiv.org/abs/2210.02747 .\nLiu, X., Gong, C., and Liu, Q. Flow straight and fast: Learn-\ning to generate and transfer data with rectified flow, 2022.\nURL https://arxiv.org/abs/2209.03003 .\nLiu, Y ., Zhang, K., Li, Y ., Yan, Z., Gao, C., Chen, R.,\nYuan, Z., Huang, Y ., Sun, H., Gao, J., He, L., and Sun, L.\nSora: A review on background, technology, limitations,\nand opportunities of large vision models, 2024. URL\nhttps://arxiv.org/abs/2402.17177 .\nLuo, Z., Shi, F., Ge, Y ., Yang, Y ., Wang, L., and Shan, Y .\nOpen-magvit2: An open-source project toward democ-\nratizing auto-regressive visual generation, 2024. URL\nhttps://arxiv.org/abs/2409.04410 .\nNVIDIA, :, Agarwal, N., Ali, A., Bala, M., Balaji, Y .,\nBarker, E., Cai, T., Chattopadhyay, P., Chen, Y ., Cui,\nY ., Ding, Y ., Dworakowski, D., Fan, J., Fenzi, M., Fer-\nroni, F., Fidler, S., Fox, D., Ge, S., Ge, Y ., Gu, J., Gu-\nrurani, S., He, E., Huang, J., Huffman, J., Jannaty, P.,\nJin, J., Kim, S. W., Kl ´ar, G., Lam, G., Lan, S., Leal-\nTaixe, L., Li, A., Li, Z., Lin, C.-H., Lin, T.-Y ., Ling, H.,\nLiu, M.-Y ., Liu, X., Luo, A., Ma, Q., Mao, H., Mo, K.,\nMousavian, A., Nah, S., Niverty, S., Page, D., Paschali-\ndou, D., Patel, Z., Pavao, L., Ramezanali, M., Reda, F.,\nRen, X., Sabavat, V . R. N., Schmerling, E., Shi, S., Ste-\nfaniak, B., Tang, S., Tchapmi, L., Tredak, P., Tseng, W.-\nC., Varghese, J., Wang, H., Wang, H., Wang, H., Wang,\n10\n--- Page 11 ---\nHumanoid World Models\nT.-C., Wei, F., Wei, X., Wu, J. Z., Xu, J., Yang, W.,\nYen-Chen, L., Zeng, X., Zeng, Y ., Zhang, J., Zhang, Q.,\nZhang, Y ., Zhao, Q., and Zolkowski, A. Cosmos world\nfoundation model platform for physical ai, 2025. URL\nhttps://arxiv.org/abs/2501.03575 .\nPeebles, W. and Xie, S. Scalable diffusion models with trans-\nformers, 2023. URL https://arxiv.org/abs/\n2212.09748 .\nPfeifer, R. and Iida, F. Embodied Artificial Intelligence:\nTrends and Challenges , pp. 1–26. Springer Berlin Heidel-\nberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-27833-7.\ndoi: 10.1007/978-3-540-27833-7 1. URL https://\ndoi.org/10.1007/978-3-540-27833-7_1 .\nPolyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A.,\nLee, A., Vyas, A., Shi, B., Ma, C.-Y ., Chuang, C.-Y ., Yan,\nD., Choudhary, D., Wang, D., Sethi, G., Pang, G., Ma,\nH., Misra, I., Hou, J., Wang, J., Jagadeesh, K., Li, K.,\nZhang, L., Singh, M., Williamson, M., Le, M., Yu, M.,\nSingh, M. K., Zhang, P., Vajda, P., Duval, Q., Girdhar,\nR., Sumbaly, R., Rambhatla, S. S., Tsai, S., Azadi, S.,\nDatta, S., Chen, S., Bell, S., Ramaswamy, S., Sheynin,\nS., Bhattacharya, S., Motwani, S., Xu, T., Li, T., Hou,\nT., Hsu, W.-N., Yin, X., Dai, X., Taigman, Y ., Luo, Y .,\nLiu, Y .-C., Wu, Y .-C., Zhao, Y ., Kirstain, Y ., He, Z., He,\nZ., Pumarola, A., Thabet, A., Sanakoyeu, A., Mallya, A.,\nGuo, B., Araya, B., Kerr, B., Wood, C., Liu, C., Peng, C.,\nVengertsev, D., Schonfeld, E., Blanchard, E., Juefei-Xu,\nF., Nord, F., Liang, J., Hoffman, J., Kohler, J., Fire, K.,\nSivakumar, K., Chen, L., Yu, L., Gao, L., Georgopoulos,\nM., Moritz, R., Sampson, S. K., Li, S., Parmeggiani, S.,\nFine, S., Fowler, T., Petrovic, V ., and Du, Y . Movie\ngen: A cast of media foundation models, 2024. URL\nhttps://arxiv.org/abs/2410.13720 .\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-\nford, A., Chen, M., and Sutskever, I. Zero-shot text-to-\nimage generation, 2021. URL https://arxiv.org/\nabs/2102.12092 .\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models, 2022. URL https://arxiv.org/\nabs/2112.10752 .\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N.,\nand Ganguli, S. Deep unsupervised learning using\nnonequilibrium thermodynamics, 2015. URL https:\n//arxiv.org/abs/1503.03585 .\nSong, Y ., Sohl-Dickstein, J., Kingma, D. P., Kumar, A.,\nErmon, S., and Poole, B. Score-based generative model-\ning through stochastic differential equations, 2021. URL\nhttps://arxiv.org/abs/2011.13456 .Su, J., Lu, Y ., Pan, S., Murtadha, A., Wen, B., and Liu,\nY . Roformer: Enhanced transformer with rotary position\nembedding, 2023. URL https://arxiv.org/abs/\n2104.09864 .\nTong, S., Liu, Z., Zhai, Y ., Ma, Y ., LeCun, Y ., and Xie, S.\nEyes wide shut? exploring the visual shortcomings of\nmultimodal llms, 2024. URL https://arxiv.org/\nabs/2401.06209 .\nvan den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural\ndiscrete representation learning, 2018. URL https:\n//arxiv.org/abs/1711.00937 .\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need, 2023. URL https://arxiv.org/\nabs/1706.03762 .\nVianello, L., Penco, L., Gomes, W., You, Y ., Anzalone,\nS. M., Maurice, P., Thomas, V ., and Ivaldi, S. Human-\nhumanoid interaction and cooperation: a review. Current\nRobotics Reports , 2(4):441–454, December 2021. doi:\n10.1007/s43154-021-00068-z. URL https://doi.\norg/10.1007/s43154-021-00068-z .\nWang, S., Han, M., Jiao, Z., Zhang, Z., Wu, Y . N., Zhu,\nS.-C., and Liu, H. Llm3:large language model-based task\nand motion planning with motion failure reasoning, 2024.\nURL https://arxiv.org/abs/2403.11552 .\nWu, J., Yin, S., Feng, N., He, X., Li, D., Hao, J., and Long,\nM. ivideogpt: Interactive videogpts are scalable world\nmodels. In Advances in Neural Information Processing\nSystems , 2024.\nXiang, J., Liu, G., Gu, Y ., Gao, Q., Ning, Y ., Zha, Y ., Feng,\nZ., Tao, T., Hao, S., Shi, Y ., Liu, Z., Xing, E. P., and\nHu, Z. Pandora: Towards general world model with\nnatural language actions and video states, 2024. URL\nhttps://arxiv.org/abs/2406.09455 .\nXing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu,\nZ., and Jiang, Y .-G. A survey on video diffusion mod-\nels, 2024. URL https://arxiv.org/abs/2310.\n10647 .\nYang, M., Du, Y ., Ghasemipour, K., Tompson, J., Schuur-\nmans, D., and Abbeel, P. Learning interactive real-world\nsimulators. arXiv preprint arXiv:2310.06114 , 2023.\nYang, S., Walker, J., Parker-Holder, J., Du, Y ., Bruce, J.,\nBarreto, A., Abbeel, P., and Schuurmans, D. Video as\nthe new language for real-world decision making, 2024.\nURL https://arxiv.org/abs/2402.17139 .\nYang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu,\nJ., Yang, Y ., Hong, W., Zhang, X., Feng, G., Yin, D.,\n11\n--- Page 12 ---\nHumanoid World Models\nZhang, Y ., Wang, W., Cheng, Y ., Xu, B., Gu, X., Dong,\nY ., and Tang, J. Cogvideox: Text-to-video diffusion\nmodels with an expert transformer, 2025. URL https:\n//arxiv.org/abs/2408.06072 .\nYu, L., Cheng, Y ., Sohn, K., Lezama, J., Zhang, H., Chang,\nH., Hauptmann, A. G., Yang, M.-H., Hao, Y ., Essa, I.,\nand Jiang, L. Magvit: Masked generative video trans-\nformer, 2023. URL https://arxiv.org/abs/\n2212.05199 .\nYu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn,\nK., Minnen, D., Cheng, Y ., Birodkar, V ., Gupta, A., Gu,\nX., Hauptmann, A. G., Gong, B., Yang, M.-H., Essa,\nI., Ross, D. A., and Jiang, L. Language model beats\ndiffusion – tokenizer is key to visual generation, 2024.\nURL https://arxiv.org/abs/2310.05737 .\nZhang, J., Huang, J., Jin, S., and Lu, S. Vision-language\nmodels for vision tasks: A survey, 2024. URL https:\n//arxiv.org/abs/2304.00685 .\nZhang, L., Xiong, Y ., Yang, Z., Casas, S., Hu, R., and\nUrtasun, R. Copilot4d: Learning unsupervised world\nmodels for autonomous driving via discrete diffusion.\narXiv preprint arXiv:2311.01017 , 2023.\nZhao, T. Z., Kumar, V ., Levine, S., and Finn, C. Learn-\ning fine-grained bimanual manipulation with low-cost\nhardware, 2023. URL https://arxiv.org/abs/\n2304.13705 .\nZhu, F., Wu, H., Guo, S., Liu, Y ., Cheang, C., and Kong, T.\nIrasim: Learning interactive real-robot action simulators.\narXiv:2406.12802 , 2024.\n12",
  "text_length": 50699
}