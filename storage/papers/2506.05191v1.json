{
  "id": "http://arxiv.org/abs/2506.05191v1",
  "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
  "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning\nmethods are hindered by a key limitation: they are directly borrowed from LLMs,\noften neglecting the intrinsic differences of multimodal scenarios and even\naffecting the full utilization of all modalities. Inspired by our empirical\nobservation, we argue that unimodal adaptation and cross-modal adaptation are\ntwo essential parts for the effective fine-tuning of MLLMs. From this\nperspective, we propose Multimodal low-rank Adaptation (MokA), a\nmultimodal-aware efficient fine-tuning strategy that takes multimodal\ncharacteristics into consideration. It compresses unimodal information by\nmodality-specific parameters while explicitly enhancing cross-modal\ninteraction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility\nof the proposed method. Ablation studies and efficiency evaluation are also\nconducted to fully asses our method. Overall, we think MokA provides a more\ntargeted solution for efficient adaptation of MLLMs, paving the way for further\nexploration. The project page is at https://gewu-lab.github.io/MokA.",
  "authors": [
    "Yake Wei",
    "Yu Miao",
    "Dongzhan Zhou",
    "Di Hu"
  ],
  "published": "2025-06-05T16:04:08Z",
  "updated": "2025-06-05T16:04:08Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05191v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05191v1  [cs.CV]  5 Jun 2025MokA : Multimodal Low-Rank Adaptation for MLLMs\nYake Wei1,2,3, Yu Miao1,2,3, Dongzhan Zhou4, Di Hu1,2,3‚àó\n1Gaoling School of Artificial Intelligence Renmin University of China Beijing, China\n2Beijing Key Laboratory of Research on Large Models and Intelligent Governance\n3Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE\n4Shanghai Artificial Intelligence Laboratory\nyakewei@ruc.edu.cn, miaoyu823@outlook.com\nzhoudongzhan@pjlab.org.cn, dihu@ruc.edu.cn\nAbstract\nIn this paper, we reveal that most current efficient multimodal fine-tuning methods\nare hindered by a key limitation: they are directly borrowed from LLMs, often\nneglecting the intrinsic differences of multimodal scenarios and even affecting the\nfull utilization of all modalities. Inspired by our empirical observation, we argue\nthat unimodal adaptation and cross-modal adaptation are two essential parts for the\neffective fine-tuning of MLLMs. From this perspective, we propose Multimodal\nlow-ran k Adaptation (MokA ), a multimodal-aware efficient fine-tuning strategy\nthat takes multimodal characteristics into consideration. It compresses unimodal\ninformation by modality-specific parameters while explicitly enhancing cross-\nmodal interaction, ensuring both unimodal and cross-modal adaptation. Extensive\nexperiments cover three representative multimodal scenarios (audio-visual-text,\nvisual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2,\nQwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of\nthe proposed method. Ablation studies and efficiency evaluation are also conducted\nto fully asses our method. Overall, we think MokA provides a more targeted\nsolution for efficient adaptation of MLLMs, paving the way for further exploration.\nThe project page is at https://gewu-lab.github.io/MokA .\n1 Introduction\nLarge language models (LLMs) have gained remarkable popularity due to their impressive ability to\nunderstand and generate content. To extend their capabilities to more general multimodal scenarios,\nrecent advancements of Multimodal Large Language Models (MLLMs) [ 35,37,18] have focused\non aligning other modalities, such as images, with text tokens, thereby equipping LLMs with the\nability to interpret and process content of other modalities. However, due to the massive parameter\nscale of LLMs, fully fine-tuning such models on downstream tasks is computationally prohibitive\nand inefficient in most cases.\nA promising direction has emerged in the field of LLM fine-tuning before, which involves selectively\nupdating a subset of parameters rather than the full model. These Parameter-Efficient Fine-Tuning\n(PEFT) strategies have seen widespread adoption and have been successfully extended to the fine-\ntuning of MLLMs. In particular, LoRA [ 11] and its variants, which assume that over-parameterized\nmodels in fact reside on a low intrinsic dimension, have been broadly applied [ 5,6,36], demonstrating\nstrong adaptability and efficiency. However, the development of efficient multimodal LLM fine-\ntuning is at present obscured by a ‚Äúdark cloud‚Äù: most current methods are directly borrowed from\nLLMs, often overlooking the fundamental differences of multimodal scenarios . Indeed, prior studies\n‚àóCorresponding Author\nPreprint. Under review.\n--- Page 2 ---\nLLM (LoRA\nüî•)Audio tokenTexttokenVisual tokenLLM Tokenizer\nQuestion: what is the baby doing?\nProjectorProjector\n‚ùÑEncoderEncoder(a) MLLM framework.\nPretrainedWeightsBAAudio tokenTexttokenVisual token\n‚ùÑ\nüî•\nüî• (b) Classic LoRA module.\nPretrainedWeightsBAAudio tokenTexttokenVisual token\n‚ùÑ\n‚ùÑ\n‚ùÑ (c) Partial modality inference.\nText-only Full modality Audio-only Visual-only5560657075Acc\nMUSIC-AVQA\n(d) Audio-visual-text case.\nText-only Full modality Visual-only60626466687072Acc\nPOPE (e) Visual-text case.\nText-only Full modality Speech-only2628303234Acc\nAIR-Benchspeechen\n (f) Speech-text case.\nFigure 1: (a): Common MLLM fine-tuning framework. (b): Sketch of classic LoRA module for\nMLLM fine-tuning. (c): Partial modality inference setting. Take tokens of visual modality as an\nexample. (d-f) : Partial modality inference performance of LoRA. Full modality : Regular case where\nall multimodal tokens are processed by the LoRA module. Text-/Audio-/Visual-/Speech-only : Only\ntext/audio/visual/speech tokens are passed through the LoRA pathway at the first generation step\nduring inference. Results are based on LLaMA2.\nin multimodal learning have demonstrated that the inherent heterogeneity of different modalities\nnecessitates modality-specific utilization strategies, rather than a fully unified way [22, 31].\nTo this end, we are motivated to observe the fine-tuning efficacy of the widely used LoRA strategy. A\ncommon MLLM fine-tuning framework is shown in Figure 1a: encoded representation of non-text\nmodality (e.g., audio or visual) is first aligned with the text embedding space via a projector (usually\nQ-former or MLP), after which the resulting multimodal tokens are integrated and processed jointly by\nthe LLM. In the efficient fine-tuning case, the LLM backbone is frozen, and parameters of additional\nLoRA modules are optimized. Figure 1b provides the sketch of classic LoRA module. AandB\nmatrices are shared across different modalities.\nTo further observe how well tokens of different modalities are utilized, we conduct partial modality\ninference experiments. The training stage retains the original setting, wherein all multimodal tokens\nare processed by the LoRA module. Specifically, we evaluate the model‚Äôs performance when\nonly tokens from a selected modality are passed through the LoRA adaptation pathway at the first\ngeneration step during inference. As illustrated in Figure 1c, visual token inference is used as an\nexample. And it should be noted that the pre-trained weights still receive full tokens of all modalities.\nResults shown in Figure 1d-1f demonstrate a surprising phenomenon across three representative\nmultimodal scenarios, audio-visual-text case, visual-text case, and speech-text case. Text token\ninference can achieve quite comparable performance to the regular full modalities case. However,\nNon-text token inference (e.g., audio or visual) leads to a noticeable drop in performance.\nThe above results suggest that the optimization of all-modality-shared LoRA parameters is overly\ninfluenced by text tokens, resulting in non-text tokens being less effectively utilized during fine-\ntuning. Although these all-modality-shared parameters implicitly improve cross-modal interaction,\nthis phenomenon reveals the need to consider individual modality during fine-tuning. This fact\ninspires us that unimodal and cross-modal adaptation are equally critical in the fine-tuning of\nMLLMs , which is mostly ignored as mentioned above.\nTo this end, we propose the Multim odal low-ran k Adaptation ( MokA ), a fine-tuning strategy designed\nto achieve unimodal adaptation while explicitly enhancing cross-modal interaction. While MokA\nretains the widely adopted low-rank decomposition matrices, it redefines the roles of matrices A\nandBto better accommodate multimodal characteristics. Specifically, matrix Ais designed to be\nmodality-specific, allowing each modality to compress information independently and thus avoid\n2\n--- Page 3 ---\ninterference from others. After that, a cross-attention mechanism is introduced to strengthen the\ninteraction between text tokens and non-text tokens, emphasizing task-relevant features. Finally, a\nshared multimodal matrix Bprojects the unimodal low-rank representations into a unified space,\nfacilitating effective alignment across modalities. These three parts jointly ensure both unimodal and\ncross-modal adaptation. In experiments, noticeable improvement in multiple multimodal scenarios\ndemonstrates the effectiveness of our method. We think MokA represents a first-step attempt at\nmultimodal-aware adaptation, and further possibilities exist under our basis that simultaneously\naccounts for both unimodal and cross-modal adaptation.\n2 Method\n2.1 Rethinking of low-rank adaptation in the multimodal scenario\nLoRA [ 11] is based on the assumption that the weight updates during fine-tuning lie in a subspace\nof low ‚Äúintrinsic rank.‚Äù Rather than updating the entire pre-trained weight matrix directly, LoRA\nintroduces a low-rank decomposition approach, where the update ‚àÜW‚ààRd√ókto a pre-trained matrix\nW0‚ààRd√ókis parameterized as the product of two much smaller matrices: B‚ààRd√órandA‚ààRr√ók,\nwithr‚â™min(d, k). The resulting fine-tuned weight matrix W‚Ä≤is given by W0+ ‚àÜW=W0+BA.\nTherefore, for h=W0x, the modified update forward pass yields:\nh=W0x+ ‚àÜWx=W0x+BAx. (1)\nHere, W0remains fixed during training, while only the matrices AandBare learned. To ensure\nstable training, Ais initialized using a uniform Kaiming distribution [ 10], and Bis initialized to\nzero, leading to an initial update ‚àÜW=BA= 0at the beginning of fine-tuning. LoRA [ 11] and its\nvariants have been extensively employed in the parameter-efficient fine-tuning of MLLMs [ 5,6,36].\nThese methods typically employ shared parameters to uniformly process tokens from all modalities,\nimplicitly facilitating cross-modal interactions during adaptation. However, our empirical results\nreveal that such shared tuning leads to limited utilization of all modalities. This highlights the need to\nconsider individual modality during fine-tuning.\nTo better support multimodal adaptation, we argue that both unimodal and cross-modal updates\nshould be considered during fine-tuning. In other words, the model should be able to learn from each\nmodality independently while also ensuring the cross-modal interaction. Therefore, the design of\nthe update mechanism should ensure that both types of information are properly captured during the\nforward pass:\nh=W0x+ ‚àÜWx=W0x+ ‚àÜW[xm1;xm2;¬∑¬∑¬∑;xmn], (2)\n=W0x+ [‚àÜW1xm1; ‚àÜW2xm2;¬∑¬∑¬∑; ‚àÜWnxmn]| {z }\nunimodal adaptation+ ‚àÜWcross[xm1;xm2;¬∑¬∑¬∑;xmn]| {z }\ncross-modal adaptation, (3)\nwhere nis the number of modalities. xmiis the token sequence of modality i.‚àÜWiis the unimodal\nupdate parameters of modality i, and ‚àÜWcross is cross-modal update parameters.\n2.2 Multimodal low-rank Adaptation (MokA)\nPretrainedWeightsMultimodal BCross Attention\nAudio AVisual AText A\nFigure 2: Illustration of MokA strategy.Based on the above perspective, we propose Multim odal\nlow-ran k A daptation ( MokA ) strategy, a parameter-\nefficient fine-tuning method tailored for the multimodal\nnature of MLLMs. Considering the efficiency advantage of\nLoRA, MokA retains the core idea of low-rank adaptation,\nbut redefines the roles of the projection matrices AandB\nto better reflect the characteristics of multimodal scenarios.\nBy unimodal compression and explicitly reinforcing cross-\nmodal interaction, MokA enables both unimodal and cross-\nmodal adaptation, leading to more effective fine-tuning\nof MLLMs. The overall structure of MokA is depicted\nin Figure 2.\nConcretely, MokA has three core parts: unimodal matrix A,\ntask-centric cross-attention, and shared multimodal matrix\nB. Here we take the audio-visual-text case as an example, and other cases can be well extended.\n3\n--- Page 4 ---\n2.2.1 Unimodal matrix A\nFor an arbitrary pretrained weight W0in the LLMs, we suppose its input sequence is x=\n[xa\n1;xa\n2;¬∑¬∑¬∑;xa\nNa;xv\n1;xv\n2;¬∑¬∑¬∑;xv\nNv;xt\n1;xt\n2;¬∑¬∑¬∑;xt\nNt]. Here {Ni}i‚àà{a,v,t}is the token length of\nmodality i.xa\n1is the first token of modality a, and so on. For simplicity, we use xito denote the\ntoken sequence of modality i. Then the whole input sequence can be rewritten as x= [xa;xv;xt].\nTo ensure the well compression of unique unimodal information and avoid the interruption from others,\nmatrix Ais designed to be individual for each modality, allowing tokens from different modalities\nto be processed independently through their respective parameter. The compressed sequence after\nmatrix Ais:\nAx= [Aaxa;Avxv;Atxt], (4)\nwhere {Ai}i‚àà{a,v,t}is the parameter of modality i. After processing by unimodal matrix A, embed-\ndings of each modality are individually mapped into a low-rank space, without the potential influence\nof other modalities.\n2.2.2 Task-centric cross-attention\nIn the fine-tuning process of MLLMs, text and non-text tokens typically serve distinct roles. Specifi-\ncally, under supervised instruction tuning, text tokens often function as task descriptions or prompts,\nwhereas non-text tokens (e.g., audio or visual inputs) primarily convey contextual information upon\nwhich the task is based. The following example illustrates a typical instruction format:\n<audio> <visual> Please answer the question: which clarinet makes the sound first?\nText tokenKeyVisualtokenQueryTexttokenValue(    )softmax\nNewvisualtokenshortcut\nFigure 3: Cross-attention part of MokA.\nTake the visual token as an example.In this case, <audio> and<visual> provide the event\ninformation. ‚Äú Please answer the question: which clarinet\nmakes the sound first? ‚Äù describes the concrete task for\nLLMs. Successfully answering such questions relies on\neffectively capturing the semantic association between the\ntask description conveyed by text tokens and the event\ncues provided by non-text tokens. Therefore, it becomes\nintuitive and necessary to explicitly emphasize the most\nrelevant cross-modal information to support accurate rea-\nsoning. Since unimodal information has been extracted\nindividually after the processing of unimodal matrices A, this stage is well-suited for introducing\ncross-modal interaction. Additionally, as the token embeddings are projected into a low-rank space,\nthe computational burden of performing cross-modal interaction is significantly reduced. Hence,\nwe place the cross-attention part after the low-rank compression to ensure both effectiveness and\nefficiency. The concrete attention mechanism is illustrated in Figure 3, and is conducted as follows:\nAtt\u0000\nAaxa, Atxt, Atxt\u0001\n=softmax\u0012(Aaxa)(Atxt)‚ä§\n‚àöNt\u0013\nAtxt, (5)\nAtt\u0000\nAvxv, Atxt, Atxt\u0001\n=softmax\u0012(Avxv)(Atxt)‚ä§\n‚àöNt\u0013\nAtxt. (6)\nThen, the enhanced audio and visual tokens are:\nAaxa+Att\u0000\nAaxa, Atxt, Atxt\u0001\n, (7)\nAvxv+Att\u0000\nAvxv, Atxt, Atxt\u0001\n. (8)\nFinally, the sequence after cross-attention is:\nAx= [Aaxa+Atta,t,t;Avxv+Attv,t,t;Atxt]. (9)\nHere we use Atti,t,tto simply denote the cross-attention between modality iand text. It should be\nnoted that while we adopt a cross-attention module to explicitly enhance the interaction between text\nand non-text tokens, alternative designs that serve a similar purpose can also be considered. Further\ndiscussion is provided in Section 4.4. In addition, in MokA, linear projections ( Wq,Wk, andWv) are\nnot included in the cross-attention module, since low-rank matrices Aof each modality actually can\nbe considered as the linear projection in attention in this case. We also provide more discussion and\ncomparison in Appendix B.\n4\n--- Page 5 ---\n2.2.3 Shared multimodal matrix B\nAfter unimodal compression and explicit cross-modal interaction enhancement, it becomes crucial to\nproject the resulting unimodal representations into a shared space to facilitate cross-modal alignment.\nTo this end, a shared multimodal matrix Bis employed to perform this projection. The final output of\nthe MokA pathway is thus given by:\nBAx= [B(Aaxa+Atta,t,t);B(Avxv+Attv,t,t);BAtxt]. (10)\n2.2.4 Overview\nIn conclusion, in MokA, for a pretrained weight matrix W0‚ààRd√ók, its update ‚àÜW‚ààRd√ókis\nparameterized as the product of much smaller matrices: B‚ààRd√órand{Ai‚ààRr√ók}i‚àà{a,v,t}, with\nr‚â™min(d, k). For input sequence x, the forward pass yields:\nh=W0x+ ‚àÜWx=W0x+ ‚àÜW[xa;xv;xt], (11)\n=W0x+ [B(Aaxa+Atta,t,t);B(Avxv+Attv,t,t);BAtxt], (12)\n=W0x+ [BAaxa;BAvxv;BAtxt]| {z }\nunimodal adaptation+ [BAtta,t,t;BAttv,t,t;0Nt]| {z }\ncross-modal adaptation, (13)\nwhere 0Ntdenotes the zero vector of dimension Nt, since text-token remains unchanged after cross-\nattention. During fine-tuning, W0remains unchanged, with AiandBbeing subject to optimization.\nAlso, Aiis initialized using the uniform Kaiming distribution [ 10], while Bis initialized to zero. It\nleads to an initial update ‚àÜW= 0at the beginning of fine-tuning, to provide a smooth starting point.\nBased on Equation 13, MokA ensures both unimodal and cross-modal adaptation, offering a more\ntailored solution for fine-tuning MLLMs.\n3 Training and evaluation details\n3.1 Implement details\nOur framework follows the common MLLM framework as illustrated in Figure 1a, but with MokA\nstrategy. Text input is processed by the corresponding LLM tokenizer, and non-text input is first\nencoded by its encoder, and then aligned with the text embedding space via a projector. Here we use\nQ-former followed by a two-layer MLP as the projector. Finally, all tokens are fed into LLM.\nFor the visual branch of audio-visual-text and visual-text scenarios, we use CLIP-ViT/L-14 [ 23] as\nthe visual encoder to extract the last layer patch level embedding of each frame or image. For the\naudio branch of the audio-visual-text scenario, we use the BEATs [ 4] encoder to extract features. For\nthe speech branch of the speech-text scenario, OpenAI‚Äôs Whisper model [ 24] is used. The number of\nquery tokens in Q-Former of all branches is 32.\n3.2 Training procedure and benchmarks\nOur experiment of MLLM follows the widely used two-stage training paradigm: pre-training stage\nthat aims to cross-modal alignment and supervised instruction-tuning for downstream tasks.\nPre-training : LLM backbone is frozen. Projectors are trainable for cross-modal alignment. For\nthe visual branch of audio-visual-text and visual-text scenarios, trainable modules are trained on\nvideo-LLaV A [ 17] dataset, including the video captioning and the image captioning tasks. For the\naudio branch of the audio-visual-text scenario, trainable modules are trained on AudioCaps [ 12]\ndataset on the audio captioning task. For the speech branch of the speech-text scenario, trainable\nmodules are trained on GigaSpeech-M [ 3] dataset on the speech recognition task. During pre-training,\neach branch is trained for one epoch, using the AdamW optimizer with a cosine learning rate schedule.\nThe initial learning rate is 1e‚àí4with a warmup ratio of 0.03.\nInstruction-tuning : At this stage, we train the model on downstream tasks in different scenarios.\nTrainable parameters include all projectors and our MokA module. For the audio-visual-text case,\nthe model is fine-tuned on the train set of MUSIC-A VQA [ 15], and A VE [ 28], respectively. For\nthe visual-text case, the model is fine-tuned on the LLaV A-Instruct-150K [ 18]. For the speech-text\n5\n--- Page 6 ---\nLLMBackboneAudio tokenTexttokenVisual tokenMultimodal AMultimodal BAudio AVisual AText AShared B\n(a) Uni LoRA + MM LoRA.\nLLMBackboneAudio tokenTexttokenVisual tokenMultimodal AMultimodal BGateAudio AVisual AText AShared B (b) Uni LoRA + MM LoRA + Gate.\nFigure 4: Framework of baselines that also follow our multimodal-aware basis, yet relying on more\nparameters and offering limited cross-modal interaction.\ncase, the model is fine-tuned on the LibriSpeech [ 21], using the annotations provided by [ 26]. The\nvisual-text case is trained for one epoch, and other cases are trained for three epochs. Rank of\nlow-rank matrices is 4. The remaining settings are the same as the first stage.\nInference : To well assess the effectiveness of our fine-tuning strategy, we evaluate our trained models\non in-domain test sets or public benchmarks. Details are provided in the supplementary materials.\n‚Ä¢Audio-visual-text : in-domain test set of MUSIC-A VQA and A VE dataset.\n‚Ä¢Visual-text : public benchmarks: MME percep [8], MMBench [ 20], POPE [ 16], SEED-Bench [ 14].\n‚Ä¢Speech-text : public benchmarks: MMAU min‚àíspeech [25] as well as the foundation subset of\nAIR-Bench speech ‚àíen[34].\nLarge Language Model. For all three cases, LLaMA-2-7b-Chat [ 29], LLaMA-3-8B-Instruct [ 9],\nand Qwen2-7B-Instruct [ 33] are used as the LLM base model, respectively. For the audio-visual-text\ncase, Qwen2.5-VL-7B-Instruct [ 2] is also used as the LLM base model. Throughout the training\nprocess, weights of LLM are kept frozen. More experiments of Qwen3 are provided in Appendix A.\n4 Experiments\n4.1 Audio-visual-text scenario\nTo validate the effectiveness of our MokA fine-tuning strategy, we compare it with LoRA [11]\nand its variants, including multiple LoRA ,LoRAMoE [7],DoRA [19],HydraLoRA [27],Uni-modal\nLoRA [1]. In addition, we also compare with two additional baselines, whose frameworks are provided\nin Figure 4. Concretely, the Uni LoRA + MM LoRA strategy employs unimodal low-rank matrices\nAto extract unimodal information independently, while incorporating an additional fully shared\nmultimodal LoRA module to implicitly promote cross-modal interaction. The Uni LoRA + MM\nLoRA + Gate variant further introduces a gating mechanism to dynamically integrate the outputs of\nthe Uni LoRA and MM LoRA branches for improved fusion. These two baselines incorporate our\nmultimodal-aware basis that ensures both unimodal and cross-modal adaptation, but involve more\nparameters and offer limited cross-modal interaction. Based on the results in Table 1, we can have\nthe following observations:\nOur proposed MokA method achieves the superior overall performance across multiple audio-visual-\ntext datasets, consistently outperforming other baselines and compared methods. While MokA\nintroduces a slight increase in parameter scale compared to standard LoRA, this does not account for\nthe observed performance improvements. Based on the Table 1, multiple LoRA, a baseline that uses\n3A matrices and BA matrices, underperforms both standard LoRA and MokA. Simply increasing\nthe number of low-rank matrices does not necessarily lead to better fine-tuning performance. This\nsuggests that MokA‚Äôs advantage stems not from parameter quantity, but from its insurance for both\nunimodal and multimodal adaptation.\nThe mentioned two baselines, Uni LoRA + MM LoRA andUni LoRA + MM LoRA + Gate, achieve\ncompetitive results. These results further support the validity of our multimodal-aware basis that\nunimodal and cross-modal adaptation are both essential for the fine-tuning of MLLMs. However, de-\nspite their effectiveness, MokA achieves superior results with fewer parameters and further enhanced\ncross-modal interactions.\nIn addition, Qwen2.5VL with LoRA outperforms both LLaMA2 and Qwen2 under the LoRA fine-\ntuning setting. But when using MokA, the performance of Qwen2.5VL is slightly lower than that\nof LLaMA2 and Qwen2. A possible reason is that the official visual connector in Qwen2.5VL,\n6\n--- Page 7 ---\nTable 1: Evaluation results of LoRA, its variants, and our MokA on audio-visual-text datasets,\nMUSIC-A VQA and A VE. #A and #B are the number of low-rank matrices. Here N= 3refers to the\nnumber of modalities.\nLLM Method MUSIC-A VQA A VE #A #B\nLoRA [11] 73.41 69.84 1 1\nMultiple LoRA 72.66 71.77 N N\nLoRAMoE [7] 73.57 72.81 N N\nDoRA [19] 73.97 72.18 1 1\nHydraLoRA [27] 74.12 72.27 1 N\nUni-modal LoRA [1] 74.37 71.44 N N\nUni LoRA + MM LoRA 74.43 72.36 N+ 1 2\nUni LoRA + MM LoRA + Gate 74.94 73.56 N+ 1 2LLaMA2\nMokA 75.71 74.68 N 1\nLoRA 72.83 72.13 1 1\nMultiple LoRA 72.71 72.11 N N\nLoRAMoE [7] 73.48 72.83 N N\nDoRA [19] 73.29 72.91 1 1\nHydraLoRA [27] 73.14 72.59 1 N\nUni-modal LoRA [1] 73.62 73.14 N N\nUni LoRA + MM LoRA 74.09 73.35 N+ 1 2\nUni LoRA + MM LoRA + Gate 74.71 73.96 N+ 1 2Qwen2\nMokA 75.26 74.48 N 1\nLoRA 73.00 71.38 1 1\nMultiple LoRA 73.13 71.27 N N\nLoRAMoE [7] 73.28 71.91 N N\nDoRA [19] 73.37 71.06 1 1\nHydraLoRA [27] 73.04 71.26 1 N\nUni-modal LoRA [1] 73.46 72.11 N N\nUni LoRA + MM LoRA 73.75 72.27 N+ 1 2\nUni LoRA + MM LoRA + Gate 73.81 72.68 N+ 1 2Qwen2.5-VL\nMokA 74.87 73.14 N 1\nLoRA 78.31 76.91 1 1\nMultiple LoRA 78.63 77.02 N N LLaMA3\nMokA 79.15 77.81 N 1\nwhich serves a similar role to the projector used in our other LLM variants, remains frozen during\nfine-tuning. As a result, only the newly introduced audio branch is trainable, which may have limited\nthe full potential of our method. But MokA still introduces noticeable improvement in this case,\ncompared to other methods. In summary, our method achieves considerable improvement across\nvarious LLM backbones, demonstrating its broad versatility.\n4.2 Visual-text and speech-text scenarios\nTo further validate our method across a broader range of multimodal scenarios, we conducted\nexperiments beyond the challenging audio-visual-text case. Specifically, our method is further\nverified on two representative multimodal scenarios: visual-text and speech-text. For these tasks, we\nadopted three different LLM backbones, LLaMA2, LLaMA3, and Qwen2. The corresponding results\nare presented in Table 2 and Table 3. The experimental results demonstrate that our method achieves\nstable and consistent performance gains across multiple benchmark datasets, further confirming its\neffectiveness. This indicates the versatility of MokA in handling different multimodal combinations\nand LLM architectures.\n7\n--- Page 8 ---\nTable 2: Evaluation results of LoRA, its variants, and our MokA on visual-text benchmarks. #A and\n#B are the number of low-rank matrices. Here N= 2refers to the number of modalities.\nLLM Method MME percep MMBench POPE SEED-Bench #A #B\nLoRA 908.52 50.64 70.28 39.71 1 1\nMultiple LoRA 882.87 49.83 68.20 38.44 N N\nLoRAMoE [7] 938.52 51.98 71.15 39.13 N N\nDoRA [19] 786.47 51.31 71.07 38.96 1 1\nHydraLoRA [27] 774.47 47.33 70.87 38.81 1N\nUni-modal LoRA [1] 992.31 51.98 72.24 39.27 N N\nUni LoRA + MM LoRA 972.87 50.96 73.39 39.74 N+ 1 2\nUni LoRA + MM LoRA + Gate 988.37 52.01 73.48 39.91 N+ 1 2LLaMA2\nMokA 1025.86 52.74 74.23 40.45 N 1\nLoRA 1062.34 57.89 81.17 55.25 1 1\nMultiple LoRA 1103.28 57.01 80.96 55.13 N N\nLoRAMoE [7] 1157.39 57.29 81.29 56.39 N N\nDoRA [19] 1024.42 56.19 80.75 55.03 1 1\nHydraLoRA [27] 1098.25 56.42 81.34 54.67 1N\nUni-modal LoRA [1] 1189.47 57.39 81.12 56.21 N N\nUni LoRA + MM LoRA 1191.81 57.17 81.46 56.84 N+ 1 2\nUni LoRA + MM LoRA + Gate 1201.49 57.91 81.72 57.18 N+ 1 2Qwen2\nMokA 1292.37 59.06 82.33 58.10 N 1\nLoRA 1030.64 68.45 77.47 56.34 1 1\nMultiple LoRA 1032.74 68.79 78.73 56.06 N N LLaMA3\nMokA 1072.67 69.90 79.27 56.60 N 1\nText-only Full modality Audio-only Visual-only5560657075Acc\nMUSIC-AVQA\n(a) Audio-visual-text case.\nText-only Full modality Visual-only646668707274Acc\nPOPE (b) Visual-text case.\nText-only Full modality Speech-only2025303540Acc\nAIR-Benchspeechen\n (c) Speech-text case.\nFigure 5: Partial modality inference performance of MokA w/o cross-attention .Full modality :\nRegular case where all multimodal tokens are processed by the MokA module w/o cross-attention.\nText-/Audio-/Visual-/Speech-only : Only text/audio/visual/speech tokens are passed through the LoRA\npathway at the first generation step during inference. Results are based on LLaMA2.\n4.3 Partial modality inference of MokA\nTo further examine how effectively MokA leverages tokens from different modalities, we also conduct\npartial modality inference experiments where only tokens from a selected modality are passed through\nthe LoRA adaptation pathway at the first generation during inference. It should be noted that the\nevaluated model is MokA w/o cross-attention , as cross-attention computation requires the presence\nof both text and non-text tokens. The results, presented in Figure 5, show that MokA w/o cross-\nattention significantly enhances the utilization of individual modalities compared to LoRA (as shown\nin Figure 1d-1f). These findings highlight that the multimodal-aware design of MokA facilitates more\neffective use of all available modalities.\n4.4 Cross-modal interaction variants\nIn the original MokA framework, cross-attention is employed to explicitly strengthen the interaction\nbetween text and non-text tokens, thereby facilitating improved cross-modal adaptation. As previously\ndiscussed, alternative modules that similarly enhance this interaction can also be considered. In this\n8\n--- Page 9 ---\nTable 3: Evaluation results of LoRA, its variants, and our MokA on speech-text benchmarks,\nMMAU min‚àíspeech and AIR-Bench speech ‚àíen. #A and #B are the number of low-rank matrices. Here\nN= 2refers to the number of modalities.\nLLM Method MMAU AIR-Bench #A #B\nLoRA 30.33 31.75 1 1\nMultiple LoRA 29.73 31.91 N N\nLoRAMoE [7] 27.63 33.97 N N\nDoRA [19] 26.73 34.36 1 1\nHydraLoRA [27] 29.13 31.66 1 N\nUni-modal LoRA [1] 38.14 35.14 N N\nUni LoRA + MM LoRA 37.54 32.56 N+ 1 2\nUni LoRA + MM LoRA + Gate 32.13 33.04 N+ 1 2LLaMA2\nMokA 38.44 39.64 N 1\nLoRA 50.15 44.55 1 1\nMultiple LoRA 50.45 41.13 N N\nLoRAMoE [7] 50.75 42.99 N N\nDoRA [19] 52.55 44.11 1 1\nHydraLoRA [27] 53.45 43.94 1 N\nUni-modal LoRA [1] 54.05 47.01 N N\nUni LoRA + MM LoRA 54.16 43.55 N+ 1 2\nUni LoRA + MM LoRA + Gate 54.35 46.15 N+ 1 2Qwen2\nMokA 55.26 49.17 N 1\nLoRA 46.25 43.04 1 1\nMultiple LoRA 44.74 43.87 N N LLaMA3\nMokA 51.05 44.39 N 1\nTable 4: Evaluation results of MokA and variants on audio-visual-text and visual-text cases. Results\nare based on LLaMA2.\nMethod Music-A VQA A VE MME percep MMBench POPE SEED-Bench\nLoRA 73.41 69.84 908.52 50.64 70.28 39.71\nMultiple LoRA 72.66 71.77 882.87 49.83 68.20 38.44\nCross-attention* 74.94 72.59 955.18 51.25 72.94 39.91\nNaive interaction 75.04 73.18 996.73 51.49 73.52 40.17\nMokA 75.71 74.68 1025.86 52.74 74.23 40.45\nsection, we explore several variants of the cross-modal interaction module, as summarized in Table 4.\nThecross-attention* variant also adopts a cross-attention mechanism; however, it uses text tokens\nas queries. Consequently, the updated text tokens integrate information from the relevant non-text\ntokens‚Äîreversing the direction of interaction compared to the original MokA. The naive interaction\nvariant performs a simple, uniform mapping from text tokens to non-text tokens without employing\nany attention mechanism.\nExperimental results show that all proposed variants outperform the LoRA baseline, demonstrating the\ngeneral effectiveness of explicitly enhancing cross-modal interactions. However, the cross-attention*\nvariant performs slightly worse than the others. One possible explanation is that, unlike in other\nvariants where text tokens remain unchanged, this variant alters text tokens by integrating non-text\nfeatures. Although cross-modal interaction is enhanced, the modification of text representations\nmay adversely affect language modeling capabilities. In addition, while naive interaction yields\ncompetitive results, MokA achieves further improvement through its dynamic attention mechanism.\nThese findings suggest that the core idea of explicitly reinforcing cross-modal interactions is beneficial,\nand the effectiveness is not restricted to one specific module design.\n9\n--- Page 10 ---\n4.5 Ablation study\nTable 5: Ablation study of MokA. CA denotes Cross-\nAttention. Results are based on LLaMA2.\nMethod MUSIC-A VQA POPE AIR-Bench\nLoRA [11] 73.41 70.28 31.75\nMultiple LoRA 72.66 68.20 31.97\nMokA w/o CA 74.85 73.57 33.25\nMokA 75.71 74.23 39.64To thoroughly validate the efficacy of\nour method, we conduct ablation stud-\nies across all three multimodal sce-\nnarios. Results are shown in Table 5.\nBased on the results, even without the\ncross-attention module, MokA w/o\nCA outperforms the LoRA baseline,\ndemonstrating the effectiveness of en-\nhancing unimodal adaptation. Furthermore, the introduction of the cross-attention module leads to\nadditional performance improvements, indicating the benefit of explicitly enhancing cross-modal\nadaptation. These results indicate the necessity of each part in MokA.\n4.6 Efficiency evaluation\nTable 6: Efficiency evaluation and performance com-\nparison. Here, trainable parameters include low-rank\nmatrices and all projectors. Results are based on\nLLaMA2.\nMethodTrainable / Total\nParametersInference\nLatencyPOPE\nAcc\nLoRA [11] 1.27% 1.00√ó 70.28\nMultiple LoRA 1.43% 1.36√ó 68.20\nMokA 1.33% 1.13√ó 74.23To enable a more comprehensive compari-\nson, we further evaluate the proposed MokA\nand LoRA baselines on the proportion of\ntrainable parameters in the full model, and\ninference latency. As reported in Table 6, al-\nthough MokA introduces additional param-\neters due to the inclusion of more low-rank\nmatrices, the increase is quite modest com-\npared to the full LLM. Also, despite MokA\nincurs a slight increase in inference latency\ncompared to standard LoRA, it achieves a\nnotable performance gain of 3.95% on the\nPOPE benchmark. These results suggest\nthat the additional computational cost introduced by MokA is acceptable, and the performance\nimprovement is considerable.\n5 Related works\nMLLMs built upon powerful LLM backbones are increasingly demonstrating impressive capabilities\nacross diverse downstream tasks. However, fine-tuning these models remains computationally\nexpensive, prompting growing interest in parameter-efficient fine-tuning (PEFT) techniques that\nreduce memory and storage overhead during adaptation. Among them, LoRA has emerged as a\nwidely adopted, and researchers have proposed several variants to further improve its efficiency and\nflexibility [ 7,19,27,1]. For instance, LoRAMoE [ 7] introduces multiple LoRA heads combined via\na gating mechanism, while DoRA [ 19] focuses solely on optimizing the gradient direction, enabling\nmore efficient updates. Despite these advancements, most PEFT strategies for MLLMs are direct\nextensions of LLM techniques and fail to account for the inherent characteristics of multimodal\nlearning. To address this gap, we propose MokA, a fine-tuning strategy specifically designed for\nMLLMs. It explicitly ensures both unimodal and cross-modal adaptation to better preserve unimodal\nrepresentations and enhance cross-modal interaction, offering a targeted solution for efficient and\neffective multimodal adaptation.\n6 Discussion\nIn this paper, we argue that both unimodal adaptation and cross-modal adaptation are essential parts\nfor the effective fine-tuning of MLLMs, yet have largely been neglected before. To this end, we\npropose Multimodal low-ran k Adaptation (MokA ) for efficient multimodal fine-tuning. MokA\nredefines the roles of low-rank matrices AandB, ensuring unimodal information is preserved while\nenhancing cross-modal interaction by cross-attention. We think MokA is a preliminary step toward\nmultimodal-aware adaptation, highlighting the potential for future extensions that jointly consider\nboth unimodal and cross-modal adaptation.\n10\n--- Page 11 ---\nReferences\n[1]Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin\nBao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report:\nCompact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743 ,\n2025.\n[2]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025.\n[3]Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel\nPovey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000\nhours of transcribed audio. arXiv preprint arXiv:2106.06909 , 2021.\n[4]Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, and Furu Wei. Beats:\nAudio pre-training with acoustic tokenizers. arXiv preprint arXiv:2212.09058 , 2022.\n[5]Shaoxiang Chen, Zequn Jie, and Lin Ma. Llava-mole: Sparse mixture of lora experts for mitigating data\nconflicts in instruction finetuning mllms. arXiv preprint arXiv:2401.16160 , 2024.\n[6]Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna C Puvvada, Jason Li, Subhankar\nGhosh, Jagadeesh Balam, and Boris Ginsburg. Salm: Speech-augmented language model with in-context\nlearning for speech recognition and translation. In ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pages 13521‚Äì13525. IEEE, 2024.\n[7]Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao\nWang, Xiaoran Fan, et al. Loramoe: Alleviate world knowledge forgetting in large language models via\nmoe-style plugin. arXiv preprint arXiv:2312.09979 , 2023.\n[8]Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\nKe Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for\nmultimodal large language models, 2024.\n[9]Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783 , 2024.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In Proceedings of the IEEE international conference\non computer vision , pages 1026‚Äì1034, 2015.\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.\n[12] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions\nfor audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers) , pages 119‚Äì132, 2019.\n[13] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint\narXiv:2001.04451 , 2020.\n[14] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension, 2023.\n[15] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. Learning to answer\nquestions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 19108‚Äì19118, 2022.\n[16] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355 , 2023.\n[17] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united\nvisual representation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023.\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485 , 2023.\n11\n--- Page 12 ---\n[19] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting\nCheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International\nConference on Machine Learning , 2024.\n[20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In\nEuropean conference on computer vision , pages 216‚Äì233. Springer, 2024.\n[21] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based\non public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP) , pages 5206‚Äì5210. IEEE, 2015.\n[22] Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu. Balanced multimodal learning via\non-the-fly gradient modulation. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 8238‚Äì8247, 2022.\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning , pages 8748‚Äì8763. PMLR,\n2021.\n[24] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. In International conference on machine learning ,\npages 28492‚Äì28518. PMLR, 2023.\n[25] S Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani\nDuraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: A massive multi-task audio understanding and\nreasoning benchmark. arXiv preprint arXiv:2410.19168 , 2024.\n[26] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and\nChao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint\narXiv:2310.13289 , 2023.\n[27] Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Cheng-Zhong Xu. Hydralora: An asymmetric lora\narchitecture for efficient fine-tuning. Advances in Neural Information Processing Systems , 37:9565‚Äì9584,\n2024.\n[28] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in\nunconstrained videos. In Proceedings of the European conference on computer vision (ECCV) , pages\n247‚Äì263, 2018.\n[29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n[30] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768 , 2020.\n[31] Yake Wei, Di Hu, Henghui Du, and Ji-Rong Wen. On-the-fly modulation for balanced multimodal learning.\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 2024.\n[32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li,\nMingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang\nLuo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren,\nYang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru\nZhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025.\n[33] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,\nXipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024.\n12\n--- Page 13 ---\n[34] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv,\nZhou Zhao, Chang Zhou, et al. Air-bench: Benchmarking large audio-language models via generative\ncomprehension. arXiv preprint arXiv:2402.07729 , 2024.\n[35] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178 , 2023.\n[36] Jeong Hun Yeo, Seunghee Han, Minsu Kim, and Yong Man Ro. Where visual speech meets language: Vsp-\nllm framework for efficient and context-aware visual speech processing. arXiv preprint arXiv:2402.15151 ,\n2024.\n[37] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 ,\n2023.\nA Extension to Qwen3\nTo further validate our MokA method, we equip it with the latest Qwen3 [ 32] model. Qwen3-8B is\nused as the LLM base model. Throughout the training process, weights of LLM are kept frozen.\nResults are shown in Table 7. We conduct experiments in the audio-visual-text case. Our method\ncan still deliver improvements compared to the LoRA baseline, with the latest Qwen3 model. These\nfindings provide additional evidence of the effectiveness and scalability of MokA.\nTable 7: Evaluation results of LoRA, its variants, and our MokA on audio-visual-text datasets,\nMUSIC-A VQA and A VE. #A and #B are the number of low-rank matrices. Here N= 3refers to the\nnumber of modalities.\nMethod MUSIC-A VQA A VE #A #B\nLoRA 78.57 74.17 1 1\nMultiple LoRA 78.24 74.01 N N\nMokA 79.54 75.38 N 1\nB MokA with linear projection\nIn MokA, linear projections ( Wq,Wk, andWv) are not included in the cross-attention module:\nAtt\u0000\nAaxa, Atxt, Atxt\u0001\n=softmax\u0012(Aaxa)(Atxt)‚ä§\n‚àöNt\u0013\nAtxt, (14)\nAtt\u0000\nAvxv, Atxt, Atxt\u0001\n=softmax\u0012(Avxv)(Atxt)‚ä§\n‚àöNt\u0013\nAtxt. (15)\nThe reason is that low-rank matrices Aof each modality actually can be considered as the linear\nprojection in attention in this case. Therefore, we do not introduce other linear projections in the\ncross-attention module. In addition, projections of key and value are shared in this case ( At). In\nfact, this kind of projection sharing strategy has been widely used to increase the efficiency of\nattention [ 13,30]. For example, in the Linformer [ 30], it also utilizes the sharing key-value projection\nstrategy to reduce computation cost. What‚Äôs more, here non-text tokens are used as queries to\nmerge textual information into non-text modalities. In this way, cross-modal interaction is explicitly\nenhanced, while text tokens are kept unchanged to avoid potential disruption to the model‚Äôs original\nstrong text understanding capability.\nTo further validate the idea of cross-attention, we conduct experiments that include linear projections\nin cross-attention. The concrete attention mechanism with linear projection is conducted as follows,\n13\n--- Page 14 ---\nand the notation is consistent with the main manuscript:\nAtt\u0000\nAaxa, Atxt, Atxt\u0001\n=softmax \n(Wa\nqAaxa)(Wt\nkAtxt)‚ä§\n‚àöNt!\nWt\nvAtxt, (16)\nAtt\u0000\nAvxv, Atxt, Atxt\u0001\n=softmax \n(Wv\nqAvxv)(Wt\nkAtxt)‚ä§\n‚àöNt!\nWt\nvAtxt. (17)\nHere, Wa\nqis the linear projection of the audio query, and the others are similar.\nIn Table 8, we provide the results in the audio-visual-text and visual-text cases. Based on the results,\nMoKA with linear projection, yields improvements compared with LoRA baseline. However, it does\nnot consistently outperform the original MoKA and introduces additional trainable parameters along\nwith increased computational overhead.\nTable 8: Experiments of MokA with linear projection under the audio-visual-text and visual-text\ncases. The LLM backbone is LLaMA2.\nMethod Music-A VQA MME percep MMBench POPE SEED-Bench\nLoRA 73.41 908.52 50.64 70.28 39.71\nMultiple LoRA 72.66 882.87 49.83 68.20 38.44\nMokA w/ linear projection 73.83 926.77 53.97 72.43 41.01\nMokA 75.71 1025.86 52.74 74.23 40.45\nC Case study of cross-attention in MokA\nIn this section, we conduct a case study on the cross-attention module in MokA. This module explicitly\nintegrates task description information from text tokens with non-text tokens, thereby facilitating\ncross-modal interaction. Here we provide two samples from an audio-visual-text scenario, as shown\nin Figure 6. The visualization shows the cross-attention weights of the qproj at the 10‚àíth layer.\nThe LLM model is LLaMA2. The results indicate that during the process of explicit cross-modal\nintegration (e.g., cross-attention), text tokens that are more related to a given modality can receive\nhigher attention weights. For example, the token ‚Äúsound‚Äù receives greater attention in relation to\nthe audio modality. This cross-modal integration can better facilitate the alignment and interaction\nbetween text tokens and non-text tokens.\nD Broader impacts\nIn this paper, we aim to contribute to the efficient fine-tuning of MLLM, particularly how they well\nprocess and integrate information from different modalities. Improvements in this area may support\ndownstream applications in fields like autonomous driving and education. At the same time, this\nline of research carries certain risks. For example, there is a possibility that MLLM could reflect or\namplify biases present in the training data, or be misused in sensitive contexts. We do not directly\naddress these issues in this work, but acknowledge them as important areas for future research. All\ndatasets used are publicly available, and we follow standard filtering procedures to reduce exposure\nto harmful content.\nE Datasets\nInformation about the datasets used in our experiments is provided in this section.\nVideo-LLaV A [ 17] used a mixed dataset of images and videos for video captioning and image\ncaptioning tasks. The dataset includes a 665k image-text instruction and a 100k video-text instruction.\nThis dataset is used for pre-training the visual branch.\nAudioCaps [ 12] dataset is used for the audio captioning task. It consists of 46K pairs of audio clips\nand text descriptions. This dataset is used for pre-training the audio branch.\n14\n--- Page 15 ---\nhowmanytypesofmusicalinstrucmentssoundinthevideo?\nhowmanytypesofmusicalinstrucmentssoundinthevideo?\nWeight of visual-text attention \nWeight of audio-text attention (a)Sample 1 .\nwhereisthefirstsoundinginstrument?whereisthefirstsoundinginstrument?\nWeight of visual-text attention \nWeight of audio-text attention \n(b)Sample 2 .\nFigure 6: Cross-attention weight visualization, where deeper colors indicate higher attention weights.\nGigaSpeech-M [ 3] is a 1000h dataset for speech recognition task. This dataset is used for pre-training\nthe speech branch.\nMUSIC-A VQA [ 15] is an audio-visual-text dataset, which is introduced to support spatio-temporal\nunderstanding of musical content. It offers 45K QA pairs across 33 question templates that span\nmultiple modalities and question types.\nA VE [ 28] is an audio-visual-text dataset. It focuses on the audio-visual event localization task. This\ndataset covers 28 event classes and consists of 4,143 samples.\nLLaV A-Instruct-150K [ 18] is a set of GPT-generated multimodal instruction-following data. It is\nused for instruction fine-tuning for the visual-text case.\nLibriSpeech [ 21] is a 960-hour dataset. We use the instruction from [ 26] for instruction fine-tuning of\nthe speech-text case.\nMME percep [8] is the perception subset of the MME benchmark, covering a total of 10 subtasks for\nthe evaluation of the visual-text perception ability.\nMMBench [ 20]is a collection of benchmarks to evaluate the visual-text understanding capability. It\nhas 3,000 multiple-choice questions covering object detection, text recognition, action recognition,\nimage captioning, relation reasoning, and so on.\nPOPE [ 16] is a benchmark that is used for evaluating the visual-text understanding ability of MLLM.\nThe used image is the test set of MSCOCO dataset.\nSEED-Bench [ 14] consists of 19K multiple-choice questions with accurate human annotations for\nevaluating the visual-text understanding ability of MLLM.\nMMAU min‚àíspeech [25] is the speech subset of MMAU-mini benchmark. This benchmark is used\nfor evaluating the speech-text understanding ability of MLLM.\nAIR-Bench speech ‚àíen[34] is the English speech subset of the foundation part of AIR-Bench. This\nbenchmark is used for evaluating the speech-text understanding ability of MLLM.\n15",
  "text_length": 51406
}