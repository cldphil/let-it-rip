{
  "id": "http://arxiv.org/abs/2506.01062v1",
  "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models",
  "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.",
  "authors": [
    "Thinh Pham",
    "Nguyen Nguyen",
    "Pratibha Zunjare",
    "Weiyuan Chen",
    "Yu-Min Tseng",
    "Tu Vu"
  ],
  "published": "2025-06-01T16:04:34Z",
  "updated": "2025-06-01T16:04:34Z",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01062v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01062v1  [cs.CL]  1 Jun 2025\nSEALQA: Raising the Bar for Reasoning in\nSearch-Augmented Language Models\nThinh Pham Nguyen Nguyen Pratibha Zunjare\nWeiyuan Chen Yu-Min Tseng Tu Vu\nVirginia Tech\nBlacksburg, V A 24061\n{thinhphp,tuvu}@vt.edu\nAbstract\nWe introduce SEALQA, anew challenge benchmark for evaluating SEarch-\nAugmented Language models on fact-seeking questions where web search yields\nconflicting, noisy, or unhelpful results. SEALQAcomes in three flavors: (1)\nSEAL-0(main ) and (2) SEAL-HARD, which assess factual accuracy and reasoning\ncapabilities—with SEAL-0focusing on the most challenging questions, where chat\nmodels (e.g., GPT-4.1 ) typically achieve near-zero accuracy; and (3) LONG SEAL,\nwhich extends SEALQAto test long-context, multi-document reasoning in “needle-\nin-a-haystack” settings. Our evaluation reveals critical limitations in current mod-\nels: Even frontier LLM Sperform poorly across all SEALQAflavors. On SEAL-0,\nfrontier agentic models equipped with tools like O3and O4-MINI achieve only\n17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that\nadvanced reasoning models such as DEEPSEEK-R1-671B and O3-MINI are highly\nvulnerable to noisy search results. Notably , increasing test-time compute does\nnot yield reliable gains across O3-MINI,O4-MINI, and O3, with performance often\nplateauing or even declining early. Additionally, while recent models are less\naffected by the “lost-in-the-middle” issue, they still fail to reliably identify relevant\ndocuments in LONGSEALwhen faced with numerous distractors. To facilitate future\nwork, we release SEALQAathuggingface.co/datasets/vtllms/sealqa .\n1 Introduction\nFigure 1: Test-time scaling does not lead to reliable gains on SEALQAquestions, with performance\noften plateauing or even declining early.\nPreprint.\n--- Page 2 ---\nGPT-4o o3-mini-high o3-high o4-mini-high01020304050Accuracy (%)\n0.01.814.4\n4.515.014.232.7\n19.3\n2.713.420.7\n15.738.8\n13.848.6\n19.3\n1.9\nN/A49.7\n28.3S-0\nS-H\nH 's Ls Ex\nS QA\nB sC\nFigure 2: Accuracy of LLM Sacross benchmarks. SEALQAposes significant challenges to frontier\nmodels.\nLarge language models ( LLM S) have entered a new scaling paradigm: test-time scaling , where\nmodels dynamically allocate more compute during inference time to improve performance [Ope-\nnAI, 2024c, 2025c, Guo et al., 2025, Google, 2025, xAI, 2025, Anthropic, 2025]. This shift is\nembodied in reasoning models , which leverage reinforcement learning and other techniques to guide\ninference-time strategies such as chain-of-thought reasoning, recursive refinement, and real-time\nsearch [Muennighoff et al., 2025, Guo et al., 2025, Snell et al., 2024, Geiping et al., 2025]. These\nmodels can decompose questions into sub-queries, decide when and how to query a search engine,\nand fuse retrieved content into structured reasoning paths [OpenAI, 2025c, Google, 2025, xAI, 2025,\nAnthropic, 2025, Jin et al., 2025].\nAsLLM Scontinue to advance, existing benchmarks are failing to keep pace. Many academic QA\ndatasets—designed under assumptions of static knowledge and straightforward reasoning—are now\nsaturated; for example, frontier models now achieve over 90% accuracy on MMLU [Phan et al., 2025].\nFurthermore, most evaluations of search-augmented LLM Sfocus on short and simple factual queries\nwhere top-ranked search results directly answer the question [Vu et al., 2024, Kasai et al., 2023].\nThese setups require only shallow comprehension and fail to reflect the messy, ambiguous nature of\nreal-world search.\nTo properly evaluate today’s LLM S, benchmarks that go beyond simple fact lookup are needed.\nReal-world search often returns documents that are outdated, misleading, or superficially relevant\nbut ultimately unhelpful. Navigating this noise requires deeper reasoning: filtering inconsistencies,\nreconciling contradictions, and identifying trustworthy signals. Yet benchmarks that simulate these\nchallenges are rare, in part because they are difficult to curate and validate at scale. We introduce\nSEALQA, asmall butextremely challenging benchmark (Figure 2) for evaluating search-augmented\nLLM Son fact-seeking questions. Each question is carefully crafted by NLP researchers to trigger\nambiguous, conflicting, or noisy search results (Figure 3). This design makes it difficult to answer\nquestions through simple keyword matching or by relying on top-ranked documents. SEALQAspans\na range of question types (including time-sensitive questions) across diverse domains such as science,\ntechnology, sports, entertainment, politics, history, geography, etc.\nSEALQAquestions probe a broad spectrum of complex reasoning skills. These include distinguishing\nbetween similar entities or events, tracking changes to the same entity over time, interpreting\ninformation embedded in search-result plots, charts, or tables, counting multiple items, reasoning\nover non-English content, and debunking false premises or common misconceptions (Figure 4). All\nquestions are self-contained, verifiable, and require intensive reasoning to resolve ambiguity, filter\nmisinformation, or reconcile conflicting evidence—capabilities that are central to modern LLM S\nbut not adequately captured by existing benchmarks. Every SEALQAquestion presents a substantial\nchallenge for current frontier models.\nTo ensure both difficulty and quality, each SEALQAquestion undergoes a rigorous multi-round vetting\nprocess: an initial phase involving two or more graduate-level reviewers, followed by approval from\nexpert reviewers.\nSEALQAcomes in three flavors :\n•SEAL-0(main ; 111 questions): A carefully curated core set where even frontier models\nlike CHATGPT 4.1 with browsing consistently fail. Each question is iteratively refined until\nmultiple models fail across several attempts (0% accuracy, hence the “0” in the name).\n2\n--- Page 3 ---\nWhich company most recently surpassed a $1 trillion market capitalization for the first time in its history?\nThe most recent company to surpass a $1 trillion market capitalization for the first time in its history was Berkshire Hathaway.ﬀThis occurred on August 28, 2024. Berkshire Hathaway was the first US company outside of tech to reach this milestone.naive model: relies on direct  lexical or semantic matchingBroadcom surpassed a $1 trillion market capitalization for the first time in its history on December 13, 2024, becoming the eighth U.S. tech company to reach this milestone.strong model: e ectively retrieves and reasons beyond lexical or semantic matching\nFigure 3: SEALQArequires intensive reasoning to resolve ambiguity, filter out misinformation, or\nreconcile conflicting evidence.\n•SEAL-HARD (254 questions): A broader set that includes SEAL-0and additional difficult\nquestions that did not meet our strict failure threshold but remain highly challenging.\n•LONGSEAL(254 questions): A “needle-in-a-haystack” variant that tests long-context, multi-\ndocument reasoning. Each question is paired with a large set of retrieved documents, among\nwhich only one contains or implies the correct answer—buried within irrelevant, noisy, or\nmisleading content.\nWe intentionally kept SEALQAsmall due to the high cost and complexity of question development.1\nBuilding the full benchmark required a team of sixNLP researchers working over eight months\nthrough multiple development cycles. A smaller benchmark also reduces APIevaluation costs, allows\nmore frequent updates, and aligns with recent emphasis on high-quality, targeted evaluations over\nlarge, noisy ones [Rein et al., 2024, Maia Polo et al., 2024].2SEALQAis also designed for stable\nevaluation with lowrun-to-run variance.3\nOur key contributions are as follows: (1) We introduce SEALQA, a dynamic benchmark designed\nto evaluate reasoning under noisy, conflicting, and ambiguous search results. SEALQAincludes\nthree flavors: SEAL-0,SEAL-HARD, and LONG SEAL, each targeting different challenges in search-\naugmented reasoning; (2) We benchmark a range of LLM Sand uncover significant limitations in\ncurrent retrieval-augmented approaches. Even state-of-the-art models struggle across SEALQAflavors\nwhen faced with conflicting or misleading context. On SEAL-0, performance remains low even for\nagentic models equipped with tools. We find that advanced reasoning models can be highly vulnerable\nto noisy search results. Notably , increasing test-time compute does not reliably improve performance\nacross OPENAI’sO-series of models—performance often plateaus or declines. LONG SEALfurther\nreveals major weaknesses in long-context reasoning: while current frontier LLM Sare more robust to\n“lost-in-the-middle” effects [Liu et al., 2024], they still fail to reliably identify and prioritize relevant\nevidence amid distractors; and (3) We release SEALQApublicly and commit to regular updates, which\nprovides a stable and evolving benchmark for advancing search-augmented reasoning in LLM S.\n1Each question required over an hour on average—roughly 45 minutes to draft, plus additional time for\nreview and revision. Many initial ideas were discarded as they failed to meaningfully challenge frontier LLM S.\n2For example, the widely used GPQA-D IAMOND [Rein et al., 2024], a compact set of 198 expert-vetted questions,\ndemonstrates how a small, carefully curated dataset can effectively assess a model’s reasoning ability.\n3Our questions often lead multiple models to fail across repeated attempts.\n3\n--- Page 4 ---\nQuestionTypeFreshnessAnswerExplanationWhat is the smallest cube number which can be expressed as the sum of two diﬀerent positive cube numbers in two diﬀerent ways?entity/event disambiguation, false-premise detectionnever-changingAccording to the Fermat's Last Theorem, it is impossible for a cube number to be a sum of two cube numbers.This question is designed to trigger recall of the concept of the Ramanujan's number or the Ramanujan–Hardy number: 1729 is the smallest number that can be expressed as the sum of two cubes in two diﬀerent ways. Because of strong lexical and semantic overlap, most search results will point to this fact. As a result, \"naive\" models might incorrectly answer 1729. According to the Fermat's Last Theorem, it is impossible for a cube number to be a sum of two cube numbers.What was the most recent award did Yann LeCun, Geoﬀrey Hinton, and Yoshua Bengio win together for their work on deep neural networks?temporal trackingslow-changingThe 2025 Queen Elizabeth Prize for EngineeringThis question aims to recall the 2018 Turing Award, won by Yann LeCun, Geoﬀrey Hinton, and Yoshua Bengio. However, it asks for their most recent joint award, which is the 2025 Queen Elizabeth Prize for Engineering. Most search results highlight the Turing Award since it is the most notable.How many Fields medalists were affiliated with a French institution at the time they received the award?advanced reasoningslow-changing16This question requires identifying Fields Medalists and their institutional affiliations at the time they received the award, and determining how many were affiliated with a French institution. Based on the list of Fields Medalists from Wikipedia, the correct answer is 16.Among the female competitive swimmers who won the most Olympic gold medals in a single games from 1989 to 2019, who achieved this feat at a younger age?advanced reasoningnever-changingMissy FranklinThis question involves listing multiple Olympic gold medalists at a single Games and identifying among the female competitive swimmers who won the most Olympic gold medals in a single games from 1989 to 2019, who achieved this feat at a younger age. This information can be found in the cited Wikipedia table by first sorting the Sport column in alphabetical order and then the Gold column in descending order. Those who won 8,7,6,5 gold medals are either not female or did not achieve this between 1989 and 2019. The following are female competitive swimmers who won 4 medals in a single Games between 1989 and 2019: Katie Ledecky (born 1997, Summer Olympic 2016, so she was around 19 years old), Missy Franklin (born 1995, Summer Olympic 2012, around 17 years old), and Amy Van Dyken (born 1973, Summer Olympic 1996, around 23 years old). Therefore, the correct answer is Missy Franklin.Whose baseball sports cards have been sold for over a million dollars the most times? For the same card, only the highest sale value is counted.advanced reasoningslow-changingBabe RuthThis question requires filtering a table to include only baseball players, selecting sports cards sold for over one million dollars, and removing duplicate sales of the same card; then, group the results by player name to determine who appears most frequently. The correct answer is Babe Ruth. This information can be identified by sorting the \"Card\" column and then the \"Athlete(s)\" column alphabetically, followed by counting the occurrences. Both Babe Ruth and LeBron James had 4 cards. However, LeBron James is not a baseball player.I am a father of two undergraduate sons enrolling in Virginia Tech's Virtual Campus for Summer 2025. One is a Virginia resident taking 6 credit hours, and the other is a non-resident taking 8 credit hours. What is the total tuition I need to pay for both?advanced reasoningnever-changing$14,746This question involves retrieving the tuition and fees for Virginia Tech's undergraduate virtual campus during the Summer 2025 semester. Tuition is $548.00 per credit hour for Virginia residents and $1,432.25 per credit hour for non-Virginia residents. Therefore, the correct answer is 6*548.00 + 8*1,432.25 = $14,746.\nHow many American movies are in the top 100 highest-grossing films at the French box office?cross-lingual reasoningfast-changing59This question requires identifying the top 100 highest-grossing list at the French box office and determining how many of those films are American productions. Based on the cited Wikipedia article written in French, the correct answer is 59.What is the valency of mercury in Mercury(I) chloride?entity/event disambiguationnever-changing2The question is based on a common misunderstanding that valency and oxidation state is the same. In Hg2Cl2, the oxidation state of mercury is +1 while its valency is 2.Figure 4: SEALQAquestions test a broad range of reasoning skills that are often overlooked in existing\nbenchmarks.\n2 Data collection\nIn this section, we describe SEALQA, a challenging QAbenchmark designed to capture the com-\nplexity of real-world information-seeking. SEALQArigorously evaluates a model’s reasoning ability,\nrobustness to noisy search results, and capacity to handle dynamic, real-world knowledge.\nHuman annotators: To build SEALQA, we recruited NLP researchers4as human annotators. They\nwere shown a small, diverse set of exemplars to illustrate the types of questions we sought to collect\n(Figure 4).\nQuestion types: Our questions span several categories: ( Q1)advanced reasoning , which includes\nmulti-hop reasoning, interpreting search-result plots, charts, or tables, and performing counting or\ncalculations; ( Q2)entity/event disambiguation , focused on distinguishing between similar entities or\nevents; ( Q3)temporal tracking , which requires identifying and differentiating instances of entities\n4including the authors and their colleagues\n4\n--- Page 5 ---\nover time; ( Q4)cross-lingual reasoning , where the question is in English but answering it requires\nretrieving and reasoning over non-English sources; and ( Q5)false-premise questions , aimed at\ndebunking false premises.\nAnnotation criteria: Annotators were instructed to write questions with a single ,unambiguous\nanswer (e.g., specifying “on what date” rather than asking “when”). Each question must be supported\nby one or more webpages that justify the reference answer, ensuring that it is verifiable . For questions\nthat involve fresh knowledge , annotators were required to cite regularly updated sources to support\nfuture answer updates. All questions were designed to appear natural yettrigger ambiguous, conflict-\ning, or misleading search results when entered into a search engine such as GOOGLE . Annotators also\nprovided an explanation for each answer, including any necessary clarification or subtle reasoning.\nFinally, each question was refined iteratively until it consistently caused multiple models to fail across\nrepeated attempts.\nQuality control: We employed a rigorous multi-round review process. Each question was first\nreviewed by two or more graduate-level annotators, followed by expert approval. We conducted\nmultiple rounds of data cleaning, including verification of supporting URL S, answer correctness, and\nquestion clarity. We excluded questions whose answers change too frequently. For each question, we\nalso annotated the effective year (i.e., when the answer last changed) and an expected next review\ndate to support future maintenance.\nDiversity: SEALQAquestions vary in length, with an average of 31 tokens and a maximum of 69.\nSEALQAalso spans diverse domains: science and technology (26.8%), sports (22.0%), entertainment\n(21.7%), politics (9.1%), history and geography (8.3%), and others (12.2%).5By question category,\n72.4% involve advanced reasoning ( Q1), 58.3% entity/event disambiguation ( Q2), 13.7% temporal\ntracking ( Q3), 5.5% cross-lingual reasoning ( Q4), and 4.3% false-premise detection ( Q5). We also\nclassify questions by freshness [Vu et al., 2024]: 31.1% are never-changing (NEVER ; answers never\nchange), 43.7% slow-changing (SLOW ; answers change over several years), and 25.2% fast-changing\n(FAST; answers typically change within a year). By effective year, 22.0% reference 2025 events, 19.3%\n2024, and 58.7% prior to 2024. Finally, we manually labeled each question based on whether the\nsearch results at evaluation time contain conflicting answers, including the correct one ( CONFLICT .),\nor are entirely unhelpful ( UNHELPFUL ).\nDatasets: To curate SEAL-0, we tested each question against GPT-4 O,GPT-4.1 , their MINI vari-\nants [OpenAI, 2024a,b, 2025a], and LLAMA -4-S COUT [Meta, 2025], each with and without browsing.6\nOnly questions where all models failed across 10-15 attempts were retained. This follows cur-\nrent practices for building challenging benchmarks; for example, SIMPLE QA[Wei et al., 2024]\nwas also adversarially collected against GPT-4 responses. SEAL-0was then combined with other\nrejected-but-difficult questions to form SEAL-HARD.\nForLONG SEAL, each SEAL-HARD question is paired with a set of retrieved documents: onehelpful\n(gold) document from annotator-provided webpages, and up to 50 hard negatives that appear relevant\nbut are unhelpful.7To ensure difficulty, we used CHATGPT 4 Oto filter out any negatives that might\nallow the correct answer to be inferred. The gold document was then randomly inserted among\nthe negatives. LONG SEALincludes over 7.6K documents and serves as a testbed for long-context\nreasoning under noisy retrieval conditions.\nEvaluation protocol: Models are evaluated with the CHATGPT 4 Oauto-rater from Wei et al. [2024],\nwhich takes the question, predicted answer, and reference answer as input and labels responses as\n“correct”, “incorrect”, or “not attempted”. The evaluation uses a relaxed protocol that judges only\nwhether the main answer is correct.\n5Topic labels were assigned post-hoc using CHATGPT 4 O.\n6We applied FRESH PROMPT toLLAMA -4-S COUT.\n7To collect hard negatives, we used GOOGLE to retrieve the top 10 webpages per question and extracted their\nmain content using TRAFILATURA [Barbaresi, 2021]. To add temporal diversity and potential conflicts, we retrieved\n10 more pages limited to pre-2023 content. We also used CHATGPT 4 Oto generate three semantically related\nqueries per question and collected documents for each. Duplicates were removed, and documents over 10K\ntokens were excluded.\n5\n--- Page 6 ---\nTable 1: Accuracy on SEAL-0and SEAL-HARD. Frontier LLM Sface significant challenges on SEALQA\nquestions.†indicates results using CHATGPT’s built-in search; all other search-based results use\nFRESH PROMPT [Vu et al., 2024].\nModelknowl.\ncutofftypeSEAL-0 S EAL-HARD\nw/o SEARCH w/SEARCH w/o SEARCH w/SEARCH\nClosed-source models\nGPT-4 O-MINI Sep 30, 2023 CHAT 0.0 0.0†9.1 13.4†\nGPT-4.1- MINI May 31, 2024 CHAT 0.0 0.0†13.8 11.8†\nGPT-4 O Sep 30, 2023 CHAT 0.0 0.0†11.8 15.0†\nGPT-4.1 May 31, 2024 CHAT 0.0 0.0†15.0 20.5†\nO3-MINI -MEDIUM Sep 30, 2023 REASON . 2.7 2.7 14.6 12.2\nO4-MINI -MEDIUM May 31, 2024 AGENTIC - 5.4†- 19.7†\nO3-MEDIUM May 31, 2024 AGENTIC - 17.1†- 34.6†\nOpen-weight models\nLLAMA -3.2-3B December 1, 2023 CHAT 0.0 0.0 1.6 3.5\nLLAMA -3.1-70B December 2023 CHAT 0.0 0.0 0.0 6.3\nLLAMA -4-S COUT -17B-16E (109B) August 2024 CHAT 0.0 0.0 5.9 5.9\nQWEN 3-235B-A22B - REASON . 0.0 5.4 4.3 11.4\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B - REASON . 0.0 2.7 0.0 1.6\nDEEPSEEK-R1-D ISTILL -QWEN -14B - REASON . 0.9 3.6 0.9 10.6\nDEEPSEEK-R1-671B - REASON . 5.4 1.8 22.4 11.0\nDEEPSEEK-R1-0528-671B - REASON . 5.4 4.5 19.3 15.4\nAuto-rater reliability: To assess the auto-rater’s reliability, two authors independently evaluated\n100 answers. Disagreements were resolved through discussion, yielding a unified set of human ratings\nthat agreed with the auto-rater 98% of the time.\n3 Experiments\nHaving established SEALQA, we now set out to evaluate how well today’s LLM Sreason over noisy\nsearch results when navigating dynamic, real-world knowledge. Our analysis reveals limitations in\ntheir ability to reconcile conflicting parametric ( internal ) and retrieved ( external ) knowledge.\n3.1 Experiment setup\n3.1.1 SEAL-0and SEAL-HARD\nBaselines: We benchmark a wide range of open-weight and proprietary models, including chat-\noriented models such as GPT-4 O,GPT-4.1 , their MINI variants [OpenAI, 2024a,b, 2025a], LLAMA -\n3.1-70B [Grattafiori et al., 2024], LLAMA -3.2-3B [Meta, 2024], and LLAMA -4-S COUT -17B-16E-\nINSTRUCT [Meta, 2025]; advanced reasoning models like O3-MINI [OpenAI, 2025b], DEEPSEEK-R1-\n671B ,DEEPSEEK-R1-0528-671B ,DEEPSEEK-R1-D ISTILL -QWEN -14B/1.5B [Guo et al., 2025], and\nQWEN 3-235B-A22B [Yang et al., 2025]; and agentic tool-use models such as O3and O4-MINI [OpenAI,\n2025c].8\nWe simply feed each question as a prompt into each model, using a temperature of 0 when con-\nfigurable and the default value otherwise.9For models without browsing capabilities, we apply\nFRESH PROMPT [Vu et al., 2024] or SELF -ASK[Press et al., 2023] to inject GOOGLE search results into\nthe prompt. Advanced reasoning models are evaluated under medium reasoning effort settings when\nconfigurable, unless specified otherwise.\nHuman experts: To estimate human performance, we asked three graduate-level NLP researchers\n(not involved in annotation) to independently answer a sample of 50 SEAL-HARDquestions. They had\nunlimited access to GOOGLE and could use any queries they deemed useful.10\n8We used the OPENAIand TOGETHER .AI API SforOPENAIand open-weight models, respectively.\n9OPENAI’sO-series models only support a fixed temperature of 1.0.\n10Each question had a 15-minute time limit.\n6\n--- Page 7 ---\n†indicates results using CHATGPT’s built-in search; all other search-based results use FRESH -\nPROMPT [Vu et al., 2024].\nTable 2: On SEAL-HARD,LLM Stend to underperform on cross-lingual reasoning ( Q4) and false-\npremise detection ( Q5) compared to advanced reasoning ( Q1), entity/event disambiguation ( Q2), and\ntemporal tracking ( Q3).\nModel Q1Q2Q3Q4Q5W/O SEARCHGPT-4.1 14.1 14.2 25.7 0.0 0.0\nO3-MINI 13.0 17.6 17.1 0.0 0.0\nO3 – – – – –\nLLAMA -4-S COUT 4.9 6.8 5.7 0.0 0.0\nDEEPSEEK-R1 20.7 23.0 22.9 7.1 0.0W/SEARCHGPT-4.1 20.1†17.6†25.7†21.4†9.1†\nO3-MINI 12.0 9.5 20.0 0.0 9.1\nO3 33.7†32.4†48.6†7.1†27.3†\nLLAMA -4-S COUT 4.3 6.8 8.6 0.0 0.0\nDEEPSEEK-R1 10.3 10.8 14.3 0.0 18.2\nTable 3: Questions that involve rapidly changing information, i.e., fast-changing questions, pose\nsignificant challenges for LLM SonSEAL-HARD.\nModelW/O SEARCH W /SEARCH\nNEVER SLOW FAST NEVER SLOW FAST\nGPT-4.1 21.5 18.0 1.6 17.7†24.3†17.2†\nO3-MINI 40.5 39.6 17.2 16.5 10.8 9.4\nO3 – – – 41.8†39.6†17.2†\nLLAMA -4-S COUT 10.1 4.5 4.1 6.3 4.5 7.8\nDEEPSEEK-R1 32.9 24.3 6.2 15.2 9.9 7.8\nTable 4: LLM Sstruggle with questions that involve recent information on SEAL-HARD.\nModelW/O SEARCH W /SEARCH\n<2024 2024 2025 <2024 2024 2025\nGPT-4.1 23.5 6.1 0.0 25.5†20.4†7.1†\nO3-MINI 45.6 26.5 10.7 15.4 6.1 8.9\nO3 – – – 46.3†26.5†10.7†\nLLAMA -4-S COUT 8.7 4.1 0.0 7.4 6.1 1.8\nDEEPSEEK-R1 35.6 8.2 0.0 14.8 6.1 5.4\nTable 5: On SEAL-HARD, performance degrades more when search results are uniformly unhelpful\nthan when they contain conflicting answers.\nModelW/O SEARCH W /SEARCH\nUNHELPFUL CONFLICT . UNHELPFUL CONFLICT .\nGPT-4.1 14.5 15.3 18.2†22.2†\nO3-MINI 11.8 16.6 10.9 13.2\nO3 – – 33.6†35.4†\nLLAMA -4-S COUT 3.6 7.6 4.5 6.9\nDEEPSEEK-R1 20.9 23.6 9.1 12.5\n3.1.2 LONG SEAL\nBaselines: We benchmark GPT-4 O-MINI,GPT-4.1- MINI,LLAMA -4-S COUT -17B-16E-I NSTRUCT , and\nadditionally LLAMA -3.2-11B-V ISION [Meta, 2024], with context windows of 128K, 1M, 1M, and\n128K tokens, respectively.\n7\n--- Page 8 ---\nGPT-4.1 L -4-S\n 3-\n DS-R1\n0510152025Accuracy (%)15.0\n5.914.622.4\n14.6\n5.912.2\n11.019.3\n7.113.818.9S-H\n/ s\n/ FsP\n/ -\nFigure 5: Advanced reasoning models such as DEEPSEEK-R1-671B and O3-MINI are highly vulnerable\nto noisy search results.\nWe follow Liu et al. [2024] to set up a multi-document QAtask, where a model receives a question\nand a set of documents: one gold document that suggests the correct answer, and khard negatives.\nThe gold document is randomly placed among the knegatives. To answer correctly, the model must\nidentify and use the gold document from its input context. We evaluate three values of k: 12, 20, and\n30, sampling from 50 hard negatives per question. This setup allows us to assess how performance\nvaries with the number of negatives and the position of the gold document.11\n3.2 Results on SEAL-0and SEAL-HARD\nSEAL-0and SEAL-HARD present significant challenges for frontier LLM S:Table 1 shows the\naccuracy of various LLM SonSEAL-0andSEAL-HARDwithout access to a search engine ( W/O SEARCH ).\nModels perform poorly without web access, with accuracies ranging from 0.0% to 5.4% on SEAL-0\nand 0.0% to 22.4% on SEAL-HARD.\nWhile proprietary models tend to outperform open-weight ones, DEEPSEEK-R1-671B stands out\nas a notable exception, achieving the best overall performance. Interestingly, model size does not\nconsistently correlate with performance. For example, both LLAMA -3.2-3B and LLAMA -3.1-70B\nscore 0.0% on SEAL-0, with the smaller model slightly outperforming the larger one on SEAL-HARD\n(1.6% vs. 0.0%). A similar pattern holds for DEEPSEEK-R1-D ISTILL -QWEN, which shows negligible\nimprovement when scaled from 1.5B to 14B (0.0% →0.9%) on both datasets. Large mixture-of-\nexpert (MOE) models such as LLAMA -4-S COUT -17B-16E (109B total parameters) and QWEN 3-235B-\nA22B also fail to generalize on SEAL-0(0.0%) and yield only modest gains on SEAL-HARD (5.9%\nand 4.3%, respectively). Additionally, reasoning-focused models do not consistently outperform\ngeneral-purpose chat models, as seen with QWEN 3-235B-A22B and LLAMA -4-S COUT -17B-16E —with\nDEEPSEEK-R1-671B as the exception.\nTables 2, 3, 4, and 5 show a breakdown of SEAL-HARDresults by question category (see Appendix A)\nfor full results). Overall, models perform poorly across question categories, especially on cross-\nlingual reasoning, false-premise detection, and questions that involve recent or rapidly changing\ninformation. Performance also degrades more when search results are uniformly unhelpful than when\nthey contain conflicting answers.12\nNaive search and integration can amplify noise rather than improve accuracy: Table 1 ( W/O\nSEARCH ) and Figure 5 show the effects of web search on model performance. In general, search\nimproves accuracy across models. O3and O4-MINI—agentic reasoning models capable of using tools\nwithin CHATGPT, including web search—perform significantly better than others. O3achieves the\nhighest accuracy on both datasets: 16.2% on SEAL-0and 34.3% on SEAL-HARD.\n11The average prompt lengths across all examples are 27.6K, 54.5K, and 70.1K tokens, with 100%, 99.2%,\nand 96.7% of prompts fitting within the 128K context window of GPT-4 O-MINIand LLAMA-3.2-11B , fork=12,\n20, and 30, respectively.\n12Additionally, we find that open-weight models like LLAMA -4-S COUT and DEEPSEEK-R1choose to “not attempt”\nquestions more often than proprietary models such as GPT-4.1 ,O4-MINI, and O3(see Appendix B).\n8\n--- Page 9 ---\nTable 6: SEALQAalso poses challenges for humans.\nModel accuracy\nGPT-4 O 6.0\nGPT-4.1 6.0\nO3-MINI -HIGH 8.0\nO4-MINI -HIGH 12.0\nO3-HIGH 28.0\nhuman (avg.) 23.3\nhuman (best) 30.0\nOur results suggest that training models to understand and execute search queries, as done in\nCHATGPT’s built-in search, is more effective than retrieval-based prompting methods like FRESH -\nPROMPT . While CHATGPT-4.1 gains a performance boost from built-in search (+5.5%), FRESH PROMPT\nslightly reduces its accuracy (15.0% →14.6%). Built-in search generally improves performance on\nSEAL-HARD for both GPT-4.0 and GPT-4.1 . With that said, FRESH PROMPT remains useful for most\nopen-weight models without tool-use training. For example, QWEN 3-235B-A22B and DEEPSEEK-\nR1-D ISTILL -QWEN -14B achieve gains of +7.1% and +9.7%, respectively, on SEAL-HARDwhen using\nFRESH PROMPT .\nHowever, search can sometimes be detrimental. GPT-4.1- MINI, when equipped with built-in search,\ndrops in accuracy from 13.8% to 11.8%. Since SEALQAquestions are designed to elicit conflicting or\nnoisy search results, naive retrieval and integration can harm model accuracy.\nAdvanced reasoning models can be highly vulnerable to noisy search results: As shown in\nTable 1 ( W/O SEARCH ) and Figure 5, DEEPSEEK-R1-671B and O3-MINI are dramatically more sensitive\nto input noise than other models. For example, DEEPSEEK-R1-671B ’s performance drops from 22.4%\nto 11.0% when using FRESH PROMPT . Our ablation (Table 3 and Table 4) reveals that FRESH PROMPT\nimproves DEEPSEEK-R1-671B ’s performance on fast-changing (+1.6%) and 2025-specific (+5.4%)\nquestions, but leads to large drops on static or older questions (-17.7% on never-changing, and\n-20.8% on pre-2024). GPT-4.1- MINI shows a similar trend with CHATGPT’s built-in search, though the\ndecline is less pronounced. In contrast, open-weight models with weaker reasoning capabilities (e.g.,\nQWEN 3-235B-A22B andDEEPSEEK-R1-D ISTILL -QWEN -14B ) consistently benefit from FRESH PROMPT .\nAmong retrieval-based prompting methods, SELF -ASK, which decomposes questions into sub-\nquestions, is generally more effective than FRESH PROMPT , which issues direct searches and thus\ntriggers more noise for SEALQA’s adversarial questions. However, both methods harm the accuracy\nofDEEPSEEK-R1-671B and O3-MINI.\nTest-time scaling does not lead to reliable gains on SEALQA:Models like O3-MINI,O3, and\nO4-MINI have shown strong reasoning capabilities, with consistent improvements from increased\ntest-time compute. However, we find that this approach does not yield reliable gains on SEALQA\nquestions.\nFigure 1 illustrates test-time scaling effects on SEAL-0and SEAL-HARD questions across different\nreasoning effort settings— low,medium , and high—where higher levels correspond to more reasoning\ntokens. On SEAL-0,O3-MINI’s accuracy plateaus despite scaling, with scores of 1.8, 2.7, and 1.8 at\nlow, medium, and high effort levels, respectively. O4-MINI’s accuracy peaks at low effort (6.3%), but\ndrops with more compute at medium (5.4%) and high (4.5%) settings. While O3achieves the highest\noverall accuracy, scaling also fails to provide reliable gains on SEAL-0, with accuracies of 11.7%,\n17.1%, and 14.4% across the three effort levels. Similar trends are observed on SEAL-HARD.\nWe conjecture that increased reasoning over noisy search results may impair performance. As\ntest-time compute grows, longer chains of thought can amplify spurious or irrelevant information,\nentangling the model in misleading evidence and ultimately reducing accuracy.\nThe effect of repeated sampling: We also explore the effect of repeated sampling [Brown et al.,\n2024]. Each model is sampled fivetimes, and we count an answer as correct if any of the five attempts\nis correct. Due to O3’s high APIcost, this experiment is limited to O3-MINI and O4-MINI, evaluated on\nSEAL-0at medium reasoning effort. Under this setting, O3-MINI and O4-MINI reach 9% and 16.2%\n9\n--- Page 10 ---\nFigure 6: Frontier LLM Sfail to reliably identify relevant documents in LONG SEALwhen numerous\ndistractors are present, despite being less prone to “lost-in-the-middle” failures [Liu et al., 2024].\naccuracy, respectively. These results again demonstrate that SEAL-0is extremely challenging, even\nfor agentic reasoning models with full tool access like O4-MINI.\nSEALQAalso poses challenges for humans: Table 6 reports performance on a sample of 50\nSEAL-HARD questions. Frontier LLM Sgenerally lag behind human performance: the best model,\nO3-HIGH , reaches 28.0% accuracy, slightly below the top human score of 30.0%, and is the only\nmodel to exceed the average human accuracy of 23.3%. On average, human participants skipped only\n11.3% of questions, and in 64.7% of cases reported finding an answer within five minutes. Despite\nthis, overall accuracy remained low.\n3.3 Results on LONG SEAL\nWe now switch gears to discuss our evaluation results on LONG SEAL(Figure 6).\nFrontier LLM Sstruggle on LONG SEALwith increased distractors: All models exhibit a clear\ndrop in accuracy as the number of hard negatives increases. For example, when the gold document\nappears immediately after the question (i.e., at the 1stposition), GPT-4.1- MINI’s accuracy decreases\nfrom 32.7% at k= 12 (i.e., 12 hard negatives) to 29.9% at k= 20 and 29.5% at k= 30 . The\ndegradation is more pronounced in smaller or less capable models: GPT-4 O-MINI falls from 24.0% to\n6.3% and then 3.9%, while LLAMA-3.2-11B drops from 10.2% to 2.0% and 2.4%.\nThese results suggest that simply increasing context size does not guarantee effective context use.\nWhen many hard negatives are present, models often struggle to identify and prioritize the gold\ndocument. The primary failure mode appears to be the inability to reliably filter relevant from\n10\n--- Page 11 ---\nirrelevant content at scale. High distractor density impairs relevance estimation, even when all\ninput documents fit within the context window. This suggests a need for architectural advances or\ntraining strategies that enhance implicit retrieval and salience detection to improve performance in\nlarge-context, multi-document QAsettings.\nAbsence of classic positional bias in Liu et al. [2024]: Unlike earlier work that reports a strong\n“lost in the middle” effect, our results show no clear U-shaped positional trend. GPT-4.1- MINI maintains\nstable accuracy across positions, with only minor fluctuations from start to end; even at k= 30 , its\nperformance varies little between early, middle, and late placements. LLAMA-4-S COUT shows a\nslight improvement toward later positions, but no consistent dip in the middle.\nThis absence of positional bias suggests that newer models may have mitigated some of the structural\nweaknesses previously associated with position encoding. However, the broader challenge remains:\nregardless of position, models often fail to recognize the gold document when distractors are numerous.\nThe issue has shifted from sensitivity to position to a more general difficulty in modeling relevance\nwithin large, noisy contexts.\n3.4 Qualitative analysis\nTwo authors independently evaluated 100 responses from six models: GPT-4.1 (without search, with\nFRESH PROMPT , and with built-in search); O3-MINI,O3(both under a medium reasoning effort); and\nDEEPSEEK-R1-671B . Our analysis reveals clear differences across models in their reasoning and use\nof external knowledge. Among the GPT-4.1 variants, the base model without search occasionally\nincludes relevant URL Sbut often produces inaccurate answers due to outdated knowledge. The\nFRESH PROMPT version is better at detecting false-premise questions and tends to be more concise,\nthough its accuracy depends heavily on retrieval quality. The built-in search variant produces more\nlogically coherent answers and higher-quality citations, which supports factual verification, though it\nstill exhibits occasional errors. We find that O3is capable of producing more informed and concise\nresponses; however, it sometimes overthinks and mistakenly rejects valid answers. O3-MINI’s outputs\nare easy to follow, yet the model occasionally misses relevant reasoning paths. Notably, both models\ngenerally acknowledge their knowledge cutoffs for time-sensitive queries, seek clarification, and\nsuggest alternative strategies to support user decision-making. Finally, DEEPSEEK-R1-671B tends\nto overthink and frequently repeats phrases like “wait”, “let me think”, and “alternatively” without\narriving at a clear conclusion. Its lack of structured formatting also makes its responses harder to\nfollow compared to GPT-4.1 and O3models.\n4 Related work\nReasoning under knowledge conflict: Prior work shows that LLM Scan be vulnerable to misinfor-\nmation [Pan et al., 2023], irrelevant context [Shi et al., 2023], and conflicting sources [Kazemi et al.,\n2023]. Retrieval quality strongly influences model output; however, contradictions between sources\noften have only a minimal effect on model confidence [Chen et al., 2022]. Wan et al. [2024] find that\nmodels prioritize surface-level relevance over credibility indicators such as scientific references or\nneutral tone. While LLM Scan detect conflict [Jiayang et al., 2024], they struggle to resolve it [Wang\net al., 2024, Xu et al., 2024a]. Models also exhibit confirmation bias by favoring evidence that aligns\nwith their parametric memory [Chen et al., 2022], often resolving contradictions in favor of internal\nknowledge [Jin et al., 2024, Jiayang et al., 2024]. Still, Xie et al. [2024b] show that models remain\nhighly receptive to contradictory external evidence when it is coherent and convincing. Additional\nbiases include favoring frequent evidence and relying on memory for common knowledge but external\nsources for long-tail knowledge [Jin et al., 2024]. See Xu et al. [2024b] for a comprehensive survey.\nBuilding on these insights, recent work has introduced benchmarks targeting specific types of re-\ntrieval conflicts. Some focus on specific challenges, such as entity ambiguity [ AMBIG DOCS;Lee et al.,\n2024], credible yet conflicting sources [ WIKICONTRADICT ; Hou et al., 2024], debatable questions\n[DEBATE QA; Xu et al., 2024a], and Shaier et al. [2024] for citation-aware QAunder ambiguity.\nOther assess model behavior under noisy contexts, such as faithfulness under unanswerable, incon-\nsistent, and counterfactual contexts [ FAITH EVAL; Ming et al., 2025], or reasoning over conflicting\ncontexts [ QACC ; Liu et al., 2025], as well as analyzing what shapes predictions, such as textual\nfeatures [ CONFLICTING QA; Wan et al., 2024] and conflict sources [ CONFLICT BANK; Su et al., 2024].\nMost recently, Wang et al. [2025] augment AMBIG DOCSexamples with simulated ambiguity, misinfor-\n11\n--- Page 12 ---\nmation, and noise to create RAMD OCS. Our work complements this growing body by introducing a\nunified benchmark that brings together real-world challenges—ambiguity, misinformation, temporal\ndrift, and noisy retrieval—through expert-curated, naturally occurring questions, without relying on\nsynthetic augmentation.\nMeasuring factuality and reasoning in LLM S:SEALQAaligns with a growing body of work on\ntime-sensitive QAbenchmarks [Chen et al., 2021, Zhang and Choi, 2021, Liska et al., 2022, Kasai\net al., 2023, Vu et al., 2024, inter alia ].\nSEALQAalso fits among recent challenging benchmarks that evaluate LLM Sacross factuality, rea-\nsoning, and retrieval. Benchmarks like MMLU [Hendrycks et al., 2021a], MATH [Hendrycks et al.,\n2021b], GPQA [Rein et al., 2024], and HUMANITY ’SLASTEXAM [Phan et al., 2025] focus on academic\nor expert-level reasoning. Others evaluate open-domain retrieval [ FRESH STACK ; Thakur et al., 2025],\nmulti-hop, multi-document reasoning [ FRAMES ; Krishna et al., 2025], and real-world software engi-\nneering tasks [ SWE- BENCH ; Xie et al., 2024a]. Targeted evaluations such as SIMPLE QA[Wei et al.,\n2024] and BROWSE COMP [Wei et al., 2025] measure factual recall and web browsing competence.\nThese datasets push different axes of model performance, and SEALQAcomplements them by provid-\ning a unified benchmark spanning all three dimensions—factuality, reasoning, and retrieval—through\nnaturally occurring, adversarially curated questions that reflect real-world QAcomplexity.\n5 Conclusion\nWe introduce SEALQA, a benchmark for evaluating Search-Augmented Language Models on chal-\nlenging factual questions where web search results may be conflicting, noisy, or irrelevant. SEALQA\nincludes three flavors: SEAL-0, which includes questions that challenge today’s most capable mod-\nels; SEAL-HARD, a wider collection of difficult queries; and LONG SEAL, which is designed to test\nlong-context reasoning in “needle-in-a-haystack” settings. Our evaluations show that frontier LLM S,\nincluding agentic models with search tools, perform poorly and are vulnerable to noisy search results,\nwith increased test-time compute often not leading to reliable performance gains. LONG SEALin\nparticular highlights the difficulty models face in identifying relevant information amid distractors,\nthough they exhibit reduced susceptibility to the “lost-in-the-middle” issue. We hope that SEALQA\nwill spur more fundamental research into tackling real-world challenges in retrieval-augmented\nreasoning.\n6 Limitations\nSEALQAhas several limitations that suggest directions for future work. First, most SEALQAquestions\nare constructed to have a single short answer, which does not fully capture the complexity of real-\nworld information-seeking tasks. In practice, users may pose questions with no definitive answer,\nmultiple plausible answers, or ones that require long-form responses. Addressing such cases would\nrequire LLM Snot only to generate nuanced answers but also to cite credible sources and be evaluated\nbased on source quality. Second, while some questions involve retrieving and reasoning over non-\nEnglish sources, it is not designed to systematically assess multilingual capabilities or cultural and\nlinguistic generalization, an important area for future research. Finally, although SEALQAis updated\nperiodically to maintain relevance, some answers may still become stale between releases due to the\nrapidly evolving nature of web content. Users should therefore be aware that the benchmark might\nnot always reflect the most recent developments in real time.\n7 Ethical considerations and risks\nDespite a rigorous multi-round vetting process, some concerns remain in SEALQA. First, manual\ncuration may inadvertently introduce cultural, regional, or domain-specific biases from annotators.\nWhile we aim for neutrality and diversity, such biases are difficult to fully eliminate. Second, as\nSEALQAincludes questions based on dynamic or evolving knowledge, some reference answers may\nbecome outdated. To address this, we provide verifiable sources for each question and commit\nto regularly updating answers to ensure accuracy. Finally, the paired real-world search data in\nLONG SEALmay include copyrighted, sensitive, or restricted content. We urge researchers to follow\nethical guidelines and respect intellectual property rights when publishing or sharing results.\n12\n--- Page 13 ---\n8 Acknowledgments\nWe thank Quyet Do, Amartya Dutta, and Rishab Balasubramanian for their contributions in creating\nand reviewing SEALQAquestions. We are also grateful to Yeana Bond, Pin-Jie Lin, Rishab Balasub-\nramanian, and Rituraj Sharma for their involvement in our human performance evaluation. We thank\ntheVT LLM Sgroup for valuable discussions and feedback. Finally, we thank Arie Cattan, Mohit\nIyyer, Marzena Karpinska for their helpful suggestions.\n13\n--- Page 14 ---\nReferences\nAnthropic. Introducing Claude 4. 2025. URL https://www.anthropic.com/news/claude-4 .\nAdrien Barbaresi. Trafilatura: A web scraping library and command-line tool for text discovery\nand extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing: System\nDemonstrations , pages 122–131, 2021. URL https://aclanthology.org/2021.acl-demo.\n15/.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and\nAzalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling.\narXiv preprint arXiv:2407.21787 , 2024. URL https://arxiv.org/abs/2407.21787 .\nHung-Ting Chen, Michael Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowl-\nedge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing , pages 2292–2307, 2022. URL\nhttps://aclanthology.org/2022.emnlp-main.146/ .\nWenhu Chen, Xinyi Wang, William Yang Wang, and William Yang Wang. A\ndataset for answering time-sensitive questions. In Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks , volume 1, 2021.\nURL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/\n2021/file/1f0e3dad99908345f7439f8ffabdffc4-Paper-round2.pdf .\nJonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson,\nBhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with\nlatent reasoning: A recurrent depth approach. arXiv preprint arXiv:2502.05171 , 2025. URL\nhttps://arxiv.org/abs/2502.05171 .\nGoogle. Gemini 2.5: Our most intelligent models are getting even better. 2025. URL https:\n//blog.google/technology/google-deepmind/google-gemini-updates-io-2025/ .\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783 , 2024. URL https://arxiv.org/abs/2407.21783 .\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. URL https://arxiv.org/\nabs/2501.12948 .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations , 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceed-\nings of the Neural Information Processing Systems Track on Datasets and Benchmarks , volume 1,\n2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/\npaper/2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf .\nYufang Hou, Alessandra Pascale, Javier Carnerero-Cano, Tigran Tchrakian, Radu Mari-\nnescu, Elizabeth Daly, Inkit Padhi, and Prasanna Sattigeri. WikiContradict: A bench-\nmark for evaluating llms on real-world knowledge conflicts from wikipedia. In Ad-\nvances in Neural Information Processing Systems , volume 37, pages 109701–109747,\n2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\nc63819755591ea972f8570beffca6b1b-Paper-Datasets_and_Benchmarks_Track.pdf .\nCheng Jiayang, Chunkit Chan, Qianqian Zhuang, Lin Qiu, Tianhang Zhang, Tengxiao Liu, Yangqiu\nSong, Yue Zhang, Pengfei Liu, and Zheng Zhang. ECON: On the detection and resolution of evi-\ndence conflicts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing , pages 7816–7844, 2024. URL https://aclanthology.org/2024.emnlp-main.\n447/ .\n14\n--- Page 15 ---\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-R1: Training llms to reason and leverage search engines with reinforcement\nlearning. arXiv preprint arXiv:2503.09516 , 2025. URL https://arxiv.org/abs/2503.09516 .\nZhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Li Qiuxia, and Jun\nZhao. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-\naugmented language models. In Proceedings of the 2024 Joint International Conference on\nComputational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages\n16867–16878, 2024. URL https://aclanthology.org/2024.lrec-main.1466/ .\nJungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What 's the answer right\nnow? In Advances in Neural Information Processing Systems , volume 36, pages 49025–\n49043, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n9941624ef7f867a502732b5154d30cb7-Paper-Datasets_and_Benchmarks.pdf .\nMehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak\nRamachandran. BoardgameQA: A dataset for natural language reasoning with contradictory\ninformation. In Advances in Neural Information Processing Systems , volume 36, pages 39052–\n39074, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n7adce80e86aa841490e6307109094de5-Paper-Datasets_and_Benchmarks.pdf .\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam\nUpadhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-\naugmented generation. In Proceedings of the 2025 Conference of the Nations of the Ameri-\ncas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers) , pages 4745–4759, 2025. URL https://aclanthology.org/2025.\nnaacl-long.243/ .\nYoonsang Lee, Xi Ye, and Eunsol Choi. Ambigdocs: Reasoning across documents on different\nentities under the same name. In First Conference on Language Modeling , 2024. URL https:\n//openreview.net/forum?id=mkYCfO822n .\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,\nCyprien De Masson D’Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-\nMcmahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. StreamingQA: A benchmark for\nadaptation to new knowledge over time in question answering models. In Proceedings of the 39th\nInternational Conference on Machine Learning , volume 162 of Proceedings of Machine Learning\nResearch , pages 13604–13622. PMLR, 2022. URL https://proceedings.mlr.press/v162/\nliska22a.html .\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts. Transactions of the\nAssociation for Computational Linguistics , 12:157–173, 2024. URL https://aclanthology.\norg/2024.tacl-1.9/ .\nSiyi Liu, Qiang Ning, Kishaloy Halder, Zheng Qi, Wei Xiao, Phu Mon Htut, Yi Zhang, Neha\nAnna John, Bonan Min, Yassine Benajiba, and Dan Roth. Open domain question answering with\nconflicting contexts. In Findings of the Association for Computational Linguistics: NAACL 2025 ,\npages 1838–1854, 2025. URL https://aclanthology.org/2025.findings-naacl.99/ .\nFelipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin.\ntinyBenchmarks: evaluating LLMs with fewer examples. In Proceedings of the 41st International\nConference on Machine Learning , volume 235 of Proceedings of Machine Learning Research ,\npages 34303–34326, 2024. URL https://proceedings.mlr.press/v235/maia-polo24a.\nhtml .\nMeta. Llama 3.2: Revolutionizing edge ai and vision with open,\ncustomizable models. 2024. URL https://ai.meta.com/blog/\nllama-3-2-connect-2024-vision-edge-mobile-devices/ .\nMeta. The Llama 4 herd: The beginning of a new era of natively multimodal ai innovation. 2025.\nURL https://ai.meta.com/blog/llama-4-multimodal-intelligence/ .\n15\n--- Page 16 ---\nYifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong,\nand Shafiq Joty. FaithEval: Can your language model stay faithful to context, even if “the moon is\nmade of marshmallows”. In The Thirteenth International Conference on Learning Representations ,\n2025. URL https://openreview.net/forum?id=UeVx6L59fg .\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time\nscaling. arXiv preprint arXiv:2501.19393 , 2025. URL https://arxiv.org/abs/2501.19393 .\nOpenAI. GPT-4o system card. 2024a. URL https://openai.com/index/\ngpt-4o-system-card/ .\nOpenAI. GPT-4o mini: advancing cost-efficient intelligence. 2024b. URL https://openai.com/\nindex/gpt-4o-mini-advancing-cost-efficient-intelligence/ .\nOpenAI. OpenAI o1 system card. 2024c. URL https://openai.com/index/\nopenai-o1-system-card/ .\nOpenAI. Introducing GPT-4.1 in the API. 2025a. URL https://openai.com/index/gpt-4-1/ .\nOpenAI. OpenAI o3-mini system card. 2025b. URL https://openai.com/index/\no3-mini-system-card/ .\nOpenAI. OpenAI o3 and o4-mini system card. 2025c. URL https://openai.com/index/\no3-o4-mini-system-card/ .\nLiangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. Attacking open-domain\nquestion answering by injecting misinformation. In Proceedings of the 13th International Joint\nConference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pages 525–539, 2023.\nURL https://aclanthology.org/2023.ijcnlp-main.35/ .\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint\narXiv:2501.14249 , 2025. URL https://arxiv.org/abs/2501.14249 .\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measur-\ning and narrowing the compositionality gap in language models. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2023 , pages 5687–5711, 2023. URL https:\n//aclanthology.org/2023.findings-emnlp.378/ .\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof Q&A\nbenchmark. In First Conference on Language Modeling , 2024. URL https://openreview.\nnet/forum?id=Ti67584b98 .\nSagi Shaier, Ari Kobren, and Philip V . Ogren. Adaptive question answering: Enhancing language\nmodel proficiency for addressing knowledge conflicts with source citations. In Proceedings of the\n2024 Conference on Empirical Methods in Natural Language Processing , pages 17226–17239,\n2024. URL https://aclanthology.org/2024.emnlp-main.956/ .\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nSchärli, and Denny Zhou. Large language models can be easily distracted by irrelevant\ncontext. In Proceedings of the 40th International Conference on Machine Learning , vol-\nume 202 of Proceedings of Machine Learning Research , pages 31210–31227, 2023. URL\nhttps://proceedings.mlr.press/v202/shi23a.html .\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally\ncan be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024.\nURL https://arxiv.org/abs/2408.03314 .\n16\n--- Page 17 ---\nZhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and\nYu Cheng. ConflictBank: A benchmark for evaluating the influence of knowledge conflicts in\nllms. In Advances in Neural Information Processing Systems , volume 37, pages 103242–103268,\n2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\nbaf4b960d118f838ad0b2c08247a9ebe-Paper-Datasets_and_Benchmarks_Track.pdf .\nNandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, and Andrew Drozdov.\nFreshStack: Building realistic benchmarks for evaluating retrieval on technical documents. arXiv\npreprint arXiv:2504.13128 , 2025. URL https://arxiv.org/abs/2504.13128 .\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung,\nDenny Zhou, Quoc Le, and Thang Luong. FreshLLMs: Refreshing large language models with\nsearch engine augmentation. In Findings of the Association for Computational Linguistics: ACL\n2024 , pages 13697–13720, 2024. URL https://aclanthology.org/2024.findings-acl.\n813/ .\nAlexander Wan, Eric Wallace, and Dan Klein. What evidence do language models find convincing?\nInProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 7468–7484, 2024. URL https://aclanthology.org/2024.\nacl-long.403/ .\nHan Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Retrieval-augmented generation\nwith conflicting evidence. arXiv preprint arXiv:2504.13079 , 2025. URL https://arxiv.org/\nabs/2504.13079 .\nYike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and\nYulia Tsvetkov. Resolving knowledge conflicts in large language models. In First Conference on\nLanguage Modeling , 2024. URL https://openreview.net/forum?id=ptvV5HGTNN .\nJason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese,\nJohn Schulman, and William Fedus. Measuring short-form factuality in large language models.\narXiv preprint arXiv:2411.04368 , 2024. URL https://arxiv.org/abs/2411.04368 .\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won\nChung, Alex Tachard Passos, William Fedus, and Amelia Glaese. BrowseComp: A simple yet\nchallenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516 , 2025. URL\nhttps://arxiv.org/abs/2504.12516 .\nxAI. Grok 3 beta — The age of reasoning agents. 2025. URL https://x.ai/news/grok-3 .\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. SWE-bench: Can language models resolve\nreal-world github issues? In The Twelfth International Conference on Learning Representations ,\n2024a. URL https://openreview.net/forum?id=VTF8yNQM66 .\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn\nsloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth\nInternational Conference on Learning Representations , 2024b. URL https://openreview.\nnet/forum?id=auKAUJZMO6 .\nRongwu Xu, Xuan Qi, Zehan Qi, Wei Xu, and Zhijiang Guo. DebateQA: Evaluating question\nanswering on debatable knowledge. arXiv preprint arXiv:2408.01419 , 2024a. URL https:\n//arxiv.org/abs/2408.01419 .\nRongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei\nXu. Knowledge conflicts for LLMs: A survey. In Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing , pages 8541–8565, 2024b. URL\nhttps://aclanthology.org/2024.emnlp-main.486/ .\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 ,\n2025. URL https://arxiv.org/abs/2505.09388 .\nMichael Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages\n7371–7387, 2021. URL https://aclanthology.org/2021.emnlp-main.586/ .\n17\n--- Page 18 ---\nA SEAL-HARDresults by question category\nTables 7, 8, 9, 10, and 11 show a breakdown of SEAL-HARD results by question category. Overall,\nmodels perform poorly across question categories, especially on cross-lingual reasoning, false-\npremise detection, and questions that involve recent or rapidly changing information. Performance\nalso degrades more when search results are uniformly unhelpful than when they contain conflicting\nanswers.\n18\n--- Page 19 ---\n†indicates results using CHATGPT’s built-in search; all other search-based results use FRESH -\nPROMPT [Vu et al., 2024].\nTable 7: On SEAL-HARD,LLM Stend to underperform on cross-lingual reasoning ( Q4) and false-\npremise detection ( Q5) compared to advanced reasoning ( Q1), entity/event disambiguation ( Q2), and\ntemporal tracking ( Q3).\nModelW/O SEARCH\nQ1Q2Q3Q4Q5\nClosed-source models\nGPT-4 O-MINI 6.5 7.4 22.9 7.1 0.0\nGPT-4.1- MINI 10.9 15.5 22.9 14.3 9.1\nGPT-4 O 9.8 13.5 11.4 0.0 0.0\nGPT-4.1 14.1 14.2 25.7 0.0 0.0\nO3-MINI -MEDIUM 13.0 17.6 17.1 0.0 0.0\nO4-MINI -MEDIUM – – – – –\nO3-MEDIUM – – – – –\nOpen-weight models\nLLAMA -3.2-3B 0.0 1.4 0.0 0.0 0.0\nLLAMA -3.1-70B 3.3 4.7 5.7 0.0 0.0\nLLAMA -4-S COUT -17B-16E (109B) 4.9 6.8 5.7 0.0 0.0\nQWEN 3-235B-A22B 2.2 4.1 5.7 0.0 0.0\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B 1.1 2.0 0.0 0.0 0.0\nDEEPSEEK-R1-D ISTILL -QWEN -14B 6.5 8.1 17.1 0.0 0.0\nDEEPSEEK-R1-671B 20.7 23.0 22.9 7.1 0.0\nDEEPSEEK-R1-0528-671B 18.5 19.6 20.0 7.1 9.1\nTable 8: On SEAL-HARD,LLM Stend to underperform on cross-lingual reasoning ( Q4) and false-\npremise detection ( Q5) compared to advanced reasoning ( Q1), entity/event disambiguation ( Q2), and\ntemporal tracking ( Q3).\nModelW/SEARCH\nQ1Q2Q3Q4Q5\nClosed-source models\nGPT-4 O-MINI 11.4†10.8†17.1†14.3†9.1†\nGPT-4.1- MINI 8.2†11.5†14.3†0.0†0.0†\nGPT-4 O 11.4†15.5†17.1†7.1†0.0†\nGPT-4.1 20.1†17.6†25.7†21.4†9.1†\nO3-MINI -MEDIUM 12.0 9.5 20.0 0.0 9.1\nO4-MINI -MEDIUM 17.9†20.9†14.3†0.0†9.1†\nO3-MEDIUM 33.7†32.4†48.6†7.1†27.3†\nOpen-weight models\nLLAMA -3.2-3B 2.7 2.7 8.6 0.0 0.0\nLLAMA -3.1-70B 4.3 4.7 14.3 7.1 9.1\nLLAMA -4-S COUT -17B-16E (109B) 4.3 6.8 8.6 0.0 0.0\nQWEN 3-235B-A22B 9.2 10.8 14.3 0.0 18.2\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B 1.1 2.7 0.0 0.0 0.0\nDEEPSEEK-R1-D ISTILL -QWEN -14B 8.2 9.5 25.7 0.0 18.2\nDEEPSEEK-R1-671B 10.3 10.8 14.3 0.0 18.2\nDEEPSEEK-R1-0528-671B 15.2 12.8 17.1 7.1 18.2\n19\n--- Page 20 ---\n†indicates results using CHATGPT’s built-in search; all other search-based results use FRESH -\nPROMPT [Vu et al., 2024].\nTable 9: Questions that involve rapidly changing information, i.e., fast-changing questions, pose\nsignificant challenges for LLM SonSEAL-HARD.\nModelW/O SEARCH W /SEARCH\nNEVER SLOW FAST NEVER SLOW FAST\nClosed-source models\nGPT-4 O-MINI 15.2 9.0 1.6 16.5†10.8†14.1†\nGPT-4.1- MINI 20.3 15.3 3.1 12.7†10.8†12.5†\nGPT-4 O 16.5 12.6 4.7 15.2†15.3†14.1†\nGPT-4.1 21.5 18.0 1.6 17.7†24.3†17.2†\nO3-MINI -MEDIUM 40.5 39.6 17.2 16.5 10.8 9.4\nO4-MINI -MEDIUM – – – 29.1†18.0†10.9†\nO3-MEDIUM – – – 41.8†39.6†17.2†\nOpen-weight models\nLLAMA -3.2-3B 1.3 0.9 0.0 3.8 4.5 1.6\nLLAMA -3.1-70B 7.6 2.7 3.1 6.3 8.1 3.1\nLLAMA -4-S COUT -17B-16E (109B) 10.1 4.5 4.1 6.3 4.5 7.8\nQWEN 3-235B-A22B 7.6 3.6 1.6 12.7 8.1 15.6\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B 0.0 1.8 1.6 1.3 2.7 0.0\nDEEPSEEK-R1-D ISTILL -QWEN -14B 7.6 9.0 4.7 10.1 9.0 14.1\nDEEPSEEK-R1-671B 32.9 24.3 6.2 15.2 9.9 7.8\nDEEPSEEK-R1-0528-671B 31.6 18.0 6.3 19.0 14.4 12.5\nTable 10: LLM Sstruggle with questions that involve recent information on SEAL-HARD.\nModelW/O SEARCH W /SEARCH\nBEFORE 2024 2024 2025 BEFORE 2024 2024 2025\nClosed-source models\nGPT-4 O-MINI 13.4 6.1 0.0 16.1†16.3†3.6†\nGPT-4.1- MINI 20.1 8.2 1.8 10.7†20.4†7.1†\nGPT-4 O 16.8 8.2 1.8 15.4†18.4†10.7†\nGPT-4.1 23.5 6.1 0.0 25.5†20.4†7.1†\nO3-MINI -MEDIUM 45.6 26.5 10.7 15.4 6.1 8.9\nO4-MINI -MEDIUM – – – 27.5†14.3†3.6†\nO3-MEDIUM – – – 46.3†26.5†10.7†\nOpen-weight models\nLLAMA -3.2-3B 1.3 0.0 0.0 5.4 2.0 0.0\nLLAMA -3.1-70B 6.0 2.0 1.8 8.7 6.1 0.0\nLLAMA -4-S COUT -17B-16E (109B) 8.7 4.1 0.0 7.4 6.1 1.8\nQWEN 3-235B-A22B 6.7 2.0 0.0 12.8 16.3 3.6\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B 0.7 2.0 1.8 2.7 0.0 0.0\nDEEPSEEK-R1-D ISTILL -QWEN -14B 10.7 6.1 0.0 11.4 14.3 5.4\nDEEPSEEK-R1-671B 35.6 8.2 0.0 14.8 6.1 5.4\nDEEPSEEK-R1-0528-671B 27.5 10.2 5.4 19.5 14.3 5.4\n20\n--- Page 21 ---\n†indicates results using CHATGPT’s built-in search; all other search-based results use FRESH -\nPROMPT [Vu et al., 2024].\nTable 11: On SEAL-HARD, performance degrades more when search results are uniformly unhelpful\nthan when they contain conflicting answers.\nModelW/O SEARCH W /SEARCH\nUNHELPFUL CONFLICTING UNHELPFUL CONFLICTING\nClosed-source models\nGPT-4 O-MINI 7.2 10.4 10.9†15.3†\nGPT-4.1- MINI 10.0 16.6 10.0†13.2†\nGPT-4 O 9.0 13.8 11.8†17.4†\nGPT-4.1 14.5 15.3 18.2†22.2†\nO3-MINI -MEDIUM 11.8 16.6 10.9 13.2\nO4-MINI -MEDIUM – – 16.3†22.2†\nO3-MEDIUM – – 33.6†35.4†\nOpen-weight models\nLLAMA -3.2-3B 0.0 1.3 2.7 4.2\nLLAMA -3.1-70B 1.8 6.2 4.5 7.6\nLLAMA -4-S COUT -17B-16E (109B) 3.6 7.6 4.5 6.9\nQWEN 3-235B-A22B 3.6 4.8 8.2 13.9\nDEEPSEEK-R1-D ISTILL -QWEN -1.5B 0.0 2.0 2.7 0.7\nDEEPSEEK-R1-D ISTILL -QWEN -14B 2.7 11.1 7.3 13.2\nDEEPSEEK-R1-671B 20.9 23.6 9.1 12.5\nDEEPSEEK-R1-0528-671B 18.2 20.1 11.8 18.1\n21\n--- Page 22 ---\nB SEAL-HARDresults by answer type\nFigure 7 shows SEAL-HARD results broken down by answer type: “correct”, “incorrect”, and “not\nattempted”. We find that open-weight models like LLAMA -4-S COUT and DEEPSEEK-R1choose to “not\nattempt” questions more often than proprietary models such as GPT-4.1 ,O4-MINI, and O3.\nL -4-S\n DS-R1\n GPT-4.1 4-\n 3\n/ s\n5.9%28.7%\n65.4%22.4%9.5%\n68.1%15.0%0.8%\n84.2%- -\n/ s\n5.9%15.8%\n78.3%11.0%6.7%\n82.3%20.5%2.0%\n77.5%19.7%1.2%\n79.1%34.6%0.8%\n64.6%S-H  results by answer type\ncorrect incorrect not attempted\nFigure 7: On SEAL-HARD, open-weight models like LLAMA -4-S COUT and DEEPSEEK-R1choose to\n“not attempt” questions more often than proprietary models such as GPT-4.1 ,O4-MINI, and O3.\n22",
  "text_length": 65530
}