{
  "id": "http://arxiv.org/abs/2506.01074v1",
  "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
  "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
  "authors": [
    "Amir Hossein Kargaran",
    "Yihong Liu",
    "François Yvon",
    "Hinrich Schütze"
  ],
  "published": "2025-06-01T16:24:13Z",
  "updated": "2025-06-01T16:24:13Z",
  "categories": [
    "cs.CL",
    "cs.PL",
    "cs.SE"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01074v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01074v1  [cs.CL]  1 Jun 2025How Programming Concepts and Neurons Are Shared in\nCode Language Models\nAmir Hossein Kargaran1Yihong Liu1François Yvon2Hinrich Schütze1\n1LMU Munich & Munich Center for Machine Learning\n2Sorbonne Université & CNRS, ISIR\n{amir, yihong}@cis.lmu.de\nAbstract\nSeveral studies have explored the mechanisms\nof large language models (LLMs) in coding\ntasks, but most have focused on programming\nlanguages (PLs) in a monolingual setting. In\nthis paper, we investigate the relationship be-\ntween multiple PLs and English in the concept\nspace of LLMs. We perform a few-shot trans-\nlation task on 21 PL pairs using two Llama-\nbased models. By decoding the embeddings\nof intermediate layers during this task, we ob-\nserve that the concept space is closer to English\n(including PL keywords) and assigns high prob-\nabilities to English tokens in the second half\nof the intermediate layers. We analyze neuron\nactivations for 11 PLs and English, finding that\nwhile language-specific neurons are primarily\nconcentrated in the bottom layers, those exclu-\nsive to each PL tend to appear in the top layers.\nFor PLs that are highly aligned with multiple\nother PLs, identifying language-specific neu-\nrons is not feasible. These PLs also tend to have\na larger keyword set than other PLs and are\ncloser to the model’s concept space regardless\nof the input/output PL in the translation task.\nOur findings provide insights into how LLMs\ninternally represent PLs, revealing structural\npatterns in the model’s concept space. Code\nis available at https://github.com/cisnlp/\ncode-specific-neurons .\n1 Introduction\nMost state-of-the-art autoregressive large lan-\nguage models (LLMs) perform well on coding\ntasks (Chen et al., 2021; Hou et al., 2024; Lyu\net al., 2024; DeepSeek-AI et al., 2024; Jiang et al.,\n2024). Including code in the pre-training data has\nbecome a common practice in LLM pre-training,\neven for models not specifically designed for cod-\ning (Aryabumi et al., 2024). Most of these LLMs\ninvolved in coding tasks are pre-trained on multi-\nple programming languages (PLs) (Li et al., 2023;\nDeepSeek-AI et al., 2024; Jiang et al., 2024; GuoObservation NLs PLs\n1) English detour ✓ ✓ (shared with PLs)\n2) High alignment ✓(English) ✓(other PLs, e.g., C#)\n3) English neuron ID ✗ ✓\n4) Non-English/PL neuron ID ✓ ? (inconsistent)\nTable 1: Differences between natural languages (NLs)\nand programming languages (PLs) in English-centric\nLLMs. 1) LLMs’ layers reach non-English tokens\nthrough a detour via English (Wendler et al., 2024).\nThe same occurs when outputting PLs, though English\nis shared with PL tokens (§3.1). 2) Non-English lan-\nguages show high cross-lingual alignment with English\nin LLMs’ intermediate layers (Kargaran et al., 2024),\nwhile PLs, including C#, exhibit high alignment with\neach other (§3.2). 3), 4) It is challenging to identify\nEnglish-specific neurons whose ablation does not af-\nfect non-English. It is easy to find neurons specific for\nNon-English (e.g., French) (Tang et al., 2024). For PLs,\nthere are English ablatable neurons with minimal perfor-\nmance degradation over PLs, but for some PLs (e.g., C#,\nsee §3.3) finding ablatable neurons without affecting\nother PLs is hard.\net al., 2024). This raises an intriguing question:\nHow does pre-training on multiple PLs and En-\nglish affect the behavior of the models’ “concept\nspace” in coding tasks? More specifically: RQ1.\nDoes the model use English or a PL as a kind of\n“pivot” language? RQ2. Can we identify language-\nspecific neurons for PLs and English? Do PLs\nand English influence one another and neurons are\nshared across PLs and English?\nAs summarized in Table 1, we observe both sim-\nilarities and differences in how LLMs represent\nnatural languages versus PLs.\nContributions. To investigate the relationship\nbetween English and multiple PLs in the LLM’s\nconcept space, we apply methods from the field of\ninterpretability. Specifically, we focus on two mod-\nels from the Llama family: CodeLlama 7B (Roz-\nière et al., 2023) and Llama 3.1 8B (Dubey et al.,\n2024). We use the logit lens technique (Nostal-\n--- Page 2 ---\ngebraist, 2020), which involves applying the “un-\nembedding” operation prematurely at intermediate,\nnon-final layers. This approach provides insight\ninto the model’s internal numerical representations,\nwhich are otherwise difficult to interpret. We cre-\nate a super-parallel dataset of 42 translation direc-\ntions (21 pairs) across seven PLs using the dump of\nGeeksforGeeks (GeeksforGeeks, 2008) prepared\nby Zhu et al. (2022b). Our results show that the se-\nlected Llama models assign high probabilities and\ntop ranks to expected tokens during translation in\nthe last layer, meaning they completely understand\nthe translation task. Tracking token probabilities\nacross layers for different PLs and English using\nlogit lens (see Figure 1), we observed: (1) Most\ntokens in the first half of the layers have low prob-\nabilities, near zero, across PLs and English. (2)\nTokens from English and various PLs appear in the\nintermediate layers, mostly in the second half of the\nlayers. (3) Most tokens belong to English, followed\nby all PLs. Among individual PLs, the output PL\ncomes next, followed by PLs such as C++ and C#,\nwhich have some of the largest keyword sets. We\nuse our super-parallel data and measure the cross-\nlingual alignment for these PLs and find that C#\nis more aligned with most languages but not in all\ncases. For example, the best-aligned PL for PHP\nand Python is JavaScript.\nWe also explore how neuron activations are\nshared across 11 PLs and English. We use language\nactivation probability entropy (Tang et al., 2024) to\nidentify language-specific neurons. Our analysis re-\nveals the following insights: (1) Language-specific\nneurons are more concentrated in the bottom layers,\nfollowed by another smaller peak observed around\nthe three quarter point of the layers. (2) Among\nlanguage-specific neurons, those exclusive to a sin-\ngle PL tend to appear in the top layers. (3) For\nPLs such as C# and Java, which closely align with\nmultiple other PLs, identifying language-specific\nneurons is challenging.\n2 Materials and methods\nWe use three established methods to uncover the\nconcept space of LLMs, using datasets from PLs\nat parallel, keyword, and raw levels.\n2.1 Datasets\nSuper-parallel PL. Most of the parallel datasets\navailable in code community research (Zhu et al.\n(2022a,b); Lachaux et al. (2020), inter alia ) come\nFigure 1: Illustration of logit lens (Nostalgebraist, 2020)\napplied to CodeLlama 7B for the task of translating a\nforloop from Java to Rust (showing only Rust loop\nhere). The y-axis shows layers, the x-axis input to-\nkens, and color next-token probabilities (red: low, blue:\nhigh). Terms decoded in intermediate layers, such as\ninterval ,range ,until , and ten, are not keywords in\nJava or Rust but belong to other PLs (Python, Go, Ruby)\nand English. In Rust, the ..syntax defines a range. The\ntokens until andthrough , which decode for the same\nposition but with lower probabilities or in earlier layers,\nshare similar semantics with this syntax.\nfrom GeeksForGeeks (GeeksforGeeks, 2008), a\nwebsite that hosts data structure and algorithm\nproblems along with solutions in up to seven differ-\nent PLs: C++, Java, Python, C#, JavaScript, PHP,\nand C. According to GeeksForGeeks, the solution\nprograms for the same problem follow the same\nstructure, including consistent variable names, re-\nsulting in semantic consistency across the different\nlanguages. In most cases, the programs for the\nsame problem share the same set of comments in\nthe same order, indicating that they are parallel at\nthe snippet level. We use the (Zhu et al., 2022b)\ndump of GeeksForGeeks and create a super-parallel\ndataset for all seven PLs, containing 581 parallel\ncode snippets, each available for all seven PLs. We\nretain only programs that are available in all PLs\nand that have the same number of snippets to en-\nsure alignment across all seven PLs.\nEnglish and PL keywords. We gather, for 22\nPLs, programming-specific keywords, as well as\nthe names of other built-ins starting from (Meyer\nand McCulloch, 2022). For brevity, we refer to\nthese as PL keywords. We also extract English key-\nwords from PanLex (Kamholz et al., 2014), which\ncontains words from several thousand languages,\nincluding English. We only keep keywords that\nthe model’s tokenizer represents as a single token\nand remove numbers from this list (if represented\nnumerically). Note that PLs have a limited vocab-\nulary consisting primarily of keywords whereas\n--- Page 3 ---\nnatural languages have an extensive and continu-\nously evolving lexicon. Additionally, many PLs are\ninfluenced by older PLs (Sebesta, 2016), leading\nto shared structures and common keywords like if,\nfor,while , and return .\nRaw PL and English. We take raw code\nof eleven popular PLs from the GitHub Code\ndataset (CodeParrot, 2022). It consists of 115 mil-\nlion code files from GitHub in 32 PLs. We select\nthe following eleven popular PLs (GitHut, 2024):\nC, C++, C#, Go, HTML, Java, JavaScript, PHP,\nPython, Ruby, and Rust. We also use the English\nWikipedia as the source for English texts. We limit\neach language to 50,000 code files/articles.\n2.2 Models\nWe focus on models from the Llama family, which\nare autoregressive and decoder-only transform-\ners (Vaswani et al., 2017). We focus on pre-trained\nmodels rather than fine-tuned ones with instruction\ntuning or RLHF to minimize confounding factors.\nWe select two models: CodeLlama 7B (Rozière\net al., 2023) and Llama 3.1 8B (Dubey et al., 2024).\nWe choose models around 7B parameters, which\nare considered a base size in the LLM community.\nCodeLlama 7B is pre-trained on 500B tokens from\na code-heavy dataset. It is first initialized with\nLlama 2 (Touvron et al., 2023b) model weights,\nwhich are pre-trained on general-purpose text and\ncode. CodeLlama is the only family of Llama mod-\nels introduced primarily for coding tasks. The latest\nmodels in the Llama family are general foundation\nmodels. Llama 3.1 8B is the latest model in the\nfamily with a base size around 7B parameters and is\npre-trained on 15 trillion tokens of general-purpose\ntext and code.\n2.3 Method 1: Interpreting latent embeddings\nFollowing Wendler et al. (2024), we use logit lens\n(Nostalgebraist, 2020) instead of tuned lens (Bel-\nrose et al., 2023) to decode intermediate embed-\ndings, as tuned lens is trained to map internal states\nto the final next-token prediction, which may lose\nthe signal of interest. We use logit lens to find\nwhich of the PLs or English is closer to the abstract\nconcept space of the selected models.\nLogit lens. A transformer model at layer ℓcan\nbe viewed in two parts: (i) a lower part, which in-\ncludes layers up to and including layer ℓ, that maps\ninput tokens to hidden states, and (ii) an upper part,\nwhich includes layers after ℓthat convert hidden\nstates into logits. The core idea of logit lens is tosee the lower part as a complete transformer and\napplyWU, the “unembedding” matrix, to project\nthe hidden state at layer ℓ,h(ℓ), into logit scores.\nThese logit scores are then transformed into token\nprobabilities via the softmax operation. The logit\nlens operation can be defined as:\nLogitLens( h(ℓ)) = LayerNorm[ h(ℓ)]WU.\nFew-shot translation. The task is to translate\nthe preceding PL (e.g., Java) code snippet into an-\nother PL (e.g., Python). We show the model four\ncode snippets with their correct translations, fol-\nlowed by a fifth code snippet without its translation,\nand ask the model to predict the next tokens. With\nsuch a prompt, the model can infer that it should\ntranslate the fifth code snippet. Since the fifth pre-\ndicted code snippet could diverge at some point\nand affect all the subsequent tokens, we predict the\ntokens one by one and replace the previous tokens\nwith the expected ones. We use our super-parallel\nPL dataset (§2.1) for the fifth code snippet (both\ninput and output PLs). For every input token, at\neach layer, we compute the probabilities of the top\nα= 10 decoded tokens using logit lens and clas-\nsify them as belonging to English or one or more\nPLs using the keywords dataset (§2.1).\nAs for the four-shot code snippets, we always\nuse parallel data for basic structures, as shown in\nthe example below (Input PL: Java, Output PL:\nPython).\nJava: String message = \"\"; - Python: message = \"\"\nJava: public class MyClass {} - Python: class MyClass:\nJava: public int value = 5; - Python: value = 5\nJava: public void doSomething() {} - Python: def do_something():\nJava: for (int i = 0; i < 10; i++) - Python:\n2.4 Method 2: Cross-lingual alignment\nWe employ MEXA (Kargaran et al., 2024), a mea-\nsure of cross-lingual alignment, to determine which\nPL aligns most closely with the majority of the se-\nlected PLs in the model’s intermediate layers. To\ncompute MEXA, we generate code snippet embed-\ndings using position-weighted averaging (Muen-\nnighoff, 2022) and assess alignment based on co-\nsine similarity comparisons. The higher the score,\nthe greater the alignment, with values ranging be-\ntween 0 and 1.\nMEXA. Given a decoder-only transformer\nmodel m, MEXA computes the cross-lingual align-\nment score for language L1relative to a pivot lan-\nguage L2. Let S={s1, s2, . . . , s n}be a set of n\nparallel sentences (i.e., code snippets) in L1and\n--- Page 4 ---\nL2. We use our super-parallel dataset (§2.1) for\neach pair. First, we compute sentence embeddings\nusing model mat layer ℓwith position-weighted\naveraging. Given a sentence s, its corresponding\nembedding is denoted as e(ℓ)(s). We construct a\nsimilarity matrix C(L1, L2, m, ℓ )∈Rn×n, where\neach element cij(ℓ)represents the cosine similarity\nbetween the embeddings of sentence siinL1and\nsentence sjinL2. The diagonal elements cii(ℓ)\ncorrespond to the similarity between parallel sen-\ntence pairs. The MEXA alignment score for matrix\nC(L1, L2, m, ℓ )is defined as:\n1\nnnX\ni=1I\u0012\ncii(ℓ)>max\nj̸=i{cij(ℓ), cji(ℓ)}\u0013\n,\nwhere Iis the indicator function, which returns 1 if\nthe condition holds and 0 otherwise. This measures\nhow often a parallel sentence pair has the highest\nsimilarity compared to any non-parallel pairs.\n2.5 Method 3: Language-specific neurons\nWe use language activation probability entropy\n(LAPE) (Tang et al., 2024), which outperforms\nsimilar methods in identifying language-specific\nregions across natural languages. We use LAPE to\nidentify language-specific neurons in each model\nand analyze their impact on other languages.\nNeurons in FFN. Llama-based models (Touvron\net al., 2023a) use a transformer architecture with\na GLU variant (Shazeer, 2020). Like other trans-\nformer architectures, their core building blocks in-\nclude multi-head self-attention (MHA) and feed-\nforward networks (FFNs). Let ˜h(ℓ)denote the out-\nput of the MHA module in the ℓ-th layer, computed\nusing the previous layer’s hidden states and train-\nable parameters. The FFN module, which outputs\nthe hidden state h(ℓ)∈Rd1, in a GLU variant trans-\nformer is given by:\nh(ℓ)=\u0000\nϕ(˜h(ℓ)W(ℓ)\n1)⊗˜h(ℓ)W(ℓ)\n3\u0001\n·W(ℓ)\n2,\nwhere W(ℓ)\n1,W(ℓ)\n3∈Rd1×d2andW(ℓ)\n2∈Rd2×d1\nare learnable parameters, and ϕ(·)denotes the ac-\ntivation function. In LAPE, a neuron is defined\nas the linear transformation of a single column in\nW(ℓ)\n1followed by the application of the non-linear\nactivation function. Thus, each FFN module con-\ntainsd2neurons. A neuron indexed by rin the ℓ-th\nFFN layer is considered “active” if its activation\nvalue ϕ(˜h(ℓ)W(ℓ)\n1)rexceeds zero.\nLAPE. To compute LAPE, we feed LLMs dif-\nferent texts, each written in a single language fromraw PL and English texts (§2.1). For the r-th neu-\nron in the ℓ-th layer, we calculate the activation\nprobability when processing texts in language z:\npz\nℓ,r=E\u0010\nI\u0000\nϕ(˜h(ℓ)W(ℓ)\n1)r>0\u0001\n|language z\u0011\n,\nwhere Iis the indicator function. This probability\nis empirically estimated as the likelihood that the\nneuron’s activation value exceeds zero. We obtain\nthe probability distribution across languages and\nnormalize it via sum normalization to compute the\nnormalized probability p′z\nℓ,rfor each language z.\nThe entropy of this distribution is:\nLAPE ℓ,r=−X\nz∈Lp′z\nℓ,rlog(p′z\nℓ,r).\nwhereLis the set of languages. We designate neu-\nrons with low LAPE scores as “language-specific\nneurons,” as they show a predilection for activa-\ntion in response to one or two languages, while\nshowing reduced activation probabilities for others.\nA neuron is deemed specific to language zif its\ncorresponding activation probability pz\nℓ,rsurpasses\na predefined threshold.\nLAPE is highly dependent on hyperparameter\nthresholds. The first hyperparameter is the activa-\ntion threshold, set at the activation probability cor-\nresponding to the τquantile. The default for LAPE\nisτ= 0.95. For CodeLlama 7B/Llama 3.1 8B, this\ncorresponds to activation probability thresholds of\n0.531 and 0.554, respectively, meaning selected\nneurons must exhibit activation probabilities ex-\nceeding these values for at least one language. The\nsecond threshold, the filter threshold γ, retains only\na small fraction of neurons as language-specific by\nselecting those in the bottom γof LAPE scores.\nThe default setting is γ= 0.01. However, since\nthis results in varying numbers of selected neurons\nacross languages and makes the comparison be-\ntween different languages harder to interpret, we\ninstead compute the average number of selected\nneurons and select the same number, ν, for each\nlanguage. For the default settings of both selected\nmodels, νis around 400.\nControlled generation. To assess the impact of\nthe selected neurons, we set their activation values\nto zero or zero out the corresponding parameters\nand then measure changes in model performance.\nSpecifically, we compute language model perplexi-\nties (PPLs) to examine how much removing these\nneurons affects various languages.\n--- Page 5 ---\n0 510 15 20 25 30\nLayers0.00.20.40.60.81.0Probability\nExpected Output\nAll PL Keywords\nInput PL Keywords\nOutput PL Keywords\nC++ Keywords\nEnglish Keywords\nEnglish (Excl. PL Keywords)(a) Code Llama 7B; probability value.\n0 510 15 20 25 30\nLayers0.00.20.40.60.81.0Probability\nExpected Output\nAll PL Keywords\nInput PL Keywords\nOutput PL Keywords\nC# Keywords\nEnglish Keywords\nEnglish (Excl. PL Keywords) (b) Llama 3.1 8B; probability value.\n0 510 15 20 25 30\nLayers0.00.20.40.60.81.01/rank\nExpected Output\nAll PL Keywords\nInput PL Keywords\nOutput PL Keywords\nC++ Keywords\nEnglish Keywords\nEnglish (Excl. PL Keywords)\n(c) Code Llama 7B;1\nrankvalue.\n0 510 15 20 25 30\nLayers0.00.20.40.60.81.01/rank\nExpected Output\nAll PL Keywords\nInput PL Keywords\nOutput PL Keywords\nC++ Keywords\nEnglish Keywords\nEnglish (Excl. PL Keywords) (d) Llama 3.1 8B;1\nrankvalue.\nFigure 2: Language keyword probability or1\nrankvalue (best keyword rank) during translation task. The PLs\ncontributing the most to each score, selected from the 22 PL keywords, are C++ and C#.\n3 Results\n3.1 Method 1: Interpreting latent embeddings\nWe present the results of interpreting latent embed-\ndings for the translation task in Figure 2. Neither\nEnglish nor PL keywords exhibit noticeable proba-\nbility during the first half of the layers (Figures 2a,\n2b). Although these probabilities remain negligi-\nble, English keywords still appear among the top\nrank decoded tokens in the first half of the layers\n(Figures 2c, 2d); this occurs much less frequently\nfor PL keywords.\nAround the half point (rougly, layer 15), the prob-\nabilities of English and PL keywords, as well as\nexpected tokens, begin to rise sharply (Figures 2a,\n2b). English and PL keywords overtake the ex-\npected tokens at first. While expected token prob-\nability continues increasing until the final layers,\nEnglish and PL keyword probabilities decline, par-\nticularly when English token probability crosses\nover the expected token probability.\nIn the final layer, while the expected token holds\nrank = 1, English keywords (excluding PL key-\nwords) and PL keywords maintain a high and simi-\nlar1\nrankvalue of 0.4each (Figures 2c, 2d), indicat-\ning their presence among the top decoded tokens.\nAmong individual PL keywords, the output PL\ndominates in both rank and probability measures,\nfollowed by popular PLs like C++ and C# (which\nhave some of the largest keyword sets), while theinput PL has less influence. This distribution holds\nacross different PL keywords: rising in the second\nhalf of the layers, peaking, and then decreasing in\nthe final layers. Notably, many expected tokens\nare variable names, symbols, numbers, or punctua-\ntion, which typically fall outside the different PL\nkeyword sets.\nRegarding the comparison between CodeLlama\n7B and Llama 3.1 8B: In terms of token probabil-\nity, Llama 3.1 8B exhibits a slower initial rise in\nexpected token probability, followed by a sharp\nincrease in the top three layers. In contrast, CodeL-\nlama 7B demonstrates a more gradual increase\nthroughout. In terms of rank, CodeLlama 7B con-\nsistently shows a mainstream presence of English\nkeywords. However, their distribution shifts: in\nthe first half of the layers, they primarily consist of\nEnglish keywords excluding PL keywords, while\nin the second half, they increasingly include En-\nglish keywords that overlap with PL keywords. For\nLlama 3.1 8B, the presence of PL keywords also\nincreases in the second half of the layers, reaching\na1\nrankvalue of 0.7at layer index 23 for English\nkeywords shared with PL keywords.\n3.2 Method 2: Cross-lingual alignment\nWe present the results of cross-lingual alignment\nin Figure 3. We compute alignment scores for all\npairs of PLs and determine which PL aligns bet-\nter with others. C# achieves the best alignment\n--- Page 6 ---\n0 5 10 15 20 25 30\nLayers0.40.50.60.70.80.91.0Alignment Score\nPLs\nC#\nC\nJava\nC++\nJavascript\nPHP\nPython(a) Code Llama 7B\n0 5 10 15 20 25 30\nLayers0.40.50.60.70.80.91.0Alignment Score\nPLs\nC#\nJava\nC\nC++\nJavascript\nPHP\nPython\n(b) Llama 3.1 8B\nFigure 3: MEXA alignment score. The minimum value\nof the MEXA alignment score is 0. The figures are\nlimited to scores above 0.4 for better visualization.\noverall across all layers in both models, though\nthe difference between C-family PLs and Java is\nminimal. Both models show fewer alignments for\nPython. JavaScript is the best-aligned PL for both\nPHP and Python. The high alignment of C# and\nC++ further supports the influence of popular PLs,\nas discussed in Section 3.1. This finding is also\naligned with Quan et al. (2025), who find that al-\nthough Python is the most familiar language for\nexisting LLMs and competition-level code bench-\nmarks, model performance improves over Python\nwhen responding in C++ for most of the models,\nincluding for the instruction-tuned version of the\nLlama 3.1 8B model.\nThe alignment scores consistently increase ex-\ncept for two instances: first, in the bottom layers\n(layer index 2 in Figures 3a, 3b), where representa-\ntions diverge from the “input space”; and second,\nimmediately before the final layer (layer index 31\nin Figures 3a, 3b), where they transition into the fi-\nnal “output space”. The alignment of different PLs,\nespecially in the layer preceding the final layer, in-\ndicates the high quality of the parallel data, as the\nalignment reaches values of 0.9in average.\nC\nC++\nC#\nJava\nRust\nPython\nJavaScript\nPHP\nHTML\nGo\nRuby\nEnglishC\nC++\nC#\nJava\nRust\nPython\nJavaScript\nPHP\nHTML\nGo\nRuby\nEnglish0.01 0 -0 -0 0 0 -0 0 0 0.01 0 0\n0 0.01 -0 -0 0.01 0 -0.01 -0 -0 0 -0 0.01\n0 0 0 0 0 0 0 0 0 0.01 0 0\n0 -0 0 0 0 -0 0.01 0.01 0.01 0 0 -0.01\n-0 -0 0 0 0.04 0.01 0 -0 -0.01 0.02 0 0.01\n-0 0 0 -0 0.01 0.07 0.01 -0 -0 0 0 0\n0 0 0 -0 0.01 -0 0.15 0 0.01 0 0 -0\n-0 0 0 0 0.01 -0.01 0 0.06 0 -0 0 -0\n-0 0 0 -0 0 -0.01 -0 0.01 0.01 0 0 0\n0 0.01 0 0.01 0.01 0.01 0.02 -0 0 0.09 0 -0\n0 0.01 -0 -0 0.01 0.04 0.02 0.02 0 0 0.04 -0.01\n0 0.01 0 0 0 -0 0 0.01 0.01 0 0.01 0.19(a) Code Llama 7B\nC\nC++\nC#\nJava\nRust\nPython\nJavaScript\nPHP\nHTML\nGo\nRuby\nEnglishC\nC++\nC#\nJava\nRust\nPython\nJavaScript\nPHP\nHTML\nGo\nRuby\nEnglish0.43 0.2 0.02 0.01 0.05 -0 0.05 0.14 0 0.14 0.01 0.08\n0.32 0.23 0.03 0.01 0.07 -0.01 0.04 0.16 0.01 0.09 0.01 0.1\n0.04 0.1 0.17 0.05 0.03 -0 0.03 0.04 0.02 0.11 0.05 0.03\n0.03 0.08 0.04 0.07 0.03 0.02 0.09 0.05 0.02 0.06 0.03 0.04\n0.15 0.14 0.01 0 0.35 0 -0.02 0.03 0.01 0.27 0.07 0.05\n0.05 0.02 0.02 0.02 0.06 0.4 0.09 0 0.02 0.06 0.21 0.05\n-0.01 0.06 0.01 0.02 0.02 0.03 0.46 0.08 0.05 0.03 0.02 0.04\n0.02 0.03 0.01 0.01 0.03 0.26 0.14 0.64 0.02 0.03 0.05 0.04\n0.01 0.06 0.01 0.03 0.03 0.02 0.09 0.08 0.1 0 0 0.04\n0.02 0.02 0.02 0.03 0.16 0.05 0.05 0.03 0.01 1 0.03 0.07\n0 -0.01 0.01 0.01 0.09 0.27 0.08 0.07 0.01 0.01 0.46 0.01\n0.04 0.06 0.02 0.03 0.07 0.06 0.13 0.03 0.02 0.03 0.04 0.58\n(b) Llama 3.1 8B\nFigure 4: Impact of LAPE identification ( ν= 400 , τ=\n0.95) on PPL increase. The element at row i, column j\nrepresents the PPL change for language jdue to pertur-\nbations in the language iregion.\nLlama 3.1 8B achieves better MEXA alignment\nscores across all pairs compared to CodeLlama\n7B. This is not entirely unexpected: even though\nCodeLlama 7B and its instruction-tuned version are\nspecifically trained for code, newer generic models\nof Llama, including Llama 3 8B (Dubey et al.,\n2024) and its instruction-tuned version, achieve\nbetter scores in code generation tasks (as evaluated\non LiveCodeBench (Jain et al., 2024)).1\n3.3 Method 3: Language-specific neurons\nWe identify language-specific neurons for 11 PLs\nand English using LAPE. PPL change results ( ν=\n400, τ= 0.95) in Figure 4 show that deactivating\nlanguage-specific neurons has negligible effects on\nother languages, while more noticeably impacting\nthe primary language—though this may not hold\nacross all settings. For other νvalues, we apply\nthe LAPE neuron identification method and mea-\nsure PPL changes by incrementally deactivating\nlanguage-specific neurons for each primary lan-\nguage. The results for τ= 0.95are shown in\n1hf.co/spaces/livecodebench/leaderboard\n--- Page 7 ---\n0 2500 5000 7500 10000 12500 150000246810\nC#\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJava\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nGo\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJavaScript\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPython\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nEnglish\nAVG over other languages(a) Code Llama 7B\n0 2500 5000 7500 10000 12500 150000246810\nC#\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJava\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nGo\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJavaScript\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPython\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nEnglish\nAVG over other languages (b) Llama 3.1 8B\nFigure 5: Impact of LAPE neuron identification. X-axis: Number of shared neurons for each language. Y-axis:\nChange in PPL across languages when deactivating the primary language’s neurons (e.g., English in the lower-right\nfigure). Figure 7 in Appendix A shows the same figure for more languages.\n0 5 10 15 20 25 30\nLayers050100150200250300Number of Neurons\nLlama 3.1 8B\nCode Llama 7B\nFigure 6: Number of “language-specific” neurons, i.e.,\nneurons that are exclusive to one PL and not shared\nwith any other PL, in the LAPE experiment with ν=\n1400, τ= 0.95. The figure shows the total number of\nlanguage-specific neurons summed over all PLs.\nFigure 5, where we observe that LAPE fails to iden-\ntify “effective” language-specific neurons for some\nlanguages. Effectiveness is indicated by a larger\nPPL change for the primary language compared\nto other languages when deactivating the primary\nlanguage-specific neurons.\nIn most cases, increasing the number of neu-\nrons enlarges the PPL gap between primary and\nother languages in Llama 3.1. However, for C#\nand Java (and C++ and HTML in Figure 7) the\ngap is less pronounced. Interestingly, C# and Java\nare the PLs with the highest alignment in Llama\n3.1, as shown in Section 3.2. Additionally, C#\nhas one of the largest PL keyword sets appearing\nfrequently in intermediate layers, as shown in Sec-\ntion 3.1. This suggests that the identified specific\nneurons for these languages are more shared acrosslanguages. In other words, for PLs such as C# and\nJava, which closely align with multiple other PLs,\ndistinguishing language-specific neurons is more\nchallenging.\nFor CodeLlama 7B, even though some effective\nlanguage-specific neurons exist, the PPL change is\nnot significant (Figures 4a, 5a). When the number\nof deactivated neurons exceeds 12,500, the impact\non other languages sometimes surpasses that on the\nprimary language. The only language for which\nCodeLlama 7B identifies effective specific neurons\nwith a larger PPL change margin is English. This\nsuggests that PL neurons in CodeLlama 7B are\nhighly shared, possibly due to its training recipe,\nwhere the pre-training phase following Llama 2\ninitialization primarily focuses on code.\nTo further investigate what makes language-\nspecific neurons effective in Llama 3.1 8B but\nnot in CodeLlama 7B, we examine the language-\nspecific neurons selected for all PLs that are “ex-\nclusive” to each PL, as shown in Figure 6 for\nν= 1400 , τ= 0.95. Other νsettings exhibit a\nsimilar distribution. In general, most language-\nspecific neurons in both models and across most\nlanguages are selected from the bottom layers (in-\ndices 0 to 4), followed by layer indices 18 to 22 in\nboth models. However, those that are exclusive to a\nspecific PL are predominantly selected from the top\nlayers (indices 29 to 31). Notably, LAPE selects\nmore exclusive neurons from top layers for Llama\n3.1 8B than CodeLlama 7B as shown in Figure 6.\nThis aligns with the fact that top layers serve for\ntoken generation, where the LLM must handle the\n--- Page 8 ---\n“output space” and map it to the expected token. If\nexclusive language-specific neurons exist for each\nprimary language at the top layers, deactivating\nthat language’s neurons only affects the PPL of\nthat language. However, if there are no such exclu-\nsive neurons, it affects PPL of other languages as\nwell.\n4 Discussion and Implications\nOur findings suggest several strategies for building\nmore efficient multilingual code models.\n1) Since English and certain PLs are centrally\nlocated in the model’s concept space, these could\nserve as intermediate representations for multilin-\ngual code translation, minimizing the distance be-\ntween source and target languages.\n2) The distribution of neuron types across layers\n– shared/general in bottom layers, specific in top –\nsupports modular architectures where base layers\nencode general syntax/semantics and top layers can\nbe swapped or specialized for specific languages.\n3) For closely aligned PLs (e.g., Java and C#),\nshared representations could enable parameter shar-\ning or adapter-based methods for lightweight mul-\ntilingual support, while only tuning minimal addi-\ntional weights.\n4) Some languages enforce object-oriented pro-\ngramming, while others support it optionally. This\nstructural difference may lead the model to develop\nstronger internal representations for languages with\nstricter paradigms, potentially introducing some\nbias in code generation. Other differences in lan-\nguage design and idiomatic usage can influence\nthe model’s behavior when generating code across\nlanguages. Recognizing these factors could help\nimprove the generalization capabilities of multilin-\ngual code models.\n5 Related work\nPivot language. Wendler et al. (2024) use logit\nlens (Nostalgebraist, 2020) to show that English\nacts as a kind of “pivot” language in English-centric\nLLMs, such as Llama-2 (Touvron et al., 2023b),\nenabling these models to solve complex seman-\ntic tasks in a non-English language by detouring\nthrough English internal states before generating\nnon-English text. Building on this idea, Wu et al.\n(2025) propose the semantic hub hypothesis, which\nsuggests that the same phenomenon could occur\nnot only across different languages but also across\ndifferent modalities. As one of these modalities,they introduce code. Their analysis focuses solely\non Python within the Llama 2 model. Since obtain-\ning semantically equivalent English-Python pairs\nis challenging, they test only a few targeted cases,\nsuch as the English token “and” and its Python\ncounterpart “,”. Using logit lens, they show that in\nthe intermediate layers, the expected Python token\nis closer to “and” than to other tokens such as “or”\nand “not.” In our work, we focus exclusively on\nPLs and consider seven PLs. As noted by Wu et al.\n(2025), obtaining semantically equivalent English-\nPL pairs is challenging. Instead, we analyze key-\nword sets—comprising keywords from 22 PLs and\nan English dictionary—through a translation task\nacross 42 directions. This allows us to examine\nwhich PLs and English-derived tokens appear in\nthe model’s intermediate layers and are closer to\nits concept space, both in terms of probability and\nrank. Our findings reveal that not only English but\nalso other PLs contribute to the model’s concept\nspace.\nNeuron-level interpretability. Initially,\nlanguage-specific components were studied in\nneural machine translation using small language\nmodels (Lin et al., 2021; Xie et al., 2021; Zhang\net al., 2021). Later, the role of FFNs within LLMs\nwas explored in several studies, highlighting\ntheir function as key-value memories for storing\nfactual and linguistic knowledge (Geva et al., 2021,\n2022; Ferrando et al., 2023). However, these\nanalyses typically investigate neuron behavior,\nfocusing on monolingual settings in natural\nlanguages and PLs. Building on methods explored\nin investigations on the role of FFNs within\nLLMs and considering clear evidence that LLMs\nexhibit significant overlap in their embeddings\nacross languages—particularly among those from\nthe same linguistic family (Doddapaneni et al.,\n2021)—several recent studies (Xie et al., 2021;\nTang et al., 2024; Zhao et al., 2024; Kojima et al.,\n2024; Wang et al., 2024; Bhattacharya and Bojar,\n2023, 2024; Mueller et al., 2022; Liu et al., 2024a;\nDumas et al., 2024; Liu et al., 2025) have investi-\ngated the existence of language-specific neurons\nand internal mechanisms for natural languages,\nespecially within the FFN layers of LLMs. Just as\nthere are many natural languages, there are also\nmany PLs. However, no research has explored the\nexistence of language-specific neurons for PLs,\neven though LLMs are typically pre-trained on\na mixture of these languages. Building on this,\nour work adopts the method proposed by Tang\n--- Page 9 ---\net al. (2024) to identify PL-specific neurons. This\napproach enables a scalable and targeted analysis\nof neurons for many PLs using only raw PL data.\nInterpretability for code. Interpretability in lan-\nguage models for code-related tasks remains under-\nexplored, with most research focusing on attention\nlayers (Mohammadkhani et al., 2023; Wan et al.,\n2022; Paltenghi and Pradel, 2021; Liu et al., 2024b).\nOur work is closest to Haider et al. (2024), who\nanalyze FFN layers. They analyze two GPT-based\nmodels (Xu et al., 2022; Nijkamp et al., 2022) for\nthree PLs, showing that lower layers capture syn-\ntax while higher layers encode abstract concepts\nand semantics. They demonstrate that concepts are\nstored in the FFN layers and can be edited without\ncompromising code language model performance.\nHowever, their analysis is performed in monolin-\ngual settings, while our work investigates the re-\nlationship between PLs to determine if they share\nconcepts and neurons in coding tasks.\n6 Conclusion\nIn this study, we investigate how LLMs represent\nprogramming languages (PLs) in their concept\nspace using the logit lens method. We observe\nthat English and PL keywords appear in interme-\ndiate layers, with notable probabilities in the lat-\nter half. Initially, these keywords surpass the ex-\npected output tokens, but as the probabilities of\nexpected tokens increase and overtake those of En-\nglish and PL keywords, the probabilities of the\nlatter decline. We further investigate the existence\nof language-specific neurons using the language ac-\ntivation probability entropy (LAPE) method. Our\nanalysis reveals that language-specific neurons can\nbe identified for most languages in the Llama 3.1\nmodel, but not for PLs such as Java and C#, which\nalign closely with other PLs. We find that language-\nspecific neurons are concentrated in the bottom lay-\ners, while neurons exclusive to each PL are located\nin the top layers. These findings deepen our under-\nstanding of LLMs’ inner workings in the context of\nPLs and provide valuable insights for interpretabil-\nity in code-related tasks.\nLimitations\nWe are aware of three main limitations of our work.\nFirst, parts of our analysis rely on a super-\nparallel dataset, which is limited to seven languages\ndue to source constraints. To our knowledge, no\nsuper-parallel dataset with a broader language setis publicly available. A potential solution is to gen-\nerate super-parallel data for more languages using\nmore powerful LLMs and validate it through unit\ntests to ensure quality and consistency.\nSecond, while we use keywords to interpret la-\ntent embeddings, a more precise approach would\ninvolve constructing a dictionary mapping PL key-\nwords to each other and their English equivalents.\nHowever, this is not always feasible, as some PL\nkeywords lack direct English meanings or map to\nmultiple tokens.\nThird, we hypothesize that the ineffectiveness\nof neuron identification for CodeLlama 7B stems\nfrom its training recipe, but further investigation\nacross other models could be beneficial. Our anal-\nysis focuses on PLs in Llama-based architectures,\nwhich underlie many state-of-the-art models, but\nit’s important to explore other architectures for\nbroader validation.\nAcknowlegments\nThis research was supported by DFG (grant SCHU\n2246/14-1).\nReferences\nViraat Aryabumi, Yixuan Su, Raymond Ma, Adrien\nMorisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee,\nAhmet Üstün, and Sara Hooker. 2024. To code, or\nnot to code? exploring impact of code in pre-training.\nPreprint , arXiv:2408.10914.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nPreprint , arXiv:2303.08112.\nSunit Bhattacharya and Ond ˇrej Bojar. 2023. Unveil-\ning multilinguality in transformer models: Exploring\nlanguage specificity in feed-forward networks. In\nProceedings of the 6th BlackboxNLP Workshop: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 120–126, Singapore. Association for Compu-\ntational Linguistics.\nSunit Bhattacharya and Ond ˇrej Bojar. 2024. Under-\nstanding the role of ffns in driving multilingual be-\nhaviour in llms. Preprint , arXiv:2404.13855.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde De Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, and 1 others. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374 .\nCodeParrot. 2022. GitHub code dataset.\n--- Page 10 ---\nDeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao,\nDejian Yang, Peiyi Wang, Runxin Xu, Y . Wu,\nYukun Li, Huazuo Gao, Shirong Ma, and 1 others.\n2024. DeepSeek-Coder-v2: Breaking the barrier of\nclosed-source models in code intelligence. Preprint ,\narXiv:2406.11931.\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M.\nKhapra, Anoop Kunchukuttan, and Pratyush Kumar.\n2021. A primer on pretrained multilingual language\nmodels. Preprint , arXiv:2107.00676.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\nPreprint , arXiv:2407.21783.\nClément Dumas, Veniamin Veselovsky, Giovanni\nMonea, Robert West, and Chris Wendler. 2024. How\ndo llamas process multilingual text? a latent explo-\nration through activation patching. In ICML 2024\nWorkshop on Mechanistic Interpretability .\nJavier Ferrando, Gerard I. Gállego, Ioannis Tsiamas,\nand Marta R. Costa-jussà. 2023. Explaining how\ntransformers use context to build predictions. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 5486–5513, Toronto, Canada.\nAssociation for Computational Linguistics.\nGeeksforGeeks. 2008. Geeksforgeeks: A computer\nscience portal for geeks.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nGitHut. 2024. GitHut 2.0: Language popularity in\nGitHub repositories.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY . Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wen-\nfeng Liang. 2024. DeepSeek-Coder: When the large\nlanguage model meets programming – the rise of\ncode intelligence. Preprint , arXiv:2401.14196.\nMuhammad Umair Haider, Umar Farooq, A. B.\nSiddique, and Mark Marron. 2024. Looking\ninto black box code language models. Preprint ,\narXiv:2407.04868.Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong\nWang, Li Li, Xiapu Luo, David Lo, John Grundy,\nand Haoyu Wang. 2024. Large language models\nfor software engineering: A systematic literature re-\nview. ACM Transactions on Software Engineering\nand Methodology , 33(8):1–79.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\nCodeBench: Holistic and contamination free evalu-\nation of large language models for code. Preprint ,\narXiv:2403.07974.\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim,\nand Sunghun Kim. 2024. A survey on large\nlanguage models for code generation. Preprint ,\narXiv:2406.00515.\nDavid Kamholz, Jonathan Pool, and Susan Colowick.\n2014. PanLex: Building a resource for panlingual\nlexical translation. In Proceedings of the Ninth In-\nternational Conference on Language Resources and\nEvaluation (LREC‘14) , pages 3145–3150, Reykjavik,\nIceland. European Language Resources Association\n(ELRA).\nAmir Hossein Kargaran, Ali Modarressi, Nafiseh\nNikeghbal, Jana Diesner, François Yvon, and Hin-\nrich Schütze. 2024. MEXA: Multilingual evaluation\nof english-centric LLMs via cross-lingual alignment.\nPreprint , arXiv:2410.05873.\nTakeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hit-\nomi Yanaka, and Yutaka Matsuo. 2024. On the multi-\nlingual ability of decoder-based pre-trained language\nmodels: Finding and controlling language-specific\nneurons. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers) , pages 6919–6971,\nMexico City, Mexico. Association for Computational\nLinguistics.\nMarie-Anne Lachaux, Baptiste Rozière, Lowik Chanus-\nsot, and Guillaume Lample. 2020. Unsupervised\ntranslation of programming languages. Preprint ,\narXiv:2006.03511.\nRaymond Li, Yangtian Zi, Niklas Muennighoff, Denis\nKocetkov, Chenghao Mou, Marc Marone, Christo-\npher Akiki, LI Jia, Jenny Chim, Qian Liu, and 1\nothers. 2023. Starcoder: may the source be with you!\nTransactions on Machine Learning Research .\nZehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.\n2021. Learning language specific sub-network for\nmultilingual machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 293–305, Online.\nAssociation for Computational Linguistics.\n--- Page 11 ---\nWeize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xum-\ning Hu, and Jian Wu. 2024a. Unraveling babel: Ex-\nploring multilingual activation patterns of LLMs and\ntheir applications. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 11855–11881, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nYihong Liu, Runsheng Chen, Lea Hirlimann, Ah-\nmad Dawar Hakimi, Mingyang Wang, Amir Hossein\nKargaran, Sascha Rothe, François Yvon, and Hinrich\nSchütze. 2025. On relation-specific neurons in large\nlanguage models. Preprint , arXiv:2502.17355.\nYue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and\nLi Li. 2024b. On the reliability and explainability\nof language models for program generation. ACM\nTransactions on Software Engineering and Method-\nology , 33(5):1–26.\nMichael R Lyu, Baishakhi Ray, Abhik Roychoudhury,\nShin Hwei Tan, and Patanamon Thongtanunam. 2024.\nAutomatic programming: Large language models and\nbeyond. ACM Transactions on Software Engineering\nand Methodology .\nNick Meyer and Leigh McCulloch. 2022. Keywords: A\nlist and count of keywords in programming languages.\nhttps://github.com/e3b0c442/keywords .\nAhmad Haji Mohammadkhani, Chakkrit Tantithamtha-\nvorn, and Hadi Hemmatif. 2023. Explaining\ntransformer-based code models: What do they learn?\nwhen they do not work? In 2023 IEEE 23rd Interna-\ntional Working Conference on Source Code Analysis\nand Manipulation (SCAM) , pages 96–106. IEEE.\nAaron Mueller, Yu Xia, and Tal Linzen. 2022. Causal\nanalysis of syntactic agreement neurons in multi-\nlingual language models. In Proceedings of the\n26th Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 95–109, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nNiklas Muennighoff. 2022. Sgpt: Gpt sen-\ntence embeddings for semantic search. Preprint ,\narXiv:2202.08904.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. CodeGen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474 .\nNostalgebraist. 2020. Interpreting\nGPT: The logit lens. https://www.\nlesswrong.com/posts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens .\nMatteo Paltenghi and Michael Pradel. 2021. Thinking\nlike a developer? comparing the attention of humans\nwith neural models of code. In 2021 36th IEEE/ACM\nInternational Conference on Automated Software En-\ngineering (ASE) , pages 867–879. IEEE.Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng,\nDayiheng Liu, An Yang, Xuancheng Ren, Bofei\nGao, Yibo Miao, Yunlong Feng, Zekun Wang,\nJian Yang, Zeyu Cui, Yang Fan, Yichang Zhang,\nBinyuan Hui, and Junyang Lin. 2025. CodeElo:\nBenchmarking competition-level code generation of\nllms with human-comparable elo ratings. Preprint ,\narXiv:2501.01257.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Romain Sauvestre, Tal Remez, and 1\nothers. 2023. Code Llama: Open foundation models\nfor code. arXiv preprint arXiv:2308.12950 .\nRobert W. Sebesta. 2016. Concepts of Programming\nLanguages , 11 edition. Pearson Education Limited,\nHarlow, England.\nNoam Shazeer. 2020. Glu variants improve transformer.\nPreprint , arXiv:2002.05202.\nTianyi Tang, Wenyang Luo, Haoyang Huang, Dong-\ndong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei,\nand Ji-Rong Wen. 2024. Language-specific neurons:\nThe key to multilingual capabilities in large language\nmodels. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 5701–5715, Bangkok,\nThailand. Association for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. Preprint ,\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, and 1 others. 2023b. Llama 2: Open\nfoundation and fine-tuned chat models. Preprint ,\narXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems .\nYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guan-\ndong Xu, and Hai Jin. 2022. What do they capture?\na structural analysis of pre-trained language models\nfor source code. In Proceedings of the 44th Interna-\ntional Conference on Software Engineering , pages\n2377–2388.\nWeixuan Wang, Barry Haddow, Minghao Wu, Wei\nPeng, and Alexandra Birch. 2024. Sharing matters:\nAnalysing neurons across languages and tasks in llms.\nPreprint , arXiv:2406.09265.\nChris Wendler, Veniamin Veselovsky, Giovanni Monea,\nand Robert West. 2024. Do llamas work in english?\n--- Page 12 ---\non the latent language of multilingual transformers.\nPreprint , arXiv:2402.10588.\nZhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Ji-\nasen Lu, and Yoon Kim. 2025. The semantic hub\nhypothesis: Language models share semantic repre-\nsentations across languages and modalities. In In-\nternational Conference on Learning Representations\n(ICLR) .\nWanying Xie, Yang Feng, Shuhao Gu, and Dong Yu.\n2021. Importance-based neuron allocation for multi-\nlingual neural machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 5725–5737, Online.\nAssociation for Computational Linguistics.\nFrank F Xu, Uri Alon, Graham Neubig, and Vincent Jo-\nsua Hellendoorn. 2022. A systematic evaluation of\nlarge language models of code. In Proceedings of\nthe 6th ACM SIGPLAN International Symposium on\nMachine Programming , pages 1–10.\nBiao Zhang, Ankur Bapna, Rico Sennrich, and Orhan\nFirat. 2021. Share or not? learning to schedule\nlanguage-specific capacity for multilingual transla-\ntion. In Ninth International Conference on Learning\nRepresentations 2021 .\nYiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji\nKawaguchi, and Lidong Bing. 2024. How do large\nlanguage models handle multilingualism? In The\nThirty-eighth Annual Conference on Neural Informa-\ntion Processing Systems .\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan\nRavindran, Sindhu Tipirneni, and Chandan K.\nReddy. 2022a. Xlcost: A benchmark dataset\nfor cross-lingual code intelligence. Preprint ,\narXiv:2206.08474.\nMing Zhu, Karthik Suresh, and Chandan K Reddy.\n2022b. Multilingual code snippets training for pro-\ngram translation. In Proceedings of the AAAI con-\nference on artificial intelligence , volume 36, pages\n11783–11790.\nA Impact of LAPE neuron identification\nWe show the complete version of Figure 5 in Fig-\nure 7, covering more PLs.\n--- Page 13 ---\n0 2500 5000 7500 10000 12500 150000246810\nC#\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nC++\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJava\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nHTML\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nGo\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJavaScript\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nRust\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nRuby\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nC\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPHP\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPython\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nEnglish\nAVG over other languages(a) Code Llama 7B\n0 2500 5000 7500 10000 12500 150000246810\nC#\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nC++\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJava\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nHTML\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nGo\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nJavaScript\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nRust\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nRuby\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nC\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPHP\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nPython\nAVG over other languages\n0 2500 5000 7500 10000 12500 150000246810\nEnglish\nAVG over other languages (b) Llama 3.1 8B\nFigure 7: Impact of LAPE neuron identification. X-axis: Number of shared neurons for each language. Y-axis:\nChange in PPL across languages when deactivating the primary language’s neurons (e.g., English in the lower-right\nfigure).",
  "text_length": 53155
}