{
  "id": "http://arxiv.org/abs/2506.04209v1",
  "title": "Language-Image Alignment with Fixed Text Encoders",
  "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
  "authors": [
    "Jingfeng Yang",
    "Ziyang Wu",
    "Yue Zhao",
    "Yi Ma"
  ],
  "published": "2025-06-04T17:51:56Z",
  "updated": "2025-06-04T17:51:56Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04209v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04209v1  [cs.CV]  4 Jun 2025Language-Image Alignment with Fixed Text Encoders\nJingfeng Yang1∗Ziyang Wu1∗Yue Zhao1Yi Ma1,2\n1UC Berkeley2The University of Hong Kong\nAbstract\nCurrently, the most dominant approach to establishing language-image alignment\nis to pre-train text and image encoders jointly through contrastive learning, such as\nCLIP and its variants. In this work, we question whether such a costly joint training\nis necessary. In particular, we investigate if a pre-trained fixed large language\nmodel (LLM) offers a good enough text encoder to guide visual representation\nlearning. That is, we propose to learn Language-Image alignment with a Fixed\nText encoder (LIFT) from an LLM by training only the image encoder. Somewhat\nsurprisingly, through comprehensive benchmarking and ablation studies, we find\nthat this much simplified framework LIFT is highly effective and it outperforms\nCLIP in most scenarios that involve compositional understanding and long captions,\nwhile achieving considerable gains in computational efficiency. Our work takes\na first step towards systematically exploring how text embeddings from LLMs\ncan guide visual learning and suggests an alternative design choice for learning\nlanguage-aligned visual representations. Our code and checkpoints are available at\nhttps://github.com/Jingfeng0705/LIFT .\n1 Introduction\nContrastive pre-training on massive collections of text-image pairs has recently emerged as a powerful\nparadigm for learning language-aligned visual representations and demonstrates strong performance\nin applications such as vision-language models (VLMs) [ 29,30]. Representative works such as\nCLIP [ 43] and SigLIP [ 59] align text and image embeddings in a shared space by training separate\ntext and image encoders on paired data with a contrastive loss. This approach is shown to be highly\neffective — most VLMs today employ such pre-trained image encoders due to their language-aligned\nvisual representations.\nDespite its wide adoption, contrastive pre-training on text-image pairs has some widely known\nlimitations. For instance, training both encoders from scratch makes the method computationally\nexpensive [ 18,37]. This issue is further exacerbated by the large batch sizes and the substantial\namounts of training data demanded by CLIP. Moreover, CLIP’s text and image encoders struggle to\naccurately encode compositional information, including word order (in texts), spatial locations (in\nimages), object–attribute associations (both), and object-object relations (both) [ 16,35,48]. Prior\nstudies attribute this limitation to the fact that contrastive pre-training on general-purpose retrieval\ndatasets incentivizes CLIP’s encoders trained from scratch to adopt a shortcut strategy that suppresses\n(i.e., discards) features related to compositional information [13, 45].\nCrucially, a core assumption held by dominant contrastive approaches lies at the heart of these\nlimitations: optimal language-image alignment requires jointly training text and image encoders from\nscratch. In this work, we question the necessity of this joint training and prove that large language\nmodels (LLMs) already provide good enough text embeddings to guide visual representation learning.\nConcretely, we use a pre-trained text encoder fine-tuned on an LLM to embed texts offline and solely\ntrain the image encoder to align visual representations with the text embeddings. We name this\napproach Language- Image alignment with a Fixed Text encoder (LIFT) . The name “LIFT” also\nsuggests that it aligns raw visual inputs with their corresponding higher-level semantics. Fig. 2\nillustrates the overall pipeline of the proposed framework.\nPreprint. Under review.\n--- Page 2 ---\n❄\nA vintage red car in front of a vintage army prop plane \n     A vintage red car behind  a vintage army prop plane \n❄\nThree  teddy bears  and a stuffed cat  on a stone wall \n     A stuffed  teddy bear  and three  cats on a stone wall \n     Impressionism \n     Post-Impressionism      LIFT : The dog is looking up\n     CLIP : The dog is looking right      Top-right  corner doesn’t have any fruit \n     Bottom-left corner doesn’t have any fruit \nWhich direction is the dog looking at? Which corner doesn't have any fruits? What style is depicted? \nFigure 1: The qualitative comparisons between LIFT and CLIP [ 43]. The first line shows the caption or option\nselected by LIFT, and the second line shows the one selected by CLIP. In every case, LIFT selects the correct\none, while CLIP does not. We observe that LIFT compensates for CLIP’s shortcomings in tasks involving\ncompositional information (e.g., spatial locations, object-attribute associations, object-object relations).\nTo investigate whether, when, and why LIFT might offer advantages over vanilla CLIP, we perform\nextensive benchmarking and ablation studies to address four fundamental questions:\n•Across tasks evaluating different model capabilities, on which does LIFT demonstrate strengths\ncompared to CLIP , and on which does it fall short?\nAnswer : LIFT outperforms CLIP by an average accuracy gain of 7.4% across seven compositional\nunderstanding tasks and also leads CLIP on five out of six LLaV A [ 29,30] downstream tasks,\nall driven by its superior ability to encode compositional information. It also matches CLIP’s\nzero-shot retrieval performance. Some qualitative results are shown in Fig. 1.\n•On which types of training data does LIFT outperform CLIP , and why?\nAnswer : When trained on short, web-scraped captions, CLIP has a slight edge over LIFT on\nthree zero-shot retrieval tasks and one LLaV A downstream task. However, all of these advantages\ntransfer to LIFT when both are trained on long, synthetic captions. We attribute LIFT’s better\nperformance to its robustness against the inverse effect induced by synthetic captions.\n•What design choices of LLM-based text encoders enable better language-image alignment?\nAnswer : Vanilla LLMs generally perform poorly as the text encoder for LIFT. Contrastive fine-\ntuning targeted at improving text encoding is typically necessary, whereas additional embedding\nextraction modules are not.\n•Given its powerful LLM-based text encoder, can LIFT simplify some of the design choices in\nmainstream contrastive language-image alignment approaches?\nAnswer : We find that a simpler yet efficient cosine similarity loss can substitute for the contrastive\nloss while achieving comparable performance on the compositional understanding tasks and\nLLaV A downstream tasks.\n2\n--- Page 3 ---\nImage Encoder \n(ViT) LLM-based Text Encoder \n(NV-Embed-V2) Text \nEmbedding \nImage \nEmbedding Alignment \nObjective Offline Calculation \n❄\n “A row of brightly \npainted Victorian \nhouses set against a \nbackdrop of modern \nhigh-rise buildings \nunder a clear, blue sky.” \nMLP \nFigure 2: The pipeline of LIFT, which adopts a dual-encoder architecture similar to CLIP [ 43]. LIFT uses an\nLLM-based text encoder ftextto pre-compute the embedding zTfor each text sample Toffline . During training,\nwe solely update the image encoder fimg\nθand the projection head fhead\nϕto align image embeddings with the\npre-computed text embeddings by optimizing an alignment objective.\n2 Related Work\n2.1 Language-Image Representation Learning\nCurrently, language-image alignment is typically achieved through contrastive learning. Seminal\nworks CLIP [ 43] and ALIGN [ 20] pioneer large-scale pre-training on text-image pairs to learn joint\nembeddings, enabling zero-shot transfer to various downstream tasks. These approaches rely on\na dual-encoder architecture in which text and image encoders are jointly trained from scratch to\nmaximize the alignment (e.g., measured by cosine similarity) between paired samples’ embeddings\nwhile minimizing the alignment between non-matching pairs’ embeddings. Subsequently, SigLIP\n[59] improves training stability using a sigmoid contrastive loss function. However, these methods\ndemand significant computational resources to train both encoders.\nRecently, SuperClass [ 18] and CatLIP [ 37] have explored an alternative training paradigm without\ntext encoders. They extract class labels from captions and cast language-image alignment as a\nclassification problem by training image encoders with a binary cross-entropy loss. However, since\nthese approaches ignore word order in captions, the resulting representations behave like a bag-of-\nwords and lack compositional understanding.\nIn this work, we aim to simplify the language-image alignment pipeline while avoiding the shortcom-\nings of existing text encoder-free approaches.\n2.2 The Limitations of CLIP\nIt is well known that CLIP lacks compositional understanding, largely because contrastive pre-training\non general-purpose retrieval datasets encourages CLIP’s encoders to adopt a shortcut strategy that\nignores compositional information [ 13,58]. To tackle this issue, [ 58] incorporates designed negative\nsamples during training; [ 49,50] combine features from multiple image encoders. More recently,\n[36,39,41,51] merge self-supervised learning methods with contrastive learning, leading to image\nencoders with stronger vision-centric capabilities. Closer to our study, [ 5,47] replace CLIP’s text\nencoder with LLMs and jointly train them with image encoders, achieving better scaling behavior\nand improved compositional understanding. However, by introducing additional modifications to\ntheir training pipelines, they fail to isolate the use of unaltered LLM text embeddings and therefore\ndo not systematically study their effect on language-image alignment.\n3\n--- Page 4 ---\nCLIP(77) CLIP(128) LIFT (77,128)100200300400500\n4659\n27134155\n105425464\n368FLOPs (G)\nViT-S\nViT-B\nViT-L\nFigure 3: The estimated training FLOPs per text-\nimage sample for CLIP [ 43] and LIFT trained with\naverage per-batch max caption length 77 and 128.CLIP(77) CLIP(128) LIFT (77,128)1020304050\n14 15132022\n1936 37\n33Memory Usage (GB)\nViT-S\nViT-B\nViT-L\nFigure 4: The estimated H800 GPU memory usage\nfor CLIP and LIFT trained with average per-batch max\ncaption length 77 and 128. The batch size is 1024.\nFurthermore, [ 32,33,54] reveal that CLIP yields suboptimal zero-shot performance when trained\non full-length long captions, which are typically synthesized by VLMs to include detailed object\ndescriptions. [ 33] conjectures that CLIP’s text encoder is distracted by the syntactic similarity of\nsynthetic captions and fails to attend to semantically meaningful content. Truncation strategies, such\nas text shearing [ 33] and sub-caption sampling [ 60], can bring empirical improvements. However,\nthey inevitably sacrifice the rich information that long captions provide.\nIn summary, current approaches remain insufficient in resolving both issues. We attempt to tackle\nboth by questioning a prevailing setup in contrastive language-image alignment.\n2.3 Text Embedding Models\nText embedding models are widely used to extract semantic embeddings of texts and are crucial in\ntasks such as Retrieval-Augmented Generation (RAG) [ 24]. Traditional embedding models such as\nBERT [ 9] and T5 [ 44] are typically trained with bidirectional attention mechanisms. Recently, LLM-\nbased embedding models such as LLM2Vec [ 3], E5-Mistral [ 55], SFR-Embedding-Mistral [ 2], and\nNV-Embed-V2 [ 23] aim to leverage the rich semantics of auto-regressive LLMs and have emerged as\npowerful embedding models on Massive Text Embedding Benchmark (MTEB) [ 40]. In particular,\nNV-Embed-V2 [ 23] introduces a novel latent attention layer and a two-stage contrastive instruction\ntuning, making the resulting text embeddings rich yet distinct between texts with different semantic\nmeanings. Hence, it is adopted as the text encoder of LIFT.\n3 Method\nAs illustrated in Fig. 2, LIFT adopts a dual-encoder architecture similar to CLIP [ 43]. Let (T, I)\ndenotes a text-image pair, where T∈ VLis a sequence of Ltokens from a vocabulary Vand\nI∈RC×H×W. We apply a pre-trained and frozen LLM-based text encoder ftextwith an output\ndimension Dto extract the text embedding\nzT=ftext(T)∈R1×D.\nWe parameterize an image encoder neural network fimg\nθ:RC×H×W→R1×dand a projection head\nfhead\nϕ:R1×d→R1×D.fimg\nθis implemented by a ViT [ 10] of width dandfhead\nϕis a 2-layer MLP. Let\nzI=fhead\nϕ◦fimg\nθ(I)∈R1×D\nbe the final image embedding. We use CLIP’s contrastive loss1[43,52] as our objective, which is\n4\n--- Page 5 ---\nLarge four sided clock hangs on  the corner of the building \nA large four sided clock leans against  the corner of the building \nA sandwich is sitting in a white  plate  on top of a wood table \nA sandwich is sitting on a wood plate  on top of a white table \nFigure 5: The original captions (top) and their negative counterparts (bottom) from two SugarCrepe [ 16] tasks:\nreplace relation (left) and swap attribute (right).\ncalculated between a batch of normalized zTandzIas:\nLcontrastive =−1\n2BBX\ni=1\"\nlogexp(zT\ni·zI\ni/τ)PB\nj=1exp(zT\ni·zI\nj/τ)+ logexp(zI\ni·zT\ni/τ)PB\nj=1exp(zI\ni·zT\nj/τ)#\n.\nOffline Text Embeddings Generation . Since we don’t optimize ftext, the entire text embedding\nprocess can be performed offline . Specifically, before training LIFT, we embed all the captions in our\ndataset once, allowing subsequent trainings to reuse the pre-computed caption embeddings without\nthe text encoder. LIFT utilizes NV-Embed-V2 [ 23] asftext. As a point of reference, eight H800 GPUs\ncan embed 100M captions per day at bfloat16 precision.\nAs shown in Fig. 3 and Fig. 4, embedding texts offline leads to significant computational and memory\nefficiency. Concretely, given average per-batch max caption token length n, the FLOPs and memory\nfootprint of CLIP scale with O(n2)complexity, whereas LIFT achieves O(1)amortized complexity.\nWe also quantitatively benchmark CLIP and LIFT on both short ( n= 77 ) and long ( n= 128 )\ncaptions. On average, LIFT reduces FLOPs by 25.5% for short captions and 35.7% for long ones,\nwhile lowering memory usage by 6.8% and12.6% . The calculation details of FLOPs and memory\nusage are attached in Appendix A.4.\n4 Experiments\nExperimental Settings . We train both LIFT and CLIP [ 43] (implemented by OpenCLIP [ 6]) using a\nViT-B/16 [ 10] vision backbone on a dataset containing 400 million text-image pairs. Each image has\ntwo types of captions: a short, web-scraped caption from DataComp-1B [ 12], and a long, synthetic\ncaption from Recap-DataComp-1B [ 25]. Their respective max caption token lengths are set to 77 and\n323. To ensure a fair comparison, both LIFT and CLIP are trained using the same hyperparameters:\n500 warmup steps, a weight decay of 0.2, a learning rate of 1e-3 with a cosine schedule, and a batch\nsize of 16,384. We use an input resolution of 224 ×224. The largest experiment uses 1.28 billion\nsamples and is trained on eight H800 GPUs over 12 days.\nIn the following sections, we systematically address the four questions raised in Sec. 1.\n4.1 On which tasks does LIFT offer advantages, and on which does it fall short?\nCompositional Understanding . Compositional understanding is a known limitation of CLIP. We\nevaluate it using seven SugarCrepe [ 16] tasks. As shown in Fig. 5, for each caption, SugarCrepe\ngenerates a negative caption by add,replace , orswap anobject ,attribute , orrelation\nin the original caption. Models are asked to identify the correct caption based on caption-image\ncosine similarity.\n1In practice (see Sec. 4.4), we find a simple cosine similarity loss also leads to reasonably strong performance.\n5\n--- Page 6 ---\nMethod DatasetSample\nSeenAdd Replace Swap\nObj Att Obj Att Rel Obj Att\nOpenCLIP DataComp 1.28B 82.3 73.7 91.7 79.4 61.2 59.6 56.9\nLIFT DataComp 1.28B 89.0 86.1 93.2 86.0 70.6 64.1 63.4\nOpenCLIP Recap 512M 77.0 73.7 88.9 80.8 63.4 62.0 76.3\nLIFT Recap 512M 88.8 92.2 92.3 88.2 76.8 66.5 72.8\nTable 1: The performance of LIFT and CLIP [ 43] on seven SugarCrepe [ 16] tasks. For each task, SugarCrepe\ngenerates negative captions by add,replace , orswap anobject ,attribute , orrelation in the\noriginal captions. We report the accuracy of each task, and the best results are bolded.\nAs shown in Table 1, when trained on the short captions from DataComp-1B, LIFT outperforms\nCLIP on all seven tasks with a 6.8% average accuracy gain; when trained on the long, synthetic\ncaptions from Recap-DataComp-1B, it leads on six tasks with a 7.9% gain. In both settings,\nLIFT achieves significant gains on add attribute ,replace attribute , and replace\nrelation tasks. These improvements are strong evidence that ftext’s auto-regressive training\nobjective avoids the compositional oversight induced by contrastive learning and enables more\naccurate modeling of object–attribute associations and object–object relations. More visualizations\ncan be found in Appendix A.2.\nAdmittedly, LIFT ’s ability to capture compositional information is not yet complete. It shows\nrelatively low accuracy on swap object andswap attribute compared to other SugarCrepe\ntasks. We further attribute this limitation to the fact that the contrastive learning objective still focuses\non aligning primarily lower-order statistics. Addressing this challenge requires exploring more refined\ninformation-theoretic measures for language-image alignment, a key direction for future work.\nLLaV A [ 29,30] Downstream Tasks . [50] shows that large multimodal models (LMMs) using CLIP\nas the vision tower inherit CLIP’s limitations in modeling certain visual patterns. We train LMMs\nusing LLaV A to examine whether LIFT transfers its superior compositional understanding to the\nLMM built on it. Vicuna-7B-V1.5 [ 7] is used as the base language model and is fine-tuned with\nLoRA [ 17]. All training hyperparameters follow the original LLaV A setup, except that we initialize\nLLaV A’s projector with the weights of our 2-layer MLP fhead\nϕ.\nMethod DatasetSample\nSeenTextVQAMMBenchMME POPE SciQA\nEN CN\nOpenCLIP DataComp 1.28B 47.9 53.3 45.9 1283.2 86.4 69.9\nLIFT DataComp 1.28B 48.9 57.8 50.6 1289.2 85.2 70.2\nOpenCLIP Recap 512M 46.2 48.4 42.0 1245.1 85.6 69.0\nLIFT Recap 512M 47.8 54.3 46.5 1341.6 85.8 69.4\nTable 2: The performance of the LMM with either LIFT or CLIP [ 43] as the vision tower on LLaV A [ 29,30]\ndownstream tasks. The results are reported for TextVQA [ 46], MMBench [ 31] (English and Chinese), MME [ 11],\nPOPE [27] (random accuracy), and ScienceQA [34] (accuracy). The best results are bolded.\nMethod DatasetSample\nSeenAR CP FP-C FP-S LR RR\nOpenCLIP DataComp 1.28B 61.3 65.9 50.3 52.6 27.1 39.1\nLIFT DataComp 1.28B 67.8 72.3 51.0 56.0 28.0 47.0\nOpenCLIP Recap 512M 58.3 63.5 45.5 43.0 26.3 32.2\nLIFT Recap 512M 60.0 70.6 50.3 51.9 28.0 40.9\nTable 3: The performance of the LMM with either LIFT or CLIP [ 43] as the vision tower on specific MM-\nBench [ 31] subtasks. Abbreviations: AR for Attribute Reasoning ; CP for Coarse Perception ; FP-\nC for Fine-grained Perception (Cross Instance) ; FP-S for Fine-grained Perception\n(Single Instance) ; LR for Logical Reasoning ; RR for Relation Reasoning . The visualiza-\ntions are provided in Appendix A.3. The best results are bolded.\n6\n--- Page 7 ---\nA white car                                              with a black grille and black wheels               is parked on a white background. \nA white background                       with the text 'STEVE MADDEEN'  in black,                  centered and in a bold font. \nA blue umbrella                                    with a design of owls and butterflies                                                           is open. \nA blue pen                                                 with a silver tip and a silver clip                      is lying on a white background. A sleek, metallic silver bicycle pump            with a black grip handle                 is isolated against a white background. \nA red colored pencil with a black tip and the word 'BIC' written in white on the side  is lying on a white background. \nCLIP : 0.57\nLIFT : 0.30\nThe Average Pairwise Cosine Similarity of 1000 Captions: \nCLIP : 24.0\nLIFT : 19.0\nPair 1 Cosine Similarity: \nCLIP : 0.59\nLIFT : 0.31\nPair 2 Cosine Similarity: \nCLIP : 0.57\nLIFT : 0.29\nPair 3 Cosine Similarity: \nFigure 6: The examples of syntactically similar but semantically different caption pairs from Recap-DataComp-\n1B [25]. The synthetic captions follow the template “ A {Adj.} {Noun} with {N. Phrase} {Verb}\n{Location} ”. CLIP’s [ 43] text encoder often assigns higher scores to caption pairs with similar syntax, while\nLIFT better captures semantic differences and assigns lower ones. The scores are calculated based on the\nembeddings from LIFT’s ftextand CLIP’s text encoder trained on 512M Recap-DataComp-1B samples.\nAs shown in Table 2, LIFT outperforms CLIP on five out of six LLaV A downstream tasks when both\nare trained on short captions, and leads on all tasks when trained on long captions. In both settings,\nLIFT shows substantial improvements on MMBench [ 31]. We further examine their performance on\nspecific MMBench (English) subtasks to identify the exact visual patterns that offer the gains. As\nshown in Table 3, LIFT achieves significant accuracy gains on fine-grained perception\n(single-instance) andrelational reasoning . The former subtask involves object\nlocalization and attribute recognition, while the latter includes identifying physical relations, all\nlargely benefiting from LIFT’s accurate encoding of compositional information. The visualizations\nare provided in Appendix A.3.\nZero-shot Classification and Retrieval . Similar to CLIP, LIFT can perform zero-shot transfer for\nimage classification and cross-modal retrieval by employing the LLM-based text encoder ftextto\nembed captions during inference. Following CLIP’s evaluation protocol, we evaluate the models on\nImageNet-1K validation set [ 8] by constructing image captions using the prompt template “ It is\na photo of {label} .” More evaluation details are provided in Appendix A.6.\nAs shown in Table 4, when trained on short captions, LIFT outperforms CLIP on two text-to-image\ntasks, while performing similarly on the remaining three tasks. When trained on long captions,\nhowever, LIFT leads CLIP by substantial margins on all these tasks, achieving an average accuracy\ngain of 11.0% . We analyze the cause of this decline in CLIP’s relative performance in Sec. 4.2.\nMethod DatasetSample\nSeenImageNetCOCO Flickr\nI2T T2I I2T T2I\nOpenCLIP DataComp 1.28B 58.4 31.0 27.2 62.9 59.6\nLIFT DataComp 1.28B 58.3 29.1 28.1 58.8 63.7\nOpenCLIP Recap 512M 34.6 25.7 26.7 56.4 57.9\nLIFT Recap 512M 43.6 34.6 36.0 69.1 72.9\nTable 4: The zero-shot performance of LIFT and CLIP [ 43] on ImageNet-1K [ 8] classification, COCO [ 28]\nImage-to-Text and Text-to-Image retrieval, and Flickr30K [ 57] Image-to-Text and Text-to-Image retrieval. For\nall tasks, we report top-1 accuracy. The best results are bolded.\n4.2 On which types of training data does LIFT outperform CLIP, and why?\nAs reported in Sec. 4.1, when trained on short captions, CLIP [ 43] has a slight edge over LIFT on\nPOPE [ 27], ImageNet-1K [ 8] zero-shot classification, and two image-to-text retrieval tasks. However,\nall of these advantages are overtaken by LIFT when both are trained on long, synthetic captions. In\nthis section, we investigate the reasons behind CLIP’s loss of performance advantages.\n7\n--- Page 8 ---\nOne contributing factor is the inverse effect [26,32], which observes that CLIP trained on full-length\nsynthetic captions yields suboptimal zero-shot performance but shows noticeable improvements as\nthe captions are progressively truncated. This effect likely stems from the homogeneous caption\nsyntax introduced by caption generators (usually fine-tuned VLMs), which can distort the original\ncaption distribution and become a “shortcut” feature [ 33,45]. Such homogeneous syntax is most\npronounced in full-length synthetic captions and weakens with increasing truncation.\nAs shown in Fig. 6, CLIP’s text encoder is misled by this shortcut feature during training from scratch.\nBy computing the average pairwise cosine similarity of 1,000 captions randomly drawn from Recap-\nDataComp-1B [ 25], we find that CLIP’s text encoder overemphasizes syntactic similarity, assigning\nhigh similarity scores to caption pairs that are syntactically similar but semantically different. In\ncontrast, LIFT employs the LLM-based encoder ftextpre-trained on large-scale data, which yields an\nembedding space more robust to syntactic homogeneity. It focuses more on semantic content and\nassigns significantly lower similarity scores to such misleading caption pairs.\nHowever, the 11.0% accuracy gap between LIFT and CLIP cannot be fully attributed to the inverse\neffect. We argue that another key factor is the expressive power of text encoders, which is especially\ncritical for handling the greater semantic complexity of long, synthetic captions. Specifically, the\nCLIP’s text encoder in our implementation has 63M parameters, whereas LIFT’s ftexthas 7B\nparameters. Despite its larger architecture, LIFT remains more efficient for long-caption processing\ndue to its use of offline embeddings.\n4.3 What design choices of LLM-based text encoders enable better language-image alignment?\nSeveral key design choices have contributed to the transformation of LLMs into state-of-the-art text\nencoders, and this section examines which of these choices are most helpful for the language-image\nalignment achieved by LIFT. [ 38,42] first show that the hidden state of end-of-sentence token <eos>\ncontains sufficient information about an input sequence and can be used as its embedding. Later\nmethods explore alternative embedding extraction strategies, as <eos> tends to bias attention toward\nnearby tokens and ignore distant ones [ 19,23]. Meanwhile, contrastive fine-tuning using instructions\ndesigned for general text embedding tasks (retrieval, clustering, etc.) has been shown to further\nimprove embedding quality [2, 22, 23].\nTo disentangle the effects of these design choices on language-image alignment, we train LIFT\nusing five representative LLMs as ftext. Mistral-7B-V0.1 [ 21] and Vicuna-7B-V0.1 [ 7] are vanilla\nLLMs without any fine-tuning or architectural changes; SFR-Embed-Mistral [ 2] and Linq-Embed-\nMistral [ 22] both apply contrastive fine-tuning; NV-Embed-V2 [ 23] represents the most evolved\nvariant, combining contrastive fine-tuning with a latent attention layer to extract embeddings. All\nmodels have 7B parameters, with a hidden size and final embedding dimension of 4096.\nAs shown in Table 5, two vanilla LLMs lag significantly behind their fine-tuned counterparts across\nall reported tasks — for instance, by an average accuracy decline of 22.8% on ImageNet-1K [ 8]\nzero-shot classification. Vanilla Mistral-7B-V0.1 even performs no better than random guessing on\nSugarCrepe’s [ 16]replace relation task. These results indicate that LLMs are not inherently\neffective as the text encoder for LIFT, and contrastive fine-tuning is necessary. On the other hand,\nall three fine-tuned models achieve comparable performance, suggesting that <eos> token alone\ncan accurately encode input captions, and that advanced embedding extraction mechanisms, such as\nNV-Embed-V2’s additional latent attention layer, may not be required.\nLLM EncoderContrastive\nFine-tuningEmbedding\nExtractionImageNetFlickr Replace\nI2T T2I Obj Att Rel\nMistral-7B-V0.1 ✗ <eos> 12.5 5.4 3.3 69.0 58.4 50.0\nVicuna-7B-V1.5 ✗ <eos> 23.8 14.1 13.2 78.1 73.1 58.8\nSFR-Embed-Mistral ✓ <eos> 39.9 44.9 42.1 87.4 82.3 70.9\nLinq-Embed-Mistral ✓ <eos> 39.8 44.2 41.6 84.8 81.5 64.2\nNV-Embed-V2 ✓ Latent Attention 40.9 42.7 44.1 87.2 81.2 67.3\nTable 5: The ablation study on the choice of LIFT’s LLM-based text encoder. The results are reported for\nImageNet-1K [ 8] classification, Flickr30K [ 57] retrieval tasks, and SugarCrepe’s [ 16]replace task. All\nmodels are trained on 128M samples from DataComp-1B [ 12]. The best results are bolded, and the second-best\nresults are underlined.\n8\n--- Page 9 ---\n4.4 Can LIFT simplify some of the design choices in mainstream contrastive language-image\nalignment approaches?\nA Simple Cosine Similarity Loss . CLIP [ 43] and its variants have demonstrated the effectiveness of\ncosine similarity for language-image alignment. To avoid mode collapse (i.e., identical outputs from\ntext and image encoders regardless of the inputs), CLIP employs a contrastive InfoNCE [ 52] loss\nbased on pairwise cosine similarities. However, this approach is computationally intensive, with both\nFLOPs and memory scaling asymptotically as O(B2)with batch size B. It also requires a large batch\nsize to ensure sufficient negative samples. SigLIP [ 59] introduces a chunked strategy that relaxes the\nlarge-batch requirement, yet its FLOPs and memory still grow quadratically with local batch size.\nSince the embedding space of ftextis fixed, mode collapse is no longer a concern. This motivates\nus to explore whether a simple cosine similarity loss, computed solely on positive text-image pairs\nwithout involving negative pairs, can be effective as well. Specifically, for a batch of size B, we\noptimize\nLcosine =1\nBBX\ni=1(1−zT\ni·zI\ni),\nwhere zT=ftext(T)andzI=fhead\nϕ◦fimg\nθ(I)is a normalized embedded text-image pair in the\nbatch. This simple loss has O(B)FLOPs and memory complexity with respect to batch size B\nand removes the reliance on negative samples, thereby easing the batch size constraint. [ 4,14] also\nexplore conceptually similar cosine similarity losses, but they employ more sophisticated techniques\nto address mode collapse.\nLoss DatasetSample\nSeenImageNetFlickr Add MMBench\nI2T T2I Obj Att EN CN\nContrastive DataComp 426M 45.5 45.8 51.9 82.4 84.4 54.0 43.4\nCosine Sim DataComp 426M 26.8 10.2 19.5 85.4 74.0 53.4 43.0\nContrastive Recap 512M 43.6 69.1 72.9 88.8 92.2 54.3 46.5\nCosine Sim Recap 512M 38.8 58.8 68.9 86.9 88.2 57.5 50.3\nTable 6: The ablation study on the choice of LIFT’s loss function. The results are reported for ImageNet-1K [ 8]\nclassification, Flickr30K [ 57] retrieval tasks, SugarCrepe’s [ 16]Add task, and MMBench [ 31] (English and\nChinese). The best results are bolded.\nAs shown in Table 6, the simple cosine similarity loss performs comparably to the contrastive loss on\nthe compositional understanding tasks and LLaV A [ 29,30] downstream tasks. Notably, when trained\non long captions, LIFT using the simple cosine similarity loss outperforms its contrastive loss variant\non both English and Chinese MMBench [ 31] by non-trivial margins. However, it suffers significant\nperformance drops in the zero-shot retrieval tasks, particularly when trained on short, web-scraped\ncaptions. The simple cosine similarity loss shows an accuracy decline of 18.7% on ImageNet-1K [ 8]\nand 34.0% on Flickr30K [ 57]. We attribute this performance gap to the contrastive loss’s use of\nnegative samples, which encourages more discriminative representations that benefit classification\nand retrieval tasks.\n5 Conclusion\nIn this paper, we question a core assumption held by dominant language-image alignment approaches\nlike CLIP [ 43] — that text and image encoders should be jointly trained from scratch to achieve\noptimal language-image alignment. We present LIFT, which pre-computes fixed text embeddings\nfrom LLMs and solely trains the image encoder. Comprehensive benchmarking and ablation studies\nconfirm that LIFT outperforms CLIP in key scenarios that involve compositional understanding and\nlong captions. Further experiments show that contrastive fine-tuning is crucial for LIFT’s LLM-based\ntext encoder to generate effective text embeddings and that LIFT also enables the use of a simpler\nyet effective loss function. Our work initiates a systematic exploration of how text embeddings\nfrom LLMs can guide visual representation learning and opens a new avenue for language-image\nalignment.\n9\n--- Page 10 ---\nReferences\n[1] AI@Meta. Llama 3 model card. 2024.\n[2]R. M. amd Ye Liu, S. R. Joty, C. Xiong, Y . Zhou, and S. Yavuz. Sfr-embedding-mistral:enhance text\nretrieval with transfer learning. Salesforce AI Research Blog, 2024.\n[3] P. BehnamGhader, V . Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\nlanguage models are secretly powerful text encoders, 2024.\n[4] X. Chen and K. He. Exploring simple siamese representation learning, 2020.\n[5]Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu,\nY . Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic\ntasks, 2024.\n[6]M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt,\nand J. Jitsev. Reproducible scaling laws for contrastive language-image learning. In 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , page 2818–2829. IEEE, June 2023.\n[7]W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez,\nI. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\n[8]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 248–255, 2009.\n[9]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding, 2019.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers\nfor image recognition at scale, 2021.\n[11] C. Fu, P. Chen, Y . Shen, Y . Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y . Wu, and R. Ji.\nMme: A comprehensive evaluation benchmark for multimodal large language models, 2024.\n[12] S. Y . Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh,\nJ. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V . Ramanujan, Y . Bitton, K. Marathe, S. Mussmann,\nR. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi,\nR. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y . Carmon, V . Shankar, and L. Schmidt. Datacomp: In search\nof the next generation of multimodal datasets, 2023.\n[13] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann. Shortcut\nlearning in deep neural networks. Nature Machine Intelligence , 2(11):665–673, Nov. 2020.\n[14] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D.\nGuo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new\napproach to self-supervised learning, 2020.\n[15] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A.\nHendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc,\nA. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal\nlarge language models, 2022.\n[16] C.-Y . Hsieh, J. Zhang, Z. Ma, A. Kembhavi, and R. Krishna. Sugarcrepe: Fixing hackable benchmarks\nfor vision-language compositionality. In Thirty-Seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track , 2023.\n[17] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\nadaptation of large language models, 2021.\n[18] Z. Huang, Q. Ye, B. Kang, J. Feng, and H. Fan. Classification done right for vision-language pre-training,\n2024.\n[19] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. Perceiver: General perception\nwith iterative attention, 2021.\n[20] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. V . Le, Y . Sung, Z. Li, and T. Duerig. Scaling\nup visual and vision-language representation learning with noisy text supervision, 2021.\n10\n--- Page 11 ---\n[21] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang,\nT. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n[22] J. Kim, S. Lee, J. Kwon, S. Gu, Y . Kim, M. Cho, J. yong Sohn, and C. Choi. Linq-embed-mistral:elevating\ntext retrieval with improved gpt data through task-specific control and quality refinement. Linq AI Research\nBlog, 2024.\n[23] C. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. Nv-embed: Improved\ntechniques for training llms as generalist embedding models, 2025.\n[24] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih,\nT. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks,\n2021.\n[25] X. Li, H. Tu, M. Hui, Z. Wang, B. Zhao, J. Xiao, S. Ren, J. Mei, Q. Liu, H. Zheng, Y . Zhou, and C. Xie.\nWhat if we recaption billions of web images with llama-3?, 2024.\n[26] X. Li, Z. Wang, and C. Xie. An inverse scaling law for clip training, 2023.\n[27] Y . Li, Y . Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large\nvision-language models, 2023.\n[28] T.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick,\nand P. Dollár. Microsoft coco: Common objects in context, 2015.\n[29] H. Liu, C. Li, Y . Li, and Y . J. Lee. Improved baselines with visual instruction tuning, 2024.\n[30] H. Liu, C. Li, Q. Wu, and Y . J. Lee. Visual instruction tuning, 2023.\n[31] Y . Liu, H. Duan, Y . Zhang, B. Li, S. Zhang, W. Zhao, Y . Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin.\nMmbench: Is your multi-modal model an all-around player?, 2024.\n[32] Y . Liu, X. Li, Z. Wang, B. Zhao, and C. Xie. Clips: An enhanced clip framework for learning with synthetic\ncaptions, 2024.\n[33] Y . Liu, K. Wang, W. Shao, P. Luo, Y . Qiao, M. Z. Shou, K. Zhang, and Y . You. Mllms-augmented\nvisual-language representation learning, 2024.\n[34] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to\nexplain: Multimodal reasoning via thought chains for science question answering, 2022.\n[35] Z. Ma, J. Hong, M. O. Gul, M. Gandhi, I. Gao, and R. Krishna. Crepe: Can vision-language foundation\nmodels reason compositionally?, 2023.\n[36] K.-K. Maninis, K. Chen, S. Ghosh, A. Karpur, K. Chen, Y . Xia, B. Cao, D. Salz, G. Han, J. Dlabal,\nD. Gnanapragasam, M. Seyedhosseini, H. Zhou, and A. Araujo. Tips: Text-image pretraining with spatial\nawareness, 2025.\n[37] S. Mehta, M. Horton, F. Faghri, M. H. Sekhavat, M. Najibi, M. Farajtabar, O. Tuzel, and M. Rastegari.\nCatlip: Clip-level visual recognition accuracy with 2.7x faster pre-training on web-scale image-text data,\n2024.\n[38] J. X. Morris, W. Zhao, J. T. Chiu, V . Shmatikov, and A. M. Rush. Language model inversion, 2023.\n[39] N. Mu, A. Kirillov, D. Wagner, and S. Xie. Slip: Self-supervision meets language-image pre-training,\n2021.\n[40] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark, 2023.\n[41] M. F. Naeem, Y . Xian, X. Zhai, L. Hoyer, L. V . Gool, and F. Tombari. Silc: Improving vision language\npretraining with self-distillation, 2023.\n[42] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy,\nJ. Heidecke, P. Shyam, B. Power, T. E. Nekoul, G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu,\nM. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng. Text and code embeddings by\ncontrastive pre-training, 2022.\n11\n--- Page 12 ---\n[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language\nsupervision, 2021.\n[44] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a unified text-to-text transformer, 2023.\n[45] J. Robinson, L. Sun, K. Yu, K. Batmanghelich, S. Jegelka, and S. Sra. Can contrastive learning avoid\nshortcut solutions?, 2021.\n[46] A. Singh, V . Natarajan, M. Shah, Y . Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa\nmodels that can read, 2019.\n[47] A. Stone, H. Soltau, R. Geirhos, X. Yi, Y . Xia, B. Cao, K. Chen, A. Ogale, and J. Shlens. Learning visual\ncomposition through improved semantic guidance, 2025.\n[48] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground: Probing\nvision and language models for visio-linguistic compositionality, 2022.\n[49] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, Z. Wang,\nR. Fergus, Y . LeCun, and S. Xie. Cambrian-1: A fully open, vision-centric exploration of multimodal llms,\n2024.\n[50] S. Tong, Z. Liu, Y . Zhai, Y . Ma, Y . LeCun, and S. Xie. Eyes wide shut? exploring the visual shortcomings\nof multimodal llms, 2024.\n[51] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans,\nL. Beyer, Y . Xia, B. Mustafa, O. Hénaff, J. Harmsen, A. Steiner, and X. Zhai. Siglip 2: Multilingual\nvision-language encoders with improved semantic understanding, localization, and dense features, 2025.\n[52] A. van den Oord, Y . Li, and O. Vinyals. Representation learning with contrastive predictive coding, 2019.\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need, 2023.\n[54] A. J. Wang, K. Q. Lin, D. J. Zhang, S. W. Lei, and M. Z. Shou. Too large; data reduction for vision-language\npre-training, 2023.\n[55] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large\nlanguage models, 2024.\n[56] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought\nprompting elicits reasoning in large language models, 2023.\n[57] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descriptions. Transactions of the Association for\nComputational Linguistics , 2:67–78, 2014.\n[58] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models be-\nhave like bags-of-words, and what to do about it? In International Conference on Learning Representations ,\n2023.\n[59] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training, 2023.\n[60] K. Zheng, Y . Zhang, W. Wu, F. Lu, S. Ma, X. Jin, W. Chen, and Y . Shen. Dreamlip: Language-image\npre-training with long captions, 2024.\n12\n--- Page 13 ---\nA Appendix\nA.1 Limitations\nAs discussed in the experiments section, LIFT ’s ability to capture compositional information is\nnot yet complete. It shows relatively low accuracy on swap object andswap attribute\ncompared to other SugarCrepe [ 16] tasks. We attribute this limitation to the fact that contrastive\nlearning objectives still focus on aligning primarily lower-order statistics. Addressing this challenge\nrequires exploring more refined information-theoretic measures for language-image alignment, which\nhighlights a promising avenue for future research.\nAlso, due to computational constraints, we are unable to evaluate the scalability of LIFT beyond 1.28\nbillion training samples. We acknowledge that CLIP [ 43] and its variants may exhibit more favorable\nscaling behavior, as they jointly train both text and image encoders, whereas LIFT keeps its text\nencoder frozen. Prior studies have shown that selectively unfreezing the last four layers of LLMs\ncan substantially improve the scalability of image encoders without incurring heavy computational\ncosts [ 5,47]. How to efficiently fine-tune LLMs within mainstream language-image alignment\npipelines remains an important direction for future work.\nA.2 More Visualizations from SugarCrepe [16]\nWe present the visual results from evaluating LIFT and CLIP [ 43] on the seven compositional\nunderstanding tasks from SugarCrepe. For each caption, SugarCrepe generates a challenging negative\ncaption by add,replace , orswap anobject ,attribute , orrelation in the original\ncaption. Models are asked to identify the correct caption based on caption-image cosine similarity.\nBoth models use a ViT-B/16 [ 10] backbone and are trained on 1.28B samples from DataComp-\n1B [12]. Each subfigure shows the captions selected by LIFT (top) and CLIP (bottom). In every case,\nLIFT selects the correct caption, while CLIP does not.\nA man  that is skiing down a snowy field \nA man and a woman  are skiing down a snowy field \nA hand adding a cherry to some small tarts \nA hand adding a cherry to some small tarts and macarons \nAn antelope  is eating grass in between two zebra \nA giraffe is eating grass with an antelope  in between two zebra \nA baseball player with a bat  on the field \nA baseball player with a bat and a mitt on the field \nFigure A1: The visualizations of the add object task from SugarCrepe [ 16]. Each subfigure shows the\ncaptions selected by LIFT (top) and CLIP [ 43] (bottom). In every case, LIFT selects the correct caption, while\nCLIP does not.\n13\n--- Page 14 ---\nTwo airplanes  flying in the air near one another \nTwo dotted airplanes  flying in the air near one another \nA vase  with a handle has wild flowers in it \nA cracked vase  with a handle has wild flowers in it \nThe inside view  of a large decorated church \nA dimly-lit inside view  of a large decorated church \nOne tray of rice and a tray  of fruits and veggies \nOne tray of rice and a wooden tray  of fruits and veggies \n(a)Add Attribute\nThe cross country skiers  are enjoying their run \nThe snowboarders  are enjoying their run \nA highway with a green  exit sign  and a yellow traffic sign \nA highway with a green rest stop  and a yellow traffic sign \nA moped and utility truck next to a small building \nA moped and utility truck next to a small skyscraper \nA roasting pan full with apples, carrots, potatoes , and meat \nA roasting pan full with apples, carrots, onions , and meat \n(b)Replace Object\nFigure A2: The visualizations of the add attribute andreplace object tasks from SugarCrepe [ 16].\nEach subfigure shows the captions selected by LIFT (top) and CLIP [ 43] (bottom). In every case, LIFT selects\nthe correct caption, while CLIP does not.\n14\n--- Page 15 ---\nA lime green room  with old style furnishings and curtains \nA red room with old style furnishings and curtains \nA two tier cake  with some frosting and strawberries in the middle \nA three tier cake  with some frosting and strawberries in the middle \nA close up of many drinks  in a fridge \nA far shot of many drinks  in a fridge \nA big herd of cows walking down a road in a row with green tags  on their ears \nA big herd of cows walking down a road in a row with yellow tags  on their ears \n(a)Replace Attribute\nA baseball player throws a pitch while others watch  from the dugout \nA baseball player throws a pitch while others are cheering  from the dugout \nA hand is holding  a carrot for a llama to chew \nA hand is dropping  a carrot for a llama to chew \nA sign with the word \"Pub\" hangs above  several liquor bottles \nA sign with the word \"Pub\" hangs behind  several liquor bottles \nThere is a white horse pulling a trolley behind it \nThere is a white horse pulling a trolley in front of it \n(b)Replace Relation\nFigure A3: The visualizations of the replace attribute andreplace relation tasks from Sugar-\nCrepe [ 16]. Each subfigure shows the captions selected by LIFT (top) and CLIP [ 43] (bottom). In every case,\nLIFT selects the correct caption, while CLIP does not.\n15\n--- Page 16 ---\nA bright kitchen with tulips  on the table and plants  by the window \nA bright kitchen with plants  on the table and tulips  by the window \nCow  eating grass while a bull  watches nearby \nBull eating grass while a cow  watches nearby \nA sidewalk  next to the outdoor sitting area  of a restaurant \nAn outdoor sitting area  next to the sidewalk  of a restaurant \nA close up of a sandwich  with a drink  in the back \nA close up of a drink  with a sandwich  in the back \n(a)Swap Object\nA group of adult elephants  and one baby elephant  drinking water \nA group of baby elephants  and one adult elephant  drinking water \nA bus  and a few cars  are riding in the rain \nA car  and a few buses  are riding in the rain \nA living room with a white carpet  and black furniture \nA living room with black carpet  and white furniture \nA black and white dog  laying in the grass with a frisbee \nA dog  laying in the grass with a black and white frisbee \n(b)Swap Attribute\nFigure A4: The visualizations of the swap object andswap attribute tasks from SugarCrepe [ 16].\nEach subfigure shows the captions selected by LIFT (top) and CLIP [ 43] (bottom). In every case, LIFT selects\nthe correct caption, while CLIP does not.\n16\n--- Page 17 ---\nA.3 More Visualizations from MMBench [31]\nWe present the visual results from evaluating LIFT and CLIP [ 43] on five MMBench subtasks that\nspecifically test models’ compositional understanding. All questions are in the form of multiple-\nchoice. Both models use a ViT-B/16 [ 10] backbone and are trained on 1.28B samples from DataComp-\n1B [12]. Each subfigure shows the options selected by LIFT (highlighted in orange) andCLIP\n(highlighted in cyan) . In every case, LIFT selects the correct option, while CLIP does not.\nWhere is the cat? \nRoughly how much of the picture is \noccupied by the cat in the picture? \nA: 0.1 \nB: 0.4 \nC: Less than 20% \nD: More than 80% \nA: Top-right \nB: Top-left \nC: Bottom-left \nD: Bottom-right \nWhere is the dog? \nA: Bottom-left \nB: Bottom-right \nC: Top-right \nD: Top-left \nFigure A5: The visualizations of the object localization subtask from MMBench [ 31]. Each subfigure\nshows the options selected by LIFT andCLIP [43]. In every case, LIFT selects the correct option, while CLIP\ndoes not.\n17\n--- Page 18 ---\nWhich property do these two \nobjects have in common? \nWhat is the shape of this \nobject? \nA: Oval \nB: Heart \nC: Star \nD: Hexagon A: Yellow \nB: Salty \n(a)Attribute Recognition\nWhere is the sheep? \nWhich option describes the object \nrelationship in the image correctly? \nA: The cat is at the edge of the sink \nB: The book is beside the cat \nC: The sink contains the cat \nD: The cat is beside the microwave \nA: The sheep is on the left of the car \nB: The sheep is behind the car \nC: The sheep is in the front of the car \nD: The sheep is on the right of the car \n(b)Physical Relation\nFigure A6: The visualizations of the attribute recognition andphysical relation subtasks\nfrom MMBench [ 31]. Each subfigure shows the options selected by LIFT andCLIP [43]. In every case, LIFT\nselects the correct option, while CLIP does not.\n18\n--- Page 19 ---\nWhich of the following statements \nmatch the image? \nA: A yellow triangle is to the right of a blue shape \nB: A triangle is to the right of a blue rectangle \nC: A magenta triangle is to the left of a blue rectangle \nD: A magenta rectangle is to the left of a magenta shape \nWhich of the following statements \nmatch the image? \nA: A cyan rectangle is below a red shape \nB: A yellow triangle is below a red rectangle \nC: A cross is above a cyan shape \nD: A rectangle is above a cyan shape \n(a)Spatial Relation\nHow many trucks are in this \nphoto? \nA: 2 apples and 1 bananas \nB: 3 apples and 1 bananas \nC: 3 apples and 2 bananas \nD: 1 apples and 1 bananas \nA: Seven \nB: Eight \nC: Six \nD: Five \nHow many apples are there in \nthe image? And how many \nbananas are there? \n(b)Counting\nFigure A7: The visualizations of the spatial relation andcounting subtasks from MMBench [ 31].\nEach subfigure shows the options selected by LIFT andCLIP [43]. In every case, LIFT selects the correct\noption, while CLIP does not.\n19\n--- Page 20 ---\nA.4 The Calculation Details of FLOPs and Memory Usage\nAlgorithm 1 FLOP S OF LANGUAGE TRANSFORMERS [53]\nRequire:\nnctx(average per-batch max caption token length), nvocab (vocab size), dmodel (model width),\nnheads (attention head number), dkey(key dimension), dff(feed-forward width),\nnlayers (layer number).\nEnsure: Ftotal\nEmbedding FLOPs\n1:Femb←2nctxnvocabdmodel\nAttention FLOPs (per layer)\n2:Fqkv←2nctx(3dmodel) (dkeynheads)\n3:Fqk←2n2\nctx(dkeynheads)\n4:Fsoft←3nheadsn2\nctx\n5:Fred←2n2\nctx(dkeynheads)\n6:Fproj←2nctx(dkeynheads)dmodel\n7:Fattn←Fqkv+Fqk+Fsoft+Fred+Fproj\nFeed-forward FLOPs (per layer)\n8:Fff←4nctx(dmodeldff)\nTotal FLOPs\n9:Ftotal←Femb+nlayers(Fattn+Fff)\n10:return 3×Ftotal\nAlgorithm 2 FLOP S OF VISION TRANSFORMERS [10]\nRequire:\nnpatch(patch number), dpatch(patch size), nchannels (channel number), dmodel (model width),\nnheads (attention head number), dkey(key dimension), dff(feed-forward width),\nnlayers (layer number).\nEnsure: Ftotal\nEmbedding FLOPs\n1:Femb←2npatchd2\npatchnchannels dmodel\nAttention FLOPs (per layer)\n2:Fqkv←2npatch(3dmodel) (dkeynheads)\n3:Fqk←2n2\npatch(dkeynheads)\n4:Fsoft←3nheadsn2\npatch\n5:Fred←2n2\npatch(dkeynheads)\n6:Fproj←2npatch(dkeynheads)dmodel\n7:Fattn←Fqkv+Fqk+Fsoft+Fred+Fproj\nFeed-forward FLOPs (per layer)\n8:Fff←4npatch(dmodeldff)\nTotal FLOPs\n9:Ftotal←Femb+nlayers(Fattn+Fff)\n10:return 3×Ftotal\nFLOPS . Due to the truncation and padding applied by text tokenizers, the average per-sample FLOPs\nof each model should be calculated based on the average per-batch max caption token length. In\nour calculation, we approximate this value using the global max caption token length. Although this\nintroduces some deviation from the exact values, our goal is not to report precise measurements but\nto demonstrate the difference in scaling behavior between the two models: CLIP [ 43]’s FLOPs and\nmemory footprint scale with O(n2)complexity, while LIFT achieves an amortized complexity of\nO(1).\nIn Algorithm 1 and Algorithm 2, we present steps to calculate the per-sample FLOPs of the lan-\nguage [ 53] and vision transformers [ 10] used in this study. Both algorithms are adapted from [ 15].\nFor CLIP, the per-sample FLOPs is approximated as the sum of the FLOPs from its language and\nvision transformers. Since LIFT keeps its text encoder frozen, the per-sample FLOPs is estimated as\n20\n--- Page 21 ---\nthat of its vision transformer alone. Throughout, we assume that a backward pass incurs twice the\nFLOPs of a forward pass.\nMemory Usage . The memory usage of each model is estimated by monitoring GPU status using\nthenvidia-smi command. We observe that the exact memory consumption varies depending on\nfactors such as training stages and GPUs. The reported results represent the average of five training\nruns conducted on different H800 GPUs with varying random seeds.\nA.5 Broader Impacts\nSimilar to other vision-language models (VLMs), LIFT has significant potential for positive societal\nimpacts. For instance, it advances multimodal understanding and can be fine-tuned to generate\nhigh-quality image captions for visually impaired individuals. As a weakly supervised method, LIFT\nalso reduces the reliance on heavily labeled datasets. However, LIFT also carries potential negative\nsocietal impacts. It may inherit societal biases present in its training data, leading to harmful outcomes\nwhen deployed in sensitive applications. We advocate for cautious use of the model and recommend\nmitigating risks through careful dataset curation, bias analysis, and responsible deployment.\nA.6 Dataset Details\nA.6.1 Training Datasets\nDataComp-1B [ 12]is a large-scale, multimodal dataset consisting of approximately 1.4 billion\ntext-image pairs curated from a massive pool of 12.8 billion web-crawled samples. The dataset\nis constructed using various filtering strategies — including CLIP [ 43] score filtering, text-based\nfiltering, and image-based filtering — to identify high-quality, semantically aligned pairs. Models\ntrained on DataComp-1B have been shown to achieve notable gains on downstream tasks, including\nzero-shot classification on ImageNet-1K [8].\nRecap-DataComp-1B [ 25]builds upon DataComp-1B to address inherent noise and semantic\nmisalignment in web-crawled captions. It fine-tunes a LLaV A-1.5 [ 29] on LLaMA-3-8B [ 1] to\nrecaption images with longer, richer, and more semantically aligned descriptions. Empirical studies\nindicate that Recap-DataComp-1B leads to gains in both cross-modal retrieval and text-to-image\ngeneration under complex queries.\nA.6.2 Evaluation Datasets\nImageNet [ 8]is a large-scale, publicly available image dataset containing over 14 million images\nacross more than 20,000 categories. For our zero-shot classification experiments, we use ImageNet-\n1K validation set, which consists of 50,000 images, with 50 images for each of the 1,000 classes. The\ninput captions are constructed using the prompt template “ It is a photo of {label} .”\nCOCO [ 28]is a large-scale dataset for object detection, segmentation, keypoint detection, and image\ncaptioning, comprising 328,000 images. For our zero-shot retrieval experiments, we use Val2017\nsplit of 5,000 images and randomly select one of the five ground-truth captions for each image.\nFlickr30K [ 57]consists of 31,000 images sourced from Flickr, each paired with five human-annotated\nreference captions. For our zero-shot retrieval experiments, we evaluate vision-language models on\nthe test set of 1,000 images and randomly select one of the five ground-truth captions for each image.\nSugarCrepe [ 16]is a benchmark designed to evaluate compositional understanding of vision-\nlanguage models while addressing biases present in existing datasets. SugarCrepe employs large\nlanguage models to generate fluent and challenging negative captions by add,replace , orswap\nanobject ,attribute , orrelation in the original captions. Models are tasked with selecting\nthe correct captions among these compositional distractors based on caption-image cosine similarity.\nMMBench [ 31]is a benchmark designed to evaluate vision-language models across a broad range\nof perception and reasoning abilities. It features a diverse set of evaluation questions curated with\nstrict quality control and introduces a CircularEval strategy that uses large language models to map\nfree-form outputs to multiple-choice answers. It supports bilingual evaluation in English and Chinese.\nMME [ 11]is a benchmark designed to evaluate vision-language models across 14 subtasks covering\nboth perception and cognitive abilities. It provides systematic evaluations using manually crafted\n21\n--- Page 22 ---\ninstruction-answer pairs. MME’s standardized instructions enable fair comparisons between models\nwithout relying on prompt engineering.\nPOPE [ 27]is a benchmark designed to systematically assess object hallucination in vision-language\nmodels, a common issue in which models tend to generate objects that are inconsistent with the target\nimages. It innovates a polling-based query method that offers a more stable and flexible evaluation of\nhallucinated content.\nScienceQA [ 34]is a vision-language model benchmark comprising about 21,000 multiple-choice\nscience questions. Each question is accompanied by rich annotations that encourage models to gener-\nate chain-of-thought [ 56] (CoT) responses and enable the study of multi-hop reasoning. ScienceQA\nsupports evaluation across textual, visual, and diagrammatic modalities.\nTextVQA [ 46]is a benchmark designed to assess vision-language models’ ability to recognize text\nwithin images. It consists of 45,336 questions across 28,408 images, where successful answering\nrequires models to accurately read text in the image and reason about it in the context of both the\nimage and the question.\n22",
  "text_length": 59566
}