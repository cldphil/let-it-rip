{
  "id": "http://arxiv.org/abs/2505.24878v1",
  "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
  "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
  "authors": [
    "Yaxin Luo",
    "Zhaoyi Li",
    "Jiacheng Liu",
    "Jiacheng Cui",
    "Xiaohan Zhao",
    "Zhiqiang Shen"
  ],
  "published": "2025-05-30T17:59:55Z",
  "updated": "2025-05-30T17:59:55Z",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.CV",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24878v1"
}