{
  "id": "http://arxiv.org/abs/2506.04161v1",
  "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing",
  "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%.",
  "authors": [
    "Parsa Alian",
    "Martin Tang",
    "Ali Mesbah"
  ],
  "published": "2025-06-04T17:00:38Z",
  "updated": "2025-06-04T17:00:38Z",
  "categories": [
    "cs.SE"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04161v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04161v1  [cs.SE]  4 Jun 2025VISCA : Inferring Component Abstractions\nfor Automated End-to-End Testing\nParsa Alian\nUniversity of British Columbia\nVancouver, Canada\npalian@ece.ubc.caMartin Tang\nUniversity of British Columbia\nVancouver, Canada\nmt8422@student.ubc.caAli Mesbah\nUniversity of British Columbia\nVancouver, Canada\namesbah@ece.ubc.ca\nAbstract —Providing optimal contextual input presents a signif-\nicant challenge for automated end-to-end (E2E) test generation\nusing large language models (LLMs), a limitation that current\napproaches inadequately address. This paper introduces Visual-\nSemantic Component Abstractor ( VISCA ), a novel method that\ntransforms webpages into a hierarchical, semantically rich com-\nponent abstraction. VISCA starts by partitioning webpages into\ncandidate segments utilizing a novel heuristic-based segmentation\nmethod. These candidate segments subsequently undergo classifi-\ncation and contextual information extraction via multimodal LLM-\ndriven analysis, facilitating their abstraction into a predefined\nvocabulary of user interface (UI) components. This component-\ncentric abstraction offers a more effective contextual basis than\nprior approaches, enabling more accurate feature inference and\nrobust E2E test case generation. Our evaluations demonstrate\nthat the test cases generated by VISCA achieve an average feature\ncoverage of 92%, exceeding the performance of the state-of-the-art\nLLM-based E2E test generation method by 16%.\nI. I NTRODUCTION\nBefore deploying applications, developers need to thoroughly\ntest them to ensure the quality and integrity of the features.\nEnd-to-end (E2E) testing, in particular, focuses on ensuring\nthat all components of an application work together correctly\nand the features designed for the end-user perform smoothly.\nWhile manual E2E test creation by human testers can yield\nhigh-quality, semantically rich tests, this approach is expensive,\ntime-consuming, and scales poorly with application complexity,\nnecessitating automated solutions. Traditional automated E2E\ntesting techniques, often relying on application modeling [ 1],\n[2], [3], [4] or Reinforcement Learning (RL) [ 5], aimed to\nreduce manual effort. However, the generated tests lack the\nquality and semantic coherence of human-written ones, often\nmanifesting as random sequences of actions due to their reliance\non criteria that do not inherently capture user-centric goals.\nRecent advances in Large Language Models (LLMs) have\nsignificantly expanded the potential for automating software\ntesting across a variety of domains, including unit, API,\nand accessibility testing [ 6], [7], [8], [9], [10]. Building\non this momentum, LLMs are also explored for E2E test\ngeneration. An emerging direction in this space involves\nleveraging LLMs to generate test cases that cover application\nfeatures [ 11]. Feature inference in current techniques, such\nasAUTOE2E [11], involves extracting HTML snippets of\nuser actions and supplying them to an LLM to identify the\nunderlying features. While intuitive, this strategy is hindered byinsufficient contextual information. The HTML representation\nof an interactive element often lacks the surrounding semantics\nneeded for the model to infer its role within the application.\nAs a result, the absence of meaningful context can lead to\ninaccurate feature interpretations and limit the effectiveness of\nthe generated E2E tests.\nTo address the challenge of finding optimal context for\nLLM-based feature inference in E2E testing, we introduce\nVisual- Semantic Component Abstractor ( VISCA ). Our method\ntransforms a given webpage into an abstract structured compo-\nnent abstraction , which is analogous to the modular component\nmodels widely adopted in modern UI development frameworks\nsuch as React [ 12], VueJS [ 13], and Svelte [ 14]. By encapsu-\nlating logically related structures, content, and behaviors, this\nhigher-level component-centric abstraction captures the context\nof UI actions much more effectively than isolated raw HTML\nelements. Our insight is that this contextual abstraction, in turn,\nfacilitates more accurate feature inference and the generation\nof more robust and meaningful E2E test cases.\nThe contributions of this work are as follows:\n•Defining a standard vocabulary for describing the webpages\nas an abstraction.\n•A heuristic-based segmentation technique, resulting in seg-\nments that are suitable for transformation to component\nabstractions.\n•A novel method for transforming an unstructured webpage\nDocument Object Model (DOM) into a hierarchical, seman-\ntically enriched component abstraction, identifying logical\nUI units and their semantic context.\n•A feature inference technique that leverages the rich contex-\ntual information embedded within the component abstraction\nto more accurately identify the application features and\ngenerate test cases for them.\nOur evaluations demonstrate the multi-faceted effectiveness\nofVISCA . The core component abstraction module achieves\na 91.5% accuracy in preserving the structure and content\nof original applications. Within this, our novel heuristic-\nbased segmentation algorithm attains a precision of 83.4%,\noutperforming the state-of-the-art VIPS algorithm by 26.9% in\nthis regard. Ultimately, for end-to-end test generation, VISCA\nachieves an average feature coverage of 92%, surpassing\nthe leading feature-driven E2E testing baseline by a 16%\n1\n--- Page 2 ---\nimprovement. These results collectively highlight the significant\nbenefits of using detailed component abstractions to enhance\ncontextual understanding for automated LLM-driven E2E test\ngeneration.\nII. M OTIVATION\nA critical factor in the success of LLM-based systems is the\ncontext provided to the model; its nature and scope significantly\ninfluence whether an LLM performs a task effectively. For\ninstance, AutoE2E utilizes the HTML of individual actions\nas its input context for feature inference. However, relying on\nsuch highly localized context presents considerable challenges.\nConsider Amazon’s shopping cart page, depicted in Figure 1.\nOn this page, users can “delete a product from the cart.” The\nsimplified HTML for the “Delete” action is shown in Listing 1.\nFig. 1: Amazon’s shopping cart page\n1<span class=\"size-small action-delete\">\n2 <input value=\"Delete\" class=\"color-link\"/>\n3</span>\nListing 1: Delete action’s HTML from Amazon’s cart page\nBased on this action’s HTML alone, the precise target of\nthe “Delete” is ambiguous. This snippet lacks the surrounding\ninformation necessary to determine its purpose. Furthermore,\nsince multiple items are in the cart, their respective “Delete”\nbuttons share identical or similar HTML. This makes it\nchallenging for an LLM to distinguish which action corresponds\nto which item or to infer distinct features. In such cases, the\nprovided context is too narrow , potentially leading to inaccurate\nor incomplete feature inference.\nTo mitigate narrow context, another solution can be to\nprovide the LLM with the entire page’s HTML. While this\ntheoretically grants access to all information needed to under-\nstand element relationships and infer features, it introduces\nnew problems. Webpage HTML can span tens of thousands of\nlines of code; for example, the Amazon cart page (Figure 1)\nalone comprises roughly 36,000 lines of HTML, translating to\napproximately 348,000 LLM tokens. Feeding such extensive\ndata to LLMs risks overlooking crucial information, increases\nthe likelihood of model hallucination, and faces context window\nlimitations. In this scenario, the context becomes too broad ,\npotentially obscuring relevant signals within excessive noise.The challenge of selecting an optimal context is compounded\nby HTML’s inherent design, primarily for presentation, not for\nsemantic understanding. While the DOM provides a hierarchical\nstructure, it offers no explicit cues for the ideal contextual\nboundary of any given interactive element. This ambiguity in\ndefining the contextual scope makes it challenging to isolate a\nDOM segment that is both semantically complete for the task\nand efficiently processable.\nTo solve the problem of finding optimal context, one might\nconsider segmentation to partition a webpage into distinct,\nvisually and structurally coherent regions (e.g., headers, item\nlists, forms). Such segments could potentially offer more\nappropriately scoped, coherent contextual units for an LLM to\nanalyze UI actions and infer features. However, despite their\npotential, existing segmentation algorithms [ 15], [16], [17],\n[18], [19], [20], [21], [22] often fall short of providing optimal\ncontext for LLM-based feature inference. A primary limitation\nis their reliance on incomplete contextual analysis, as many\nsuch algorithms prioritize a single modality—such as DOM\nstructure, textual content, or visual layout—each offering only\na partial view when used in isolation. More critically, even\nwhen a collection of segments is generated, there is typically\nno inherent guidance on selecting which segments provide\nthe appropriate contextual scope for a given UI element or\nfeature. Individual segments can still be too broad, combining\nseveral distinct logical sections, or too narrow, fracturing a\nsingle coherent functional unit or providing only a partial view\nof an action’s necessary context.\nFig. 2: Amazon’s cart page segmented by VIPS [15]\nFor instance, Figure 2 shows Amazon’s cart page segmented\nby the VIPS algorithm [ 15], the state-of-the-art segmentation\ntechnique [ 23]. As depicted in the figure, the segment with the\n“Delete” button (Listing 1) is excessively broad: it also includes\nthe entire shopping cart, the checkout, and a “Buy it again” sec-\ntion. This example demonstrates how a segmentation algorithm\ncan produce an overly broad contextual unit. Consequently,\nrelying on segmentation outputs without a deeper contextual\nunderstanding of each segment is unreliable for providing the\noptimal LLM context.\n2\n--- Page 3 ---\nIII.A.1\nCandidate\nSegment\nExtraction\nOriginal Page Segmented Page\nIII.A.2\nClassification\nIII.A.3\nContext Extraction\nIII.A.4\nTransformation\n Component\nRepresentation\nIII.B\nFeature Extraction\nand Aggregation\nTest CasesFig. 3: Overview of V ISCA\nWe posit that an effective and balanced level of granularity\nfor context can be found at the level of UI Components .\nWeb application front-ends are built using HTML, CSS, and\nJavaScript. However, traditional development relying solely\non these technologies often struggled with modularity, as\nHTML lacks inherent mechanisms for encapsulating the view\nand behavior of distinct application elements. Component-\nbased frameworks (e.g., React, VueJS, Svelte) have emerged\nto address this, enabling developers to structure applications\nas collections of reusable, self-contained modules. Each com-\nponent encapsulates a specific piece of functionality—like a\ncart item, a search bar, or a navigation menu—along with its\nassociated UI elements, behavior, and styling.\nUI components, by their very design, offer a more promising\nfoundation for defining contextual boundaries than algorithmi-\ncally derived segments. Developers create them as logically\ncoherent and self-contained units. This means a component\ninherently groups related information and interactive elements\nthat work together to fulfill a specific purpose. For example,\nan individual “cart item” on the Amazon shopping cart page\n(Figure 1) would likely be implemented as a component. This\ncomponent would naturally contain the product title, price,\nquantity controls, and the “Delete” action, all related to that\nsingle entry. Such a component provides a context that is neither\ntoo narrow (it has all specifics for its actions) nor too broad\n(it is focused on one item), inherently forming a semantically\nmeaningful boundary ideal for LLM comprehension.\nIII. A PPROACH\nTo overcome the limitations of insufficient context for\nrobust feature inference, we introduce VISCA . Our method\ntransforms webpages into a semantically rich, component-\nbased abstraction through a multi-stage process, and then\nleverages this abstraction for feature inference and test case\ngeneration. The VISCA workflow, illustrated in Figure 3, begins\nby processing nodes from the DOM tree, applying heuristics\nto generate an initial hierarchy of candidate segments. These\ncandidates are subsequently classified while simultaneously\nextracting contextual descriptions using multimodal analysis.\nThe classified segments are then transformed into a component\nabstraction. Utilizing the structure and descriptions provided\nby these components, we infer application features and auto-\nmatically generate corresponding E2E test cases.TABLE I: Component Libraries\nLibrary Name Component Count GitHub Stars\nBootstrap [24] 25 172K\nMaterial UI [25] 50 95K\nAnt Design [26] 70 94K\nShadcn [27] 49 85K\nChakra UI [28] 95 39K\nA. Transforming to Component Abstraction\nTo facilitate modular development over raw HTML, CSS, and\nJavaScript development, component-based frameworks enabled\nthe encapsulation of logic, structure, and style into discrete\nUI Components . Building upon this paradigm, popular design\nlibraries, such as those detailed in Table I, provide collections\nof widely-used Common Templates . These templates offer\npredefined, reusable implementations of UI components that\nadhere to established design patterns, facilitating more efficient\nand consistent application development by reducing the need\nto build every interface element from scratch.\nGiven that common templates are designed for broad\napplicability across diverse websites, we posit that they can\nalso serve as a descriptive vocabulary to express the structure\nand function of virtually any webpage, regardless of its original\nimplementation technology or framework usage. For instance,\nconsider Amazon’s shopping cart page, depicted in Figure 1.\nThis page was not built with any of the specific libraries\nlisted in Table I, and its underlying HTML presents a deeply\nnested structure composed of semantically generic elements.\nNevertheless, it is possible to conceptualize an abstraction of\nthis page using common templates analogous to those found\nin such libraries:\n1<Container name =\"Shopping Cart Page\">\n2 <Navbar >\n3 <Image name =\"Logo\" src=\"...\" />\n4 <Input name =\"Search\" placeholder =\"Search Amazon\" />\n5 ...\n6 <Button name =\"Cart\" value =\"2\" />\n7 ...\n8 </Navbar >\n9 ...\n10 <List name =\"Shopping Cart Item List\">\n11 <Card name =\"Shopping Cart Item\" title=\"A Philosophy\nof Software Design...\">\n12 ...\n13 <Button name =\"Delete Cart Item\" />\n14 ...\n15 </Card >\n16 <Card name =\"Shopping Cart Item\" title=\"The Software\nEngineer’s Guidebook...\">\n17 ...\n3\n--- Page 4 ---\n18 <Button name =\"Delete Cart Item\" />\n19 ...\n20 </Card >\n21 </List >\n22 ...\n23</Container >\nListing 2: Abstraction of Amazon’s shopping cart page\nThe abstracted form in Listing 2 is designed such that if\nrendered using UI libraries (Table I), the resulting webpage\nwould be visually and structurally congruent with the original\nview in Figure 1. This abstraction, unlike the page’s raw HTML,\noffers significant advantages for contextual understanding. It\ndirectly addresses the limitations outlined in section II by\nsemantically grouping related content and behavior, thereby\naiding LLMs in feature inference. For instance, each “cart item”\nis represented as a distinct <Card name=\"Cart Item\">\nin this abstraction. This structured representation delineates\nboundaries for actions within each conceptual unit.\nMoreover, each component instance inherently carries se-\nmantic context based on its type. For example, the Navbar\npresented in Listing 2 immediately implies that its children\nare navigation-related functionality and content. Such an\nunderstanding provides richer information than analyzing raw\nHTML elements alone.\nTherefore, VISCA first generates a component abstraction\n(similar to Listing 2) using a curated set of common templates .\nThese templates were systematically derived by analyzing\nwidely-used UI design libraries (Table I), focusing on elements\nwith clear semantic and functional roles while consolidating\nsynonyms and generalizing overly specialized items to ensure\nbroad applicability. This curation resulted in a standardized\nvocabulary of 50 common templates and their names.Examples\nof these common templates include Card ,Table ,Navbar , or\nTab components.\nThe resulting abstraction serves as semantically rich, context-\naware segments, providing a more meaningful foundation for\nfeature inference compared to raw DOM analysis or traditional\nsegmentation methods. The following sections detail the multi-\nstage process V ISCA employed to achieve this abstraction.\n1) Candidate Segment Extraction: The initial DOM tree\nof a webpage is often large and computationally intensive to\nanalyze. Therefore, VISCA initially focuses on simplifying this\ninto a more manageable set of Candidate Segments . This begins\nby identifying all visible DOM nodes and reconstructing their\nhierarchical relationships. Subsequently, a recursive, visual\nprocess prunes structurally redundant nodes, added for styling\nor layout purposes, that offer no semantic value distinct from\ntheir children. This process iteratively examines parent nodes\nwith exactly one child: if the parent’s visual rendering is\nidentical to, or a close derivative of, its child’s (e.g., differing\nonly by padding or minor positional shifts), the child node\nis considered redundant and is removed. The children of any\npruned node are then re-parented to their grandparent.\nWhile the pruning step reduces the complexity of the initial\nDOM hierarchy, the number of remaining nodes can still be\nsubstantial for complex webpages. Processing a large numberof fine-grained nodes in subsequent LLM-based stages might\nbe computationally expensive or inefficient. To address this\npotential scalability challenge, VISCA includes a heuristic-\nbased segmentation step. This step aims to identify a set of\ncandidate segments by selecting nodes that likely represent\ncomponent instances based on two key principles: appropriate\nsize and structural similarity to siblings, reflecting common\npatterns in component-based design.\nFirst, component instances typically encapsulate a mean-\ningful unit of content or functionality, suggesting they should\nnot be granular (e.g., single text nodes) nor large (e.g., entire\npage sections). The idea of balancing segment size has been\nexplored in techniques like VIZMOD[29] for wireframe to\ncomponent transformation, where the approach considers a\nnode’s subtree size in its heuristics. Our approach similarly\nfavors nodes representing reasonably sized subtrees.\nSecond, developers leverage component templates for\nreusability. This results in multiple instances of the same\ncomponent template appearing adjacent or repeated within\na section. An example of this can be seen in the repeated\nuse of Cart Item in Amazon’s shopping cart page (Figure 1).\nTherefore, a node representing a component instance is likely\nto be structurally similar to its siblings if they originate from\nthe same template.\nTo quantitatively capture both size and sibling similarity, we\ndefine the following potential function Ψnfor each node nin\nthe pruned hierarchy from the previous step:\nΨn= log \nsize(n)\n1 +X\nidist\u0000\nn,sibi(n)\u0001!\n(1)\nIn this function, size(n)measures the size of the subtree\nrooted at node n, and Σdist\u0000\nn,sibi(n)\u0001\ndenotes the total\nstructural dissimilarity between node nand its sibling nodes\nsibi(n). This is calculated using an appropriate tree comparison\nmetric, such as Tree Edit Distance [ 30], summed over all\nsiblings. The function Ψnassigns higher potential to nodes\nthat are larger and similar to their siblings. To select the\ncandidate segments based on this potential function, we employ\nAlgorithm 1.\nAlgorithm 1 Candidate Segment Extraction Algorithm\n1:function MARKCANDIDATE SEGMENTS (node)\n2: nodePotential ←CalculatePotential (node) ▷Calculate Ψnodeusing Eq. (1)\n3: childrenCombinedPotential ←0\n4: for all child innode.Children do\n5: MarkCandidateSegments (child)\n6: childrenCombinedPotential +=child .assignedPotential\n7: end for\n8: ifnodePotential ≥childrenCombinedPotential then\n9: node.isCandidate ←true\n10: node.assignedPotential ←nodePotential\n11: for all descendant innode.Descendants do\n12: descendant.isCandidate = false\n13: end for\n14: else\n15: node.isCandidate ←false\n16: node.assignedPotential ←childrenCombinedPotential\n17: end if\n18: return node\n19: end function\n4\n--- Page 5 ---\nThe MarkCandidateSegments function first calculates\nthe potential ( Ψn) for the current node (line 2). It then\nrecursively calls itself on all children and aggregates the\nassignedPotential evaluated by these calls (lines 3-7).\nThis assignedPotential represents the maximum potential\nachievable within each child’s subtree (either from the child\nitself or its descendants). The algorithm compares the node’s\nown potential ( nodePotential ) with the sum passed up from\nits children ( childrenCombinedPotential , line 8). If the\nnode’s potential is higher or equal, it indicates that this node is\na better candidate than any combination of candidates within\nits children. Therefore, the node is marked ( isCandidate\n= true ), its own potential is assigned, and any candidates\npreviously marked within its subtree are unmarked (lines 12).\nIf the children’s combined potential is higher, the node itself\nis not marked, and the combined potential from the children\nis assigned to the node to be passed up the recursion (lines\n15-16). After the traversal completes on the root, the set of\nnodes with isCandidate = true represents the heuristically\nselected candidate segments.\n2) Candidate Segment Classification: Following the can-\ndidate segment extraction, VISCA classifies each candidate\nsegment to determine its semantic role. This classification is\ncrucial for identifying which segments represent UI component\ninstances suitable for transformation into the target component\nabstraction and understanding the overall page structure.\nWe categorize candidate segments into three distinct types:\nContainers, Lists, and Components.\nContainers. The primary function of a container is organiza-\ntional; it groups various subsegments based on spatial layout or\nstructural hierarchy without enforcing strong semantic cohesion\namong the contained elements. An example of a Container is\ntheVIPS segment in Figure 2, containing the shopping cart,\ncheckout section, and Amazon’s reward program.\nLists. The second candidate segment category is the List.\nWhile Lists, like Containers, serve a grouping function, they\nare distinguished by stronger internal semantic cohesion:\nthe subsegments organized within a List represent multiple\ninstances of the same conceptual entity or fulfill the same\nsemantic role. An example of lists includes the “cart item” list\nshown in Figure 1.\nComponents. The final category, and the primary target for our\nsubsequent transformation, is the Component . Components are\ninstances of common templates, representing a coherent unit\nof information and functionality. An example of a Component\nincludes a single “cart item” (Figure 1).\nThe defined segment categories exhibit a hierarchical contain-\nment relationship, illustrated in Figure 4. Containers function as\ngeneral structural groupings and may contain any segment type.\nLists, enforcing semantic consistency among their children, and\ncontain only multiple instances of a single type of Component.\nFinally, Components support recursive composition, allowing\nthem to encapsulate nested Lists or other smaller Components.\nTo assign a category to each candidate segment produced by\nthe preceding steps (Section III-A 1),VISCA utilizes prompting\nContainer List\nComponentFig. 4: Segment types’ composition relations\nof a multimodal LLM. This step determines the semantic\nrole of each segment and identifies those suitable for trans-\nformation into our target component abstraction. For each\ncandidate segment, the multimodal LLM is provided with the\ndefinitions and hierarchical relationships of the three segment\ncategories, and the visual rendering of the candidate segment.\nWe employ Chain-of-Thought (CoT) prompting to guide the\nLLM’s reasoning process. The prompt encourages the LLM\nto analyze the segment’s visual characteristics, considering\nfactors like internal homogeneity of the content and structure\nto determine the most appropriate category.\nWe use the recursive procedure detailed in Algorithm 2 to\napply this classification efficiently across the entire candidate\nsegment hierarchy.\nAlgorithm 2 Segment Tree Classification\n1:function CLASSIFY TREE(node)\n2: node .class←MULTIMODAL LLMC LASSIFY (node )\n3: ifnode.class=“Container” then\n4: for all child innode.Children do\n5: CLASSIFY TREE(child )\n6: end for\n7: else if node.class=“List” then\n8: for all child innode.Children do\n9: child.class←“Component”\n10: end for\n11: end if ▷Otherwise node.class is Component: Base case, recursion stops down\nthis path\n12: return node\n13: end function\nThe algorithm begins by invoking the multimodal LLM\nprocess ( MultimodalLLMClassify ) to determine the class\nfor the current node (line 2). The subsequent steps depend on\nthe assigned class. If a node is classified as a Container (line\n3), its children can be of any category. Therefore, the algorithm\nmust recursively call itself on each child to determine their\nclasses (lines 4-6). If a node is classified as a List (line 7), its\nchildren are, by definition of our List category, expected to be\nmultiple instances of a Component. The algorithm thus directly\nassigns the Component class to these children (lines 8-10).\nFinally, if a node is classified as a Component, it represents a\nstructural leaf in terms of this classification process. No further\nrecursive calls are made down this branch, as the segment is\nnow identified as a candidate for the final transformation step.\n3) Candidate Segment Context Extraction: A primary moti-\nvation for component abstraction is acquiring richer context for\n5\n--- Page 6 ---\nfeature inference and test generation (Section II). In practice,\ndevelopers often assign descriptive names to UI components\n(e.g., <CartItemCard> or<SearchResultCard> rather than\na generic <Card> ), improving the understandability of the\napplication structure. To achieve a similar level of semantic\ninsight, VISCA extracts a title and a natural language contextual\ndescription for each candidate segment, performing this task\nsimultaneously with the classification process described in\nSection III-A 2. We utilize the following prompt structure to\nextract the contextual description for the candidate segment:\n1)Page Context: The overall page description is generated\nbased on the full-page visual rendering, providing a con-\ntextual understanding of the application under test.\n2)Visual Rendering: The candidate segment’s visual ren-\ndering serving as the primary source for its content and\nlayout.\n3)Ancestor Context: The contextual description(s) previously\ngenerated for the current segment’s ancestors. Knowing that\na segment containing the product information is nested\nwithin “ Shopping Cart ” (Figure 1) helps clarify its role\ncompared with when the same information is in other\npossible contexts.\nTo determine these contextual descriptions across the hierar-\nchy, we employ a top-down recursive approach. By combining\npage-level, ancestor, and local visual context, VISCA aims to\ngenerate descriptions that accurately reflect the specific role and\npurpose of each segment. An example of the final generated\ncontextual description for the “cart item” from Figure 1 can\nbe seen in Listing 3.\n1{\n2 \"title\": \"Shopping Cart Item\",\n3\n4 \"context\": \"This segment displays the detailed\ninformation and interactive options for a product\nin a shopping cart or order summary.\"\n5}\nListing 3: Extracted context for cart item\nThese generated contextual descriptions are subsequently\nassociated with each respective segment in its final abstraction.\nFor example, in the abstracted structure illustrated in Listing 2,\nthis concept is exemplified by each node featuring a name\nattribute that encapsulates its derived contextual understanding.\n4) Candidate Segment Transformation: The final step in the\nsegmentation phase of VISCA is to generate the hierarchical\ncomponent abstraction. This structured abstraction serves as the\nenriched input for the feature inference and test generation stage.\nThe specific transformation strategy employed depends directly\non the category assigned to each segment during classification:\nForContainers , we recursively generate the abstraction for\neach child segment it contains and then assemble these child\nabstractions according to the parent’s structure. When handling\nLists, we avoid redundancy by selecting a single child segment\nas a representative sample; the component abstraction is then\ngenerated solely for this sample, which defines the structure and\ntype for all similar items in the original list. Lastly, segments\nthat are directly classified as Components serve as the base levelfor our transformation and are generated through a multimodal\nLLM prompting process.\nThe core transformation in VISCA occurs for segments\nclassified as Components . We employ a multimodal LLM\nprompt to map each such segment onto the most appropriate\ncommon templates from our predefined vocabulary.To guide the\nLLM towards an accurate transformation, the prompt integrates\nseveral key pieces of information for each target Component\nsegment:\n1)Segment’s Visual Rendering: The visual rendering pro-\nvides the primary evidence of the component’s appearance,\nlayout, and visible sub-elements.\n2)Segment HTML Code: The corresponding HTML source\nis included to allow the LLM to extract precise textual\ncontent, specific attributes (e.g., href for URLs, src for\nimage paths), or structural nuances potentially missed by\nvisual analysis alone.\n3)Hierarchical Context: The titles and the natural language\ndescriptions generated for the segment’s ancestors (Section\nIII-A 3) are provided. This informs the LLM about the\ncomponent’s placement and likely role within the broader\napplication structure, aiding disambiguation (e.g., differen-\ntiating visually similar cards used for different purposes).\n4)Common Template Definitions: The list of the 50 target\ncommon templates is provided to the LLM. This grounds\nthe generation process, instructing the LLM to structure its\noutput according to this predefined vocabulary.\nOnce component abstractions have been generated for the\nrelevant segments, VISCA completes the page transformation\nvia a bottom-up composition phase. This recursive assembly\nculminates at the hierarchy’s root node; its final, fully composed\nabstraction constitutes the complete component-based model\nderived from the original web page, ready to be used in the\nsubsequent feature inference and test generation phase. The\nresult of this transformation is an abstracted version of the web\npage, such as the example illustrated in Listing 2 for Amazon’s\nshopping cart page (Figure 1).\nB. Test Generation\nThe final phase of VISCA utilizes the generated component\nabstraction to drive feature inference and E2E test case\ngeneration. For this, we adapt the probabilistic framework\nestablished by AUTOE2E [11]. As previously noted (Section\nII), the original AUTOE2E framework processes the raw\nHTML of individual UI actions to infer features. Our primary\nmodification is to replace this direct HTML input with our\ncomponent abstraction inference. Consequently, instead of\nanalyzing isolated action elements, the adapted framework\nnow leverages the semantically richer components to infer\napplication features and subsequently generate targeted E2E\ntest cases.\nC. Implementation\nVISCA is implemented in Python, leveraging GEMINI 2.5\nFLASH as its core LLM. We selected GEMINI 2.5 F LASH\nprimarily for its affordability when operating at larger scales,\n6\n--- Page 7 ---\nwhich facilitates the practical application of VISCA in real-\nworld test generation scenarios. To promote consistent and\nreproducible outcomes, GEMINI 2.5 F LASH was utilized with\na temperature setting of 0. Furthermore, although GEMINI 2.5\nFLASH natively supports reasoning capabilities, all operations\nwithin VISCA that generate the results presented in this work\nwere performed using its non-reasoning version. The features\ninferred for the components and the generated test cases\nare stored on MONGO DB A TLAS [31]. The final test cases\ngenerated for the features utilize the Selenium [ 32] framework\nfor testing and assertions.\nIV. E VALUATION\nTo measure the effectiveness of our approach, we address\nthe following research questions.\n•RQ1 : How accurate is the component abstraction module in\nVISCA ?\n•RQ2 : How effective is the heuristic-based segmentation\nalgorithm?\n•RQ3 : How does VISCA compare to the state-of-the-are E2E\ntest generation methods?\nA. Component Abstraction Accuracy (RQ1)\nTo evaluate the accuracy of our component abstraction\nmodule, the core of VISCA , we need to compare its generated\ncomponent-based abstraction against a ground-truth abstraction\nthat also utilizes our defined common templates. One practical\nmethod to achieve this is to obtain subject webpages that were\noriginally constructed using UI components directly analogous\nto these common templates. For such pages, their inherent,\nlibrary-defined component structure effectively serves as the\nground-truth abstraction. VISCA ’s generated abstraction for\nthese pages can then be compared against this known structure\nto determine how accurately it captures the intended component-\nbased design.\nFollowing this methodology to construct our ground-truth\ndataset, we investigated the open-source UI libraries listed in\nTable I. Our selection process involved examining their official\ndocumentation and publicly available showcase applications,\nas these directly exemplify pages built with well-defined\nUI components. Among the five UI libraries we initially\nconsidered (Table I), three provided open-source demonstration\npages suitable for our analysis: Material UI, Shadcn, and\nBootstrap. Our selection process was as follows: we included\nall available showcase pages from both Bootstrap and Shadcn.\nFor Material UI, given that its operational showcases primarily\nfeature dashboard interfaces, we selected two distinct dashboard\napplications and incorporated all unique webpages within them.\nThis methodology yielded 26 distinct webpages that serve as\nour evaluation subjects.\nEach subject is thus a webpage natively constructed using\nUI components from its respective library. Crucially for\nestablishing the ground truth, the library-specific component\ntypes (e.g., markup explicitly defining a Button or a Card )\nare embedded within the subject’s HTML code. This embed-\nded type information, alongside the page’s complete HTMLstructure and visual rendering, constitutes the ground truth\nagainst which the component abstractions generated by VISCA\nare evaluated.\nMethodology. For each subject webpage, we execute the\ncomponent abstraction phase of VISCA . The input to VISCA\nconsists of the page’s raw HTML code and its visual rendering.\nCrucially, to assess VISCA ’s ability to generalize, any explicit\nlibrary-specific component identifiers (e.g., attributes like\nclass=\"card\" ) present in the original subject HTML are\nomitted from the input. The output of this phase is a component\nabstraction for the page (similar to Listing 2).\nThe evaluation then focuses on the segments that VISCA has\nidentified and transformed into instances of our Component\ncategory (as defined in Section III-A 2). For each such generated\nComponent instance, we compare its abstracted representation\nagainst the corresponding ground-truth component’s original\nHTML snippet from the subject page. This comparison is\nperformed across two primary dimensions: structural accuracy\nandcontent preservation .\nStructural accuracy assesses how well the common template\nselected by VISCA and its structure align with the actual type\nand structural characteristics of the ground-truth component\ninstance (e.g., a ground-truth Card component is represented\nin the abstraction using Card and its structure). Content preser-\nvation verifies whether all essential textual content, interactive\nelements, and key functional attributes (e.g., hyperlink URLs,\nimage sources, button labels) from the ground-truth compo-\nnent’s HTML are accurately and completely captured within\nVISCA ’s generated component abstraction. Each comparison\nfor structural accuracy and, separately, for content preservation,\nyields one of the following three outcomes:\n•Full Match: For structural accuracy , this indicates the\ncomponent abstraction’s type and essential structure perfectly\nalign with, or are an appropriate generalization of, the\nground-truth component. For content preservation , this\nsignifies that all essential information (text, key attributes,\ninteractive elements) from the source HTML is accurately\nand completely captured in the abstraction.\n•Partial Match: Forstructural accuracy , the component\nabstraction does not exactly match the ground-truth type or\nprecise structure, but it represents a semantically plausible\nalternative or a reasonable generalization that could achieve\na similar visual and functional outcome (e.g., using a List\ntemplate for navigation options instead of a specific Navbar\ntemplate, if the navigational purpose is maintained). For\ncontent preservation , most essential content is present, but\nsome minor attributes, secondary text values, or non-critical\nelements are missing or differ slightly in the abstraction,\nwithout fundamentally altering the core information or\nfunction of the component.\n•No Match: If the criteria for neither a Full Match nor a\nPartial Match are met for the dimension being evaluated\n(either structure or content), the abstraction is considered\nincorrect in that specific aspect.\n7\n--- Page 8 ---\n0% 50% 100%SubjectStructural Accuracy\n0% 50% 100%Content Preservation\nMatch Type\nFull Match Partial Match No MatchFig. 5: Abstraction Effectiveness for V ISCA\nTo determine the match category (Full, Partial, or No Match)\nfor both structural accuracy and content preservation, we first\nemploy GEMINI 2.5 F LASH , configured in its reasoning mode.\nFor each abstraction generated by VISCA ,GEMINI 2.5 F LASH\nis provided with the abstraction, its original HTML code, and\na visual rendering of the original segment. The LLM is then\nprompted to assess these inputs against our defined criteria\nand assign the appropriate match category for structure and,\nseparately, for content. Following the LLM’s assessment, the\nassigned match category for each comparison is independently\nvalidated by multiple authors to ensure the accuracy and\nconsistency of the final evaluation.\nThe results for structural accuracy and content preservation\nare presented in Figure 5. Each bar in this figure illustrates the\npercentage of generated abstract Component in different match\ncategories. In total, 386 components have been generated by\nVISCA across the 26 webpages. For structural accuracy, 91.4%\nof these components are a full match, 5.7% partial match, and\n2.9% no match. For content preservation, VISCA achieves a\n91.5% full match, 7.1% partial match, and 1.4% no match. This\nmeans that in total, around 91.5% of the components generated\nbyVISCA are perfect representations of the underlying UI\ncomponent used in creating them.\nTo ensure that VISCA accounts for all relevant parts of\nthe page, we measure the completeness of the abstraction.\nThis metric calculates the proportion of the visual elements\nfrom the subject’s ground-truth component structure that\nare not represented in our generated abstraction. A lower\nproportion of absent elements indicates a more comprehensive\ntransformation. In total, VISCA only missed seven components\nin its abstractions in the entire dataset, meaning that the\ncompleteness of V ISCA is evaluated at 98.2%.\n1 3 5 7 9 11\nVIPS (PDoC)0.000.250.500.751.00\nVISCA0.000.250.500.751.00Metric\nFB3 PB3 RB3Fig. 6: Comparison of B-Cubed scores for VIPS and V ISCA\nB. Segmentation Performance (RQ2)\nOur approach incorporates a novel heuristic-based segmen-\ntation method, detailed in Section III-A 1. We conduct two\nprimary evaluations for this segmentation technique. First, we\nassess its overall segmentation effectiveness by comparing it\nagainst a state-of-the-art baseline. To establish a robust baseline\nfor comparison, we referred to previous empirical studies of\npopular webpage segmentation algorithms [ 23]. This prior\nresearch identifies the VIPS algorithm [ 15] as a top-performing\nmethod overall. Consequently, we benchmark our segmentation\nheuristic’s performance against VIPS.\nTo evaluate the segmentation results, we utilize a dataset\nof human-annotated real-world websites [ 33], which closely\nmirrors the complex DOM structures found on live websites.\nThis dataset contains the top 45 most popular websites from\nYahoo’s Web Directory, where each page has been manually\ndivided into semantic segments. As evaluation metrics, we\nemploy PB3(precision), RB3(recall), and FB3(F1-score),\nwhich were previously introduced for assessing web page\nsegmentation results [ 34]. These metrics assess segmentation by\ntreating the segments as clusters and measuring their similarity\nbased on shared pairs, a metric known as B-Cubed [35].\nFigure 6 displays the results of our segmentation evaluation,\ncomparing VISCA with the VIPS baseline using the PB3\n(precision), RB3(recall), and FB3(F1-score) metrics. The\nVIPS algorithm’s performance is tuned via its Permitted\nDegree of Coherence (PDoC) parameter, which is a threshold\ndetermining whether the algorithm should divide a segment\ninto smaller subsegments, influencing the size, number, and\ncoherence of the segment’s content. We varied PDoC in its\nallowed range from 1 to 11, reporting its strongest performance.\nAs illustrated, VISCA achieves a PB3of 83.4%, an RB3of\n23.3%, and an FB3of 27.2%. For comparison, the optimal\nPB3,RB3, andFB3scores achieved by VIPS across all tested\nPDoC values are 65.7%, 60.8%, and 48.5%, respectively. Our\n8\n--- Page 9 ---\nmethod’s precision outperforms the best-performing VIPS ’s\nprecision by 26.9%.\nThe primary objective of our segmentation heuristic (Section\nIII-A 1) is to generate candidate segments that align closely\nwith our Component category, as defined in Section III-A 2.\nTo assess how well we achieve this goal, we evaluate the\nproportion of segments produced by our heuristic that are\nsubsequently identified as Components during our multimodal\nLLM classification stage (Section III-A 2). We compare this\nproportion against that achieved by the VIPS algorithm. For\nVIPS , we consider its output at three PDoC settings: 1 and\n11 (the range extremes), and 6, which has been identified\nas a generally optimal parameter [ 23]. The results of this\ncomparative analysis are presented in Table II.\nTABLE II: Classification Statistics for VIPS and VISCA\nSegments\nMethod Avg Segments Component Non-Component\nVIPS 1 10 43.0% 57.0%\nVIPS 6 13 44.5% 55.5%\nVIPS 11 108 72.5% 27.5%\nVISCA 56 74.5% 25.5%\nAs demonstrated in Table II, VISCA ’s segmentation heuris-\ntic successfully fulfills its design objective of prioritizing\ncomponent-like structures. On average, 74.5% of the segments\ngenerated by VISCA are subsequently classified as Components .\nThis represents a 2.8% increase over the rate achieved by\nthe best-performing VIPS variant, while VISCA concurrently\nproduces, on average, 48.1% fewer segments overall.\nC. Feature Inference and Test Generation Comparison (RQ3)\nTo evaluate the efficacy of VISCA in E2E test generation,\nwe benchmark it against AUTOE2E.AUTOE2E introduced\nthe paradigm of feature-driven E2E testing and demonstrated\nsuperior performance compared to various traditional and earlier\nLLM-based E2E test generation techniques [ 11]. To the best of\nour knowledge, AUTOE2E remains the state-of-the-art method\nin this specific domain and serves as our primary baseline.\nOur experimental setup utilizes the E2EB ENCH [11] bench-\nmark, used also by AUTOE2E. This benchmark comprises eight\nopen-source web applications of varying complexities, provid-\ning a standardized suite for evaluation. We adopt their proposed\nmetric, feature coverage , which quantifies the proportion of\nan application’s features that are exercised by a generated test\nsuite. This metric directly measures the effectiveness of feature-\ndriven testing approaches, and we use it to evaluate the test\nsuites produced by V ISCA .\nThe feature coverage for the test suites generated by VISCA\nis detailed in Table III. For each E2EB ENCH subject, the table\npresents the total number of test cases generated by VISCA , the\ncount of those correctly covering actual application features,\nand the resulting precision, recall, and F1-score. Aggregating\nperformance across all applications, VISCA achieves a global\nprecision of 40% (i.e., the proportion of all features inferred\nbyVISCA that were indeed correct) and a global recall of 83%TABLE III: Feature Inference of V ISCA on E2EB ENCH\nApp Name Total Correct Precision Recall F1\nPetClinic 26 22 0.85 0.96 0.90\nConduit 28 17 0.61 1.00 0.76\nTaskcafe 98 31 0.32 0.89 0.47\nDimeshift 41 23 0.56 1.00 0.72\nMantisBT 143 26 0.18 0.93 0.30\nEverTraduora 64 37 0.58 0.90 0.70\nStorefront 47 13 0.28 1.00 0.43\nDashboard 198 87 0.44 0.67 0.53\nTotal 645 256 0.40 0.83 0.54\nPetClinicStorefrontConduitTaskCafe Traduora Dimeshift MantisBTDashboard0%20%40%60%80%100%Feature CoverageAutoE2E VISCA\nFig. 7: Feature Coverage of VISCA and AUTOE2E on\nE2EB ENCH\n(i.e., the proportion of all actual features across E2EB ENCH\nthat VISCA successfully identified). These aggregate scores\nyield a global F1-score of 0.54.\nFurthermore, a direct comparison of feature coverage\nachieved by VISCA versus AUTOE2E is presented in Figure 7.\nOur method, VISCA , attains an average feature coverage\nof92% , exceeding the reported 79% for AUTOE2E and\ndemonstrating a 16% improvement over this baseline. Notably,\nVISCA shows superior or equal feature coverage compared to\nAUTOE2E on every application in the benchmark, achieving\nperfect coverage on three of the eight applications. In terms\nof feature identification accuracy, VISCA achieves an overall\nF1-score of 0.54, compared to 0.62 for A UTOE2E.\nV. D ISCUSSION\nSegmentation Precision/Recall. Our segmentation evaluation\n(Section IV-B ) indicates that VISCA achieves higher precision\nthan VIPS at the cost of lower recall and F1-score. We\ncontend this trade-off is beneficial for our primary objective of\ncomponent abstraction and feature inference. High precision\nensures that the segments fed into our subsequent LLM-\nbased transformation stage are coherent units, representing\npotential components, directly enhancing the quality of the\nabstraction and feature inference. The lower recall for VISCA\nis a consequence of its heuristic design, producing a flat list\n9\n--- Page 10 ---\nof segments, contrasting with hierarchical segments that VIPS\nand the ground-truth dataset exhibit. Therefore, for the goal of\ngenerating high-quality inputs for reliable component transfor-\nmation and feature inference, VISCA prioritizes segmentation\nprecision over maximizing recall.\nVanilla Prompting for Abstraction. A simpler alternative to\nour multi-stage VISCA might be direct, single-prompt LLM\nabstraction using a page’s full HTML and visual rendering.\nHowever, this holistic method, while potentially viable for small\npages, proves unreliable for complex commercial applications\ndue to common LLM limitations such as overlooking details,\nhallucination, and exceeding context windows. For instance, a\nsingle-prompt GEMINI 2.5 F LASH attempt on Amazon’s cart\npage (Figure 1) consistently failed to capture more fine-grained\ndetails in smaller sections (e.g., navigation, footer, list items)\nthat VISCA ’s structured process successfully abstracted. This\ndisparity underscores the necessity of a methodical, multi-stage\ntechnique like VISCA for robust component abstraction from\nreal-world webpages.\nThreats to Validity. It is important to acknowledge the potential\nthreats to the validity of our work and the steps to mitigate\nthem. One such threat lies in the representativeness of subjects\nfor component abstraction evaluation in Section IV-A . We have\ncarefully examined all the possible webpages for this evaluation,\nand aimed to diversify and reduce the potential redundancy\nin the subjects that might skew our results by selecting fewer\nsubjects from Material UI that showcase the same category of\nwebpages.\nAnother threat lies in the use of LLMs in the evaluation of\ncomponent abstraction (Section IV-A ). Using LLMs can be an\nunreliable solution, which is why we utilized the evaluation\nLLM in its reasoning mode to ensure a better understanding\nof the results, and independent authors validated the responses\nfrom the LLM to ensure its validity.\nFinally, a temperature of 0 has been used for the LLM in\nour evaluations to ensure consistency. However, the results may\nstill slightly vary in separate runs.\nVI. R ELATED WORK\nSegmentation. Extensive research has been conducted on\ndesigning segmentation techniques for webpages. Many es-\ntablished techniques rely heavily on a single modality. For\ninstance, DOM-based approaches [ 36], [37], [16], [17] like\nthe widely adopted VIPS algorithm [ 15] primarily analyze\nthe HTML structure. While useful, DOM properties do not\nalways align with visual presentation or semantic meaning\n[22], and the heuristics employed can be brittle or outdated,\nfailing to capture how elements are contextually grouped from\na user’s perspective. Conversely, text-based methods [ 38], [18]\nfocus on linguistic analysis of textual content, often ignoring\nthe crucial roles of visual layout, styling, and imagery in\ndefining contextual relationships on the page. More recent\nvisual approaches [ 22], [19], [20], [21] leverage positioning\nand appearance but may lack grounding in the underlying\nsemantic or structural information conveyed through text and\nthe DOM.UI Code Generation. Automated UI code generation aims\nto convert diverse design specifications into functional code,\ntackling the inefficiencies of manual development [ 39]. Earlier\nwork in the field has ranged from utilizing heuristics to\ntranslate mockups into web components [ 29] to utilizing neural\nnetworks and deep learning to translate screenshots of the\ntarget UI into code [ 40]. Currently, LLMs and generative\nAI, including diffusion models [ 41], are at the forefront,\nenabling sophisticated multimodal understanding and code\nsynthesis [ 42], though achieving high semantic fidelity and\nmaintainability in generated code remains a primary research\nfocus [39], [42].\nLLMs for Test Generation. Recent research in software testing\nincreasingly emphasizes automation, with LLMs emerging as\nsignificant enablers across diverse testing domains. While a\nsubstantial body of work has leveraged LLMs for unit test\ngeneration [ 43], [44], [6], [7], [8], and notable advancements\nhave been made in automated API testing [ 9] and accessibility\ntesting [ 45], [10] using various techniques, a prominent recent\ntrend is the focused application of LLMs to E2E test automation.\nWithin this E2E context, current techniques are addressing\nspecific sub-problems, such as enhancing semantic form\ntesting [ 46], or inferring application features to drive more\ncomprehensive and fully automated E2E test generation [11].\nIn the specific domain of mobile application testing, LLMs\nare also being actively investigated. These efforts range\nfrom utilizing multi-stage LLM prompting to guide action\nselection for functional testing [ 47], [48], to deploying agentic\nframeworks where multiple LLM-driven agents interact with\nthe application for thorough testing [ 49]. More recent work\nin mobile accessibility also explores leveraging LLMs for\nscenario-based evaluation [50].\nVII. C ONCLUSION\nIn this work, we introduced VISCA for transforming web\napplications into a component abstraction and utilizing the\nabstraction for automated E2E test generation. Our evaluations\ndemonstrate that VISCA achieves a 92% feature coverage in\nits generated test cases, outperforming the baseline by 16% .\nFurthermore, the evaluation of our core component abstraction\nmethodology reveals a high degree of fidelity, attaining an\naverage structural match of 91.4% and an average content\nmatch of 91.5% between the original webpages and their\ngenerated representations. This high-quality abstraction not\nonly validates the effectiveness of our transformation process\nbut also underscores its strong potential for enhancing various\ndownstream tasks, prominently including the improved E2E\ntest generation demonstrated in this paper.\nREFERENCES\n[1]A. Mesbah, A. Van Deursen, and S. Lenselink, “Crawling ajax-based web\napplications through dynamic analysis of user interface state changes,”\nACM Transactions on the Web (TWEB) , vol. 6, no. 1, pp. 1–30, 2012.\n[2]M. Biagiola, A. Stocco, F. Ricca, and P. Tonella, “Diversity-based web\ntest generation,” in Proceedings of the 2019 27th ACM Joint Meeting\non European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering , 2019, pp. 142–153.\n10\n--- Page 11 ---\n[3]R. K. Yandrapally and A. Mesbah, “Fragment-Based Test Generation\nfor Web Apps,” IEEE Transactions on Software Engineering , vol. 49,\nno. 3, pp. 1086–1101, 2023.\n[4]M. Biagiola, A. Stocco, F. Ricca, and P. Tonella, “Dependency-Aware\nWeb Test Generation,” in 2020 IEEE 13th International Conference on\nSoftware Testing, Validation and Verification (ICST) , 2020, pp. 175–185.\n[5]X. Chang, Z. Liang, Y . Zhang, L. Cui, Z. Long, G. Wu, Y . Gao,\nW. Chen, J. Wei, and T. Huang, “A Reinforcement Learning Approach\nto Generating Test Cases for Web Applications,” in 2023 IEEE/ACM\nInternational Conference on Automation of Software Test (AST) , 2023,\npp. 13–23.\n[6]Z. Nan, Z. Guo, K. Liu, and X. Xia, “Test intention guided llm-based\nunit test generation,” in 2025 IEEE/ACM 47th International Conference\non Software Engineering (ICSE) . IEEE Computer Society, 2025, pp.\n779–779.\n[7]X. Cheng, F. Sang, Y . Zhai, X. Zhang, and T. Kim, “Rug: Turbo llm\nfor rust unit test generation,” in 2025 IEEE/ACM 47th International\nConference on Software Engineering (ICSE) . IEEE Computer Society,\n2025, pp. 634–634.\n[8]X. Yin, C. Ni, X. Xu, and X. Yang, “What you see is what you get:\nAttention-based self-guided automatic unit test generation,” arXiv preprint\narXiv:2412.00828 , 2024.\n[9]M. Kim, T. Stennett, S. Sinha, and A. Orso, “A multi-agent approach\nfor rest api testing with semantic graphs and llm-driven inputs,” arXiv\npreprint arXiv:2411.07098 , 2024.\n[10] S. F. Huq, M. Tafreshipour, K. Kalcevich, and S. Malek, “Automated\ngeneration of accessibility test reports from recorded user transcripts,” in\n2025 IEEE/ACM 47th International Conference on Software Engineering\n(ICSE) . IEEE Computer Society, 2024, pp. 534–546.\n[11] P. Alian, N. Nashid, M. Shahbandeh, T. Shabani, and A. Mesbah, “Feature-\ndriven end-to-end test generation,” in 2025 IEEE/ACM 47th International\nConference on Software Engineering (ICSE) . IEEE Computer Society,\n2025, pp. 678–678.\n[12] “React,” https://react.dev/, 2025, accessed: 2025-04-10.\n[13] “Vuejs,” https://vuejs.org/, 2025, accessed: 2025-04-10.\n[14] “Svelte,” https://svelte.dev/, 2025, accessed: 2025-04-10.\n[15] D. Cai, S. Yu, J.-R. Wen, and W.-Y . Ma, “Vips: a vision-based page\nsegmentation algorithm,” 2003.\n[16] G. Vineel, “Web page dom node characterization and its application to\npage segmentation,” in 2009 IEEE International Conference on Internet\nMultimedia Services Architecture and Applications (IMSAA) . IEEE,\n2009, pp. 1–6.\n[17] J. Kang, J. Yang, and J. Choi, “Repetition-based web page segmentation\nby detecting tag patterns for small-screen devices,” IEEE Transactions\non Consumer Electronics , vol. 56, no. 2, pp. 980–986, 2010.\n[18] A. Kołcz and W.-t. Yih, “Site-independent template-block detection,” in\nEuropean Conference on Principles of Data Mining and Knowledge\nDiscovery . Springer, 2007, pp. 152–163.\n[19] B. Meier, T. Stadelmann, J. Stampfli, M. Arnold, and M. Cieliebak,\n“Fully convolutional neural networks for newspaper article segmentation,”\nin2017 14th IAPR International conference on document analysis and\nrecognition (ICDAR) , vol. 1. IEEE, 2017, pp. 414–419.\n[20] M. Cormer, R. Mann, K. Moffatt, and R. Cohen, “Towards an improved\nvision-based web page segmentation algorithm,” in 2017 14th Conference\non Computer and Robot Vision (CRV) . IEEE, 2017, pp. 345–352.\n[21] K. Chen, J. Wang, J. Pang, Y . Cao, Y . Xiong, X. Li, S. Sun, W. Feng,\nZ. Liu, J. Xu et al. , “Mmdetection: Open mmlab detection toolbox and\nbenchmark. arxiv 2019,” arXiv preprint arXiv:1906.07155 , 1906.\n[22] M. Bajammal and A. Mesbah, “Page segmentation using visual adjacency\nanalysis,” arXiv preprint arXiv:2112.11975 , 2021.\n[23] J. Kiesel, L. Meyer, F. Kneist, B. Stein, and M. Potthast, “An empir-\nical comparison of web page segmentation algorithms,” in European\nConference on Information Retrieval . Springer, 2021, pp. 62–74.\n[24] “Bootstrap,” https://getbootstrap.com/, 2025, accessed: 2025-04-10.\n[25] “Material ui,” https://mui.com/, 2025, accessed: 2025-04-10.\n[26] “Ant design,” https://ant.design/, 2025, accessed: 2025-04-10.\n[27] “Shadcn,” https://ui.shadcn.com/, 2025, accessed: 2025-04-10.\n[28] “Chakra ui,” https://chakra-ui.com/, 2025, accessed: 2025-04-10.\n[29] M. Bajammal, D. Mazinanian, and A. Mesbah, “Generating reusable\nweb components from mockups,” in Proceedings of the 33rd ACM/IEEE\nInternational Conference on Automated Software Engineering , 2018, pp.\n601–611.[30] K. Zhang and D. Shasha, “Simple fast algorithms for the editing distance\nbetween trees and related problems,” SIAM journal on computing , vol. 18,\nno. 6, pp. 1245–1262, 1989.\n[31] “Mongodb atlas,” https://www.mongodb.com, 2024, accessed: 2025-04-\n10.\n[32] “Selenium,” https://www.selenium.dev, 2024, accessed: 2025-04-10.\n[33] R. Kreuzer, “A quantitative comparison of semantic web page segmenta-\ntion algorithms,” Master’s thesis, 2013.\n[34] J. Kiesel, F. Kneist, L. Meyer, K. Komlossy, B. Stein, and M. Potthast,\n“Web page segmentation revisited: evaluation framework and dataset,” in\nProceedings of the 29th ACM International Conference on Information\n& Knowledge Management , 2020, pp. 3047–3054.\n[35] E. Amig ´o, J. Gonzalo, J. Artiles, and F. Verdejo, “A comparison of\nextrinsic clustering evaluation metrics based on formal constraints,”\nInformation retrieval , vol. 12, pp. 461–486, 2009.\n[36] T. Manabe and K. Tajima, “Extracting logical hierarchical structure\nof html documents based on headings,” Proceedings of the VLDB\nEndowment , vol. 8, no. 12, pp. 1606–1617, 2015.\n[37] K. Rajkumar and V . Kalaivani, “Dynamic web page segmentation\nbased on detecting reappearance and layout of tag patterns for small\nscreen devices,” in 2012 International Conference on Recent Trends in\nInformation Technology . IEEE, 2012, pp. 508–513.\n[38] C. Kohlsch ¨utter and W. Nejdl, “A densitometric approach to web page\nsegmentation,” in Proceedings of the 17th ACM conference on Information\nand knowledge management , 2008, pp. 1173–1182.\n[39] T. Kaluarachchi and M. Wickramasinghe, “A systematic literature review\non automatic website generation,” Journal of Computer Languages ,\nvol. 75, p. 101202, 2023.\n[40] T. Beltramelli, “pix2code: Generating code from a graphical user\ninterface screenshot,” in Proceedings of the ACM SIGCHI symposium\non engineering interactive computing systems , 2018, pp. 1–6.\n[41] A. Garg, Y . Jiang, and A. Oulasvirta, “Controllable gui exploration,”\narXiv preprint arXiv:2502.03330 , 2025.\n[42] S. Xiao, Y . Chen, J. Li, L. Chen, L. Sun, and T. Zhou, “Prototype2code:\nEnd-to-end front-end code generation from ui design prototypes,” in\nInternational Design Engineering Technical Conferences and Computers\nand Information in Engineering Conference , vol. 88353. American\nSociety of Mechanical Engineers, 2024, p. V02BT02A038.\n[43] M. Sch ¨afer, S. Nadi, A. Eghbali, and F. Tip, “An empirical evaluation of\nusing large language models for automated unit test generation,” IEEE\nTransactions on Software Engineering , vol. 50, no. 1, pp. 85–105, 2024.\n[44] Y . Chen, Z. Hu, C. Zhi, J. Han, S. Deng, and J. Yin, “Chatunitest: A\nframework for llm-based test generation,” in Companion Proceedings of\nthe 32nd ACM International Conference on the Foundations of Software\nEngineering , 2024, pp. 572–576.\n[45] M. Taeb, A. Swearngin, E. Schoop, R. Cheng, Y . Jiang, and\nJ. Nichols, “Axnav: Replaying accessibility tests from natural\nlanguage,” in Proceedings of the CHI Conference on Human Factors\nin Computing Systems , ser. CHI ’24. New York, NY , USA:\nAssociation for Computing Machinery, 2024. [Online]. Available:\nhttps://doi.org/10.1145/3613904.3642777\n[46] P. Alian, N. Nashid, M. Shahbandeh, and A. Mesbah, “Semantic constraint\ninference for web form test generation,” in Proceedings of the 33rd ACM\nSIGSOFT International Symposium on Software Testing and Analysis ,\n2024, pp. 932–944.\n[47] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che, D. Wang, and\nQ. Wang, “Chatting with gpt-3 for zero-shot human-like mobile automated\ngui testing. arxiv 2023,” arXiv preprint arXiv:2305.09434 , 2023.\n[48] ——, “Make llm a testing expert: Bringing human-like interaction to\nmobile gui testing via functionality-aware decisions,” in Proceedings of\nthe IEEE/ACM 46th International Conference on Software Engineering ,\n2024, pp. 1–13.\n[49] Z. Wang, W. Wang, Z. Li, L. Wang, C. Yi, X. Xu, L. Cao, H. Su,\nS. Chen, and J. Zhou, “Xuat-copilot: Multi-agent collaborative system\nfor automated user acceptance testing with large language model,” arXiv\npreprint arXiv:2401.02705 , 2024.\n[50] Y . Zhang, S. Chen, X. Xie, Z. Liu, and L. Fan, “Scenario-driven and\ncontext-aware automated accessibility testing for android apps,” in 2025\nIEEE/ACM 47th International Conference on Software Engineering\n(ICSE) . IEEE Computer Society, 2025, pp. 630–630.\n11",
  "text_length": 62035
}