{
  "id": "http://arxiv.org/abs/2506.00950v1",
  "title": "Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies:\n  A Comparative Analysis of Subjective and Objective Testing Methods",
  "summary": "The MUSHRA framework is widely used for detecting subtle audio quality\ndifferences but traditionally relies on expert listeners in controlled\nenvironments, making it costly and impractical for model development. As a\nresult, objective metrics are often used during development, with expert\nevaluations conducted later. While effective for traditional DSP codecs, these\nmetrics often fail to reliably evaluate generative models. This paper proposes\nadaptations for conducting MUSHRA tests with non-expert, crowdsourced\nlisteners, focusing on generative speech codecs. We validate our approach by\ncomparing results from MTurk and Prolific crowdsourcing platforms with expert\nlistener data, assessing test-retest reliability and alignment. Additionally,\nwe evaluate six objective metrics, showing that traditional metrics undervalue\ngenerative models. Our findings reveal platform-specific biases and emphasize\ncodec-aware metrics, offering guidance for scalable perceptual testing of\nspeech codecs.",
  "authors": [
    "Laura Lechler",
    "Chamran Moradi",
    "Ivana Balic"
  ],
  "published": "2025-06-01T10:51:33Z",
  "updated": "2025-06-01T10:51:33Z",
  "categories": [
    "eess.AS",
    "cs.SD"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00950v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00950v1  [eess.AS]  1 Jun 2025\nThis is a preprint of a paper submitted to and accepted for INTERSPEECH 2025.Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies:\nA Comparative Analysis of Subjective and Objective Testing Methods\nLaura Lechler1, Chamran Moradi1, Ivana Balic1\n1Cisco Systems, Europe\nllechler@cisco.com, cmoradia@cisco.com, ibalic@cisco.com\nAbstract\nThe MUSHRA framework is widely used for detecting subtle\naudio quality differences but traditionally relies on expert listen-\ners in controlled environments, making it costly and impractical\nfor model development. As a result, objective metrics are of-\nten used during development, with expert evaluations conducted\nlater. While effective for traditional DSP codecs, these metrics\noften fail to reliably evaluate generative models. This paper\nproposes adaptations for conducting MUSHRA tests with non-\nexpert, crowdsourced listeners, focusing on generative speech\ncodecs. We validate our approach by comparing results from\nMTurk and Prolific crowdsourcing platforms with expert lis-\ntener data, assessing test–retest reliability and alignment. Ad-\nditionally, we evaluate six objective metrics, showing that tra-\nditional metrics undervalue generative models. Our findings re-\nveal platform-specific biases and emphasize codec-aware met-\nrics, offering guidance for scalable perceptual testing of speech\ncodecs.\nIndex Terms : evaluation, MUSHRA, crowdsourcing, codec\n1. Introduction\nSubjective perceptual studies are the gold standard for evalu-\nating human perception of audio quality and related research\ntasks. Conducting these studies requires significant time and\ncost, often relying on specialized labs or resorting to small-\nsample internal listening tests with expert listeners. As a result,\nevaluation using objective metrics (e.g., PESQ [1], POLQA\n[2,3], and NiSQA [4,5]) is widely used. However, recent ad-\nvancements in generative models have compromised their relia-\nbility, as many rely on ground-truth references. Generated out-\nputs may meaningfully differ without perceptual degradation.\nSimilarly, non-intrusive objective metrics often fail to correlate\nwell with subjective ratings.\nMany subjective test methodologies have successfully been\nadapted to a crowdsourcing environment, ensuring scalability\nand cost-effectiveness, compared to internal or professional lab-\noratory tests. Examples are the application of MOS (mean opin-\nion scores) for speech quality to the crowdsourcing environment\n[6], leading to the implementation of ITU-T recommendation\nP.808 [7], and the open-sourced adaptation of the Diagnostic\nRhyme Test in multiple languages to assess speech intelligibil-\nity via crowdsourcing [8]. The MUSHRA (multiple stimuli with\nhidden reference and anchor) framework of evaluating speech\nquality [9] is a well-established test tool with high sensitivity\nto even small quality differences of high-quality speech codecs\nand other signal processing applications.\nStudies investigating the effects of conducting MUSHRA\ntests with non-expert listeners demonstrate that relative rank-\nings are largely consistent between the two participant groups[10,11], and some efforts have been made to facilitate online\nMUSHRA tests with the option to deploy these tests on crowd-\nsourcing platforms [11–13]. However, such studies note that ab-\nsolute scores may differ between expert and non-expert listen-\ners, and non-expert ratings appeared to be significantly above\nthe expert ratings, indicating a ceiling effect for very high-\nquality conditions under test.\nIt is hence still a wide-spread assumption that the\nMUSHRA test framework is only suitable for an expert-listener\naudience (e.g., [14]). This stance may be valid in certain ap-\nplication scenarios, such as evaluations for official accredita-\ntion and certification, where absolute scores are required that\ncan be compared between test laboratories. However, for other\nuse cases, such as continuous model evaluation throughout the\ndevelopment cycle and benchmarking against competitors, reli-\nable relative rankings and scalability are priorities.\nDue to the recent popularity of generative methods for\nspeech coding, we revisit the question of whether crowdsourced\nMUSHRA tests can reliably evaluate such models. We pro-\npose adaptations to the MUSHRA protocol for use with non-\nexpert listeners in a crowdsourced setting and release open-\nsource tools for replication. Our approach enables faster and\nmore scalable evaluations across diverse populations.\nThis work makes the following contributions: (i) it presents\nthe first, to our knowledge, dedicated crowdsourced MUSHRA\nevaluation design, focusing on generative speech codecs, high-\nlighting their unique perceptual challenges; (ii) it offers a di-\nrect comparison of two crowdsourcing platforms—Prolific and\nMTurk—revealing that while both replicate expert rankings\nwith high test–retest reliability, only Prolific closely aligns with\nexpert scores in absolute terms, whereas MTurk shows a ceiling\neffect; (iii) it provides a codec-specific analysis of subjective–\nobjective metric correlation, showing that traditional metrics\nlike PESQ and POLQA underestimate DNN-based codecs,\nwhile newer metrics such as SCOREQ show higher consistency\nacross architectures. These insights inform evaluation design,\nhighlight the value of subjective testing in generative codec re-\nsearch, and motivate further investigations into newer metrics.\n2. Crowdsourced MUSHRA testing\nTraditional MUSHRA testing has always faced difficulties and\nhigh costs. Setting up controlled listening environments, re-\ncruiting trained participants, and managing equipment can be\nexpensive and time-consuming. Moreover, ensuring consistent\nand reliable results can be challenging due to variations in lis-\ntener focus and fatigue. By leveraging the power of the crowd,\nwe can solve most of these difficulties. In addition, crowd-\nsourced MUSHRA offers cost-efficiency, faster data collection,\nand greater reliability through a larger sample size. The larger\n--- Page 2 ---\nThis is a preprint of a paper submitted to and accepted for INTERSPEECH 2025.and more diverse pool of participants accessible through crowd-\nsourcing may be more representative of customer opinions and\nmay avoid over-emphasizing the opinions of domain experts.\nDue to the low cost and fast collection pace, tests can be con-\nducted more frequently.\nGiven the significant variability in response reliability\namong crowdworkers—stemming from factors like fatigue, im-\npairments, carelessness, equipment differences, or malicious in-\ntent—rigorous participant screening is crucial. To address this,\nour tool enhances existing solutions (e.g., webMUSHRA [12])\nwith two key features: real-time score screening to immediately\ndiscard unreliable responses based on deviation thresholds, and\nautomatic test partitioning. This partitioning divides stimuli\ninto smaller, configurable sub-tests for individual listeners, en-\nabling evaluation of more conditions while mitigating fatigue\nand supporting diverse audio files.\nOur procedure builds upon and adapts established guide-\nlines from ITU-T P.808 [15], which outlines best practices for\ncrowdsourced MOS testing, and ITU-R BS.1534-3 [9], the stan-\ndard for MUSHRA testing. We extend these recommendations\nto address practical challenges encountered when conducting\nMUSHRA tests with crowdsourced non-expert listeners.\n2.1. Pre-screening\nIn line with subjective test recommendations for MOS tests in\ncrowdsourced environments [7], we applied quality filters on\nboth MTurk and Prolific. Participants were required to have\na success rate above 97% and at least 100 completed tasks.\nBased on empirical observations, no further filters were used\non MTurk, while on Prolific, we applied filters for first lan-\nguage (’English’), hearing difficulties (’no’), and cochlear im-\nplant (’no’). Depending on the platform, additional filters may\nbe available; researchers should consider those relevant to their\nuse case and report all filters used.\nWhether native-speaker competence is required may de-\npend on the research question. A study on cross-language eval-\nuations of codec artefacts [16] found no significant rating differ-\nences, though longer listening times were observed for foreign-\nlanguage conditions in high-quality audio. However, native\nspeakers are likely more able to judge intelligibility.\n2.2. Qualification phase\nOur survey flow included a questionnaire obtaining self-\nreported responses on several aspects relevant to participation\neligibility, a digits-in-noise hearing test, and a training session\nsimilar to the actual rating task. The questionnaire is based on\nITU recommendation P.800.1 [17] and assesses the listening de-\nvice used, the participant’s tiredness level, the last participation\nin an audio listening test, and the self-reported level of hear-\ning ability. Additionally, the participant’s gender, age, and level\nof English were recorded for statistical purposes. The hearing\ntest consisted of six sets of three digits in noise from the P.808\ntoolkit [7]. Participants had to pass a threshold of correct digits.\nThe training session consisted of one MUSHRA question with\ncarefully selected conditions. Participants had three attempts to\ncomplete this correctly. If the validation criteria (scores cannot\nbe equal to 0, reference must be ranked highest, the anchor must\nbe ranked lowest) were not met, feedback was provided.\n2.3. Main rating task\nThe main rating task contained up to three blocks of MUSHRA\nquestions. Each block contained up to 26 test stimuli (includingreferences and anchors). The number of MUSHRA questions\nper block therefore depended on the number of conditions di-\nrectly compared, with fewer questions added with increasing\nnumbers of conditions. If the validation criteria (cf. Subsection\n2.2) were also met in the main task, participants were allowed to\ncomplete up to three blocks. Allowing successful participants\nto rate several sections can reduce the error variance due to in-\ndividual rating differences [15]. We recommend limiting the\nnumber of rating jobs to three at most to prevent fatigue.\n2.4. Post-screening\nThe ITU Recommendation BS.1534-3 [9], tailored for expert-\nlistener MUSHRA tests, describes procedures based on which\ncertain listeners should be disqualified and outlier scores elimi-\nnated. We adapted these post-screening rules.\nListener-level disqualification: Listeners are disqualified en-\ntirely if they fail basic quality checks on more than 20% of the\nMUSHRA questions in a test block. A failure occurs if either\nof the following conditions is met for a given question:\n1. The anchor is rated higher than the hidden reference, i.e.\nranchor> r ref.\n2. All ratings for the non-anchor systems are identical, i.e.\nVar(r1, r2, . . . , r K) = 0 , where r1, . . . , r Kare the scores\nfor the Kconditions under test (excluding the anchor but in-\ncluding the hidden reference), and Var( ·)denotes variance.\nIf 20% of the questions in a test block is less than 1, we take 1\nas the threshold.\nScore-level outlier removal: After disqualifying certain lis-\nteners, we further proceed to removing outlying scores. A score\nis considered an outlier, and thus removed, if at least one of the\nabove-mentioned criteria is met or it is identified as an outlier\nusing the inter-quartile-range (IQR) rule explained in [9].\n2.5. Selecting test signals\nWe selected 40 clean speech wide-band test signals of high\nrecording quality in English. A balanced sample of male, fe-\nmale, and children’s speech was selected. Care was taken to\nchoose full utterances of intelligible speech. As non-expert lis-\nteners were found to make less use of the looping functionality\nthan expert listeners [10], we suggest simplifying the task by not\noffering the looping functionality and reducing the audio dura-\ntion. The listener can then focus on a manageable stretch of\naudio, while providing sufficient context. Although a duration\nof 10–12 seconds is recommended for expert listeners [9], we\nsuggest using slightly shorter files for crowdsourcing. Our test\nsignals had an average duration of 6.4 seconds (SD +/– 1.3s).\nThe vast participant pool of crowdsourcing platforms allows for\ntesting more signals instead of longer signals.\n2.6. Choosing the anchor\nIn MUSHRA tests, the anchor represents a clearly degraded ver-\nsion of the audio and defines the lowest point on the rating scale.\nIt helps listeners calibrate their judgments by providing contrast\nto the high-quality reference. The type of degradation intro-\nduced in the anchor can strongly influence how participants use\nthe scale and interpret differences between test conditions [9].\nITU-R BS.1534-3 recommends using an anchor that con-\ntains similar types of impairments as the systems being tested.\n--- Page 3 ---\nThis is a preprint of a paper submitted to and accepted for INTERSPEECH 2025.\nFigure 1: Repeatability and validity of crowdsourced test on two\ndifferent crowdsourcing platforms.\nThis makes comparisons easier and reduces confusion, espe-\ncially for non-expert listeners. Since we evaluated generative\nspeech codecs, which typically introduce coding artifacts, we\nused Opus at 6 kbps as the anchor.\nIn a pilot experiment, we tried using a low-pass filtered ver-\nsion of the original signal—an anchor sometimes used in tra-\nditional tests—but participants found it harder to judge quality\nbecause the artifacts were of a different type (bandwidth limita-\ntion instead of coding distortion). This feedback confirmed that\nmatching the anchor’s impairments to those of the test systems\nleads to more consistent and meaningful ratings.\n2.7. Aggregating results from different experiments\nFor expert-listener tests, a maximum of 12 conditions is rec-\nommended to be evaluated in parallel, including the reference\nand all anchors [9]. For crowdsourced tests with non-expert\nlisteners, we assume this poses an unmanageable task. In our\nexperience, a maximum of 6 conditions should not be exceeded\nin a crowdsourced test. However, further research into the at-\ntention span of crowdsourced non-expert listeners is required to\nestablish the recommended maximum scientifically.\nDue to this limitation, it is likely to split the conditions un-\nder test into several experiments. Care should be taken as to\nwhich systems are evaluated together, as context effects and\na varying resolution of detail for very similar conditions are\nknown challenges of this approach [9]. Aggregating results\nfrom different experiments with the same anchor and reference\nrequires renormalization, such that anchor and reference have\nthe same values in all tests. We set the reference to 100 and\nthe anchor to an average of the anchors from the three tests and\nre-normalized the other results accordingly.\n3. Experiments\n3.1. Experimental setup\nWe set two main goals for evaluating our crowdsourced\nMUSHRA listening test. First, it should correlate well with an\ninternal listening test conducted by expert listeners under con-\ntrolled conditions (i.e., validity). Second, repeating the same\ntest should yield the same results (i.e., repeatability). In this sec-\ntion, we examine both objectives through a study involving both\ninternal and crowdsourced listening tests. We then compare the\nresulting subjective evaluations against several well-known ob-\njective metrics.Table 1: Pairwise correlations between MUSHRA tests\nTest 1 Test 2 Pearson Spearman\nInternal Prolific 1 0.95 0.91\nInternal Prolific 2 0.95 0.91\nInternal MTurk 1 0.89 0.89\nInternal MTurk 2 0.90 0.89\nProlific 1 Prolific 2 0.98 0.95\nMTurk 1 MTurk 2 0.99 0.94\nWe conducted listening tests using the 40 clean speech sig-\nnals described in Section 2.5. The test included four codecs:\ntwo AI-based codecs: Webex AI Codec (6 kbps) [18] and En-\nCodec (6 kbps) [19], and two traditional DSP-based codecs:\nOpus (16 kbps) [20] and EVS (6 kbps) [21]. Additionally,\nOPUS 6 kbps was used as an anchor. A MUSHRA test was\nrun internally with in-house expert listeners in a quiet environ-\nment with wired headphones (4–6 votes per file). The same\nMUSHRA test was also run on two crowdsourcing platforms:\nMTurk and Prolific. The procedure for these tests was as de-\nscribed in Section 2. We repeated the crowdsourced tests af-\nter several months to evaluate the repeatability of the results.\nFor Prolific, we observed stable results with approximately 15\nresponses for each test item, while MTurk required around 25\nresponses per test item.\n3.2. Correlation testing\nIn the following analyses, Pearson and Spearman correlations\nwere calculated using per-condition mean scores (i.e., ratings\nfor every item in each test condition). By examining the full,\ncondition-level data, we capture how each platform’s ratings\nvary across all stimuli, providing a more fine-grained and ro-\nbust assessment of their correlations.\n4. Results and discussion\n4.1. Repeatability and Validity\nFrom Figure 1, we see that both Prolific and MTurk listeners\nproduce the same overall ranking of codecs as Internal listeners\n(i.e., “ground truth”). However, Prolific’s scores more closely\ntrace the Internal curve, with closer absolute means and nar-\nrower confidence intervals, implying better alignment across all\nconditions. MTurk achieves the same ranking, but its mean rat-\nings drift further from the Internal curve, particularly around\nmid-range codecs like EVS or Webex AI Codec. Table 1 sum-\nmarizes the Pearson and Spearman correlations between each\ncrowdsourced platform’s scores and the Internal listening test.\nThe high correlation values for both Prolific runs confirm that\nthe per-file mean scores closely track the internal scores. By\ncontrast, both MTurk tests maintain somewhat lower correla-\ntions, though still capturing the general ranking of conditions.\nWe can also see in Figure 1 that both MTurk and Prolific\nexhibit strong internal consistency across repeated runs: Pro-\nlific 1 and Prolific 2 scores nearly coincide, while MTurk 1 and\nMTurk 2 show minor differences in the absolute overall scores.\nThis reproducibility across independent trials underscores the\noverall reliability and robustness of these crowdsourced meth-\nods. The correlation test results reported in Table 1 confirm this\ntest consistency for per-file mean scores.\n--- Page 4 ---\nThis is a preprint of a paper submitted to and accepted for INTERSPEECH 2025.\nFigure 2: Linear regression plots for each objective metric against subjective scores (Prolific 1). Blue and red lines/dots correspond to\n3 DNN- and 3 DSP-based codecs, respectively, across 40 test items.\nTable 2: Pearson correlations of Objective against Subjective\nScores (Prolific 1) for all, DSP-, and DNN-based codecs.\nObj. Metric Overall DSP Codecs DNN Codecs\nPESQ 0.69 0.79 0.58\nPOLQA 0.72 0.78 0.69\nViSQOL 0.55 0.69 0.30\nNISQA 0.18 0.37 0.18\nDNSMOS-SIG 0.21 0.28 0.29\nSCOREQ –0.80 –0.79 –0.79\n4.2. Comparison with Objective Metrics\nWe compared our subjective test results with 6 objective met-\nrics. Three of these metrics are well-known intrusive sig-\nnal processing-based solutions: PESQ [1], POLQA [2,3], and\nViSQOL [22]. The remaining three metrics are DNN-based\nmetrics: NiSQA [4,5], DNSMOS-SIG [23] (both non-intrusive)\nand SCOREQ [24] (reference-based version).\nBased on the strong correlation of Prolific tests with our in-\nternal results, the availability of scores for more conditions, and\nthe higher participant numbers ensuring statistical robustness of\nfile means, we decided to use the Prolific scores for the compar-\nison of objective and subjective scores. To better explore their\nperformance on a variety of codecs, we added Opus (9 kbps)\nand Webex AI Codec (1 kbps) to the list of test conditions.\nBy means of an informal listening test, we confirmed that the\nmerged scores of a comparable test including these conditions\nwere valid and well-correlated with internal assessments.\nFigure 2 presents six scatter plots, showing linear regres-\nsion correlations between subjective scores and respective ob-\njective metrics for DSP-based (red) and DNN-based (blue)\ncodecs. POLQA, PESQ, and ViSQOL correlated better with\nsubjective scores for DSP-based codecs and systematically un-\ndervalued DNN-based codecs, evident in lower absolute scoresand weaker correlations. NISQA and DNSMOS-SIG appear to\nunderperform for both codec types, exhibiting poor alignment\nwith the subjective results. By contrast, SCOREQ shows the\nstrongest overall correlation with Prolific scores. The predic-\ntions remain consistently reliable across both DNN- and DSP-\nbased codecs. We observe less scatter for higher-quality codecs,\nsuggesting higher accuracy in predicting top-tier audio perfor-\nmance, regardless of codec architecture. Note that the y-axis for\nSCOREQ is reversed for clarity, since lower values indicate bet-\nter quality for distance-based metrics. Table 2 lists Pearson and\nSpearman correlations between objective and subjective scores\nfor all (overall), only DSP-based, and only DNN-based codecs.\nThese correlation results confirm the above findings.\n5. Conclusion\nOur study demonstrates that crowdsourced MUSHRA\ntests—when designed with appropriate screening and pro-\ncedural adjustments—can serve as reliable and repeatable\nalternatives to expert lab tests, even for generative speech\ncodecs. We show that Prolific yields closer absolute alignment\nwith expert ratings than MTurk, though both maintain reliable\nrelative rankings. Among objective metrics, PESQ, POLQA,\nand ViSQOL correlated more strongly with subjective scores\nin DSP-based codecs but tended to underestimate DNN-based\ncodecs. SCOREQ emerges as a promising tool for accurately\ntracking subjective speech quality across varying codec\narchitectures. MUSHRA results, which may provide more\nstable results than MOS tests, serve as excellent reference\ndata to further evaluate a metric’s reliability and accuracy for\na particular system under test. In future work, we plan to\nleverage the proposed test design in an in-depth analysis of\nvarious metrics for neural audio codec evaluation. Our test\ndesign and helper code to set up a MUSHRA test, including the\nqualification steps, can be found in our Github repository1.\n1https://github.com/cisco/multilingual-speech-testing\n--- Page 5 ---\nThis is a preprint of a paper submitted to and accepted for INTERSPEECH 2025.6. References\n[1] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, “Perceptual\nevaluation of speech quality (PESQ)-a new method for speech\nquality assessment of telephone networks and codecs,” in 2001\nIEEE International Conference on Acoustics, Speech, and Signal\nProcessing. Proceedings (Cat. No.01CH37221) , vol. 2. Salt\nLake City, UT, USA: IEEE, 2001, pp. 749–752. [Online].\nAvailable: http://ieeexplore.ieee.org/document/941023/\n[2] J. G. Beerends, M. Obermann, R. Ullmann, J. Pomy, and\nM. Keyhl, “Perceptual Objective Listening Quality Assessment\n(POLQA), The Third Generation ITU-T Standard for End-to-End\nSpeech Quality Measurement Part I–Temporal Alignment,” J. Au-\ndio Eng. Soc. , vol. 61, no. 6, 2013.\n[3] ——, “Perceptual Objective Listening Quality Assessment\n(POLQA), The Third Generation ITU-T Standard for End-to-End\nSpeech Quality Measurement Part II–Perceptual Model,” J. Audio\nEng. Soc. , vol. 61, no. 6, 2013.\n[4] G. Mittag and S. Moller, “Non-intrusive Speech Quality Assess-\nment for Super-wideband Speech Communication Networks,”\ninICASSP 2019 - 2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . Brighton,\nUnited Kingdom: IEEE, May 2019, pp. 7125–7129. [Online].\nAvailable: https://ieeexplore.ieee.org/document/8683770/\n[5] G. Mittag, B. Naderi, A. Chehadi, and S. M ¨oller, “NISQA:\nA Deep CNN-Self-Attention Model for Multidimensional\nSpeech Quality Prediction with Crowdsourced Datasets,” in\nInterspeech 2021 . ISCA, Aug. 2021, pp. 2127–2131. [On-\nline]. Available: https://www.isca-archive.org/interspeech 2021/\nmittag21 interspeech.html\n[6] F. Ribeiro, D. Florencio, C. Zhang, and M. Seltzer, “CROWD-\nMOS: An approach for crowdsourcing mean opinion score\nstudies,” in 2011 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . Prague, Czech Re-\npublic: IEEE, May 2011, pp. 2416–2419. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/5946971/\n[7] B. Naderi and R. Cutler, “An Open Source Implementa-\ntion of ITU-T Recommendation P.808 with Validation,” in\nInterspeech 2020 . ISCA, Oct. 2020, pp. 2862–2866. [On-\nline]. Available: https://www.isca-archive.org/interspeech 2020/\nnaderi20 interspeech.html\n[8] L. Lechler and K. Wojcicki, “Crowdsourced Multilingual\nSpeech Intelligibility Testing,” in ICASSP 2024 - 2024\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) . Seoul, Korea, Republic of:\nIEEE, Apr. 2024, pp. 1441–1445. [Online]. Available: https:\n//ieeexplore.ieee.org/document/10447869/\n[9] ITU, “Method for the subjective assessment of intermediate\nquality level of coding systems,” Geneva, Switzerland, 2015.\n[Online]. Available: https://www.itu.int/rec/R-REC-BS.1534/en\n[10] N. Schinkel-Bielefeld, N. Lotze, and F. Nagel, “Audio quality\nevaluation by experienced and inexperienced listeners,” Montreal,\nCanada, 2013, pp. 060 016–060 016. [Online]. Available: https:\n//pubs.aip.org/asa/poma/article/808581\n[11] M. Cartwright, B. Pardo, G. J. Mysore, and M. Hoffman, “Fast\nand easy crowdsourced perceptual audio evaluation,” in 2016\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . Shanghai: IEEE, Mar. 2016, pp. 619–\n623. [Online]. Available: http://ieeexplore.ieee.org/document/\n7471749/\n[12] M. Schoeffler, S. Bartoschek, F.-R. St ¨oter, M. Roess, S. Westphal,\nB. Edler, and J. Herre, “webMUSHRA — A Comprehensive\nFramework for Web-based Listening Tests,” Journal of Open\nResearch Software , vol. 6, no. 1, p. 8, Feb. 2018. [Online].\nAvailable: https://openresearchsoftware.metajnl.com/article/10.\n5334/jors.187/\n[13] M. Morrison, B. Tang, G. Tan, and B. Pardo, “Reproducible sub-\njective evaluation,” arXiv preprint arXiv:2203.04444 , 2022.[14] T. Muller, S. Ragot, L. Gros, P. Philippe, and P. Scalart,\n“Speech quality evaluation of neural audio codecs,” in\nInterspeech 2024 . ISCA, Sep. 2024, pp. 1760–1764. [On-\nline]. Available: https://www.isca-archive.org/interspeech 2024/\nmuller24c interspeech.html\n[15] ITU, “P.808 : Subjective evaluation of speech quality with\na crowdsourcing approach,” 2021. [Online]. Available: https:\n//www.itu.int/rec/T-REC-P.808/en\n[16] N. Schinkel-Bielefeld, Z. Jiandong, Q. Yili, A. K. Leschanowsky,\nand F. Shanshan, “Is it harder to perceive coding artifact in foreign\nlanguage items? – A study with Mandarin Chinese and German\nspeaking listeners,” Journal of the Audio Engineering Society , no.\n9739, May 2017.\n[17] ITU, “P.800 : Methods for subjective determination of\ntransmission quality,” 1998. [Online]. Available: https://www.itu.\nint/rec/T-REC-P.800-199608-I/en\n[18] A. Dhingra, “Webex AI Codec: Delivering Next-level\nAudio Experiences with AI/ML,” Mar. 2024. [Online].\nAvailable: https://blog.webex.com/collaboration/hybrid-work/\nnext-level-audio-with-webex-ai-codec/\n[19] A. D ´efossez, J. Copet, G. Synnaeve, and Y . Adi, “High\nfidelity neural audio compression,” Transactions on Machine\nLearning Research , 2023, featured Certification, Reproducibility\nCertification. [Online]. Available: https://openreview.net/forum?\nid=ivCd8z8zR2\n[20] K. V os, K. V . Sørensen, S. S. Jensen, and J.-M. Valin, “V oice\ncoding with opus,” in Audio Engineering Society Convention 135 .\nAudio Engineering Society, 2013.\n[21] M. Dietz, M. Multrus, V . Eksler, V . Malenovsky, E. Norvell,\nH. Pobloth, L. Miao, Z. Wang, L. Laaksonen, A. Vasilache,\nY . Kamamoto, K. Kikuiri, S. Ragot, J. Faure, H. Ehara,\nV . Rajendran, V . Atti, H. Sung, E. Oh, H. Yuan, and\nC. Zhu, “Overview of the EVS codec architecture,” in\n2015 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . South Brisbane, QLD,\nAustralia: IEEE, Apr. 2015, pp. 5698–5702. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/7179063/\n[22] M. Chinen, F. S. C. Lim, J. Skoglund, N. Gureev, F. O’Gorman,\nand A. Hines, “ViSQOL v3: An Open Source Production\nReady Objective Speech and Audio Metric,” Apr. 2020,\narXiv:2004.09584 [eess]. [Online]. Available: http://arxiv.org/\nabs/2004.09584\n[23] C. K. Reddy, V . Gopal, and R. Cutler, “Dnsmos: A non-intrusive\nperceptual objective speech quality metric to evaluate noise sup-\npressors,” in ICASSP 2021-2021 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) . IEEE,\n2021, pp. 6493–6497.\n[24] A. Ragano, J. Skoglund, and A. Hines, “SCOREQ: Speech quality\nassessment with contrastive regression,” in Advances in Neural\nInformation Processing Systems , A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds.,\nvol. 37. Curran Associates, Inc., pp. 105 702–105 729. [Online].\nAvailable: https://proceedings.neurips.cc/paper files/paper/2024/\nfile/bece7e02455a628b770e49fcfa791147-Paper-Conference.pdf",
  "text_length": 29051
}