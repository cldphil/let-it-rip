{
  "id": "http://arxiv.org/abs/2506.00883v1",
  "title": "Improve MLLM Benchmark Efficiency through Interview",
  "summary": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers.",
  "authors": [
    "Farong Wen",
    "Yijin Guo",
    "Junying Wang",
    "Jiaohao Xiao",
    "Yingjie Zhou",
    "Chunyi Li",
    "Zicheng Zhang",
    "Guangtao Zhai"
  ],
  "published": "2025-06-01T07:51:15Z",
  "updated": "2025-06-01T07:51:15Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00883v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00883v1  [cs.CL]  1 Jun 2025Improve MLLM Benchmark Efficiency through Interview\nFarong Wen\nwenfarong@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai AI Lab\nShanghai, ChinaYijin Guo\nguoyijin@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai AI Lab\nShanghai, ChinaJunying Wang\nwangjunying@pjlab.org.cn\nFudan University\nShanghai AI Lab\nShanghai, China\nJiaohao Xiao\nxiaojiahao@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nShanghai, ChinaYingjie Zhou\nzyj2000@sjtu.edu.cn\nShanghai Jiao Tong University\nShanghai, ChinaChunyi Li\nlichunyi@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nShanghai, China\nZicheng Zhang‚Ä†\nzhangzicheng@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nChinaGuangtao Zhai‚Ä†\nzhaiguangtao@pjlab.org.cn\nShanghai AI Lab\nShanghai Jiao Tong University\nChina\nHow to\nImprove \nEfficiency?\nEffective\nInstancesMLLM Benchmarks\nAre ‚ÄúRedundant‚Äù\nWe don‚Äôt need to ask all questions\nInstead of Full Benchmark\n‚ÄúInterview MLLM‚Äù\nQuestion\nPick\nEvaluate\nUpon \nResponseAdjust\nTest\nRe-search BenchmarkInterview\nCircle\nInterview with a few questions\nBetter MLLM as InterviewerInterview MLLMs\nWhen Asking 50 Questions\nHigh Rank Similarity to Full Benchmark\nInterview\nInterview Rank & Full \nBenchmark Rank Similarity\nA-Bench\nQ-Bench0.65\nMMT-Bench\nSEED-Bench0.84\n0.81\n0.81Interview 0.82\nRandom\nInterview\nRandom\nInterview\nRandom\nInterview\nRandom0.70\n0.77\n0.61\nFigure 1: Motivation for our work. Redundancy in MLLM benchmark instances has been demonstrated [ 38], reducing the\nefficiency of full benchmark evaluations. However, randomly sampling a small set of questions for evaluation is also inadequate.\nInspired by human interview processes, we propose an interview strategy that dynamically adjusts questions based on MLLM\nperformance, achieving more effective rankings than random sampling with the same number of questions.\nAbstract\nThe rapid development of Multimodal Large Language Models\n(MLLM) has led to a wide range of MLLM applications, and a num-\nber of benchmark datasets have sprung up in order to assess MLLM\nabilities. However, full-coverage Q&A testing on large-scale data\nis resource-intensive and time-consuming. To address this issue,\nwe propose the MLLM Interview (MITV) strategy, which aims to\nquickly obtain MLLM performance metrics by quizzing fewer ques-\ntion. First, First, we constructed the interview dataset, which was\nbuilt on an existing MLLM assessment dataset, by adding difficultylabels based on the performance of some typical MLLMs in this\ndataset. Second, we propose an MLLM Interview strategy, which ob-\ntains an initial performance situation of the large model by quizzing\na small number of topics and then continuously tries to test the\nmodel‚Äôs limits. Through extensive experiments, the result shows\nthat the MITV strategy proposed in this paper performs well on\nMLLM benchmark datasets, and it is able to obtain the model eval-\nuation capability faster through a small number of questions and\nanswers.\n--- Page 2 ---\nKeywords\nMLLM, Interview Strategy, Benchmark, Redundancy\n1 Introduction\nThe rapid advancement of Multimodal Large Language Models\n(MLLMs) has transformed numerous domains, enabling sophis-\nticated reasoning across text, images, and other data modalities.\nEvaluating the performance of MLLMs is critical to understand-\ning their capabilities and limitations. Traditionally, MLLM evalua-\ntion relies on comprehensive benchmarks comprising large sets of\nstandardized tasks. However, as MLLMs evolve in complexity and\nbenchmarks grow in scope, the computational and temporal costs of\nfull benchmark evaluations have become increasingly prohibitive,\nprompting the need for more efficient evaluation methodologies.\nCurrent MLLM benchmarks provide standardized metrics but of-\nten require evaluating models on thousands of questions to ensure\nrobustness. This exhaustive approach, while thorough, is redundant\nand resource-intensive, particularly when benchmarking advanced\nMLLMs with overlapping capabilities. The redundancy in bench-\nmarks stems from the inclusion of numerous questions that may\nnot significantly differentiate model performance, highlighting the\nneed for streamlined evaluation processes, as shown in the bench-\nmark redundancy principle.\nAs depicted in Fig.2, an SRCC of 0.95 is achieved at a 40% sam-\npling rate across MLLM benchmarks, indicating that evaluating\nMLLMs with just 40% of the benchmark instances yields perfor-\nmance rankings nearly identical to those using the full benchmark\ninstances. However, beyond this point, further increases in the sam-\npling rate result in minimal SRCC improvement, highlighting the\npresence of information redundancy in the benchmark.\nTo address the inefficiency of traditional benchmarks, we draw\ninspiration from the human interview process. Experienced inter-\nviewers can accurately assess a candidate‚Äôs abilities with only a few\ncarefully selected questions, adapting their inquiries based on re-\nsponses to probe specific strengths and weaknesses. This dynamic\nand adaptive approach contrasts with the static nature of bench-\nmarks, which apply a fixed set of questions regardless of a model‚Äôs\nperformance. Motivated by this, we propose an interview-based\nevaluation framework for MLLMs, designed to replicate the effi-\nciency and adaptability of human interviews while maintaining the\nrigor of benchmark assessments.\nOur approach employs a powerful MLLM as an interviewer to\nevaluate another MLLM (the interviewee). We begin by labeling\nbenchmark question instances with their difficulty and category,\ncreating a structured question pool. The interviewer MLLM lever-\nages this metadata to dynamically select questions, evaluate re-\nsponses, and adjust subsequent questions based on the intervie-\nwee‚Äôs performance. By iteratively refining the question selection,\nthe interviewer efficiently probes the interviewee‚Äôs capabilities\nacross diverse tasks and difficulty levels. The process culminates in\na final score that reflects the interviewee‚Äôs overall performance. The\nexperimental results show that the MITV strategy‚Äôs performance\nis better than the Random strategy‚Äôs performance for the same\nnumber of topics\nOur work makes the following contributions:\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSample Percentage0.700.750.800.850.900.951.00Metric Value\nMean SRCC\nMean PLCC\nMean R2\nThreshold 0.95\nAbove 0.95\nBelow 0.95Figure 2: Average instance redundancy for 20 MLLMs across\n18 MLLM benchmarks (sourced from the MLLM Benchmark\nRedundancy Principle [ 38]). Each data point reflects the\nmean of 100 sampling iterations, showing SRCC, PLCC, and\nR2 score between rankings from sampled subsets and full\nbenchmark rankings.\n‚Ä¢Construction of Interview Dataset : We construct a com-\nprehensive interview dataset by integrating existing MLLM\nbenchmarks and annotating questions with difficulty and\ncategory labels. This structured dataset enables targeted and\nefficient evaluation across diverse dimensions and difficulty\nlevels.\n‚Ä¢Interview-Based Evaluation Framework : We introduce\na dynamic, adaptive evaluation methodology for MLLMs,\ninspired by human interviews, which significantly reduces\nthe number of questions needed compared to traditional\nbenchmarks. We develop a systematic approach to labeling\nbenchmark questions by difficulty and category, paired with\nan intelligent question selection mechanism that optimizes\nevaluation efficiency.\n‚Ä¢Empirical Validation of Efficiency and Accuracy : We\ndemonstrate through experiments that our interview-based\napproach achieves comparable ranking accuracy to full bench-\nmarks with a fraction of the questions, offering a scalable\nsolution for MLLM evaluation.\nThis work paves the way for more efficient and practical eval-\nuation strategies, enabling rapid assessment of MLLMs in both\nresearch and deployment contexts.\n2 Related Works\nThis section reviews the evolution of Multi-modal Large Language\nModels (MLLMs) and current benchmark-based evaluation paradigms.\n2.1 Multi-modal Large Language Models\nMulti-modal Large Language Models (MLLMs) have rapidly ad-\nvanced, integrating diverse modalities such as vision and text to\nperform complex tasks including visual question answering (VQA),\nimage captioning, and multimodal reasoning. Early models like\nCLIP-ViT [ 24] established foundational visual-textual alignment.\nSubsequent architectures, including GPT-4o [ 2], Gemini-1.5Pro [ 27],\n--- Page 3 ---\nand Qwen2-VL [ 30], demonstrate significantly enhanced capabil-\nities in complex reasoning and understanding, driven by scaled\narchitectures and extensive datasets [ 15,16]. Notable efforts like\nInternVL [ 9] and LLaVAOneVision [ 18] further expand these capa-\nbilities for generic tasks and domain transfer. This rapid progress\nnecessitates robust evaluation frameworks to assess their multifac-\neted abilities, from perception to high-level reasoning.\n2.2 MLLMs Benchmark Evaluation\nThe evaluation of MLLMs predominantly relies on benchmarks\ndesigned to assess specific capabilities like visual perception, rea-\nsoning, and domain-specific knowledge. Traditional benchmarks\nsuch as GQA [ 15], VQA-V2 [ 4], and TextVQA [ 25] often employ\nconstrained, single-response formats, which may not fully capture\nthe sophisticated generative and interactive reasoning of advanced\nMLLMs. Consequently, newer benchmarks like MMBench [ 20],\nMMVet [ 35], and MMMU [ 36] have been introduced to evaluate\nintegrated multimodal capabilities. Specialized benchmarks such\nas MathVista [ 21] for mathematical reasoning, OmniMedVQA [ 14]\nfor medical applications, and VRSBench [19].\nHowever, the proliferation of these benchmarks has introduced\nissues, notably redundancy. Zhang et al . [38] highlighted significant\noverlaps by analyzing MLLM performance correlations, indicating\nthat many benchmarks may assess similar underlying abilities. Cri-\ntiques by Chen et al . [8] and others further point to design limita-\ntions, such as the inclusion of tasks with low discriminative power\nor relevance, which can obscure true model capabilities [ 38]. While\nthe community strives for comprehensive and diverse evaluations\n[12,34], these findings underscore the need for more principled\nbenchmark design to mitigate redundancy and enhance evaluative\nefficiency. The current paradigm, largely dependent on static, prede-\nfined test suites, may not adequately reveal the dynamic, adaptive,\nand deeper reasoning faculties of state-of-the-art MLLMs. This sug-\ngests an exigent need for novel evaluation methodologies capable\nof more effectively probing these emergent, complex abilities.\n3 Dataset Construct\n3.1 Prepare Dataset\nTraditional interview questions often lack systematicity and diffi-\nculty gradient, which makes it difficult to comprehensively examine\nthe interviewer‚Äôs ability performance in different fields and at dif-\nferent difficulty levels. In this paper, we construct a dataset with a\ndifficulty gradient that can comprehensively test the comprehen-\nsive ability of interviewers. This dataset integrates several existing\ndatasets, A-Bench[ 37], Q-Bench[ 31], MMT-Bench[ 33] and SEED-\nBench[ 17], covering multiple assessment dimensions such as logical\nreasoning, multimodal comprehension, multitasking, and safety\nethics, which makes the data more three-dimensional.\n3.2 Difficulty Calculation\nIn order to obtain the difficulty of each question quickly and fairly,\nwe judged the difficulty of the questions based on the performance\nof several typical MLLMs on the target dataset. Specifically, Duan ùëíùë°\nùëéùëô.[11] proposed VLMEvalKit, which is an open-source evaluation\ntoolkit of large vision-language models. VLMEvalKit provides a\npowerful tool that helps us test the performance of different MLLMs.\nWhere are questions from? How to choose questions?\n2) For each iteration,  repeat  as follow s:1) Choose according tolevels\n3) Finish when maintaining in one level:Existing Muli -Modal BenchmarksInterview \nQuestion Bank\nQuestions\nUse 10 MLLMs‚Äô performance \nto split the benchmarksSelected by \nInterviewer Finished by\nIntervieweeResponse\nAccurac y\nLevel 4 Level 4‚Ä¶ This interviewee \nIs in Level 4!\n‚Ä¶‚Ä¶\n<47%>53%\n47~53%\nAnswer2: ‚Ä¶‚Ä¶Answer1: ‚Ä¶‚Ä¶\nAnswer5: ‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶Question1: ‚Ä¶‚Ä¶\nQuestion2: ‚Ä¶‚Ä¶\nQuestion5: ‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶\nYourInterview isfinished! Next one.\nHow toevaluate the performance of the interviewees?\nLevel 6 Level 5Stronger Model Weaker Models\n3 51%\nLever 1Lever 2Lever 10\n‚Ä¶‚Ä¶\nLevel n Level n-1 Level n+1‚Ä¶\n‚Ä¶\n‚Ä¶‚Ä¶\n4 52%\n 5 49% ‚Ä¶‚Ä¶ >Figure 3: The proposed MITV strategy framework.\nTable 1: The question difficulty mapping based on the num-\nber of correct responses from the MLLMs in the difficulty\ncalculation process.\nDifficuty Level 1 Level 2 Level 3 Level 4 Level 5\nCorrect Num 10 9 8 7 6\nDifficuty Level 6 Level 7 Level 8 Level 9 Level 10\nCorrect Num 5 4 3 2 1\nFor each benchmark mentioned in Sec.3.1, we uniformly chose\nten models, including GPT-4o[ 23], Deepseek-VL[ 32], Qwen-2.5-\nVL[6],GeminiPro1-5[ 28], Grok-2[ 10], Kimi-VL[ 29], InternVL3[ 7],Cl-\naude3-7V[ 3], HunYuan[ 26] and Phi-3[ 1]. Then we judge the ques-\ntion difficulty based on the performance of the chosen MLLM ac-\ncording to Tab.1, it is worth noting that topics where none of the\nten models got it right we excluded.\nAfter processing, we show the distribution of the difficulty of\nthe questions for different Benchmarks in the Fig.4. From the figure,\nwe can see that A) SEEDBench has a lower overall difficulty of\nquestions. B) MMT-Bench has a more even distribution of question\ndifficulty. C) Except for the MMT-Bench, the other three bench-\nmarks have a low distribution of high difficulty questions.\n4 Proposed Method\nIn this section, we introduce the proposed MITV Strategy, which is\nframed as shown in Fig. It consists of three main modules, which are\ntopic selection, interview, and result judgement. Firstly, the question\nselection module selects the difficulty and category of the next\nquestion based on the interviewee‚Äôs response. Then the interview\n--- Page 4 ---\nFigure 4: Difficulty distribution of questions in different\nbenchmarks.\nmodule gives the selected question to the interviewee to answer.\nFinally, the result analysis module judges the interviewee‚Äôs answers\nand summarises the interviewee‚Äôs performance for the question\nselection module‚Äôs judgement. The performance of the interviewees\nis finally obtained through multiple rounds of interview questions\nand answers.\n4.1 Difficulty Determined\nThe difficulty determined module integrates information theory\nand adaptive testing theory. Its core objective is to dynamically\nadjust question difficulty to efficiently maximize information gain\nabout the model‚Äôs capabilities. Prior to evaluation, the model‚Äôs\nability interval is defined, assumed to be [0,10]. An initial estimate\nis made using a bisection-like strategy, starting from the midpoint\nof this interval.\nThe self-information formula from information theory serves as\nthe cornerstone for question selection. The information obtained\nfrom a single observation is calculated as:\nùêº=ùëù0¬∑log2\u00121\nùëù\u0013\n¬∑ùê∂(ùëù)+ùëû0¬∑log2\u00121\nùëû\u0013\n¬∑ùê∂(ùëû)\nwhereùëùandùëûdenote the prior probabilities of the model answering\ncorrectly and incorrectly, respectively, which can be manually con-\nfigured;ùëù0andùëû0represent the observed probabilities of correct\nand incorrect responses; ùê∂(ùëù)andùê∂(ùëû)are confidence functions.\nThis formula indicates that higher prior correctness probability ùëù\nyields less information, while lower ùëùresults in more information\ngain, with confidence weighting applied.\nDuring question selection, expected information gains for ques-\ntions of varying difficulty levels are computed based on the current\nability estimate. Question difficulty ranges from 1 to 10, with em-\npirical accuracy expectations: 70% for Level 3, 50% for Level 5, and\n30% for Level 7, and so on. Questions maximizing expected in-\nformation are prioritized. If the model consistently answers easy\nquestions correctly, the difficulty is increased according to adaptive\ntesting theory; conversely, failures trigger a decrease in difficulty,\niteratively converging towards the interviewee‚Äôs ability level.Table 2: MLLM Interviewees Illustration. We utilize 9 closed-\nsource MLLM interviewees accessed via API calls and 10 open-\nsource MLLM interviewees deployed locally.\nCalling Method Model\nAPI CallGpt-4.1-Nano [2], Gpt-4o-Mini [23]\nGpt-4.1 [2], Gpt-4o [23], Grok-2 [10]\nClaude-3.7-Sonnet [3], Claude-3.5-Sonnet [3]\nQwen-VL-max [5], Qwen-VL-plus [5]\nLocal CallPhi-3.5 [1], Phi-3 [1],Qwen2.5-VL-7b[6]\nQwen2.5-VL-72b [6], Qwen2.5-VL-32b [6]\nInternVL2-4b [7], InternVL2.5-4b [7]\nInternVL3-8b [7], Mini-InternVL [13]\nLlama-3.2-11b-Vision-Instruct [22]\nThe feedback from the interviewer allows us to calculate the\namount of information in the interviewer‚Äôs answer to the previous\nquestion, and if the amount of information is high, the difficulty of\nthe question is adjusted according to whether or not it answered\nthe question correctly. This process can be expressed as:\nLt+1=Ô£±Ô£¥Ô£¥Ô£¥ Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£≥Lt‚àí1,ifùêºt>ùêºthreshold and‚Ñéùëñùë°t=0\nLt+1,ifùêºt>ùêºthreshold and‚Ñéùëñùë°t=1\nLt, otherwise\nwhereùêøùë°denotes the difficulty level of the t-th question, ùêºùë°denotes\nthe amount of information in the response to the t-th question,\nand‚Ñéùëñùë°ùë°denotes the response to the t-th question, which is correct\nif it is 1, and incorrect if it is the other way round. Although the\nintroduction of response informativeness can lead interviewers to\nanswer questions of comparable difficulty at their own level to a\ngreater extent, an uneven distribution of data difficulty can keep\ninterviewees under the same low or high difficulty level for a long\ntime. So we introduce ùëÖùëôto help adjust the level selection and avoid\nthis problem. The process can be expressed as follows:\nLt+1=(\nLt+1,ifùëÖl<ùúÉ1\nLt‚àí1,ifùëÖl>ùúÉ2\nWhereùëÖùëôdenotes the percentage of correct responses at difficulty\nlevelùêø,ùúÉ1andùúÉ2denote the lower and upper thresholds, and ùúÉ1<\nùúÉ2. Ceteris paribus, ùëÖùëôhas a higher priority than ùêºùë°.\n4.2 MLLM Interview Module\nIn order to make the interview questions more effective and typ-\nical, the module selects 10 different types of questions based on\nthe target level. We chose GPT-4o as the interviewer due to its\nbalanced performance. the interviewer MLLM selects a represen-\ntative question from the selected questions, taking into account\nthe responses of the candidates, and then passes the question to\nthe candidate MLLM to answer. Through this systematic process\nof question screening and interaction, it not only ensures that the\ninterview questions can accurately match the assessment needs,\nbut also flexibly adjusts the direction of the investigation based on\nthe interviewee MLLM feedback in the early stages, which helps\nto assess the interview MLLM competency level in a more com-\nprehensive and in-depth manner, and improves the accuracy and\nrelevance of the interview assessment.\n--- Page 5 ---\nTable 3: Performance comparison between the random and the proposed interview strategy, where ‚ÄòQuestion Num.‚Äô indicates\nthe number of used questions and ‚ÄòQuestion %‚Äô stands for the proportions of (used questions)/(all questions) respectively.\nBenchmark A-Bench Q-Bench MMT-Bench SEED-Bench\nQuestion Num. SRCC PLCC KRCC SRCC PLCC KRCC SRCC PLCC KRCC SRCC PLCC KRCC\nRandom Strategy: Questions are randomly sampled from the benchmarks.\n10 0.4558 0.4816 0.3346 0.4267 0.4856 0.3157 0.483 0.6217 0.3594 0.4177 0.7412 0.3133\n20 0.5341 0.5787 0.398 0.6273 0.6447 0.4573 0.5739 0.7603 0.4387 0.4433 0.8439 0.3256\n30 0.5932 0.7500 0.4416 0.6294 0.7334 0.4707 0.6960 0.8344 0.5300 0.5765 0.8598 0.4272\n50 0.6521 0.7803 0.4906 0.7091 0.7998 0.5409 0.7746 0.8855 0.6087 0.6102 0.9066 0.4595\n100 0.7865 0.8875 0.6089 0.8614 0.8888 0.6808 0.8546 0.923 0.6858 0.7276 0.9397 0.5557\nInterview Strategy (proposed): Questions are picked during the interview process.\n10 0.5411 0.4092 0.5854 0.6526 0.4838 0.6592 0.7408 0.5622 0.7857 0.4905 0.3463 0.6268\n20 0.5989 0.4328 0.5872 0.7264 0.5192 0.7477 0.7316 0.5439 0.8320 0.6040 0.4520 0.5948\n30 0.7899 0.6295 0.7916 0.7709 0.5706 0.7634 0.7877 0.5789 0.8453 0.6547 0.4809 0.6374\n50 0.8250 0.6689 0.7946 0.8428 0.6412 0.8360 0.8100 0.6217 0.8283 0.8088 0.6491 0.8304\n100 0.8467 0.6951 0.8158 0.8867 0.7118 0.8324 0.8158 0.6374 0.8217 0.7909 0.6386 0.8130\nQuestion % SRCC PLCC KRCC SRCC PLCC KRCC SRCC PLCC KRCC SRCC PLCC KRCC\nRandom Strategy: Questions are randomly sampled from the benchmarks.\n1% 0.4915 0.5934 0.3744 0.5334 0.5979 0.395 0.6722 0.8307 0.5113 0.8062 0.9537 0.631\n2% 0.6074 0.744 0.4469 0.6507 0.7370 0.4917 0.8125 0.8974 0.6418 0.8678 0.9720 0.7002\n3% 0.6337 0.7936 0.4759 0.6915 0.8096 0.5287 0.841 0.919 0.6636 0.8699 0.9785 0.7076\n4% 0.7419 0.8276 0.5628 0.7877 0.8338 0.6065 0.8915 0.9469 0.7261 0.9296 0.9857 0.7824\n5% 0.7332 0.8392 0.5490 0.8317 0.8976 0.6652 0.8961 0.9527 0.7363 0.9310 0.9869 0.7867\nInterview Strategy (proposed): Questions are picked during the interview process.\n1% 0.5472 0.3934 0.5186 0.7586 0.5353 0.7438 0.7877 0.5789 0.8453 0.9030 0.7596 0.9153\n2% 0.7899 0.6295 0.7916 0.7709 0.5706 0.7634 0.8298 0.6491 0.8476 0.9231 0.7716 0.9323\n3% 0.8043 0.6426 0.7880 0.8323 0.6177 0.8112 0.8070 0.6140 0.8176 0.9312 0.7803 0.9385\n4% 0.8467 0.6951 0.8147 0.8669 0.6490 0.8307 0.8421 0.6725 0.8702 0.9301 0.7795 0.9368\n5% 0.8527 0.7012 0.8162 0.8806 0.6882 0.8302 0.8423 0.6726 0.8693 0.9354 0.7835 0.9414\n4.3 Judgment Module\nThe judging module is mainly responsible for data processing in\nthe interview process, one is to check the answers of the interview-\ners.Due to the random nature of the MLLM answer content, we\nuse a Large language model (LLM) to perform a comprehensive\ncalibration of the interviewee‚Äôs competence in order to avoid the\ndifficulties posed by varying lengths of answers and redundancy of\nanswer information.\nThe judgment module is mainly responsible for the data pro-\ncessing in the interview process, which is to verify the answers\nof the interviewee. Due to the randomness of MLLM answers, we\nuse LLM to comprehensively verify the interviewee‚Äôs ability to\navoid the difficulties caused by the different lengths of answers and\nthe redundancy of answer information. Second, we calculate the\nperformance of the interviewee. This includes the correct rate at\ndifferent difficulty levels, the amount of information in the question\nand answer session, and calculating the distribution of the intervie-\nwee‚Äôs ability level after the interview. The judging module is able\nto statistically analyse the interviewee‚Äôs performance information\nin order to systematically make the next decision.5 Experiment\n5.1 Experiment Detail\nIn order to validate the generalisation of MITV, we have selected 19\ntypical MLLMs, as detailed in Tab. 2. Among them, 9 large models\nwere validated using official API calls, and for the other MLLM\nmodels, we used locally deployed models for the validation. To\nverify the validity of MITV, we designed a control group whose\nBenchmark questions were quizzed through a random sampling\nstrategy. It is worth noting that for the random sampling strategy\ngroup experimental data we used VLMEvalKit [ 11] for uniform\ntesting. It is worth noting that for both sets of experiments, we\nset up an absolute number of questions comparison and a relative\nproportional number of questions comparison.\nOn each Benchmark, the full coverage test performance of the\nmodel are used as the ground truth and three commonly used\nmetrics for algorithm assessment are applied: Spearman rank order\ncorrelation coefficient (SRCC), Pearson linear correlation coefficient\n(PLCC), and Kendall rank order correlation coefficient (KRCC).\n--- Page 6 ---\n10 20 30 50100\nQuestion Number0.00.10.20.30.40.50.60.70.8SRCCStrategy\n Random Strategy\n Interview Strategy(a) A-Bench\n10 20 30 50100\nQuestion Number0.00.20.40.60.8SRCCStrategy\n Random Strategy\n Interview Strategy (b) Q-Bench\n10 20 30 50100\nQuestion Number0.00.10.20.30.40.50.60.70.8SRCCStrategy\n Random Strategy\n Interview Strategy\n(c) MMT-Bench\n10 20 30 50100\nQuestion Number0.00.10.20.30.40.50.60.70.8SRCCStrategy\n Random Strategy\n Interview Strategy (d) SEED-Bench\nFigure 5: Overview of the SRCC performance corresponding the question numbers on 4 benchmarks.\n5.2 Performance Discussion\nThe experimental results in Table 3 demonstrate the effectiveness\nof the proposed MITV strategy compared to the random sampling\nbaseline across four MLLM benchmarks: A-Bench, Q-Bench, MMT-\nBench, and SEED-Bench. With closer inspections, we can obtain\nseveral insights as follows:\n5.2.1 Superior Performance of MITV. Across all benchmarks, MITV\nachieves higher SRCC, PLCC, and KRCC scores than the random\nsampling strategy on most question number settings, both in ab-\nsolute question counts (10 to 100 questions) and relative question\npercentages (1% to 5%). The overview SRCC performance is shown\nin Fig. 5. For instance, on A-Bench with 50 questions, MITV yields\nan SRCC of 0.8250, compared to 0.6521 for random sampling‚Äîa\n26.5% improvement. These results highlight MITV‚Äôs ability to pro-\nduce rankings more aligned with full benchmark evaluations, even\nwith a limited number of questions.\n5.2.2 Efficiency with Small Question Sets. MITV exhibits a signifi-\ncant advantage when the number of questions is small. For example,\nwith just 10 questions on MMT-Bench, MITV achieves an SRCC of\n0.7408, compared to 0.4830 for random sampling‚Äîa 53.4% improve-\nment. This high starting point underscores the efficiency of MITV‚Äôs\nadaptive question selection, which leverages difficulty metadata\nand performance feedback to target the most informative questions\nearly in the evaluation process. In contrast, random sampling strug-\ngles to capture meaningful performance differences with limitedquestions, as it lacks the dynamic adjustment mechanism inherent\nin MITV.\n5.2.3 Generalization Across Benchmarks. The performance gains\nof MITV are consistent across diverse benchmarks, demonstrating\nits generalizability. On SEED-Bench, which has a larger benchmark,\nMITV achieves an SRCC of 0.8088 with 50 questions, compared to\n0.6102 for random sampling.On smaller benchmarks like Q-Bench\nand MMT-Bench, MITV maintains its superiority, with SRCC values\nexceeding 0.83 at 50 questions, approaching the accuracy of full\nbenchmark evaluations\n6 Conclusion\nSince the conventional Benchmark test is a full-coverage ques-\ntion and answer test, there is information redundancy, in order\nto optimise the evaluation method, this paper firstly constructs\nseveral datasets with difficulty labels through the performance of\nten models on the existing Benchmark. Then the MITV strategy\nis proposed, which can obtain the fastest model evaluation per-\nformance through a small number of questions and answers by\nconverting the conventional full-coverage model performance test\ninto an interview ability evaluation. Through a large number of\nexperiments, it is proved that the proposed method is effective, has\ngood generalisation, and can provide suggestions and guidance for\nMLLM assessment work.\n--- Page 7 ---\nReferences\n[1]Marah Abdin, Samyam Jacobs, Mojan Javaheripi, Ojas Pathak, Fady El Antary,\nHani Ghodrat, Sharan Gopi, Ghaith El Kurdi, Shahrzad Shojaee, Renkun Zheng,\net al.2024. Phi-3 Technical Report: A Highly Capable Language Model Locally\non Your Phone. arXiv preprint arXiv:2404.14219 (2024).\n[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al .2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774\n(2023). https://arxiv.org/abs/2303.08774\n[3]Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku .\nTechnical Report. Anthropic. https://www-cdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\n[4]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering.\nProceedings of the IEEE International Conference on Computer Vision (2015), 2425‚Äì\n2433. https://arxiv.org/abs/1505.00468\n[5]Jinze Bai, Shuai Bai, Shuo Chen, Peng Du, Ruchao Fu, Shildon Gui, Wenbin\nHu, Haodong Huang, Junfeng Huang, Xin Jiang, Chang Jin, et al .2023. Qwen-\nVL: A Frontier Large Vision-Language Model with Versatile Abilities. https:\n//arxiv.org/abs/2308.12966. arXiv:2308.12966 [cs.CV]\n[6]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai\nDang, Peng Wang, Shijie Wang, Jun Tang, et al .2025. Qwen2. 5-vl technical\nreport. arXiv preprint arXiv:2502.13923 (2025).\n[7]Keyu Chen, Sijie Zhu, Ziyi Wang, Kunchang Chen, Weidi Zhang, Jilan Yang, Xiyi\nWang, Wenhai Zhang, Xiaoyi Wang, Yu Qiao, et al .2024. InternVL 2: Scaling Vi-\nsion Foundation Models to General-Purpose Multimodal Large Language Models.\narXiv preprint arXiv:2404.16796 (2024).\n[8]Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen,\nHaodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al .2024. Are We on\nthe Right Way for Evaluating Large Vision-Language Models? arXiv preprint\narXiv:2403.20330 (2024). https://arxiv.org/abs/2403.20330\n[9]Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan\nZhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al .2024. Internvl: Scaling\nup vision foundation models and aligning for generic visual-linguistic tasks. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n24185‚Äì24198.\n[10] Murillo Edson de Carvalho Souza and Li Weigang. 2025. Grok, gemini, chatgpt and\ndeepseek: Comparison and applications in conversational artificial intelligence.\nINTELIGENCIA ARTIFICIAL 2, 1 (2025).\n[11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu,\nXiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al .2024. Vlmevalkit: An\nopen-source toolkit for evaluating large multi-modality models. In Proceedings of\nthe 32nd ACM International Conference on Multimedia . 11198‚Äì11201.\n[12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin,\nJinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024.\nMME: A Comprehensive Evaluation Benchmark for Multimodal Large Language\nModels. arXiv preprint (2024). https://arxiv.org/abs/2306.13394 Manuscript\nunder review.\n[13] Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao\nTian, Shenglong Ye, Junjun He, Xizhou Zhu, et al .2024. Mini-InternVL: a flexible-\ntransfer pocket multi-modal model with 5% parameters and 90% performance.\nVisual Intelligence 2, 1 (2024), 1‚Äì17.\n[14] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping\nLuo. 2024. Omnimedvqa: A new large-scale comprehensive evaluation benchmark\nfor medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 22170‚Äì22183.\n[15] Drew A. Hudson and Christopher D. Manning. 2019. GQA: A New Dataset for\nReal-World Visual Reasoning and Compositional Question Answering. Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2019),\n6700‚Äì6709. https://arxiv.org/abs/1902.09506\n[16] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. 2024.\nSEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with\nText-Rich Visual Comprehension. arXiv preprint arXiv:2404.16790 (2024). https:\n//arxiv.org/abs/2404.16790\n[17] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023.\nSeed-bench: Benchmarking multimodal llms with generative comprehension.\narXiv preprint arXiv:2307.16125 (2023).\n[18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen\nZhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al .2024. LLaVAOneVision: Easy\nVisual Task Transfer. arXiv preprint arXiv:2408.03326 (2024). https://arxiv.org/\nabs/2408.03326\n[19] Xiang Li, Jian Ding, and Mohamed Elhoseiny. 2024. VRSBench: A Versatile\nVision-Language Benchmark Dataset for Remote Sensing Image Understanding.\narXiv preprint arXiv:2406.12384 (2024). https://arxiv.org/abs/2406.12384\n[20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao,\nYike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al .2025. MMBench: Is Your\nMulti-Modal Model an All-Around Player? European Conference on ComputerVision (2025), 216‚Äì233. https://arxiv.org/abs/2307.06281\n[21] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi,\nHao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. MathVista:\nEvaluating Mathematical Reasoning of Foundation Models in Visual Contexts.\narXiv preprint arXiv:2310.02255 (2023). https://arxiv.org/abs/2310.02255\n[22] Meta AI. 2024. Introducing Meta Llama 3.1: Our most capable publicly available\nLlama to date. https://ai.meta.com/blog/meta-llama-3-1/.\n[23] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models\nFrom Natural Language Supervision. arXiv preprint arXiv:2103.00020 (2021).\nhttps://arxiv.org/abs/2103.00020\n[25] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv\nBatra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA Models That Can\nRead. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2019), 8317‚Äì8326. https://arxiv.org/abs/1904.08920\n[26] Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang,\nShuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al .2024. Hunyuan-large: An\nopen-source moe model with 52 billion activated parameters by tencent. arXiv\npreprint arXiv:2411.02265 (2024).\n[27] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, et al .\n2023. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint\narXiv:2312.11805 (2023). https://arxiv.org/abs/2312.11805\n[28] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol\nGulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al .2024.\nGemini 1.5: Unlocking multimodal understanding across millions of tokens of\ncontext. arXiv preprint arXiv:2403.05530 (2024).\n[29] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang,\nCheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al .2025. Kimi-vl\ntechnical report. arXiv preprint arXiv:2504.07491 (2025).\n[30] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin\nChen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al .2024. Qwen2-VL: Enhancing\nVision-Language Model‚Äôs Perception of the World at Any Resolution. arXiv\npreprint arXiv:2409.12191 (2024). https://arxiv.org/abs/2409.12191\n[31] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan\nWang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al .2023. Q-bench:\nA benchmark for general-purpose foundation models on low-level vision. arXiv\npreprint arXiv:2309.14181 (2023).\n[32] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai\nDai, Huazuo Gao, Yiyang Ma, et al .2024. DeepSeek-VL2: Mixture-of-\nExperts Vision-Language Models for Advanced Multimodal Understanding.\narXiv:2412.10302 [cs.CV] https://arxiv.org/abs/2412.10302\n[33] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao\nZhang, et al .2024. MMT-bench: a comprehensive multimodal benchmark for\nevaluating large vision-language models towards multitask AGI. In Proceedings of\nthe 41st International Conference on Machine Learning (Vienna, Austria) (ICML‚Äô24) .\nJMLR.org, Article 2359, 83 pages.\n[34] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang,\nWenbo Zhang, Yuqi Lin, Shuo Liu, et al .2024. MMT-Bench: A Comprehensive\nMultimodal Benchmark for Evaluating Large Vision-Language Models Towards\nMultitask AGI. arXiv preprint arXiv:2404.16006 (2024). https://arxiv.org/abs/2404.\n16006\n[35] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu,\nXinchao Wang, and Lijuan Wang. 2023. MM-Vet: Evaluating Large Multimodal\nModels for Integrated Capabilities. arXiv preprint arXiv:2308.02490 (2023). https:\n//arxiv.org/abs/2308.02490\n[36] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang,\nSamuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al .2024. MMMU: A\nMassive Multi-Discipline Multimodal Understanding and Reasoning Benchmark\nfor Expert AGI. Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2024), 9556‚Äì9567. https://arxiv.org/abs/2311.16502\n[37] Zicheng Zhang, Shaolin Su, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang.\n2024. A-Bench: Are LMMs Masters at Evaluating AI-Generated Images? arXiv\npreprint arXiv:2406.03070.\n[38] Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo\nMin, Haodong Duan, Kai Chen, and Guangtao Zhai. 2025. Redundancy Principles\nfor MLLMs Benchmarks. arXiv preprint arXiv:2501.13953 (2025).",
  "text_length": 36922
}