{
  "id": "http://arxiv.org/abs/2506.04156v1",
  "title": "A Dataset for Addressing Patient's Information Needs related to Clinical\n  Course of Hospitalization",
  "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.",
  "authors": [
    "Sarvesh Soni",
    "Dina Demner-Fushman"
  ],
  "published": "2025-06-04T16:55:08Z",
  "updated": "2025-06-04T16:55:08Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04156v1",
  "full_text": "--- Page 1 ---\nA Dataset for Addressing Patient’s Information Needs  \nrelated to Clinical Course of Hospitalization  \nSarvesh Soni, Dina Demner -Fushman  \nDivision of Intramural Research  \nNational Library of Medicine , National Institutes of Health , Bethesda, MD, USA  \nsarvesh.soni@nih.gov  \nAbstract  \nBackground: Patients have distinct information needs  related to their hospitalization that can be  \nfulfilled using clinical evidence from  electronic health records  (EHRs) . Advanced artificial \nintelligence (AI) systems  have the potential  to fulfill these needs;  however, robust, high -quality \ndatasets are essential to evaluate the factual accuracy and relevance of AI -generated responses. \nTo our knowledge, no existing dataset captures patient information needs in the context of their \nEHRs.  \nMethods: We developed a novel, expert -annotated dataset , ArchEHR -QA, based on  real-world \npatient cases from intensive care unit and emergency department settings. The case s comprise  \nquestions  posed by patients to public health forums , clinician -interpreted counterparts, relevant \nclinical note excerpts with sentence -level relevance annotations, and clinician -authored answers. \nTo establish benchmarks for grounded EHR question answering (QA), we evaluated three open -\nweight large language  models (LLMs) —Llama 4,  Llama 3, and Mixtral —using three prompting \nstrategies: (1) generating answers with citations  to clinical note sentences , (2) generating answers \nbefore citations, and (3) generating answers from filtered citations. We assessed performance \nbased on two metrics: Factuality  (overlap between cited note sentences and ground truth  \nrelevance annotations) and Relevance  (textual and semantic similarity between system and \nreference answers).  \nResults: The final dataset contains 134 patient cases. Across all prompting strategies and \nmodels, the answer -first prompting approach consistently yielded the highest performance. Llama \n4 outperformed the other models across strategies, achieving the best overall score using the \nanswer -first method. Manual er ror analysis supported the automated findings and highlighted \ncommon issues, including omission of critical clinical evidence and incorporation of contradictory \nor hallucinated information.  \nConclusions: We introduce a first -of-its-kind dataset to support the development and evaluation \nof patient -centered EHR QA systems. Our benchmarking results establish performance baselines \nand emphasize the need for further advancements to improve the factual consisten cy and \nrelevance of AI -generated responses in clinical contexts.  \n--- Page 2 ---\nIntroduction  \nQuestion answering (QA) is an organic way to interact with complex information systems such as \nelectronic health records (EHRs)1, where a QA system responds to user questions with exact \nanswers. The major focus of existing EHR QA work has been on addressing clinician information \nneeds2, with datasets for system development and evaluation largely prioritizing these \nrequirements3. However, with the increasing patient -involvement in their care4–8, there is a need \nfor targeted EHR QA research that incorporates the unique needs of patients from their health \nrecords9,10. To this end, datasets play an important role in developing  and evaluating tailored \nartificial intelligence (AI) systems and, thus, the datasets must be representative of the target end-\nuser's needs11, i.e., patients.  \n \nDespite growing interest in patient -centered care , most work on addressing the health information \nneeds of patients  uses general  health resources12. This disconnect  often results in systems that \nfail to incorporate the context that prompt s the patients’ questions, i.e., EHRs, contributing to non -\npatient -centered solutions13. EHRs document important clinical evidence about the care provided \nto patients. However, the rationale for drafting these documents often differs from the primary \nreasons patients access them14. This necessitates build ing assistive technologies to contextualize \nEHR information to address patients’ information needs.  \n \nMoreover , the volume of patient requests for medical information through patient portals is rising, \ncontributing to desktop medicine and increasing clinician burden15. Most existing studies on \nautomated responses to patient messages do not incorporate critical contextual information from \nEHRs16–20. Among studies  that use EHR  content , none provide comprehensive evaluations of \nhow effectively the generated responses  leverage this clinical context21,22. \n \nGrounding  is crucial  in AI applications in medicine, as it ensures that AI models are anchored to \naccurate, contextually relevant, real -world clinical data. This is particularly important when the \nintended audience lacks clinical expertise23. To effectively design and evaluate grounded QA  \nsystems , a representative dataset and evaluation framework is essential24. \n \nIn this work, we propose a benchmark dataset, ArchEHR -QA, to evaluate grounding capabilities \nof models for responding to patient -initiated queries. The proposed dataset consists of patient -\ninitiated questions  posted in public domain , the corresponding clinician -interpreted questions, \ntheir answers annotated in the EHRs, and clinician -generated free -text answers to the questions. \nWe collect true patient health information needs by aligning real -world health forum messages to \npublicly accessible real EHRs. To our knowledge, this is the first pub lic dataset that encapsulates \npatient questions and relevant clinical evidence from EHRs. We further provide an evaluation \nframework to assess two critical aspects of a  grounded EHR QA  system: does it identify relevant \ninformation from given clinical evidence  and does it  use this information in responding to user \nqueries.  \n \n \n--- Page 3 ---\nTable 1. Example annotated case  from the dataset  with a sample system -generated response . \nPatient Question  (underlined are the areas of focus)  \nTook my 59 yo father to ER ultrasound discovered he had an aortic aneurysm. He had a salvage repair \n(tube graft).  Long surgery / recovery for couple hours then removed packs. why did they do this \nsurgery?????  After this time he spent 1 month in hospital now sent home.  \nClinician Question (interpreted from the patient question)  \nWhy did they perform the emergency salvage repair on him?  \nClinical Note Excerpt  (sentences numbered for grounding) –  1 and 2 are essential; 3 is \nsupplementary; all others are not -relevant  \n1:  He was transferred to the hospital on 2025 -1-20 for emergent repair of his ruptured thoracoabdominal \naortic aneurysm. 2:  He was immediately taken to the operating room where he underwent an emergent \nsalvage repair of ruptured thoracoabdominal aortic aneurysm with a 34 -mm Dacron tube graft using deep \nhypothermic circulatory arrest. 3:  Please see operative note for details which included cardiac arrest x2. \n4:  Postoperatively he was taken to the intensive care unit for monitoring with an open chest. 5:  He \nremained intubated and sedated on pressors and inotropes. 6:  On 2025 -1-22, he returned to the \noperating room where he underwent exploration and chest closure. 7:  On 1 -25 he returned to the OR \nfor abd closure JP/ drain placement/ feeding jejunostomy placed at that time for nutritional support.  \n \n8:  Thoracoabdominal wound healing well with exception of very small open area mid wound that is \n@1cm around and 1/2cm deep, no surrounding erythema. 9:  Packed with dry gauze and covered \nw/DSD.  \nClinician Answer  (with citations to note sentences) – some sentences may be unsupported  \nThe patient needed emergency salvage repair for his aortic aneurysm because the aorta had ruptured \n[1]. This rupture is something that needs to be repaired immediately or the patient will die. The patient \nneeded a tube graft for the repair [2]. An aortic aneurysm is a very serious diagnosis and repair surgery \nis not something that can be put on hold. Additionally, it appears this surgery was absolutely necessary \nbecause the patient arrested twice during the operation [3]. \nSystem -generated  Answer  (with citations to note sentences) – some sentences may be unsupported  \nThey performed the emergency salvage repair because the patient had a ruptured thoracoabdominal \naortic aneurysm. |1,2| The surgery was necessary to address the rupture, which required immediate \nattention, and involved a 34 -mm Dacron tube graft using deep hypothermic circulatory arrest. |2| The \nprocedure was done to save the patient's life after he experienced cardiac arrest twice during the \noperation. |3| \nMethods  \nData  \nThere is no public resource containing both patient -initiated questions and their EHRs. Thus, we \ncreated ArchEHR -QA by align ing two different publicly available resources for patient questions \nand EHRs  (Figure 1 ). To incorporate true patient information needs, we derive questions using \npublic health forum s (specifically,  HealthCareMagic, now Ask A Doctor -24x7)25, one of the most \n--- Page 4 ---\npopular internet venues for patients to seek information where patients pose their information \nneeds as discussion posts26,27. To associate relevant medical context to the questions, we find \nclinical evidence using discharge summaries from publicly accessible EHR data ( specifically, \nMIMIC -III and MIMIC -IV databases  which contain deidentified patient records28,29). \nAlignment  \nHealth forum posts were filtered to reflect scenarios in which patients had recently been \ndischarged from the hospital —a common context for post -encounter online information seeking30. \nThe posts containing keywords such as “ admission ”, “emergency room ”, and “ICU” (see \nSupplementary Materials for a complete list)  were selected and reviewed by SS (clinical \ninformaticist) and DDF (MD and computational linguist). For each reviewed forum post, a set of \ncandidate relevant discharge summaries (notes) were retrieve d from the EHR. Due to the \ndifferences in vocabulary between patient -authored posts and clinician - authored  notes (e.g., \n“pelvic”  vs “inferior perineal” ), we augmented the text in both sources with standardized \nterminology.  For forum posts,  we appended  ICD code descriptions automatically generated by \nprompting the Llama  3.3 70B31 large language model ( LLM; prompt details available in the \nsupplementary materials) . For clinical notes, we appended  the corresponding ICD code \ndescriptions  associated with the patient ’s admission  (available as part of the structured data in \nthe MIMIC  database s). The expanded posts were used to retrieve candidate notes using BM25 \nalgorithm32. The top 10 notes for each post were manually reviewed by SS and DDF to select \nonly the ones that may be used as evidence to satisfy the information needs expressed in the \nforum post. For each selected note, the corresponding forum post was minimally modified to \nmatch the s urrounding details (e.g., changing “mother”  to “father” ) to ensure its alignment with the \nnote. We never  modified the underlying information needs in the post  or the associated note \ncontent . Annotation guidelines are provided in Supplementary Material.  \nQuestion and Answer Creation  \nFor each aligned forum post -note pair, SS , DDF, and a licensed clinician  annotated the query -\nrelated focus areas in the post and created a clinician’s version of the patient’s query —this is a \nquestion that a clinician would interpret and respond to (Table 1). Further, each sentence in the \nclinical note was manually annotated with a “relevance”  label, indicating its importance in \nanswering the question: “essential”  (must be used), “supplementary”  (provide s support but not \nnecessary), or “not-relevant” . Due t o the considerable length of the notes, we manu ally curated \nnote excerpts  by excluding paragraphs composed entirely of sentences  deemed “not-relevant” , \nwhile retaining those that provided important contextual information (e.g., section headers such \nas “Brief Hospital Course” ). Lastly, we tasked licensed clinicians  (hired through Centaur Labs)  to \nreview and update the relevance labels for the note sentences and compose 75 -word \n(approximately 5 -sentence) ground truth answers  to the questions.  \n--- Page 5 ---\nValidation  \nAll annotations were reviewed and revised as necessary by a second annotator.  Alignment and \nsentence labeling was performed by the authors  and reviewed by a licensed clinician, who revised \nthe annotations as needed and authored an answer. Lastly, the clinician -written answer was \nreviewed by DDF, revising as needed. We report the annotation agreements for all stages.  \n \nWe perform ed baseline experiments for the task of automatically generating an answer to the \ninput question (using both patient and clinician versions) with citations to the specific sentences \nin the input note excerpt. Based on existing literature33,34, we restricted the output answer length \nto 75 words (or approximately 5 sentences). The baseline approaches  (Figure 2 ) include a \nheuristic method  that selects the most semantically similar note sentences using cosine similarity \nover MiniLM semantic embeddings35 and restricts the response to the top -ranked sentences \nwithin the word limit . \n \nIn addition to the heuristic baseline, we evaluated  three prompting strategies using LLMs, each \nreflecting a distinct approach to citation integration : (1) generating answers with citations  \nembedded directly in the output , (2) generating answer s first followed by the addition of citations \nin a subsequent step , and (3) generating answers using only a specified set of citations as input . \nThese strategies were  designed to evaluate how citation context and ordering influence the \nmodel’s ability to produce factual and relevant responses.  \nAll prompts are structured into four components: (a) an input section  containing the patient and \nclinician questions; (b) task-specific instructions , which defined the objective of the prompt (e.g., \nfor the “Answer & Cite”  task, instructions explicitly directed the model to generate a response with \ninline citations); (c) formatting instructions , which specified  the required  output structure (e.g., for \nthe “Answer & Cite”  task, responses were to be formatted in markdown, with each answer \nsentence presented on a separate line followed by the corresponding citation s); and (d) a \nfeedback section  provid ing error -specific guidance based on prior output. During prompt \ndevelopment, we observed several  correctable errors , such as  failing to include citations or format  \ncontent on separate lines . To address these issues, we implemented an iterative prompting \nstrategy , retrying each prompt up to five times. At each iteration, the mode l received \nprogrammatically generated feedback tailored to the specific formatting or content errors identified \nin the preceding  response.  \n \nWe benchmark ed the zero -shot performance of three popular LLMs on ArchEHR -QA: Llama  3.3 \n70B31, Llama  4 17B 16E36, and Mixtral  8x22B37. These models represent top -tier performance \namong open -weight LLMs  that can be run locally . The Llama models were developed  by Meta AI , \nwhile  the Mixtral model was released  by Mistral AI . Llama 3.3 employs  a standard transformer \narchitecture, whereas  Llama  4 and Mixtral leverage  a mixture -of-experts (MoE) design  to improve \ncomputational efficiency and performance . \n--- Page 6 ---\nEvaluation  \nSystem -generated responses are evaluated along two dimensions: Factuality  (use of clinical \nevidence for grounding ) and Relevance  (similarity  to the ground -truth answer ). \n \nFactuality  is measured using  an F1 Score between the sentences cited as evidence in the system -\ngenerated answer  (which are  treated  as predicted essential  note sentences ) and the ground truth \nrelevance labels  for note sentence s. We define  two versions of Factuality : “Essential -only”  and \n“Essential + Supplementary” . In the “Essential -only” definition, only sentences labeled as \nessential in the ground truth count  as positive s. In the “Essential  + Supplementary ” version, \nground truth sentences labeled as either essential or supplementary a re counted as positive s \n(penalizing the system for failing to cite either, but not for including supplementary ones ). We \nreport  Factuality  both at the macro level (averaging per-case  F1 scores ) and the micro level \n(aggregating true positives, false positives, and false negatives across all cases ). We designate  \nthe essential -only micro F1 Score as Overall Factuality  and use it to calculate the Overall Score , \nas it captures aggregate performance across all instances  focusing on  the most  important note \nsentences . \n \nRelevance  is evaluated  by comparing the generated answer text to a clinician -authored reference \nanswer using text - and semantics -based metrics : BLEU38, ROUGE39, SARI40, BERTScore41, \nAlignScore42, and MEDCON43. Each metric is normalized, and Overall Relevance  is computed as \nthe mean of these normalized scores.  We compute another version of relevance scores by \ntreating the set of essential ground  truth sentences (together with the original question) as the \nreference . This alternative provides a feasible and scalable approximation for evaluating answer \nquality  in the absence of human -written  answers.   \n--- Page 7 ---\n \n \nFigure 1: Dataset creation  workflow . (1) Patient information needs identified  from a public health \ndiscussion forum  are aligned with clinical evidence from an electronic health record  database  to \nproduce  pairs of patient -posed questions and associated clinical notes. (2) Clinician -interpreted \nquestions are created from patient questions and sentence -level relevance labels are annotated  in the \nnote. (3) Clinicians compose natural language answers considering the notes.  Annotations are \nreviewed and updated at each stage  to ensure high  quality .  \n\n--- Page 8 ---\n \nFigure  2: Overview of baseline  approaches for the task of generating answers with citations . (A) \nPrompt structures:  Each prompt is organized into four sections: (a) Input questions ; (b) Instructions  for \nthe specific task component —“Answer & Cite ”, “Answer -only”, or “Cite-only”; (c) Desired response \nformat; and (d) Feedback from prior iterations, if available.  Each prompt is executed up to five times \nto handle errors, after which the final response is output.  (B) Baseline approaches : Four prompting \napproaches are evaluated : (1) “Together” —generating the answer and citations together; (2) “Answer \nFirst”—generating the answer first, then citations; (3) “Evidence First ”—providing the model with \nfiltered evidence sentences (based on cosine similarity to the question s); and (4) “Heuristics ”—using \nevidence filtering followed by heuristic selection of supporting sentences until a word limit is reached.  \n\n--- Page 9 ---\nResults  \nThe proposed dataset contains a total of 134 patient cases —104 using intensive care unit (ICU) \nnotes (MIMIC -III) and 30 using emergency department (ED) notes (MIMIC -IV). Table 2  details \npatient demographics and case statistics.  The average length of patient narratives is 91.3 words  \n(with a standard deviation  [SD] of 56.9)  while the clinician questions are 10.5 words (SD 3.6) long.  \nThe clinician -written answers have an average of 72.6 (SD 3.4) words.  The note excerpts contain \na mean of 25.7 sentences , 25.7% (6.6) of which are essential and 20.1% (5.2) are supplementary . \nThe mean length of ICU note excerpts exceeds that of  ED note excerpts, likely reflecting greater \nclinical complexity  of ICU cases.  This is also suggested  by the prevalen ce of  ICD codes  in each \nsetting . ICU diagnoses predominantly involv e heart, kidney, and respiratory failure , which  often \nrequire intensive  and multidisciplinary management , whereas  ED diagnoses more often include \nsingle -system , episodic , or behavioral  conditions  such as  asthma, apnea, and nicotine \ndependence . \n \nIn the first round of  sentence relevance annotations review, clinicians agreed  with the existing \nlabels  77.1%  (2653 /3450 ) of the time , with the most common edit s being changes  from essential \nto supplementary  (39.9% [318/ 797]) or to not-relevant  (26.6% [212/797]) . In the second round, \nthe reviewer ’s (DDF) agree ment  with the clinician -modified labels was 84.7%  (2874 /3450), with \nthe predominant edit again being  from essential to supplementary (56.1% [323/ 576]).  \nFurthermore, the reviewer (DDF) retained 81.3% of the clinician -written answers , calculated as \nthe proportion of word lemmas retained from the original  answers while ignoring punctuations and \nstop words . \n \nTable 3  summarizes  the benchmarking results on a held -out test set of 100 cases  (80 from ICU \nand 20 from ED) . The remaining 34 cases were used exclusively  for the development  of prompts . \nAll the model -based approaches  performed the best for Answer First  variation, followed by \nTogether  and then Evidence First  across most evaluati on metrics , including the Overall  Factuality \nand Relevance . Llama 4 consistently achieved superior Factuality F1 scores across approaches  \nwith the best Overall Factuality  (essential -only micro F1 ) of 51.8% , followed by Mixtral and Llama \n3. In contrast, for  Relevance , Mixtral outperformed the other models in most instances , achieving  \nthe best Overall Relevance of 33.8%  (compared against  human -authored answers), followed by \nLlama 3 and Llama 4.  \n \nOn Factuality , when the evaluation considered the supplementary sentences as important , F1 \nscores dropped ( driven primarily by a drop in Recall despite gains in Precision ). This indicates \nthat models were more effective at identifying the most important information from the clinical  \nnotes but struggled to consistently capture supporting details.  Notably, macro F1 score s \nconsistently  exceeded the micro F1 score s, reflecting  consistent performance across cases, \ndespite variations  in instance -level scores . Interestingly, Together  approach  often yielded  higher \nprecision, though at the expense of a  lower recall, leading to modest F1 scores.  Also, Evidence \nFirst consistently  produced the poorest F1 scores , even underperforming  the Heuristic approach, \n--- Page 10 ---\nhighlighting  that constraining the evidence for the model creates a bottleneck for model \nperformance.   \n \nOn Relevance  using  human -authored answers , Evidence First  also underperformed across most \nmetrics . This suggest s that pre -selecting evidence negatively affects not only factual correctness \nbut also the relevance  to clinician answers . Among models, Mixtral attained the highest MEDCON \nscore (40.2%, using Answer First  approach ), indicating strong alignment with clinically relevant \nmedical concepts.  Llama 3 attained the highest  AlignScore (47.0%, using Together ), underlining \nits superior  alignment with the clinician answers  despite modest performance on other metrics . \nThe Heuristic  approach showed consistently lower  performance across metrics . Notably,  its \nBERTScore dropped to 30.4% , significantly lower than  the worst -performing model -based \napproach (36.9%  using Evidence First and Llama 4 ). \n \nWhen the generated answers were compared to the essential note sentences and the question , \nthe heuristic version achieved the highest scores —an artifact of the relevance metrics ’ bias  \ntoward  lexical overlap . Interestingly,  the LLM-based variations  achieved  comparable scores using \nhuman answers  or notes . There is a statistically significant positive correlation between the overall  \nrelevance score s calculated using human answers and notes (Kendall’s  𝜏 = 0.362, p < 0.0001, \nn=1000 ; comparing 100 individual case scores  across 10 model variations ), indicating moderate \nagreement . \n  \n--- Page 11 ---\nTable 2. Descriptive statistics of the dataset.  ICU: Intensive Care Unit , ED: Emergency \nDepartment.  Population statistics  are in count (proportion)  format.  \nCategory  Value  ICU (N=104) ED (N=30)  All (N=134)  \nRace  or \nEthnicity  \n \n White  74 (71.2%)  17 (56.7%)  91 (67.9%)  \nBlack/African \nAmerican  13 (12.5%)  7 (23.3%)  20 (14.9%)  \nHispanic/Latino  4 (3.8%)  2 (6.7%)  6 (4.5%)  \nAsian  3 (2.9%)  2 (6.7%)  5 (3.7%)  \nOther  1 (1.0%)  1 (3.3%)  2 (1.5%)  \nUnknown/Declined  9 (8.7%)  1 (3.3%)  10 (7.5%)  \nSex Male 58 (55.8%)  10 (33.3%)  68 (50.7%)  \nFemale  46 (44.2%)  20 (66.7%)  66 (49.3%)  \nAge Group  0-30 8 (7.7%)  2 (6.7%)  10 (7.5%)  \n31-40 5 (4.8%)  7 (23.3%)  12 (9.0%)  \n41-50 13 (12.5%)  3 (10.0%)  16 (11.9%)  \n51-60 20 (19.2%)  6 (20.0%)  26 (19.4%)  \n61-70 25 (24.0%)  8 (26.7%)  33 (24.6%)  \n71-80 16 (15.4%)  2 (6.7%)  18 (13.4%)  \n81-89 15 (14.4%)  1 (3.3%)  16 (11.9%)  \n90+ 2 (1.9%)  1 (3.3%)  3 (2.2%)  \nTop 10 ICD \nCodes - ICU 4019 - Unspecified \nessential \nhypertension  39 (37.5%)  3 (10.0%)  42 (31.3%)  \n4280 - Congestive \nheart failure, \nunspecified  34 (32.7%)  0 (0.0%)  34 (25.4%)  \n42731 - Atrial \nfibrillation  27 (26.0%)  0 (0.0%)  27 (20.1%)  \n5849 - Acute \nkidney failure, \nunspecified  26 (25.0%)  3 (10.0%)  29 (21.6%)  \n51881 - Acute \nrespiratory failure  26 (25.0%)  0 (0.0%)  26 (19.4%)  \n5990 - Urinary \ntract infection, site \nnot specified  23 (22.1%)  0 (0.0%)  23 (17.2%)  \n2724 - Other and \nunspecified \nhyperlipidemia  22 (21.2%)  2 (6.7%)  24 (17.9%)  \n--- Page 12 ---\n41401 - Coronary \natherosclerosis of \nnative coronary \nartery  22 (21.2%)  2 (6.7%)  24 (17.9%)  \n486 - Pneumonia, \norganism \nunspecified  19 (18.3%)  0 (0.0%)  19 (14.2%)  \n2761 - \nHyposmolality  \nand/or \nhyponatremia  18 (17.3%)  1 (3.3%)  19 (14.2%)  \nTop 10 ICD \nCodes - ED K219 - Gastro -\nesophageal reflux \ndisease without \nesophagitis  0 (0.0%)  8 (26.7%)  8 (6.0%)  \nJ45909 - \nUnspecified \nasthma, \nuncomplicated  0 (0.0%)  7 (23.3%)  7 (5.2%)  \nG4733 - \nObstructive sleep \napnea (adult) \n(pediatric)  0 (0.0%)  6 (20.0%)  6 (4.5%)  \nZ87891 - Personal \nhistory of nicotine \ndependence  0 (0.0%)  6 (20.0%)  6 (4.5%)  \nI10 - Essential \n(primary) \nhypertension  0 (0.0%)  6 (20.0%)  6 (4.5%)  \nF419 - Anxiety \ndisorder, \nunspecified  0 (0.0%)  6 (20.0%)  6 (4.5%)  \nF329 - Major \ndepressive \ndisorder, single \nepisode, \nunspecified  0 (0.0%)  5 (16.7%)  5 (3.7%)  \nD649 - Anemia, \nunspecified  0 (0.0%)  4 (13.3%)  4 (3.0%)  \nF17210 - Nicotine \ndependence, \ncigarettes, \nuncomplicated  0 (0.0%)  4 (13.3%)  4 (3.0%)  \n--- Page 13 ---\nE785 - \nHyperlipidemia, \nunspecified  0 (0.0%)  4 (13.3%)  4 (3.0%)  \nCase Statistics  \nPatient \nNarrative \nWord Count  Mean  90.2 94.8 91.3 \nMedian  72.0 90.5 76.5 \nStd Dev  61.7 36.6 56.9 \nMin 33.0 54.0 33.0 \nMax 440.0  192.0  440.0  \nClinician \nQuestion \nWord Count  Mean  10.6 10.2 10.5 \nMedian  10.0 9.0 10.0 \nStd Dev  3.6 3.8 3.6 \nMin 3.0 4.0 3.0 \nMax 21.0 21.0 21.0 \nAnswer Word \nCount  Mean  72.6 72.3 72.6 \nMedian  73.0 73.5 73.0 \nStd Dev  3.4 3.4 3.4 \nMin 55.0 61.0 55.0 \nMax 78.0 75.0 78.0 \nNote Excerpt \nWord Count  Mean  410.2  280.7  381.2  \nMedian  383.5  223.0  351.5  \nStd Dev  200.1  196.8  205.9  \nMin 107.0  76.0 76.0 \nMax 1028.0  868.0  1028.0  \nMean  \nNote \nSentences \nCount  All 27.6 19.3 25.7 \nEssential  7.0 (25.5%)  5.2 (26.7%)  6.6 (25.7%)  \nSupplementary  5.9 (21.4%)  2.6 (13.4%)  5.2 (20.1%)  \nNot Required  14.7 (53.1%)  11.6 (59.8%)  14.0 (54.3%)  \n \n  \n--- Page 14 ---\nTable 3. Results  from benchmarking experiments . “Overall Relevance ” is the mean of all \nnormalized relevance scores. “Overall Score ” is the average  of “Overall Relevance ” and “Overall \nFactuality ” (Essential -only Micro F1). All scores are reported as percent ages . Macro:  average per -\ncase F1; Micro:  aggregate true/false positives  and negatives  across all cases ; P: Precision ; R: \nRecall ; F1: F Score ; Llama 3:  Llama 3.3 70B ; Llama 4:  Llama  4 17B 16E; Mixtral:  Mixtral  8x22B . \nBest and second -best scores are bolded  and underlined , respectively.  \n(a) Factuality Metrics  \nApproach  Model  Essentia l-only Essential + Supplementary  \nMacro  Micro  Macro  Micro  \nP R F1 P R F1* P R F1 P R F1 \n              \nTogether  Llama 3  74.2 38.6 47.3 65.3 32.8 43.7 89.2 32.4 42.2 83.7 22.9 35.9 \nLlama 4  67.0 47.5 53.1 62.4 41.8 50.0 81.1 39.0 48.8 79.1 28.8 42.2 \nMixtral  61.2 39.0 44.2 58.2 33.3 42.4 77.4 34.4 43.2 77.7 24.1 36.8 \n              \n              \nAnswer \nFirst Llama 3  71.5 45.7 51.2 59.5 39.8 47.7 85.4 37.2 46.2 77.7 28.2 41.4 \nLlama 4  66.8 51.3 55.0 56.9 47.6 51.8 84.7 43.7 53.6 79.1 35.9 49.4 \nMixtral  60.3 51.6 52.6 49.7 45.9 47.7 77.9 45.6 53.4 71.7 36.0 47.9 \n              \n              \nEvidence \nFirst Llama 3  69.8 35.3 43.9 67.3 29.0 40.6 87.2 30.1 39.6 85.2 20.0 32.4 \nLlama 4  64.0 38.7 44.9 59.2 32.8 42.2 80.4 33.1 42.5 78.1 23.5 36.1 \nMixtral  55.1 38.5 41.7 61.5 32.1 42.2 71.2 34.7 41.4 80.2 22.7 35.4 \n              \nHeuristic  -- 60.1 44.0 47.8 59.0 38.0 46.2 81.8 39.8 47.6 80.7 28.2 41.8 \n* Essential -only Micro F1 is considered Overall Factuality . \n(b) Relevance Metrics using Human Answers  \nApproach  Model  BLEU  ROUGE  SARI  BERTScore  MEDCON  AlignScore  Overall \nRelevance  \n              \nTogether  Llama 3  2.4 21.0 49.2 39.3 36.7 47.0 32.6 \nLlama 4  5.5 22.7 49.2 40.2 38.3 35.6 31.9 \nMixtral  5.7 23.1 51.8 40.2 39.9 38.3 33.2 \n              \n              \nAnswer \nFirst Llama 3  3.4 22.7 51.0 40.4 38.9 41.5 33.0 \nLlama 4  6.7 23.6 51.7 40.3 38.1 36.0 32.7 \nMixtral  6.8 24.2 53.1 42.5 40.2 35.8 33.8 \n              \n              \nEvidence \nFirst Llama 3  1.7 20.0 49.1 37.4 31.3 43.6 30.5 \nLlama 4  4.0 20.3 48.8 36.9 32.8 37.4 30.0 \nMixtral  4.9 22.6 52.9 38.7 39.0 39.8 33.0 \n              \nHeuristic  -- 6.7 22.1 53.6 30.4 35.0 34.0 30.3 \n \n \n \n \n \n \n \n \n--- Page 15 ---\n(c) Relevance Metrics using Note Excerpt  and Questions  \nApproach  Model  BLEU  ROUGE  SARI  BERTScore  MEDCON  AlignScore  Overall \nRelevance  \n              \nTogether  Llama 3  0.3 17.8 49.4 23.7 29.0 59.0 29.9 \nLlama 4  1.8 25.1 49.0 28.5 36.9 54.7 32.7 \nMixtral  1.5 22.9 52.9 27.1 35.2 54.2 32.3 \n              \n              \nAnswer \nFirst Llama 3  0.4 19.2 51.2 24.7 30.0 58.1 30.6 \nLlama 4  1.9 25.3 51.4 28.2 34.9 54.0 32.6 \nMixtral  1.6 24.9 54.2 28.9 37.9 52.9 33.4 \n              \n              \nEvidence \nFirst Llama 3  0.2 16.2 49.1 22.0 26.0 56.9 28.4 \nLlama 4  1.1 22.0 48.6 25.6 32.5 55.9 31.0 \nMixtral  1.4 22.7 53.0 26.5 34.5 54.3 32.1 \n              \nHeuristic  -- 11.7 37.3 72.3 38.8 44.1 66.1 45.0 \n \n(d) Overall Scores . Human : computed with relevance metrics using  Human Answers  from (b) ; \nNotes:  computed with  relevance metrics using  Note Excerpt and Questions from (c).  \nApproach  Model  Overall Score  (Human)  Overall Score  (Notes)  \n              \nTogether  Llama 3  38.1 36.8 \nLlama 4  41.0 41.3 \nMixtral  37.8 37.3 \n              \n              \nAnswer  \nFirst Llama 3  40.3 39.2 \nLlama 4  42.3 42.2 \nMixtral  40.7 40.6 \n              \n              \nEvidence  \nFirst Llama 3  35.5 34.5 \nLlama 4  36.1 36.6 \nMixtral  37.6 37.1 \n              \nHeuristic  -- 38.2 45.6 \nDiscussion  \nWe present ed a benchmarking dataset , ArchEHR -QA, comprising patient questions, clinician -\ninterpreted versions of questions , corresponding discharge summaries with sentence -level \nrelevanc e annotations, and clinician -written answers.  To our knowledge, there is no publicly \navailable  resource that couple d patient information needs from their EHRs with actual clinical \ndocuments. We achieve this by combining two different publicly accessible  databases  on a novel \npremise : “what questions do patients ask about the clinical course of their hospitalization ?”. With \na total of 134 patient cases  with detailed annotations, the proposed dataset will serve as a strong \nbenchmark to develop and evaluate AI tools for EHR -specific information seeking.  \n \nDuring our manual error analysis of the highest and lowest scoring runs  (based on Factuality and \nRelevance  metrics ) by the Llama 4 model using the Answer First  approach , we observed that the \n--- Page 16 ---\nautomated metrics generally aligned with the actual quality of the generated responses.  In most \ninstances, the model -generated answers successfully captured the core content of the clinician -\nauthored reference answers.  For example, in one case, the model correctly inferred that the \npatient “was given antibiotics because she developed pneumonia,”  but incorrectly  attributed this \ndecision to “a chest x -ray that indicated low oxygen levels” —a detail  not supported by the clinical \nnote and inconsistent  with general medical knowledge . In another case , the model failed to \nmention an important related procedure  (“thoracentesis ”), despite its  relevan ce to the question.  \n \nIn one of the lowest -performing cases, the model  incorrectly asserted  that “no specific order [was] \nmentioned for taking the prescribed medications ,” whereas the clinical note clearly included \ninstructions  for a “dexamethasone taper .” More over, in the same instance , the model \ndisproportionately reproduced much of the content from patient’s question , rather than grounding \nthe answer in clinical evidence.  Interestingly, in some cases, although the model included citations \nto relevant sentences from the note, it failed to accurately convey the associated information in \nthe generated text.  Complete examples and corresponding annotations from our manual review \nare included  in Supplementary Material (Table S2) . \n \nOur evaluation framework targeted two key dimensions: Factuality , assessing the cited note \nsentences , and Relevance , evaluating the answer  text consistency with the reference response. \nManual inspection showed alignment with these automated assessments. Nonetheless, we \nacknowledge the potential for more scalable and nuanced evaluations, such as leveraging \nadvanced LLMs to assess answer quality, particularly in relation to annotated reference \nanswers44. \n \nIn addition to the rigorous validation protocol employed  throughout the development of ArchEHR -\nQA, the dataset has also been subjected  to community -based evaluation  through  a shared task \n(also titled ArchEHR -QA), organized as part of the BioNLP  (Biomedical Natural Language \nProcessing)  Workshop at the 2025 Annual Meeting of the Association for Computational \nLinguistics (ACL 2025) . This shared task  invited researchers to explore and develop innovative \nmethods for grounded EHR QA using the dataset. A total of 76 successful system submissions \nfrom 30 participating teams were recorded, further demonstrating the usability, relevance, and \ncommunity interest in ArchEHR -QA as a benchmark resource.  \n \nThe forum posts and MIMIC EHR notes originate from different patient populations. Nonetheless, \nthe informational needs expressed in forum posts remain relevant and have been examined in \ncontexts such as Radiology45 and Laboratory Result s46,47. Additionally, no forum post was aligned \nwith a note in a way that could alter its underlying information needs . Patient portal messages \nlinked to EHRs could offer real -world data to study patient information needs. However, although \nstudies using portal messages exist , the corresponding datasets are not publicly available16,17,48,49. \nOur approximate but methodical alignment provides a publicly accessible resource for advancing \npatient -specific EHR QA . \n \nThe primary focus of studies on generat ing responses to patient portal questions  has been on \nevaluating clinicians’ perception rather than evaluating the use of EHR information21,22. The \n--- Page 17 ---\nproposed dataset addresses this gap by providing  detailed sentence -level EHR note annotations , \nenabling fine -grained assessment s of how well automated systems ground their responses in \nrelevant clinical information.  Additionally , there is growing evidence supporting the use of AI \ntechnologies to generate draft responses for clinician review21,22,50. This dataset facilitates the \ndevelopment of automated benchmarking protocols, accelerating system improvements , and \nultimately helping to reduce clinician  burden.  \n \nThe developed dataset originate s from a single medical institution  and a single health discussion \nforum. However , the underlying EHR  data are drawn from  two distinct clinical  settings  (ICU an d \nED) and were  authored  by different providers  over several  years . The associated patient \nquestions also reflect authentic information needs expressed in real -world contexts . Moreover , \nour proposed paradigm for aligning forum posts with corresponding EHR s is scalable and \nadapt able to a variety of EHR systems , clinical documentation formats , and health discussion \nforums . \n \nWhile the models are benchmarked , and demonstrate usability of the data,  on the task of \ngrounded EHR QA, the proposed dataset offers broader utility beyond this specific application . \nFor instance, the annotated note excerpts  can support the development and evaluation of clinical \ninformation retrieval systems51. Additionally, the clinician - authored questions represent a \nvaluable resource for advancing research on summarizing patient questions52. Finally , the \ngrounded QA task itself can also be extended  to full-length clinical notes, whose document \nidentifiers are preserved and will be released with the dataset, thus enabling investigations on \nlong-form clinical QA53. \nConclusions  \nWe introduced a novel dataset , ArchEHR -QA, with 134 real -world patient cases, each containing \na patient -posed question, its clinician -interpreted equivalent, associated clinical note excerpts \nwith sentence -level relevance annotations, and clinician -authored answers. We benchmarked this \ndataset using three state -of-the-art open -weight LLMs under a zero -shot setting across three \ndistinct prompting strategies. Among these, the Answer First prompti ng approach consistently \noutperformed the others, and the Llama 4 model achieved the highest overall score of 4 2.3% \n(with a Factuality micro F1 score of 51.8 % and a n overall  Relevance score of 32.7 %). Given the \nsubstantial room for improvement identified by automated evaluation and manual error analysis , \nthis dataset offers  a valuable resource  for advancing the development and evaluation of patient -\nfocused EHR QA systems.  \n \n--- Page 18 ---\nData  and Code  Availability  \nThe proposed dataset, ArchEHR -QA, is publicly available through  the controlled -access \nrepository PhysioNet  at https://doi.org/10.13026/zzax -sy62 . The accompanying code used to \nevaluate the baseline system s is available via GitHub at https://github.com/soni -sarvesh/archehr -\nqa/tree/main/evaluation . \nAuthor Contributions  \nSS and DDF conceived the idea  and developed  the dataset . SS conducted  the benchmarking  \nexperiments and wrote the initial draft of the manuscript . Both authors  reviewed , edited , and \napproved the final version of the manuscript.  \nCompeting Interests  \nThe authors declare no competing interests.  \nAcknowledgements  \nThis research was supported by the Division of Intramural Research (DIR) of the National Library \nof Medicine (NLM), National Institutes of Health , and utilized the computational resources of the \nNIH HPC Biowulf cluster ( http://hpc.nih.gov ). The content is solely the responsibility of the authors \nand does not necessarily represent the official views of the National Institutes of Health.  The \nauthors thank Dr. Saeed Arabi for his contributions to annotations and discussions of the \nevaluation parameters.  \nReferences  \n1.  Ely JW, Osheroff JA, Chambliss ML, Ebell MH, Rosenbaum ME. Answering Physicians’ \nClinical Questions: Obstacles and Potential Solutions. J Am Med Inform Assoc \n2005;12(2):217 –24. https://academic.oup.com/jamia/article/12/2/217/772831. \n10.1197/jamia.M1608  \n2.  Bardhan J, Roberts K, Wang DZ. Question Answering for Electronic Health Records: Scoping \nReview of Datasets and Models. J Med Internet Res 2024;26(1):e53636. \nhttps://www.jmir.org/2024/1/e53636. 10.2196/53636  \n--- Page 19 ---\n3.  Soni S, Datta S, Roberts K. quEHRy: a question answering system to query electronic health \nrecords. J Am Med Inform Assoc 2023;30(6):1091 –102. \nhttps://doi.org/10.1093/jamia/ocad050. 10.1093/jamia/ocad050  \n4.  Fisher B, Bhavnani V, Winfield M. How patients use access to their full health records: A \nqualitative study of patients in general practice. J R Soc Med 2009;102(12):538 –44. \n10.1258/jrsm.2009.090328  \n5.  Woods SS, Schwartz E, Tuepker A, et al. Patient experiences with full electronic access to \nhealth records and clinical notes through the my healthevet personal health record pilot: \nQualitative study. J Med Internet Res 2013;15(3):403. 10.2196/jmir.2356   \n6.  Davis Giardina T, Menon S, Parrish DE, Sittig DF, Singh H. Patient access to medical records \nand healthcare outcomes: a systematic review. J Am Med Inform Assoc 2014;21(4):737 –41. \nhttps://doi.org/10.1136/amiajnl -2013 -002239. 10.1136/amiajnl -2013 -002239   \n7.  Tapuria A, Porat T, Kalra D, Dsouza G, Xiaohui S, Curcin V. Impact of patient access to their \nelectronic health record: systematic review. Inform Health Soc Care 2021;46(2):194 –206. \nhttps://doi.org/10.1080/17538157.2021.1879810. 10.1080/17538157.2021.1 879810  \n8.  Bell SK, Amat MJ, Anderson TS, et al. Do patients who read visit notes on the patient portal \nhave a higher rate of “loop closure” on diagnostic tests and referrals in primary care? A \nretrospective cohort study. J Am Med Inform Assoc 2024;ocad250. \nhttps ://doi.org/10.1093/jamia/ocad250. 10.1093/jamia/ocad250  \n9.  Pieper B, Sieggreen M, Freeland B, et al. Discharge Information Needs of Patients After \nSurgery. J Wound Ostomy Continence Nurs 2006;33(3):281. \nhttps://journals.lww.com/jwocnonline/fulltext/2006/05000/discharge_information_needs_of_\npatients_after.9.asp x.  \n10.  Zavala S, Shaffer C. Do Patients Understand Discharge Instructions? J Emerg Nurs \n2011;37(2):138 –40. \nhttps://www.sciencedirect.com/science/article/pii/S0099176709005340. \n10.1016/j.jen.2009.11.008  \n11.  Arora A, Alderman JE, Palmer J, et al. The value of standards for health datasets in artificial \nintelligence -based applications. Nat Med 2023;29(11):2929 –38. \nhttps://www.nature.com/articles/s41591 -023-02608 -w. 10.1038/s41591 -023-02608 -w  \n12.  Welivita A, Pu P. A survey of consumer health question answering systems. AI Mag \n2023;44(4):482 –507. https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12140. \n10.1002/aaai.12140  \n13.  Zhang Y, Fu W -T. Designing Consumer Health Information Systems: What Do User -\nGenerated Questions Tell Us? In: Schmorrow DD, Fidopiastis CM, editors. Foundations of \nAugmented Cognition. Directing the Future of Adaptive Systems. Berlin, Heidelberg: \nSpringer; 2011. p. 536 –45. 10.1007/978 -3-642-21852 -1_62  \n14.  Cimino JJ. Putting the “why” in “EHR”: capturing and coding clinical cognition. J Am Med \nInform Assoc 2019;26(11):1379 –84. https://doi.org/10.1093/jamia/ocz125. \n10.1093/jamia/ocz125  \n--- Page 20 ---\n15.  Martinez KA, Schulte R, Rothberg MB, Tang MC, Pfoh ER. Patient Portal Message Volume \nand Time Spent on the EHR: an Observational Study of Primary Care Clinicians. J Gen \nIntern Med 2024;39(4):566 –72. https://doi.org/10.1007/s11606 -023-08577 -7. \n10.1007/ s11606 -023-08577 -7  \n16.  Liu S, McCoy AB, Wright AP, et al. Leveraging large language models for generating \nresponses to patient messages —a subjective analysis. J Am Med Inform Assoc \n2024;ocae052. https://doi.org/10.1093/jamia/ocae052. 10.1093/jamia/ocae052  \n17.  Biro JM, Handley JL, Malcolm McCurry J, et al. Opportunities and risks of artificial intelligence \nin patient portal messaging in primary care. Npj Digit Med 2025;8(1):1 –6. \nhttps://www.nature.com/articles/s41746 -025-01586 -2. 10.1038/s41746 -025-01586 -2  \n18.  Kaur A, Budko A, Liu K, Steitz BD, Johnson KB. Primary Care Providers Acceptance of \nGenerative AI Responses to Patient Portal Messages. Appl Clin Inform 2025;0. \nhttps://www.thieme -connect.com/products/ejournals/abstract/10.1055/a -2565 -9155. \n10.1055/a -2565 -9155  \n19.  Baxter SL, Longhurst CA, Millen M, Sitapati AM, Tai -Seale M. Generative artificial intelligence \nresponses to patient messages in the electronic health record: early lessons learned. JAMIA \nOpen 2024;7(2):ooae028. 10.1093/jamiaopen/ooae028  \n20.  Chen S, Guevara M, Moningi S, et al. The effect of using a large language model to respond \nto patient messages. Lancet Digit Health 2024;0(0). \nhttps://www.thelancet.com/journals/landig/article/PIIS2589 -7500(24)00060 -\n8/fulltext#supplementaryMaterial. 1 0.1016/S2589 -7500(24)00060 -8  \n21.  Small WR, Wiesenfeld B, Brandfield -Harvey B, et al. Large Language Model –Based \nResponses to Patients’ In -Basket Messages. JAMA Netw Open 2024;7(7):e2422399. \nhttps://doi.org/10.1001/jamanetworkopen.2024.22399. \n10.1001/jamanetworkopen.2024.22399  \n22.  Garcia P, Ma SP, Shah S, et al. Artificial Intelligence –Generated Draft Replies to Patient Inbox \nMessages. JAMA Netw Open 2024;7(3):e243201. \nhttps://doi.org/10.1001/jamanetworkopen.2024.3201. \n10.1001/jamanetworkopen.2024.3201  \n23.  Haug CJ, Drazen JM. Artificial Intelligence and Machine Learning in Clinical Medicine, 2023. \nN Engl J Med 2023;388(13):1201 –8. \nhttps://www.nejm.org/doi/full/10.1056/NEJMra2302038. 10.1056/NEJMra2302038  \n24.  Shah NH, Entwistle D, Pfeffer MA. Creation and Adoption of Large Language Models in \nMedicine. JAMA 2023; https://doi.org/10.1001/jama.2023.14217. 10.1001/jama.2023.14217  \n25.  Li Y, Li Z, Zhang K, et al. ChatDoctor: A Medical Chat Model Fine -Tuned on a Large Language \nModel Meta -AI (LLaMA) Using Medical Domain Knowledge. Cureus 2023;15(6). \nhttps://www.cureus.com/articles/152858 -chatdoctor -a-medical -chat-model -fine-tuned -on-a-\nlarge -language -model -meta -ai-llama -using -medical -domain -knowledge. \n10.7759/cureus.40895  \n--- Page 21 ---\n26.  Im E-O, Chee W. An Online Forum As a Qualitative Research Method: Practical Issues. Nurs \nRes 2006;55(4):267 –73. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2491331/.  \n27.  Seale C, Charteris -Black J, MacFarlane A, McPherson A. Interviews and internet forums: a \ncomparison of two sources of qualitative data. Qual Health Res 2010;20(5):595 –606. \n10.1177/1049732309354094  \n28.  Johnson AEW, Pollard TJ, Shen L, et al. MIMIC -III, a freely accessible critical care database. \nSci Data 2016;3(1):160035. https://www.nature.com/articles/sdata201635. \n10.1038/sdata.2016.35  \n29.  Johnson AEW, Bulgarelli L, Shen L, et al. MIMIC -IV, a freely accessible electronic health \nrecord dataset. Sci Data 2023;10(1):1. https://www.nature.com/articles/s41597 -022-01899 -\nx. 10.1038/s41597 -022-01899 -x  \n30.  Ra B, X H, Se O, Rl  K. Lingering questions and doubts: online information -seeking of support \nforum members following their medical visits. Patient Educ Couns 2011;85(3). \nhttps://pubmed.ncbi.nlm.nih.gov/21315538/. 10.1016/j.pec.2011.01.015  \n31.  Grattafiori A, Dubey A, Jauhri A, et al. The Llama 3 Herd of Models. 2024; \nhttp://arxiv.org/abs/2407.21783. 10.48550/arXiv.2407.21783  \n32.  Robertson SE, Walker S, Jones S, Hancock -Beaulieu M, Gatford M. Okapi at TREC -3. In: \nProceedings of the 3rd Text REtrieval Conference. 1994. p. 109 –26. \n33.  Lin J, Quan D, Sinha V, et al. What Makes a Good Answer? The Role of Context in Question \nAnswering. In: Proceedings of the Ninth IFIP TC13 International Conference on Human -\nComputer Interaction. Zurich, Switzerland: 2003.  \n34.  Jeon J, Croft WB, Lee JH, Park S. A framework to predict the quality of answers with non -\ntextual features. In: Proceedings of the 29th annual international ACM SIGIR conference on \nResearch and development in information retrieval. New York, NY, USA: A ssociation for \nComputing Machinery; 2006. p. 228 –35. https://doi.org/10.1145/1148170.1148212. \n10.1145/1148170.1148212  \n35.  Wang W, Wei F, Dong L, Bao H, Yang N, Zhou M. MiniLM: Deep Self -Attention Distillation for \nTask -Agnostic Compression of Pre -Trained Transformers. In: Advances in Neural \nInformation Processing Systems. Curran Associates, Inc.; 2020. p. 5776 –88. \nhttps:/ /proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa -\nAbstract.html.  \n36.  The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation. Meta AI. \nAccessed May 14, 2025. https://ai.meta.com/blog/llama -4-multimodal -intelligence/.  \n37.  Jiang AQ, Sablayrolles A, Roux A, et al. Mixtral of Experts. 2024; \nhttp://arxiv.org/abs/2401.04088. 10.48550/arXiv.2401.04088  \n38.  Papineni K, Roukos S, Ward T, Zhu W -J. BLEU: a method for automatic evaluation of machine \ntranslation. In: Proceedings of the 40th annual meeting on association for computational \nlinguistics. Association for Computational Linguistics; 2002. p. 311 –8. \n--- Page 22 ---\n39.  Lin C -Y. ROUGE: A Package for Automatic Evaluation of Summaries. In: Text Summarization \nBranches Out. Barcelona, Spain: Association for Computational Linguistics; 2004. p. 74 –81. \nhttps://aclanthology.org/W04 -1013.  \n40.  Xu W, Napoles C, Pavlick E, Chen Q, Callison -Burch C. Optimizing Statistical Machine \nTranslation for Text Simplification. Trans Assoc Comput Linguist 2016;4:401 –15. \nhttps://aclanthology.org/Q16 -1029/. 10.1162/tacl_a_00107  \n41.  Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. BERTScore: Evaluating Text Generation \nwith BERT. In: International Conference on Learning Representations. 2019. \nhttps://openreview.net/forum?id=SkeHuCVFDr.  \n42.  Zha Y, Yang Y, Li R, Hu Z. AlignScore: Evaluating Factual Consistency with A Unified \nAlignment Function. In: Rogers A, Boyd -Graber J, Okazaki N, editors. Proceedings of the \n61st Annual Meeting of the Association for Computational Linguistics (Volume 1 : Long \nPapers). Toronto, Canada: Association for Computational Linguistics; 2023. p. 11328 –48. \nhttps://aclanthology.org/2023.acl -long.634/. 10.18653/v1/2023.acl -long.634  \n43.  Yim W, Fu Y, Ben Abacha A, Snider N, Lin T, Yetisgen M. Aci -bench: a Novel Ambient Clinical \nIntelligence Dataset for Benchmarking Automatic Visit Note Generation. Sci Data \n2023;10(1):586. https://www.nature.com/articles/s41597 -023-02487 -3. 10.1038/s41597 -\n023-02487 -3  \n44.  Gao M, Hu X, Yin X, Ruan J, Pu X, Wan X. LLM -based NLG Evaluation: Current Status and \nChallenges. Comput Linguist 2025;1 –28. https://doi.org/10.1162/coli_a_00561. \n10.1162/coli_a_00561  \n45.  Alarifi M, Patrick T, Jabour A, Wu M, Luo J. Understanding patient needs and gaps in \nradiology reports through online discussion forum analysis. Insights Imaging 2021;12(1):50. \nhttps://doi.org/10.1186/s13244 -020-00930 -2. 10.1186/s13244 -020-00930 -2  \n46.  Reynolds TL, Ali N, McGregor E, et al. Understanding Patient Questions about their Medical \nRecords in an Online Health Forum: Opportunity for Patient Portal Design. In: AMIA Annual \nSymposium Proceedings. 2017. p. 1468 –77. \nhttps://www.ncbi.nlm.nih.gov/ pmc/articles/PMC5977702/.  \n47.  Reynolds TL, Zhang J, Zheng K, Chen Y. Unpacking the Use of Laboratory Test Results in \nan Online Health Community throughout the Medical Care Trajectory. In: Proceedings of the \nACM on Human -Computer Interaction. 2022. p. 361:1 -361:32. \nhttps://doi.org/ 10.1145/3555086. 10.1145/3555086  \n48.  Anderson BJ, Zia ul Haq M, Zhu Y, et al. Development and Evaluation of a Model to Manage \nPatient Portal Messages. NEJM AI 2025;2(3):AIoa2400354. \nhttps://ai.nejm.org/doi/full/10.1056/AIoa2400354. 10.1056/AIoa2400354  \n49.  Liu S, Wright AP, McCoy AB, Huang SS, Steitz B, Wright A. Detecting emergencies in patient \nportal messages using large language models and knowledge graph -based retrieval -\naugmented generation. J Am Med Inform Assoc 2025;32(6):1032 –9. \nhttps://doi.org/1 0.1093/jamia/ocaf059. 10.1093/jamia/ocaf059  \n--- Page 23 ---\n50.  Reynolds K, Tejasvi T. Potential Use of ChatGPT in Responding to Patient Questions and \nCreating Patient Resources. JMIR Dermatol 2024;7(1):e48451. \nhttps://derma.jmir.org/2024/1/e48451. 10.2196/48451  \n51.  Sivarajkumar S, Mohammad HA, Oniani D, et al. Clinical Information Retrieval: A Literature \nReview. J Healthc Inform Res 2024;8(2):313 –52. https://doi.org/10.1007/s41666 -024-\n00159 -4. 10.1007/s41666 -024-00159 -4  \n52.  Ben Abacha A, Mrabet Y, Zhang Y, Shivade C, Langlotz C, Demner -Fushman D. Overview \nof the MEDIQA 2021 Shared Task on Summarization in the Medical Domain. In: \nProceedings of the 20th Workshop on Biomedical Language Processing. Online: \nAssociation for C omputational Linguistics; 2021. p. 74 –85. \nhttps://aclanthology.org/2021.bionlp -1.8. 10.18653/v1/2021.bionlp -1.8  \n53.  Adams L, Busch F, Han T, et al. LongHealth: A Question Answering Benchmark with Long \nClinical Documents. 2024; http://arxiv.org/abs/2401.14490. 10.48550/arXiv.2401.14490  \n  \n--- Page 24 ---\nSupplementary Materials  \nAnnotation Guidelines  \n1. Alignment  \nYou will be provided with a patient -posed question from a public health discussion forum and a \nset of 10 candidate discharge summaries or notes, selected based on lexical and semantic \nsimilarity. Your task has two components:  \n(a) Note Selection for Alignment  \n• Select the discharge summary that is most appropriate for answering the question.  \n• The selected note should not contradict the question. While a perfect match is not \nexpected, the alignment must be contextually plausible.  \n• Avoid selecting discharge summaries with mismatched core clinical facts.  \no Example: If the question is about the reason for a major procedure (e.g., CABG), \nand the note does not mention such a procedure, it is not a suitable match.  \n(b) Question Editing  \n• Edit the patient -posed question to reflect the patient characteristics and clinical context in \nthe selected discharge summary.  \n• Make only the minimal changes required to ensure consistency between the question and \nthe selected note without altering the core information need.  \no Example: If the question references a 56 -year-old, but the note refers to a 58 -year-\nold with otherwise matching context, change the age to 58 in the edited question.  \n• Do not add new clinical inquiries or modify the underlying intent of the question.  \n• Note: the clinical document is read only; it cannot be modified.  \n \n2. Clinician Question Creation and Sentence Relevance \nAnnotations  \nGiven an edited patient -posed question and its associated discharge summary, you will complete \ntwo sub-tasks:  \n(a) Clinician Question Formulation  \n• Identify the main information needs embedded in the patient's question.  \n• Reformulate it from the clinician's perspective to produce a clear and focused Clinician \nQuestion.  \n• This question should express how a clinician would interpret the patient’s question and \nrespond to it.  \n• Additionally, annotate the focus areas in the patient’s question that prompted the Clinician \nQuestion.  \n(b) Sentence Relevance Annotation  \n--- Page 25 ---\n• The discharge summary is pre -split into individual sentences.  \n• For each sentence, assign one of the following relevance labels indicating its importance \nin answering the Clinician Question:  \no Essential: Provides critical and necessary information to directly answer the \nClinician Question. Note: if several sentences contain the same critical information, \nall should be labeled as essential.  \no Supplementary: Offers supporting or contextual information but is not strictly \nrequired.  \no Not-Relevant: Does not contribute meaningfully to answering the question. This is \nthe default label unless updated.  \n \n3. Answer Creation  \nGiven the patient -posed question with focus areas, the clinician -interpreted question, and the \nannotated discharge summary with sentence -level relevance annotations, you will compose an \nanswer. The following are the guidelines to create an answer:  \n• Write a succinct answer (approximately 75 words or 5 sentences) that directly addresses \nthe Clinician Question.  \n• The answer must be fact -based and entirely grounded in the discharge summary.  \no Do not introduce or infer information that is not explicitly stated in the discharge \nsummary.  \n• All Essential sentences must be cited in your answer using bracketed numeric references \n(e.g., “[1]”, “[2,3]”).  \no Supplementary sentences may be cited for clarity or completeness but are not \nrequired.  \n• Ensure that the answer:  \no Fully addresses all aspects of the Clinician Question.  \no Is concise, medically accurate, and free of speculation.  \no Is written in a neutral, informative tone, suitable for a clinical audience (layperson \nlanguage is not required).  \n  \n--- Page 26 ---\nResults Analysis  \nTable S1:  System -generated responses to the example case presented in the paper.  \nApproach  Model  Answer  \nTogether  Llama 3  The emergency salvage repair was performed due to a ruptured \nthoracoabdominal aortic aneurysm. |1,2| The surgery involved a 34 -mm \nDacron tube graft and deep hypothermic circulatory arrest. |2|  \nLlama 4  They performed the emergency salvage repair because the patient had a \nruptured thoracoabdominal aortic aneurysm. |1,2| The surgery was necessary \nto address the rupture, which required immediate attention, and involved a 34 -\nmm Dacron tube graft using deep hypothermic circulatory arrest. |2| The \nprocedure was done to save the patient's life after he experienced cardiac \narrest twice during the operation.  |3| \nMixtral  The emergency salvage repair was performed due to a ruptured \nthoracoabdominal aortic aneurysm. |1| This was an urgent procedure \ninvolving a 34 -mm Dacron tube graft and deep hypothermic circulatory arrest. \n|2| \nAnswer \nFirst Llama 3  The patient underwent emergency salvage repair due to a ruptured \nthoracoabdominal aortic aneurysm. |1,2| This surgery was necessary to \nprevent further bleeding and potential death.  \nLlama 4  The emergency salvage repair was performed because the patient had a \nruptured thoracoabdominal aortic aneurysm. |1,2| The surgery was necessary \nto address the rupture, which likely posed an immediate threat to the patient's \nlife. |1| The procedure involved  a 34 -mm Dacron tube graft and deep \nhypothermic circulatory arrest due to the complexity of the case. |2|  \nMixtral  The surgery was performed due to a ruptured thoracoabdominal aortic \naneurysm. |1| This condition is life -threatening and requires immediate \nintervention. The procedure involved an emergent salvage repair using a 34 -\nmm Dacron tube graft to prevent further rupture and restore normal blood flow. \n|2| \nEvidence \nFirst Llama 3  The emergency salvage repair was performed due to a ruptured \nthoracoabdominal aortic aneurysm. |1,2| The surgery involved a 34 -mm \nDacron tube graft and deep hypothermic circulatory arrest. |2|  \nLlama 4  They performed the emergency salvage repair because the patient had a \nruptured thoracoabdominal aortic aneurysm. |1,2| The surgery was necessary \nto address the rupture, which required immediate attention, and involved a 34 -\nmm Dacron tube graft using deep h ypothermic circulatory arrest. |2| The \nprocedure was done to save his life after he experienced cardiac arrest twice \nduring the operation. |3|  \n--- Page 27 ---\nMixtral  Your father underwent an emergency salvage repair. |1| This was due to a \nruptured thoracoabdominal aortic aneurysm. |1| The procedure involved using \na 34-mm Dacron tube graft with deep hypothermic circulatory arrest. |2|  \nHeuristic  -- He was transferred to the hospital on 2025 -1-20 for emergent repair of his \nruptured thoracoabdominal aortic aneurysm. |1| He was taken immediately to \nthe operating room where he underwent an emergent salvage repair of \nruptured thoracoabdominal aortic aneur ysm with a 34 -mm Dacron tube graft \nusing deep hypothermic circulatory arrest. |2| Postoperatively he was taken to \nthe intensive care unit for monitoring with an open chest. |4|  \n \nTable S2:  System -generated responses by Llama 4 using the Answer First approach on the two \nbest and worst performing cases on Overall Score. Note Excerpts are not included because of \nbrevity and MIMIC data use agreement.  \n(a) Example  \nCase ID: 23 (best factuality)  \nPatient \nQuestion  Hi Dr. My mom is 88 years old, no heart problems. She hit her head and was hospitalized. \nNo intubation was done in ICU. Her blood count and pressure were normal, she could eat \nand was breathing on her own. My question is about the antibiotics she was given during \nher hospital stay. Were they necessary?  Thank you,  \nClinician \nQuestion  Why was she given antibiotics during her hospital stay?  \nClinician \nAnswer  The patient was given antibiotics during her hospital stay because of a lung infection [5]. \nThe patient was noted to have low levels of oxygen and a chest x -ray was ordered [5]. It \nindicated the patient was developing pneumonia [5]. IV antibiotics were ini tiated to treat the \npneumonia [6]. The patient showed improvement right away after starting the antibiotics [6]. \nTherefore, the antibiotics were necessary during her hospital stay.  \nSystem \nAnswer  ```answer. Your mom was given antibiotics because she developed pneumonia, as shown \nby a chest x -ray that indicated low oxygen levels. |5,6| The antibiotics were started \nimmediately and she showed improvement. |6| ```  \nManual \nReview  • Answers the question?  \no Yes: The antibiotics were initiated because of pneumonia.  \n• Uses the evidence?  \no Supported: The antibiotics were initiated because of pneumonia.  \no Refuted: The chest x -ray was ordered due to low oxygen levels, not the \nother way around.  \n• Uses general knowledge?  \no Conflicting: A chest x -ray does not measure oxygen saturation.  \n• Formatting  \no Uses additional “answer” markdown block in the response.  \nCase ID: 39 (best relevance)  \nPatient \nQuestion  hello, all doctors i  am here for my mom is suffering from liver cirrhosis and stone in gall \nbladder, a few days back she felt unconcious then we took her to hospitol then she was \nkept in ICU for 10 days then after treatment she came back to censes, in the mean process \naprox 1  and half litre water pumped from chast and now again there is sign of swelling and \nfilling of water in there  then again we consult our doctor they suggested for liver \ntransplant,here now I am asking from you people that what is the best and proper way to \ncure her=.  \n--- Page 28 ---\nClinician \nQuestion  Is she getting any treatment for the swelling?  \nClinician \nAnswer  The patient was found to have a pleural effusion caused by liver disease [7]. The treatment \nshe received for the swelling consisted of thoracentesis to fix the fluid accumulation [8]. The \npatient will continue on diuretics, lasix 10 mg daily and spirolacto ne 50 mg daily to help \nprevent a reoccurrence of the pleural effusion [9,14]. The patient will continue to follow up \nwith Hepatology as indicated [16]. Her urine output and weight will be monitored and \nadjusted as needed [15].  \nSystem \nAnswer  She is currently on diuretics, specifically lasix  and spironolactone, to help prevent a \nreoccurrence of the pleural effusion. |9| Her diuretics were briefly held but have been \nrestarted at lasix 10mg daily and spironolactone 50mg daily. |13,14| Her urine output and \nweight will continue to be monitored an d adjusted as needed. |15|  \nManual \nReview  • Answers the question?  \no Partially: It accurately describes the current medication regimen (diuretics), \nmentions the specific drugs and dosages, and notes the monitoring of urine \noutput and weight. However, it does not mention thoracentesis and the plan \nto follow up with Hepatolog y. \n• Uses the evidence?  \no Supported: Uses appropriate evidence for the included clinical information.  \n• Uses general knowledge?  \no No: The system does not incorporate additional general knowledge beyond \nwhat is included in the input note.  \nCase ID: 118 (least relevance)  \nPatient \nQuestion  patient has been operated because of intestine, patient had scepticimia and surgery again \nfor cleaning inside, what are the chances of survival and what can be the consequences \nafter all these operations ??  \nClinician \nQuestion  What is the expected course of recovery after surgery and septicemia?  \nClinician \nAnswer  The patient is expected to continue on antibiotics via her PICC line when she is discharged, \ndue to GNRs and GPCs that were present in the blood cultures [18,19,23,36]. Surgery \nincluded hysterectomy, salpingo -oopherectomy and omentectomy, resulting in a co lostomy \n[9,23,33]. The patient will have an extended recovery as her abdominal wounds heal, and \nas she learns how to manage her colostomy [26,34,35]. The patient will be discharged to a \nrehab facility for extended recovery [24].  \nSystem \nAnswer  ```answer. The patient's condition improved at discharge, with stable vital signs and \ndecreasing white blood cell count. |26,35| She tolerated a regular diet, ambulated with \nassistance, and had well -controlled pain. |26| Expected course of recovery involve s \ncontinued improvement with rehabilitation. |24,25| Close monitoring and management of \npotential complications, such as infection, will be necessary. |18,21,23| Underlying \nconditions like DM2, HTN, and HL require ongoing management. |2| ```.  \nManual \nReview  • Answers the question?  \no Partially: The system output presents a general summary of the patient’s \ndischarge condition and some aspects of the expected recovery. However, \nit fails to mention the continuation of antibiotics due to GNRs/GPCs, \ncolostomy care, and wound healing.  \n• Uses the evidence?  \no No: The citations used for infection management (|18,21,23|) are not \nexplicitly tied to the continuation of antibiotics.  \n• Uses general knowledge?  \no Yes: Uses general medical knowledge in mentioning the need for \nmonitoring complications like infection and managing chronic conditions \nsuch as DM2, HTN, and HL.  \n--- Page 29 ---\n• Formatting  \no Uses additional “answer” markdown block in the response.  \nCase ID: 99 (least factuality)  \nPatient \nQuestion  Hi Doc my husband has been identified with tuburculos meningitis  as his protien was 496 / \nlympocytes 90% after lumber tap..he  spent 5 days in ICU & now he is on akt 4 treatment . \n01 Large Tablet after breakfast / rifamicin after 02 hours of breakfast and rest two after lunch \nis this the right way to take the medicines  as earlier he was taking rifamicin empty stomach  \nbut this creates nausea and vomiting so our doc changed to the sequel mentioned. Also he \nhas a fever around 100 - 101 please advise  \nClinician \nQuestion  Is there a specified order to take the prescribed medications?  \nClinician \nAnswer  Patient was diagnosed with tuberculous meningitis and was discharged home on a long list \nof important medications that must be taken exactly as directed [2,3]. The dexamethasone \ntaper medication has a specific order in which it is to be taken [21]. Patient  is to start at 6mg \nIV every 6 hours and decrease the dose over a period of time until transitioned to an oral \ndose [21]. This dose starts at 4mg daily and gradually decreases until finished [21].  \nSystem \nAnswer  There is no specific order mentioned for taking the prescribed medications. |4,6,9,10,11| \nHowever, the patient is currently taking one large tablet (likely Isoniazid) after breakfast, \nRifampin after 2 hours of breakfast, and the rest two tablets after lunc h. The current regimen \nseems to be adjusted to mitigate nausea and vomiting caused by Rifampin taken on an \nempty stomach. It is recommended to consult the doctor for further clarification on the \nmedication schedule. |24,27,28|  \nManual \nReview  • Answers the question?  \no No: It states that there is no specific order mentioned, which is incorrect. It \nfails to address the question regarding the specific order of taking the \nprescribed medications, particularly the dexamethasone taper.  \n• Uses the evidence?  \no Refutes: It misrepresents the evidence by claiming that no specific order is \nmentioned. Further, it focuses on observed patient behavior (timing of pill \nintake from the question) rather than the prescribed regimen described in \nthe evidence.  \n• Uses general knowledge?  \no No: The system does not incorporate additional general knowledge beyond \nwhat is included in the input question and the note.  \n \n(b) Factuality Metrics  \nCase ID  Strict  Lenient  \nP R F1 P R F1 \n23 \n(best factuality)  100 100 100 100 100 100 \n39 \n(best relevance)  75.0 50.0 60.0 75.0 50.0 60.0 \n118 \n(least relevance)  62.5 50.0 55.6 75.0 35.3 48.0 \n99 \n(least factuality)  0 0 0 62.5 31.3 41.7 \n \n \n \n--- Page 30 ---\n(c) Relevance Metrics using Human Answers  \nCase ID  BLEU  ROUGE  SARI  BERTScore  MEDCON  AlignScore  Overall \nRelevance  \n23 \n(best factuality)  0 29.2 43.5 43.7 90.9 56.0 43.9 \n39 \n(best relevance)  26.2 41.9 58.7 61.2 85.7 66.5 56.7 \n118 \n(least relevance)  0 14.8 38.5 24.8 0 19.0 16.2 \n99 \n(least factuality)  0 12.3 46.7 21.6 25.0 1.5 17.9 \n \n(d) Relevance Metrics using Note Excerpts and Questions  \nCase ID  BLEU  ROUGE  SARI  BERTScore  MEDCON  AlignScore  Overall \nRelevance  \n23 \n(best factuality)  0 23.5 56.6 29.2 52.6 66.4 38.1 \n39 \n(best relevance)  3.1 31.9 77.1 43.1 46.2 72.4 45.6 \n118 \n(least relevance)  1.6 24.8 40.2 22.7 18.2 44.3 25.3 \n99 \n(least factuality)  0 18.4 44.5 9.3 38.5 56.3 27.8 \n \n(e) Overall Scores  \nCase ID  Overall Score (Human)  Overall Score  \n23 \n(best factuality)  71.9 69.0 \n39 \n(best relevance)  58.4 52.8 \n118 \n(least relevance)  35.9 40.4 \n99 \n(least factuality)  8.9 13.9 \n \nOther Details  \nKeywords used to filter the health forum discussion posts  \n\"admit\", \"admitted\", \"admission\", \"admitting\", \"admits\", \"icu\", \"ccu\", \"intensive care\", \"critical care\", \n\"care unit\", \"er\", \"emergency room\", \"ed\", \"emergency department\", \"emergency dept\", \n\"emergency\"  \n \n--- Page 31 ---\nTable S3:  Prompt used to generate ICD code descriptions for forum posts.  \nGenerate the relevant ICD codes for the health forum message [MESSAGE] from a \npatient:  \n \n[MESSAGE]  \n{message}  \n \nThe output should be a markdown code snippet formatted in the following schema, \nincluding the leading and trailing \"```icd\" and \"```\":  \n \n```icd  \n[ICD Code] - [Description Text]  \n[ICD Code] - [Description Text]  \n[ICD Code] - [Description Text]  \n```",
  "text_length": 70430
}