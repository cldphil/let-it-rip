{
  "id": "http://arxiv.org/abs/2506.05166v1",
  "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective",
  "summary": "Large Language Models (LLMs) are known to exhibit social, demographic, and\ngender biases, often as a consequence of the data on which they are trained. In\nthis work, we adopt a mechanistic interpretability approach to analyze how such\nbiases are structurally represented within models such as GPT-2 and Llama2.\nFocusing on demographic and gender biases, we explore different metrics to\nidentify the internal edges responsible for biased behavior. We then assess the\nstability, localization, and generalizability of these components across\ndataset and linguistic variations. Through systematic ablations, we demonstrate\nthat bias-related computations are highly localized, often concentrated in a\nsmall subset of layers. Moreover, the identified components change across\nfine-tuning settings, including those unrelated to bias. Finally, we show that\nremoving these components not only reduces biased outputs but also affects\nother NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks.",
  "authors": [
    "Bhavik Chandna",
    "Zubair Bashir",
    "Procheta Sen"
  ],
  "published": "2025-06-05T15:43:34Z",
  "updated": "2025-06-05T15:43:34Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05166v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05166v1  [cs.CL]  5 Jun 2025Under Review\nDissecting Bias in LLMs: A Mechanistic Interpretability\nPerspective\nBhavik Chandna∗bhavikchandna@gmail.com\nUniversity of California San Diego\nZubair Bashir∗zubairbashir2004@gmail.com\nIndian Institute of Technology, Kharagpur\nProcheta Sen procheta.sen@liverpool.ac.uk\nUniversity of Liverpool\nAbstract\nLarge Language Models (LLMs) are known to exhibit social, demographic, and gender biases,\noften as a consequence of the data on which they are trained. In this work, we adopt a\nmechanistic interpretability approach to analyze how such biases are structurally represented\nwithin models such as GPT-2 and Llama2. Focusing on demographic and gender biases, we\nexplore different metrics to identify the internal edges responsible for biased behavior. We\nthen assess the stability, localization, and generalizability of these components across dataset\nand linguistic variations. Through systematic ablations, we demonstrate that bias-related\ncomputations are highly localized, often concentrated in a small subset of layers. Moreover,\nthe identified components change across fine-tuning settings, including those unrelated to\nbias. Finally, we show that removing these components not only reduces biased outputs but\nalso affects other NLP tasks, such as named entity recognition and linguistic acceptability\njudgment because of the sharing of important components with these tasks. Our code is\navailable at https://anonymous.4open.science/r/MI-Data-Sensitivity-6303 .\n1 Introduction\nWith the growing deployment of Large Language Models (LLMs) in high-impact domains such as education,\nhealthcare, law, and content moderation, ensuring their responsible use has become increasingly critical\nBirhane et al. (2023). Among the many ethical and societal challenges associated with LLMs, bias remains\none of the most pressing and pervasive concerns. Numerous studies have demonstrated that LLMs can reflect\nand even amplify harmful social, demographic, and gender biases in their outputs Acerbi and Stubbersfield\n(2023). These biases often manifest in subtle yet consequential forms, including stereotyping, uneven sentiment\nassociations, and disproportionate representation. When integrated into downstream applications, such biases\ncan result in unfair treatment, discrimination, or the spread of misinformation. Prior work on debiasing\nLLMs has predominantly explored approaches such as fine-tuning or data augmentation Han et al. (2024);\nGallegos et al. (2024). A complementary line of research has investigated the role of individual neurons or\nattention heads in encoding gender bias Vig et al. (2020). However, these studies often lack generalizability,\nas they tend to focus narrowly on a single type of bias and a specific model architecture. Moreover, there\nremains limited exploration into understanding different properties of bias-related important components,\nsuch as their degree of localization, stability under perturbation, faithfulness to attribution methods, and\noverlap with features relevant to other downstream tasks.\n∗Equal contribution.\n1\n--- Page 2 ---\nUnder Review\nFigure 1: Circuit Diagram for Positive Demographic Bias in a) GPT-2 Small, b) GPT-2 Large, and c) Llama-2. Green colour shows\nInput node, Orange colour shows Attention Head, Purple colour shows MLP layer, and Yellow color shows Logits node. The description\nof different types of nodes can be found in Table 5 in Appendix D. Circuit diagrams for gender bias are shown in Appendix G.\nTo address the above mentioned limitations, this work investigates demographic and gender bias in GPT-2\nSmall, GPT-2 Large, and Llama2. In demographic bias, LLMs generally tend to predict mostly positive or\nnegative words towards a particular nationality. For example given an input ‘Afghan people are so’, LLM\ncompletes it with negative words like ‘Afghan people are so poor’. Similarly, gender bias tends to favour\neither the male or female gender for certain professions. An example of gender bias is ‘The woman worked as\na nurse. The man worked as a software engineer’. It can be observed from the example that LLM assigned\nfemale gender with the ‘nurse’ profession and ‘male’ gender with the ’software engineer’ profession.\nIn our research scope, we use Mechanistic Interpretability (MI) Olah (2022) related methodologies to identify\nimportant components within LLMs. Our central question is: Can demographic and gender biases be localized\nwithin distinct substructures of an LLM’s architecture? If such biases can indeed be attributed to identifiable\ncomponents, this opens the possibility of targeted interventions—that is, mitigating harmful behavior by\nmodifying or ablating responsible components, rather than relying on full-scale retraining or costly data\naugmentation. Out of different types of components (e.g., nodes and edges) available for analysis, we\nspecifically focus on finding the importance of edges responsible for bias using Edge Attribution Patching\n(EAP) Syed et al. (2023) approach. An edge refers to a connection between two computational nodes, such\nas neurons, attention heads, or layer outputs, typically representing the flow of information between adjacent\nlayers during the forward pass Syed et al. (2023). Figure 1 shows a sample of different circuits (subgraph\nconsisting of important edges1) for demographic bias in GPT-2 Small, GPT-2 Large and Llama-2. The key\ncontributions of this work are as follows:\na) Localized Encoding of Demographic and Gender Bias in LLM Edges We conduct a comprehensive\nanalysis across GPT-2 Small, GPT-2 Large, and Llama-2 to investigate whether demographic and gender bias\nis encoded in a localized manner. Our study employs different approaches to identify the edges most strongly\nassociated with biased behavior. As illustrated in Figure 2 and 3, our study concludes that demographic and\ngender bias are mostly localized into certain edges and layers across different models.\nb) Instability of Important Edges Across Lexical, Syntactic, and Fine-Tuning Variations We\nevaluate the generalizability and stability of the identified edges across multiple dimensions, including different\ntypes of bias (e.g., gender, demographic), fine-tuning settings, and variations in grammatical structures. Our\nanalysis (Figure 4 and 5) concludes that the edges identified for biased behavior don’t remain consistent\nunder perturbations to both the model and input text space.\n1https://distill.pub/2020/circuits/zoom-in/#glossary-circuit\n2\n--- Page 3 ---\nUnder Review\nc) Bias-Related Edge Overlap with Other Language Understanding Tasks We investigate the\nextent to which edges identified as important for biased behavior are also functionally involved in broader\nlanguage understanding tasks. To probe this, we selectively corrupt edges associated with bias and measure\nthe resulting impact on performance across a diverse set of unrelated NLP tasks. This analysis reveals the\ndegree of functional entanglement between bias-related circuits and general linguistic competence, offering\ninsights into whether bias can be mitigated without compromising core language capabilities.\nThe paper is organized as follows. In Section 2, we describe existing works related to bias and mechanistic\ninterpretability in LLMs. Section 3 describes the methodology used for identifying the important components,\nSection 4 describes the experiment setup, Section 5 describes results, and Section 6 concludes the paper.\n2 Related Work\nBias in LLMs A growing body of research has investigated various forms of bias in LLMs. For instance, Kotek\net al. (2023) highlighted profession-related gender bias in GPT-3.5 and GPT-4, while Kamruzzaman et al.\n(2024) demonstrated that models such as GPT and Llama-2 exhibit systematic biases across underexplored\ndimensions, includingage, physicalappearance, academicaffiliation, andnationality, usingsentencecompletion\ntasks. Similarly, Soundararajan and Delany (2024) observed gender bias in LLM-generated text. A broader\noverview of bias evaluation and mitigation strategies is provided in Navigli et al. (2023).\nMIaims to reverse-engineer trained neural networks, similar to the analysis of compiled software binaries\nOlah (2022). The central goal is to identify and understand the role of internal components—such as neurons,\nattention heads, and edges that give rise to specific model behaviors, including tasks like indirect object\nidentification or bias propagation Olah et al. (2020); Meng et al. (2022); Geiger et al. (2021); Goh et al. (2021);\nWang et al. (2022). Recent work by Conmy et al. (2023) introduced an automated framework for discovering\ntask-relevant components, while Wu et al. (2023) proposed a causal testing method to determine whether a\nnetwork implements specific algorithms. Nanda et al. (2023) leveraged circuit-based analysis to explain the\ngrokking phenomenon. In complementary work, Goldowsky-Dill et al. (2023) refined our understanding of\ninduction heads in LLMs, and Katz et al. (2024) employed MI techniques to trace information flow through\ntransformer-based architectures.\nMI for Bias Analysis Another thread of MI work has recently been harnessed to dissect and mitigate\nbiases in LLMs. Vig et al. (2020) pioneered causal mediation analysis to pinpoint a small subset of neurons\nand attention heads in GPT-2 that drives gender bias. Chintam et al. (2023) combined automated circuit\ndiscovery and targeted ablations to surgically edit bias-carrying components, substantially reducing stereotype\npropagation with minimal impact on overall performance. The study in Kim et al. (2025) investigated how\npolitical bias or perspective is encoded in LLM internals. Kim et al. (2025) found that models learn a\nremarkably linear representation of political ideology (e.g. liberal to conservative) within their activation space.\nHowever, prior work has not systematically examined the generalizability of bias-related components, nor\nexplored key properties such as their localization, stability under perturbations, and overlap with components\ninvolved in broader language understanding tasks. To address these gaps, we conduct a comprehensive\nanalysis of these properties across multiple model architectures.\n3 Edge Attribution in LLMs\nTo investigate how bias is encoded within LLMs, we adopt a causal intervention framework to assess the\nrelative importance of individual edges within the model architecture. As described in Section 1, an edge\nrefers generally represents the flow of information between adjacent layers during the forward pass. Each\nedge is parameterized by a weight, which governs the strength of information transfer between the connected\nnodes.\nA naive strategy would involve iteratively ablating individual edges and measuring their effect on model\noutputs. However, such exhaustive interventions are computationally expensive at the scale of modern LLMs.\nTo address this, we leverage the Edge Attribution Patching (EAP) technique introduced by Syed et al. (2023),\nwhich offers a more efficient approximation of causal importance. EAP assigns an attribution score to each\n3\n--- Page 4 ---\nUnder Review\nedge, reflecting its contribution to a particular model behavior. Mathematically, EAP measures the value\ndescribed in Equation 1 for each edge.\n|L(xclean|do(E=ecorr))−L(xclean )| (1)\nIn Equation 1, Lis a metric with respect to which we measure the effectiveness of a task, and L(xclean )shows\nthe value of metric when a clean sample is provided to the network and L(xclean|do(E=ecorr))represents\nthe value of metric when corrupted activation (i.e. activation obtained from corrupted input ecorr) is applied\nto an edge for which we want to find the importance. For the rest of the edges, clean activation is applied. If\na set of edges is important for a particular task then providing corrupted values to those edges will reduce\nthe value of L(xclean )significantly. Attribution patching is an approach(Syed et al. (2024)) that is used to\ncompute the value of Equation 1 with only two forward passes and one backward pass through the network.\nThis makes EAP computationally efficient.\n3.1 Bias Metric Computation ( L)\nThere is currently no universally accepted metric for quantifying demographic or gender bias in LLMs. Similar\nto Qiu et al. (2023), we investigated two alternative formulations designed to capture different aspects of bias.\nIn Equation 2, mis the total number of samples on which L1is computed.\nL1=1\nmm/summationdisplay\ni=1\nk/summationdisplay\nj=1Ppos(i)j−k/summationdisplay\nj=1Pneg(i)j\n (2)\nEquation 2, shows one variation of metric denoted as L1. In Equation 2, we compute the difference in\naggregate probabilities assigned to positive/male (i.e. Ppos) versus negative/female (i.e. Pneg) tokens among\nthe top-kpredicted next tokens. This captures the relative skew in the model’s output distribution. Higher\nvalue ofL1will show higher bias in an LLM.\nL2=1\nmm/summationdisplay\ni=1\nk/summationdisplay\nj=1Ppos(i)j\n (3)\nEquation 3, shows the second variation of the bias metric denoted as L2. Equation 3 isolates the cumulative\nprobability assigned exclusively to positive/male tokens within the top- kpredictions, offering a unidirectional\nmeasure of bias. By comparing these two formulations (i.e., L1andL2), we aim to understand which metric\nmore effectively identifies the edges responsible for encoding demographic and gender bias in EAP.\n3.2 Defining Corrupted Samples\nAs shown in Equation 1, generating appropriate corrupted samples is a critical step in estimating the EAP\nscore. To maintain consistency in attribution, each corrupted sample must preserve the token length of its\ncorresponding clean input. To achieve this, we adopt the Symmetric Token Replacement (STR) strategy\nproposed in Zhang and Nanda (2024), which ensures structural alignment between clean and perturbed inputs\nwhile introducing minimal semantic drift. For demographic bias, we explored two approaches: C1, where the\ncountry name in the sample was replaced with an out-of-distribution token, such as ‘abc’, and C2, where the\ncountry name was replaced with ‘Emirati’, the nationality for which we did not observe any demographic bias\nin LLM outputs. Similarly, for gender bias, we applied two strategies: C1, where gender-sensitive tokens like\nprofession names (e.g., doctor) were replaced with an out-of-distribution word like ‘xyz’, and C2, where we\nused the gender-neutral term ‘broadcaster’, chosen because LLMs showed the most gender-neutral predictions\nfor this term. Table 1 presents the structure of both the clean and corrupted samples used in our experiment\nsetup.\n3.3 Localization of Bias\nAs described in Section 1, one of the objectives of our experiments is to understand whether demographic\nand gender bias is encode within certain components in LLMs. We employ two principal strategies to assess\n4\n--- Page 5 ---\nUnder Review\nCorrupted Sample\nClean Sample C1 C2\n(DSS1) Afghan people are so ___ Abcpeople are so ___ Emirati people are so ___\n(DSS2) As expected, Afghan people are so ___ As expected, Abcpeople are so ___ As expected, Emirati people are so ___\n(GSS1) The doctor did a mistake, and everyone scolded ___ Thexyzdid a mistake, and everyone scolded ___ Thebroadcaster did a mistake, and everyone scolded ___\n(GSS2) The doctor did a miracle, and everyone praised ___ Thexyzdid a miracle, and everyone praised ___ Thebroadcaster did a miracle, and everyone praised ___\nTable 1: Variation of Clean and Corrupted Samples (i.e. C1,C2) Used for Different Bias Setup. DSSstands for Demographic Sensitive\nStructure, GSSstands for Gender Sensitive Structure.\nlocalization. Firstly, we evaluate the extent to which the highest-scoring edges (obtained from EAP method\ndescribed in Section 3) contribute to the overall value of the bias metric ( L) compared to a baseline model\n(i.e. where all edges are used). If bias is localized within specific edges, then ablating a greater number of\nimportant edges from the model architecture should lead to a more substantial decrease in the overall metric\nvalue for bias in the model. Additionally, we also analyze the layer-wise distribution of the important edges\nto identify the origin of the important edges within the model architecture.\n3.4 Stability of Important Edges\nHere, our primary goal is to explore the stability of important edges identified using the EAP approach\ndescribed in Section 3. By stability, we refer to how much the important edges change with respect to different\ncriteria. We primarily focused on three different criteria. Each of them is described as follows.\nC1: Stability with respect to Grammatical Structures Here we investigated generalizability across\ndifferent grammatical structures (i.e. DSS1, DSS2, GSS1 and GSS2 described in Table 1). Even if the\ngrammatical structure changes, the gender/demographic bias present in a sentence is of the same nature. If\nthe LLMs could generalize regarding bias, then the important structures related to bias remain the same for\nall the grammatical structures. To investigate this, we tested the different grammatical structures mentioned\nin Table 1. The difference between the grammatical structures for both demographic and gender bias is that,\nin demographic bias, the semantic meanings of the two variations are almost similar. In contrast, for gender\nbias, the semantic meanings of the two sentences are different. To estimate the stability, we have primarily\ncomputed the overlap of top-K edges across different variations.\nC2: Stability With Respect to Different Types of Bias Any kind of bias essentially means a preference\nfor certain types of tokens over others. Hence, the motivation of this set of experiments was to check if there\nis any similarity between the important edges encoding different kinds of bias. In our research scope, we\nlooked into only demographic and gender bias.\nC3: Robustness with respect to finetuning In this set of experiments, we investigate whether the\nimportant edges change concerning fine-tuning on different types of data. Here, we primarily did two different\nsets of experiments. In the first set of experiments, we finetuned the model with a dataset where positive\nthings were said about the countries/professions where there was negative demographic bias and vice versa.\nThe objective of this experiment was to check whether debiasing the model by fine-tuning changes the\nimportant edges for generating sentences of the same grammatical structure. In the second set of experiments,\nwe finetuned the model on a dataset where there was no overlap with the bias-related sentences or test topics.\nThe bias of the model doesn’t change after this. We wanted to check whether the structures important for\nthe bias also remain the same or not.\n3.5 Leveraging Important Edges for Debiasing LLMs\nExisting debiasing techniques predominantly focus on data augmentation, modifications to the training\nobjective, or fine-tuning the model. While effective, these approaches inherently require retraining, which\ncan be computationally expensive and infeasible in scenarios where access to model internals or training\ninfrastructure is limited. To overcome this limitation, we propose a novel inference-time debiasing strategy\nthat leverages the important edges identified via the EAP method (Section 3). Our approach involves\nsubstituting the activations (logits) along bias-associated edges with those obtained from a corrupted version\nof the input, while preserving clean activations along the remaining edges. This enables us to reduce biased\nbehavior without altering the model parameters. Notably, this method requires no additional training or\n5\n--- Page 6 ---\nUnder Review\naccess to gradients and can be implemented entirely at inference time. To implement this technique, a\ncorrupted variant of the original input is provided alongside the clean input. In our research scope, we\nautomatically generated a corresponding corrupted input of the same length for each original input. The\ndetails of this setup are described in Appendix E.\nPrior work Kaneko et al. (2023) has demonstrated that debiasing can lead to performance degradation across\nvarious downstream tasks, suggesting that bias-associated components may also contribute to task-relevant\nbehavior. To assess the broader impact of our debiasing method, we evaluated its effectiveness on a range of\nstandard NLP tasks. Specifically, we apply the corrupted logits approach to multiple settings to investigate\nwhether the mitigation of bias incurs a trade-off in task performance.\n4 Experiment Setup\nFor demographic and gender bias-related experiments, we used two different types of datasets. The Demo-\ngraphic Bias dataset used in our experiment is from an existing study in Narayanan Venkit et al. (2023).\nAccording to the findings in Narayanan Venkit et al. (2023), GPT-2 and Llama-2 models exhibit nationality\nbias in text generation. This dataset was constructed by including the names of all 224nationalities globally.\nThe study in Narayanan Venkit et al. (2023) identified a negative bias towards certain nationalities among\nthese 224 nationalities. Our research specifically targets two distinct sentence structures (Described in Table\n1) that mention nationality. We then assess the bias in GPT-2 or Llama-2 by examining the text completion\nof these sentence structures. The detailed descriptions about the model architecture of GPT-2 and Llama-2\nmodels used in our experiment are given in Appendix B.\nTo compute demographic bias in a text completion setup, we get the top-k next token predictions for every\nsentence (k = 10in our case, explained in detail in Appendix A) by the model and for every sentence we\nconcatenate each of these predictions separately to the sentence and check the sentiment scores of resulting\nsentences using Distilbert-base-uncased model HF Canonical Model Maintainers (2022). If the sentiment\nof the sentence is positive, then we assume that the token predicted by the model was positive and vice\nversa. For quantitatively computing the bias of the dataset 4, we used a metric (similar to L1in Equation 2)\nwhich computes the difference between positive and negative probabilities. Consequently, the Demographic\nBias metric should be positive in the case of a Positive-Bias Dataset and negative in the case of a Negative-\nBias Dataset. Using the methods described above, we divide our demographic dataset into two categories:\nPositive-Bias dataset and Negative-Bias dataset. If the sum of the probabilities of next token predictions\n(for which the sentiment is positive) is greater than or equal to the sum of the probabilities of next token\npredictions (for which the sentiment is negative) for a sentence, it is classified into the Positive-Bias dataset\nand vice versa.\nTo understand Gender Bias in models, we used the set of 320professions chosen and annotated from\nBolukbasi et al. (2016b). It is an exhaustive list of gender-specific and gender-neutral terms extracted from\nthe w2vNEWS word embedding dataset Bolukbasi et al. (2016a). The dataset was formed on similar grounds\nto the Demographic one, using sentence structure prompts of mainly two types as described in Table 1.\nUsing the gender dataset, we compute the bias in a similar way to demographic bias. The only difference is\nChange in Metric Change in Metric\nBias Type Metric Corruption GPT-2 Small Llama-2 Bias Type GPT-2 Small Llama-2\nDSS 1posL1 C1 0.0651 0.6947 GSS 1pos 1.1858 1.0876\nDSS 1posL1 C2 0.3771 0.6354 GSS 1pos 0.9992 0.9545\nDSS 1posL2 C1 0.0596 0.2341 GSS 1pos 0.1521 1.3336\nDSS 1posL2 C2 0.0404 0.2074 GSS 1pos 0.0418 1.0586\nDSS 1negL1 C2 0.0776 0.5954 GSS 1neg 0.2276 0.1290\nDSS 1negL1 C2 0.2957 0.5501 GSS 1neg 0.0319 0.1290\nDSS 1negL2 C1 0.0062 0.0784 GSS 1neg 0.0666 0.8398\nDSS 1negL2 C2 0.0380 0.0731 GSS 1neg 0.0262 0.1286\nTable 2: Variation in change in Metric value under different metric (i.e. L1andL2) and corruption configurations C1andC2. For\neach category of bias (i.e. DSS 1pos,DSS 1neg,GSS 1pos,GSS 1neg) the lowest change in metric value is boldfaced.\n6\n--- Page 7 ---\nUnder Review\nrather than using the sentiment score of a sentence, we compare the predicted token with the exhaustive\nset of male and female-specific common words as mentioned in Bolukbasi et al. (2016b). Each predicted\nword is then assigned to three groups: Male-Stereotypical(MS), Female-Stereotypical(FS), and\nGender-Neutral(GN) . Similar to demographic dataset, we divide our gender dataset into two categories:\nMale-Biased dataset and Female-Biased dataset. If the sum of MS probabilities (of next token predictions) is\ngreater than or equal to the sum of FS probabilities (of next token predictions) for a sentence, it is classified\ninto the Male-Biased dataset and vice versa. Detailed dataset statistics for both demographic and gender\nbias and the corresponding bias estimations are given in Table 4 in Appendix A.\nImplementation Details We primarily experimented with the Hooked-Transformer fromTransformer-\nLensrepository2which offers a modular and transparent framework for studying the internal mechanisms of\ntransformer models like GPT-2 and Llama-2. For the finetuning experiments, the pretrained GPT-2 and\nLlama-2 were finetuned to examine the change in underlying circuit with respect to two different types\nof datasets. In one variation, the model was fine-tuned on a Positive Dataset where all countries were\ngiven positive tokens (for gender bias case it is male gender bias) so that the finetuned model is biased\ntoward positive sentiment irrespective of the nationality. The goal was to examine whether this fine-tuning\napproach alters the key edges responsible for generating sentences with similar grammatical structures. In the\nsecond variation, the model was fine-tuned on a Shakespeare Dataset , which consisted of Shakespeare text\ncompletely unrelated to the test sentences or topics. This experiment aimed to evaluate whether fine-tuning\non entirely different content affects the edges contributing to bias. We conducted all the experiments in a\ncomputing machine having two A100 GPUs.\nBased on the above discussion we would like to note that we will use the notations DSS 1,DSS 2to describe\ndemographic bias in the grammatical structures described in Table 1. Similarly GSS 1andGSS 2notations\nwill be used to describe gender bias in the grammatical structures described in Table 1. We will use the\nterminology DSS 1pos/neg(GSS 1pos/neg) to describe positive (male) or negative (female) bias.\n5 Results\nAs described in Section 3, we show the effect of variations of corruption technique (described in Table 1)\nand metric L(described in Section 3.1) in EAP for bias identification in Table 2. The ‘Change in Metric’\ncolumn in Table 2 computes the difference in metric value when all the edges are used vs. only the important\nedges are used. From Table 2, it can be observed that the difference in metric value (i.e. change in Metric\ncolumn in Table 2) exhibits minimal change across variations in the corrupted structures (i.e. C1andC2).\nHowever, when comparing L1andL2, it is evident that the absolute difference in metric values are mostly\nmuch smaller (except for DSS 1posdataset in Llama-2) for L2than forL1. Smaller change implies that set of\nimportant edges identified by EAP performs almost similarly as the whole model where all the edges have\nbeen cosidered. Consequently, we used L2andC2as metric for all the remaining demographic and gender\nbias analysis experiments. We also show the top 3edges corresponding to different types of bias across\ndifferent types of models in Table 6 in Appendix D.\nFigure 2 and 3 show the results for localization of bias-related components in GPT-2 and Llama-2. In\nFigure 2, we report, for each layer in the LLM, the number of important edges—displaying only those layers\nthat contain more than 20%of the total important edges to highlight the non-uniform distribution across\nthe model architecture. We can observe from Figure 2 that for all the models (i.e. GPT-2 Small, GPT-2\nLarge and Llama-2) only a few layers contribute to the important edges for bias (i.e. For GPT-2 Small it is\nlayer 2−6, for GPT-2 Large it is primarily 9,10,20,34,35and for Llama-2 it is 0−11and 30−31). The\nobservation from the Figure 2 confirms that bias is encoded in certain layers only. Since there is a significant\noverlap between important edges in DSS 1pos(GSS 1pos) andDSS 1negGSS 1neg, in Figure 2 we only show\nthe edge distribution for DSS 1posandGSS 1pos. It can be observed from Figure 3 that within 40%of the\ntop edges for GPT-2 Small and Llama-2 models, the metric value drops by more than 90%. For GPT-2 Large,\nit requires 60%of the edges to drop the metric value by 90%. Consequently, we can say that demographic\nand gender bias are encoded within a few edges in GPT-2 Small, GPT-2 Large, and Llama-2.\n2https://github.com/TransformerLensOrg/TransformerLens\n7\n--- Page 8 ---\nUnder Review\n(a) GPT-2 Small on DSS1\n (b) GPT-2 Large on DSS1\n (c) Llama-2 on DSS1\n(d) GPT-2 Small on GSS1\n (e) GPT-2 Large on GSS1\n (f) Llama-2 on GSS1\nFigure 2: Layerwise important edge distribution for demographic bias (DSS1) and Gender bias (GSS1) across different models (i.e.\nGPT-2 Small, GPT-2 Large, LLAMA-2 from left to right).\n(a) GPT-2 Small\n (b) GPT-2 Large\n (c) Llama-2\nFigure 3: Drop inL2value with % of Edge Ablation from GPT-2 Small, GPT-2 Large and Llama-2 across different configurations (i.e.\nDSS 1pos,DSS 2pos,DSS 1neg,DSS 2neg,GSS 1pos,GSS 2pos,GSS 1neg,GSS 2neg).\nFigure 4 and 5 illustrates the generalizability and robustness of the edges identified using the EAP approach.\nIn Figure 4, the three confusion matrices show the overlap among the top kedges for both demographic\nand gender bias. There is significant overlap between positive and negative biases of the same type (i.e.,\ndemographic or gender). However, there is minimal to no overlap between demographic and gender biases.\nThis pattern is consistent across GPT-2 (Small & Large) and Llama-2 models. These results suggest that\ncircuits for bias demonstrate similar generalizability patterns across both small and large models. It is\nalso shown that there is very less overlap between the edges responsible for demographic and gender bias.\nAn interesting observation from Figure 4 is that, for demographic bias under grammatical variation (i.e.,\nDSS1 and DSS2), the sets of important edges exhibit minimal overlap. In contrast, gender bias under\nsimilar variations (i.e., GSS1 and GSS2) reveals a substantial degree of overlap, suggesting greater structural\nconsistency in how gender-related information is represented within the model. We have shown a similar\nanalysis with another causal intervention technique Marks et al. (2025) in Appendix F.\nTo evaluate the stability of identified edges (i.e. described in Section 3.4), we analyzed the overlap between\nedges in the pre-trained model and the fine-tuned model in Figure 5. Fine-tuning was performed on two\ndifferent datasets (i.e. Positive andShakespeare as described in Section 4). Interestingly, as shown in the\n8\n--- Page 9 ---\nUnder Review\nPos_DSS1 Neg_DSS1 Pos_DSS2 Neg_DSS2Male_GSS1\nFemale_GSS1Male_GSS2\nFemale_GSS2Pos_DSS1\nNeg_DSS1\nPos_DSS2\nNeg_DSS2\nMale_GSS1\nFemale_GSS1\nMale_GSS2\nFemale_GSS2GPT-2 Small\nPos_DSS1 Neg_DSS1 Pos_DSS2 Neg_DSS2Male_GSS1\nFemale_GSS1Male_GSS2\nFemale_GSS2GPT-2 Large\nPos_DSS1 Neg_DSS1 Pos_DSS2 Neg_DSS2Male_GSS1\nFemale_GSS1Male_GSS2\nFemale_GSS2LLAMA 2\n20406080100\n% Overlap\nFigure 4: Plot showing overlap of top edges by EAP scores for GPT-2 Small, GPT-2 Large and Llama-2 over different bias and\nsentence structure variation.\nPretained_Pos Pretained_Neg\nShakespeare_Pos Shakespeare_NegPositive_Pos Positive_NegPretained_Pos\nPretained_Neg\nShakespeare_Pos\nShakespeare_Neg\nPositive_Pos\nPositive_NegGPT-2 Small\nPretained_Pos Pretained_Neg\nShakespeare_Pos Shakespeare_NegPositive_Pos Positive_NegGPT-2 Large\nPretained_Pos Pretained_Neg\nShakespeare_Pos Shakespeare_NegPositive_Pos Positive_NegLLAMA 2\n20406080100\n% Overlap\nFigure5: Plot Showing Overlap of top edges by EAP scores for Untuned vs Finetuned GPT-2 Small, GPT-2 Large, and Llama-2 in DSS 1\n(Demographic) configuration. Pretrained_Pos, Pretrained_Neg show positive or negative bias in pretrained LLMs. Shakespeare_Pos\nand Shakespeare_Neg denote positive or negative biased models finetuned on the Shakespeare dataset. Positive_pos and Positive_neg\nshow the underlying circuit in the pretrained model finetuned on a positive dataset.\n9\n--- Page 10 ---\nUnder Review\nfigure, fine-tuning with different datasets caused noticeable changes in the circuit components, and this\nobservation held true for both small models (GPT-2 Small) and large models (GPT-2 Large and Llama-2).\nFrom Table 3, we can see that corrupting the top edges responsible for bias reduced bias in the original\nmodel in most of the cases, except for GPT-2 Large in DSS1. We used the top 400,1000and 3000edges\nfrom GPT-2 Small, GPT-2 Large, and Llama-2, respectively. We also wanted to investigate whether this\nedge corruption also affects the performance of the model in other NLP tasks. Consequently, Table 3 shows\nthe performance on two NLP tasks: CoNLL-2003 Sang and De Meulder (2003), a named entity recognition\nbenchmark, and CoLA Warstadt (2019), a linguistic acceptability judgment task for all the models. Table 3\nshows a decrease in performance in CoLA and CoNLL-2003 tasks across all the models. This reduction shows\nthat bias-related edges have an overlap with different language understanding tasks. This differential impact\nacross tasks hints at a hierarchical organization of linguistic knowledge within the network, where certain\ncapabilities are more deeply integrated into these high-influential edges than others.\nGPT-2 Small GPT-2 Large Llama-2\nBias Type δBias CoLA NER-CoNLL2003 δBias CoLA NER-CoNLL2003 δBiasCoLA NER-CoNLL2003\nDSS1 35.88%↓ 22.6%↓ 20.40%↓ 8.89%↑ 3.09%↓ 6.03%↓ 9.16%↓ 2.10%↓ 0.01%↓\nDSS2 30.37% 18.13%↓ 12.63%↓ 71.30%↓ 4.67%↓ 3.37%↓ 35.40%↓0.01%↓ 0.01%↑\nGSS1 21.85%↓ 0.66%↓ 2.70%↓ 2.87%↓ 6.43%↓ 0.18%↓ 28.84%↓5.70%↓ 6.28%↓\nGSS2 19.86%↓ 2.55%↓ 2.83%↓ 1.15%↓ 10.00%↓ 1.23%↓ 25.00%↓7.08%↓ 3.90%↓\nTable 3:δBias shows the change in Bias between the output produced by a pretrained model and the model for which bias responsible\nedges were corrupted. ↓shows decrease in bias and ↑shows increase in bias. We also show performance change for CoLA and\nNRE-CoNL2023.\n6 Conclusion\nIn this work, we investigated how bias is structurally embedded within the architectures of GPT-2 Small,\nGPT-2 Large, and LLaMA-2, using tools from mechanistic interpretability. Our analysis revealed that, across\nmodel scales and architectures, circuits responsible for different categories of bias—such as demographic\nand gender bias—are largely disjoint, indicating that the underlying representations of bias are modular\nand specialized. Through targeted interventions on specific edges identified via our causal metrics, we\ndemonstrated that it is possible to attenuate bias in model outputs without retraining. However, we also\nobserved that these structural manipulations can negatively impact the model’s performance on unrelated\nNLP tasks, such as named entity recognition and natural language inference. Our findings underscore a\nfundamental trade-off between debiasing and general task performance, and point to the need for more\nselective interventions that can isolate bias-related functionality while preserving broader model competence.\nLimitations One of the limitations of this project is that we focus specifically on demographic and gender\nbias in this work. It can be observed from our findings that for different types of bias, the underlying circuit\nresponsible for it will be generally different. Consequently, for other biases, the circuits obtained from this\nwork may not be applied.\nReferences\nAlberto Acerbi and Joseph M. Stubbersfield. Large language models show human-like content biases in transmission\nchain experiments. Proceedings of the National Academy of Sciences , 120(44), 2023.\nAbeba Birhane, Atoosa Kasirzadeh, David Leslie, and Sandra Wachter. Science in the age of large language models.\nNature Reviews Physics , April 2023.\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Quantifying and reducing\nstereotypes in word embeddings. arXiv preprint arXiv:1606.06121 , 2016a.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer\nprogrammer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing\nsystems, 29, 2016b.\n10\n--- Page 11 ---\nUnder Review\nAbhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar Van Der Wal. Identifying and adapting\ntransformer-components responsible for gender bias in an english language model. arXiv preprint arXiv:2310.12611 ,\n2023.\nArthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards\nautomated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems ,\n36:16318–16352, 2023.\nIsabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang,\nSungchul Kim, and Franck Dernoncourt. Self-debiasing large language models: Zero-shot recognition and reduction\nof stereotypes, 2024.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. Advances\nin Neural Information Processing Systems , 34:9574–9586, 2021.\nGabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and\nChris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021.\nNicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model behavior with path\npatching. arXiv preprint arXiv:2304.05969 , 2023.\nPengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, and Anima Anandkumar. ChatGPT based\ndata augmentation for improved parameter-efficient debiasing of LLMs. In Proceedings of the Fourth Workshop on\nLanguage Technology for Equality, Diversity, Inclusion , pages 73–105, March 2024.\nHF Canonical Model Maintainers. distilbert-base-uncased-finetuned-sst-2-english (revision bfdd146), 2022. URL\nhttps://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english .\nMahammed Kamruzzaman, Md. Shovon, and Gene Kim. Investigating subtler biases in LLMs: Ageism, beauty,\ninstitutional, and nationality bias in generative models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,\neditors,Findings of the Association for Computational Linguistics ACL 2024 , pages 8940–8965, Bangkok, Thailand\nand virtual meeting, August 2024. Association for Computational Linguistics.\nMasahiro Kaneko, Danushka Bollegala, and Naoaki Okazaki. The impact of debiasing on the performance of language\nmodels in downstream tasks is underestimated. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu\nPurwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference on Natural\nLanguage Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 29–36, Nusa Dua, Bali, November 2023. Association for Computational\nLinguistics.\nShahar Katz, Yonatan Belinkov, Mor Geva, and Lior Wolf. Backward lens: Projecting language model gradients into\nthe vocabulary space. arXiv preprint arXiv:2402.12865 , 2024.\nJunsol Kim, James Evans, and Aaron Schein. Linear representations of political perspective emerge in large language\nmodels.arXiv preprint arXiv:2503.02080 , 2025.\nHadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. In Proceedings\nof The ACM Collective Intelligence Conference , page 12–24, New York, NY, USA, 2023. Association for Computing\nMachinery. ISBN 9798400701139.\nSamuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits:\nDiscovering and editing interpretable causal graphs in language models. In The Thirteenth International Conference\non Learning Representations , 2025. URL https://openreview.net/forum?id=I4e82CIDxv .\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt.\nAdvances in Neural Information Processing Systems , 35:17359–17372, 2022.\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via\nmechanistic interpretability. arXiv preprint arXiv:2301.05217 , 2023.\nPranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. Nationality\nbias in text generation. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of\nthe European Chapter of the Association for Computational Linguistics , pages 116–122, May 2023.\n11\n--- Page 12 ---\nUnder Review\nRoberto Navigli, Simone Conia, and Björn Ross. Biases in large language models: Origins, inventory, and discussion.\nJ. Data and Information Quality , 15(2), June 2023. ISSN 1936-1955.\nChris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases. transformer circuits\nthread (june 27), 2022.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An\nintroduction to circuits. Distill, 5(3):e00024–001, 2020.\nHaoyi Qiu, Zi-Yi Dou, Tianlu Wang, Asli Celikyilmaz, and Nanyun Peng. Gender biases in automatic evaluation\nmetrics for image captioning. In The 2023 Conference on Empirical Methods in Natural Language Processing , 2023.\nAlecRadford, JeffWu, RewonChild, DavidLuan, DarioAmodei, andIlyaSutskever. Languagemodelsareunsupervised\nmultitask learners. 2019.\nErik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity\nrecognition. arXiv preprint cs/0306050 , 2003.\nShweta Soundararajan and Sarah Jane Delany. Investigating gender bias in large language models through text\ngeneration. In Mourad Abbas and Abed Alhakim Freihat, editors, Proceedings of the 7th International Conference\non Natural Language and Speech Processing (ICNLSP 2024) , pages 410–424, Trento, October 2024. Association for\nComputational Linguistics. URL https://aclanthology.org/2024.icnlsp-1.42/ .\nAaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit discovery. arXiv\npreprint arXiv:2310.10348 , 2023.\nAaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit discovery. In\nYonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors,\nProceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP , pages 407–416,\nNovember 2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\nfine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288 .\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber.\nInvestigating gender bias in language models using causal mediation analysis. Advances in neural information\nprocessing systems , 33:12388–12401, 2020.\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the\nwild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593 , 2022.\nA Warstadt. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471 , 2019.\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretability at scale:\nIdentifying causal mechanisms in alpaca. In Thirty-seventh Conference on Neural Information Processing Systems ,\n2023. URL https://openreview.net/forum?id=nRfClnMhVX .\nFred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods,\n2024. URL https://arxiv.org/abs/2309.16042 .\n12\n--- Page 13 ---\nUnder Review\nA Description for Biased Dataset\nWe did an experiment varyting the number of tokens used to computed bias for GPT-2 and Llama-2 and\nfound that beyond 10tokens the amount of bias present in the model doesn’t change that much. Hence we\nopted for only using 10tokens to compute bias in LLMs. Table 4 shows the bias computed for each model on\ndifferent datasets. It also shows the number of instances present in each variation of the dataset along with\naverage length of the input sentences.\nModel Struct Type Bias Category #Samples Length Bias Metric\nGPT-2 SmallDSS1 Positive 92 7.65 0.659\nDSS1 Negative 132 6.72 0.753\nDSS2 Positive 141 9.48 0.637\nDSS2 Negative 83 9.06 0.633\nGSS1 Male 293 11.69 0.816\nGSS1 Female 27 11.74 0.751\nGSS2 Male 291 10.69 0.837\nGSS2 Female 29 10.75 0.787\nGPT-2 LargeDSS1 Positive 139 7.65 0.785\nDSS1 Negative 85 6.85 0.742\nDSS2 Positive 69 9.65 0.652\nDSS2 Negative 155 9.18 0.762\nGSS1 Male 290 11.68 0.889\nGSS1 Female 30 11.8 0.823\nGSS2 Male 274 10.66 0.847\nGSS2 Female 46 10.91 0.786\nLlama-2DSS1 Positive 216 7.0 0.853\nDSS1 Negative 8 6.125 0.718\nDSS2 Positive 216 10.0 0.813\nDSS2 Negative 8 9.0 0.665\nGSS1 Male 298 13.33 0.861\nGSS1 Female 22 13.36 0.797\nGSS2 Male 291 13.31 0.864\nGSS2 Female 29 13.55 0.760\nTable 4: Dataset Statistics for Different types of Bias Across Different Models. #Samples- Number of Samples,\nLength-Avg Length of the sentence. Bias metric - Normalized probability of Positive words per example of\nPostive Bias Category (and similarly the Normalized probability of Negative words per example for Negative\nBias Category). Similar strategy for Gender Bias.\nB LLMs Explored\nWe used three different models for our experiments. Each one of them is described as follows.\nGPT-2 Small This is a 85M parameter model with 12 layers and 12 attention heads per layer. The model\nhas a dimension of 768 and vocab size of 50257. It uses GELU as its activation function Radford et al. (2019).\nIn its computational graph we have 158 nodes and 32491 edges.\nGPT-2 Large This is a larger version of GPT having 708M parameters, with 36 layers and 20 attention\nheads per layer. This one has a dimension of 1280 and vocab size of 50257. Similar to the smaller version it\nuses GELU as its activation function Radford et al. (2019). In its computational graph we have 758 nodes\nand 810703 edges.\nLlama-2 This is a 6.5B parameter model with 32 layers and 32 attention heads per layer. The model has a\ndimension of 4096 and vocab size of 32000. Unlike GPT-2 versions it employs SiLU as its activation function\nTouvron et al. (2023). In its computational graph we have 1058 nodes and 1592881 edges.\nC Baseline Scoring\nWe have the option to calculate the baseline scores for both positive-bias and negative-bias datasets via two\nmethods i.e., evaluate-baseline scoring and evaluate-graph scoring. The evaluate-baseline function calculates\nthe difference in probabilities between positive and negative next-token predictions and averages it over the\nwhole dataset. While as in evaluate-graph function we have the option of passing the argument of Graph (A\nGraph represents a model’s computational graph. Once instantiated, it contains various Nodes, representing\n13\n--- Page 14 ---\nUnder Review\nNode Description GPT-2 Small GPT-2 Large Llama-2\ninput Input Node NA NA NA\nlogits Logit Node NA NA NA\nm{i} ithMLP layer i∈(0−11)i∈(0−35)i∈(0−31)\na{i}.h{j} jthattention head in i∈(0−11),i∈(0−35),i∈(0−31),\nithattention layer j∈(0−11)j∈(0−19)j∈(0−31)\na{i}.h{j}<x> jth[x] attention head in i∈(0−11),i∈(0−35),i∈(0−31),\nx=[q]uery, [k]ey, [v]alue ithattention layer j∈(0−11)j∈(0−19)j∈(0−31)\nTable 5: Nomenclature for Nodes in the computational graph of GPT-2 and Llama-2 (Edges described in\nManual Edge Ablation Section in Appendix)\nmostly attention heads and MLPs, and Edges representing connections between nodes. Each Node and\nEdge is either in the graph (circuit) or not; by default, all Node and Edge objects are included within the\ngraph.), and in the process of calculating the baseline score using evaluate-graph function we need to pass the\nunaltered graph where no edge or node is ablated yet. Since we are going to use the evaluate-graph function\nwhen we ablate some edges, hence it is best to use evaluate-graph function for getting the baseline score\nwhich we are going to use to check the importance of edges.\nD Finding Bias Circuits\nWe sort all the edges in the graph according to their scores and print the edges in the descending order with\ntheir respective scores for each positive-bias and negative-bias dataset. These scores are the respective edge\nscores and reveal the importance of the edges in the graph for propagating the respective bias. The more\nthe score, the more important is the edge. The goal here was to ablate the Top N edges in the graph (N\nranging from 1 to 10 in our case) and observe the variation in the evaluate-graph scores and compare it to\nthe graph-baseline score. EAP Syed et al. (2023) eventually outputs a sorted list of edges where each edge is\nrepresented by the corresponding connecting nodes. In Table 5 we the description of different types of nodes\nreferred in EAP. Thable 6 shows the top 3 edges obtained for different types of bias across different models\n(i.e. GPT2- Small, GPT2-large and Llama2).\nModel Bias TypeTop 1 Top 2 Top 3\nEdge Score Edge Score Edge Score\nGPT-2 Small Demographic Positive(DSS1) m11->logits0.3307 m0->m2 0.1900 m0->m1 0.1858\nGPT-2 Small Demographic Negative(DSS1) m11->logits0.2655 m0->m1 0.1684 m0->m2 0.1557\nGPT-2 Small Demographic Positive(DSS2) m11->logits0.1580 m0->m2 0.1205 m9->logits 0.0921\nGPT-2 Small Demographic Negative(DSS2) m11->logits0.0720 m0->m2 0.0658 m0->m1 0.0527\nGPT-2 Large Demographic Positive(DSS1) m35->logits0.3183 m33->m35 0.1732 m33->logits 0.1526\nGPT-2 Large Demographic Negative(DSS1) m35->logits0.3219 m33->m35 0.1245 a32.h2->logits 0.1123\nGPT-2 Large Demographic Positive(DSS2) m35->logits0.2458 m32->logits 0.1226 m33->m35 0.1070\nGPT-2 Large Demographic Negative(DSS2) m35->logits0.1151 m32->logits 0.0598 m0->m4 0.0457\nGPT-2 Small Gender Male(GSS1) input->m00.0663 input->a0.h5 〈k〉0.0561 input->a0.h5 〈q〉0.0483\nGPT-2 Small Gender Female(GSS1) input->m00.0675 input->a0.h5 〈k〉0.0539 m11->logits 0.0471\nGPT-2 Small Gender Male(GSS2) input->m00.0615 input->a0.h5 〈k〉0.0602 input->a0.h5 〈q〉0.0502\nGPT-2 Small Gender Female(GSS2) input->m00.0649 input->a0.h5 〈k〉0.0575 m11->logits 0.0541\nGPT-2 Large Gender Male(GSS1) m35->logits0.0281 a33.h11->logits0.0274 m33->logits 0.0261\nGPT-2 Large Gender Female(GSS1) m35->logits0.0206 a33.h11->logits0.0203 m33->logits 0.0192\nGPT-2 Large Gender Male(GSS2) m35->logits0.0306 a33.h11->logits0.0265 a32.h2->logits 0.0249\nGPT-2 Large Gender Female(GSS2) m35->logits0.0241 a32.h2->logits 0.0206 a33.h11->logits0.0204\nLlama-2 Gender Male(GSS1) m31->logits0.2467 m30->logits 0.0560 m30->m31 0.0497\nLlama-2 Gender Female(GSS1) m31->logits0.3141 m30->logits 0.1282 m30->m31 0.0852\nLlama-2 Gender Male(GSS2) m31->logits0.2618 m30->logits 0.0907 m30->m31 0.0542\nLlama-2 Gender Female(GSS2) m31->logits0.3308 m30->logits 0.1461 m30->m31 0.1095\nTable 6: Top 3 edges for Different Models and Different Bias\n14\n--- Page 15 ---\nUnder Review\nPos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1Pos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1GPT-2 Small\nPos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1Pos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1GPT-2 Large\nPos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1Pos_DSS1 Neg_DSS1 Male_GSS1 Female_GSS1LLaMA\nFigure 6: Overlap Results from SAE Approach\nE Creating Corrupted Edges for Downstream Tasks\nFor CoLA dataset we filter the sentence containing atleast one noun, duplicate the sentence and swap every\nnoun token with “XYZ”, preserving positions. For CoLA dataset we swap any two randomly chosen words in\nthe sentence.\nF SAE Approach Results\nSAE Marks et al. (2025) is another recent approach to find important components from a model similar to\nEAP Syed et al. (2023). In Figure 6, we have done similar experiments like Figure 2 using SAE. Figure 6\nshows that there is not much overlap between demographic and gender bias in general.\nG Circuit Diagram\nFigure 7 shows the circuit diagram for different types of bias (i.e. demographic and gender) obtained from\nEAP approach Syed et al. (2023). A circuit is a subgraph of a neural network3.\n3https://distill.pub/2020/circuits/zoom-in/#glossary-circuit\n15\n--- Page 16 ---\nUnder Review\n(a)\n (b)\n (c)\n (d)\n (e)\n(f)\n(g)\n (h)\n (i)\n (j)\n (k)\n(l)\nFigure 7: Circuit Diagram: (a) GPT-2-Small-DSS1-Positive, (b) GPT-2-Small-DSS2-Positive, (c) GPT-2-\nLarge-DSS1-Positive, (d) GPT-2-Large-DSS2-Positive, (e) Llama-2-DSS1-Positive, (f) Llama-2-DSS2-Positive,\n(g) GPT-2-Small-GSS1-Positive, (h) GPT-2-Small-GSS2-Positive, (i) GPT-2-Large-GSS1-Positive, (j) GPT-2-\nLarge-GSS2-Positive, (k) Llama-2-GSS1-Positive, (l) Llama-2-GSS2-Positive.\n16",
  "text_length": 53827
}