{
  "id": "http://arxiv.org/abs/2506.05274v1",
  "title": "From Play to Replay: Composed Video Retrieval for Temporally\n  Fine-Grained Videos",
  "summary": "Composed Video Retrieval (CoVR) retrieves a target video given a query video\nand a modification text describing the intended change. Existing CoVR\nbenchmarks emphasize appearance shifts or coarse event changes and therefore do\nnot test the ability to capture subtle, fast-paced temporal differences. We\nintroduce TF-CoVR, the first large-scale benchmark dedicated to temporally\nfine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K\ntriplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing\non temporal aspect, link each query to a single target segment taken from the\nsame video, limiting practical usefulness. In TF-CoVR, we instead construct\neach <query, modification> pair by prompting an LLM with the label differences\nbetween clips drawn from different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting real-world tasks such\nas sports-highlight generation. To model these temporal dynamics we propose\nTF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video\nencoder on fine-grained action classification to obtain temporally\ndiscriminative embeddings; (ii) align the composed query with candidate videos\nusing contrastive learning. We conduct the first comprehensive study of image,\nvideo, and general multimodal embedding (GME) models on temporally fine-grained\ncomposed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR,\nTF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and\nafter fine-tuning raises the state-of-the-art from 19.83 to 25.82.",
  "authors": [
    "Animesh Gupta",
    "Jay Parmar",
    "Ishan Rajendrakumar Dave",
    "Mubarak Shah"
  ],
  "published": "2025-06-05T17:31:17Z",
  "updated": "2025-06-05T17:31:17Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05274v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05274v1  [cs.CV]  5 Jun 2025From Play to Replay: Composed Video Retrieval for\nTemporally Fine-Grained Videos\nAnimesh Gupta1Jay Parmar1Ishan Rajendrakumar Dave2Mubarak Shah1\n1Center for Research in Computer Vision, University of Central Florida2Adobe\nAbstract\nComposed Video Retrieval (CoVR) retrieves a target\nvideo given a query video and a modification text describing\nthe intended change. Existing CoVR benchmarks emphasize\nappearance shifts or coarse event changes and therefore\ndo not test the ability to capture subtle, fast-paced tem-\nporal differences. We introduce TF-CoVR, the first large-\nscale benchmark dedicated to temporally fine-grained CoVR.\nTF-CoVR focuses on gymnastics and diving and provides\n180K triplets drawn from FineGym and FineDiving. Pre-\nvious CoVR benchmarks focusing on temporal aspect, link\neach query to a single target segment taken from the same\nvideo, limiting practical usefulness. In TF-CoVR, we in-\nstead construct each <query, modification> pair by prompt-\ning an LLM with the label differences between clips drawn\nfrom different videos; every pair is thus associated with\nmultiple valid target videos (3.9 on average), reflecting\nreal-world tasks such as sports-highlight generation. To\nmodel these temporal dynamics we propose TF-CoVR-Base,\na concise two-stage training framework: (i) pre-train a\nvideo encoder on fine-grained action classification to ob-\ntain temporally discriminative embeddings; (ii) align the\ncomposed query with candidate videos using contrastive\nlearning. We conduct the first comprehensive study of im-\nage, video, and general multimodal embedding (GME) mod-\nels on temporally fine-grained composed retrieval in both\nzero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-\nBase improves zero-shot mAP@50 from 5.92 (Language-\nBind) to 7.51, and after fine-tuning raises the state-of-the-art\nfrom 19.83 to 25.82. Dataset and code are available at\nhttps://github.com/UCF-CRCV/TF-CoVR .\n1. Introduction\nRecent progress in content-based image retrieval has evolved\ninto multimodal composed image retrieval (CoIR) [ 1,21,\n46], where a system receives a query image and a short\ntextual modification and returns the image that satisfies the\ncomposition. Composed video retrieval (CoVR) [ 38] gen-\neralizes this idea, asking for a target video that realizes auser-described transformation of a query clip, for example,\n“same river landscape, but in springtime instead of autumn”\n(Fig. 1a) or “same pillow, but picking up rather than putting\ndown”(Fig. 1b).\nExisting CoVR benchmarks cover only a limited portion\nof the composition space. For example, WebVid-CoVR [ 38]\n(Fig. 1a) is dominated by appearance changes and de-\nmands minimal temporal reasoning, while Ego-CoVR [ 9]\nrestricts the query and target to different segments of a single\nvideo(Fig. 1b). In practice, many high-value applications\ndepend on fine-grained motion differences: surgical moni-\ntoring of subtle patient movements [ 35], low-latency AR/VR\ngesture recognition [ 44], and sports analytics where distin-\nguishing a 1.5-turn from a 2-turn somersault drives coaching\nfeedback [ 8,26]. The commercial impact is equally clear:\nthe Olympic Broadcasting Service AI highlight pipeline in\nParis 2024 increased viewer engagement 13 times in 14\nsports [ 13]. No public dataset currently evaluates CoVR at\nthis temporal resolution.\nTo address these limitations, we present TF-CoVR\n(Temporally Fine-grained Composed Video Retrieval), a\nlarge-scale benchmark for composed retrieval in gymnastics\nand diving constructed from the temporally annotated Fine-\nGym [ 31] and FineDiving [ 43] datasets. Previous work such\nas Ego-CoVR restricts query and target clips to different seg-\nments of a single video; in practice, however, relevant results\noften come from distinct videos. TF-CoVR instead provides\n180K triplets, each containing a query video, a textual mod-\nification, and one or more ground-truth target videos. We\ncall each ⟨query, modification ⟩pair a composed query . The\nbenchmark covers both event-level changes (e.g. the same\nsub-action on different apparatuses) and fine-grained sub-\naction transitions (e.g. varying rotation counts or entry/exit\ntechniques), yielding a setting that reflects real-world tem-\nporally fine-grained retrieval far more closely than exist-\ning datasets. A thorough comparison with prior datasets is\nshown in Table 1.\nExisting CoVR models, trained on appearance-centric\ndata, usually obtain video representations by simply averag-\ning frame embeddings, thereby discarding temporal structure.\nFine-grained retrieval demands video embeddings that pre-\n1\n--- Page 2 ---\n(A) WebVid-CoVR\n(Vault) round-off, flic-flac on, stretched salto\nbackward off\n(Balance Beam) Switch Leap with 1 turn \nchange season to springtime\nRiver landscape in springtime River landscape in autumnQuery Video Target Video\n(Vault) round-off, flic-flac on, stretched salto\nbackward with 2 turn off Pick it up\n#C C picks pillow #C C puts pillow down\n(Floor Exercise) Switch Leap with 1 turn (C) TF-CoVR (B) EgoCVR\nShow with 2 turn\nShow on Balance Beam\nFigure 1. Comparison of composed-retrieval triplets in WebVid-CoVR, Ego-CoVR, and TF-CoVR. (a) WebVid-CoVR targets appearance\nchanges. (b) Ego-CoVR selects the target clip from a different time-stamp of the same video, showing a new interaction with the same\nobject. (c) TF-CoVR supports two fine-grained modification types: temporal change- varying sub-actions within the same event (row 3)- and\nevent change- the same sub-action performed on different apparatuses (row 4).\nserve these dynamics. To this end we introduce a strong\nbaseline, TF-CoVR-Base . Unlike recent video–language sys-\ntems that depend on large-scale descriptive caption rewriting\nwith LLMs, TF-CoVR-Base follows a concise two-stage\npipeline. Stage 1 pre-trains a video encoder on fine-grained\naction classification, producing temporally discriminative\nembeddings. Stage 2 forms a composed query by concatenat-\ning the query-video embedding with the text-modification\nembedding and aligns it with candidate video embeddings\nvia contrastive learning.\nWe benchmark TF-CoVR with image-based CoIR base-\nlines, video-based CoVR systems, and general multimodal\nembedding (GME) models such as E5-V , evaluating every\nmethod in both zero-shot and fine-tuned regimes. TF-CoVR-\nBase attains 7.51 mAP@50 in the zero-shot setting, surpass-\ning the best GME model (E5-V , 5.22) and all specialized\nCoVR methods. Fine-tuning further lifts performance to\n25.82 mAP@50, a sizeable gain over the previous state-of-the-art BLIP CoVR-ECDE (19.83). These results underscore the\nneed for temporal granularity and motion-aware supervi-\nsion in CoVR, factors often missing in current benchmarks.\nTF-CoVR provides the scale to support this and exposes\nlimitations of appearance-based models.\nTo summarize, our main contributions are as follows:\n•We release TF-CoVR , a large-scale composed-video\nbenchmark focused on sports actions, with 180K training\ntriplets and a test set queries having average 3.9 valid\ntargets each to support realistic evaluation.\n•We propose TF-CoVR-Base , a simple yet strong baseline\nthat captures temporally fine-grained visual cues without\nrelying on descriptive, LLM-generated captions.\n•We provide the first comprehensive study of image, video,\nand GME models on temporally fine-grained retrieval\nunder both zero-shot and fine-tuning protocols, where\nTF-CoVR-Base yields consistent gains across settings.\n2\n--- Page 3 ---\nFigure 2. Overview of our automatic triplet generation pipeline for TF-CoVR. We start with temporally labeled clips from FineGym and\nFineDiving datasets. Using CLIP-based text embeddings, we compute similarity between temporal labels and form pairs with high semantic\nsimilarity. These label pairs are passed to GPT-4o along with in-context examples to generate natural language modifications describing\nthe temporal differences between them. Each generated triplet consists of a query video, a target video, and a modification text capturing\nfine-grained temporal action changes.\n2. Related Work\nVideo Understanding and Fast-Paced Datasets: Video\nunderstanding [ 25] often involves classifying videos into\npredefined action categories [ 11,16,36]. These tasks are\nbroadly categorized as coarse- or fine-grained. Coarse-\ngrained datasets like Charades [ 33] and Breakfast [ 17] cap-\nture long, structured activities, but lack the temporal resolu-\ntion and action granularity needed for composed retrieval. In\ncontrast, fine-grained datasets like FineGym [ 31] and Fine-\nDiving [ 43] provide temporally segmented labels for sports\nactions. They cover high-motion actions where subtle differ-\nences (e.g., twists or apparatus) lead to semantic variation,\nmaking them suitable for retrieval tasks with fine-grained\ntemporal changes. Yet these datasets remain unexplored in\nthe CoVR setting, leaving a gap in leveraging temporally rich\ndatasets. TF-CoVR bridges this gap by introducing a bench-\nmark that explicitly targets temporally grounded retrieval in\nfast-paced, fine-grained video settings.\nComposed Image Retrieval: CoIR retrieves a target\nimage using a query image and a modification text describ-\ning the desired change. CoIR models are trained on large-\nscale triplets of query image, modification text, and target\nimage [ 7,18,39], which has proven useful for general-\nizing across open-domain retrieval. CIRR [ 24] provides\n36K curated triplets with human-written modification texts\nfor CoIR, but suffers from false negatives and query mis-\nmatches. CIRCO [ 2] improves on this by using COCO [ 20]\nand supporting multiple valid targets per query. More re-\ncently, CoLLM [ 12] released MTCIR, a 3.4M triplet dataset\nwith natural captions and diverse visual scenes, addressing\nthe lack of large-scale, non-synthetic data. Despite these\nadvances, CoIR datasets are inherently image-centric and\nlack temporal depth, limiting their generalization to video\nretrieval tasks with fine-grained alignment.Composed Video Retrieval: WebVid-CoVR [ 38] first\nintroduced CoVR as a video extension of CoIR, using query-\nmodification-target triplets sampled from open-domain\nvideos. However, its lack of temporal grounding limits\nWebVid-CoVR’s effectiveness in retrieving videos based\non fine-grained action changes. EgoCVR [ 9] addressed this\nby constructing triplets within the same egocentric video to\ncapture temporal cues. FineCVR [ 47] advanced CoVR by\nconstructing a fine-grained retrieval benchmark using exist-\ning video understanding datasets such as ActivityNet [ 4], Ac-\ntionGenome [ 14], HVU [ 6], and MSR-VTT [ 42]. Addition-\nally, it introduced a consistency attribute in the modification\ntext to guide retrieval more effectively. While an important\nstep, the source datasets are slow-paced and coarse-grained,\nlimiting their ability to capture subtle action transitions. De-\nspite progress, CoVR benchmarks remain limited, relying\nmostly on slow-paced or object-centric content and offer\nonly a single target per query, limiting real-world evaluation\nwhere multiple valid matches may exist.\nMultimodal Embedding Models for Composed Re-\ntrieval: Recent advances in MLLMs such as GPT-4o [ 10],\nLLaVa [ 22,23], and QwenVL [ 40] have significantly ac-\ncelerated progress in joint visual-language understanding\nand reasoning tasks [ 5,29,30]. VISTA [ 50] and MAR-\nVEL [ 51] extend image-text retrieval by pairing pre-trained\ntext encoders with enhanced vision encoders to better cap-\nture joint semantics. E5-V [ 15] and MM-Embed [ 19] further\nimprove retrieval by using relevance supervision and hard\nnegative mining to mitigate modality collapse. Zhang et\nal. recently introduced GME [ 48], a retrieval model that\ndemonstrates strong performance on CoIR, particularly in\nopen-domain image-text query settings. However, GME and\nsimilar MLLM-based retrievers remain untested in CoVR,\nespecially in fast-paced scenarios requiring fine-grained tem-\n3\n--- Page 4 ---\nTable 1. Comparison of existing datasets for composed image and video retrieval, highlighting the unique features of TF-CoVR. Datasets are\ncategorized by modality (Type), where ὏7indicates image-based and /videoindicates video-based triplets.\nDataset Type #Triplets Train Eval Multi-GT Eval Metrics #Sub-actions\nCIRR [24] ὏7 36K ✓ ✓ ✗ Recall@K ✗\nFashionIQ [41] ὏7 30K ✓ ✓ ✗ Recall@K ✗\nCC-CoIR [38] ὏7 3.3M ✓ ✗ ✗ Recall@K ✗\nMTCIR [12] ὏7 3.4M ✓ ✗ ✗ Recall@K ✗\nWebVid-CoVR [38] /video 1.6M ✓ ✓ ✗ Recall@K ✗\nEgoCVR [9] /video 2K ✗ ✓ ✗ Recall@K ✗\nFineCVR [47] /video 1M ✓ ✓ ✗ Recall@K ✗\nCIRCO [3] ὏7 800 ✗ ✓ ✓ mAP@K ✗\nTF-CoVR (Ours) /video 180K ✓ ✓ ✓ mAP@K 306\nporal alignment.\n3. TF-CoVR: Dataset Generation\nFineGym and FineDiving for Composed Video Retrieval:\nComposed video retrieval (CoVR) operates on triplets\n(Vq, Tm, Vt), where Vq,Tm, and Vtdenote the query video,\nmodification text, and target video, respectively. Prior works\n[9,38] construct such triplets by comparing captions and\nselecting pairs that differ by a small textual change, often\na single word. This approach, however, relies on the avail-\nability of captions, which limits its applicability to datasets\nwithout narration. To overcome this, we use FineGym [ 31]\nand FineDiving [ 43], which contain temporally annotated\nsegments but no captions. Instead of captions, we utilize the\ndatasets’ fine-grained temporal labels, which describe pre-\ncise sub-actions. FineGym provides 288 labels over 32,697\nclips (avg. 1.7s), from 167 long videos, and FineDiving\nincludes 52 labels across 3,000 clips.\nTo identify meaningful video pairs, we compute CLIP-\nbased similarity scores between all temporal labels and select\nthose with high semantic similarity [ 27]. These pairs are\nthen manually verified and categorized into two types: (1)\ntemporal changes, where the sub-action differs within the\nsame event (e.g., (Vault) round-off, flic-flac with 0.5 turn\non, stretched salto forward with 0.5 turn off vs....with 2\nturn off ), and (2) event changes, where the same sub-action\noccurs in different apparatus contexts (e.g., (Floor Exercise)\nswitch leap with 1 turn vs.(Balance Beam) switch leap with\n1 turn ). These examples show that even visually similar\nactions can have different semantic meanings depending\non temporal or contextual cues. We apply this strategy to\nboth FineGym and FineDiving to generate rich, fine-grained\nvideo triplets. (See Figure 1 for illustrations.)\nModification Instruction and Triplet Generation: To\ngenerate modification texts for TF-CoVR, we start with thefine-grained temporal labels associated with gymnastics and\ndiving segments, such as Forward, 1.5 Soms.Pike, Entry or\n(Vault) tsukahara stretched with 2 turn . Using CLIP, we\ncompute pairwise similarity scores between all labels and\nselect those that differ in small but meaningful aspects, rep-\nresenting source and target actions connected by a semantic\nmodification.\nEach selected label pair is passed to GPT-4o [ 10] along\nwith a prompt and 15 in-context examples capturing typical\nsub-action and event-level changes [ 37]. GPT-4o generates\nconcise natural language instructions that describe how to\ntransform the source into the target, e.g., Show with 2.5 som-\nersaults orShow on Balance Beam . Unlike prior work such\nas FineCVR [ 47], which emphasizes visual consistency, our\nmodifications focus exclusively on semantic changes, mak-\ning them better suited for real-world use cases like highlight\ngeneration where visual similarity is not required.\nTo form triplets, we split the original long-form videos\ninto training and testing sets to avoid overlap. From these,\nsub-action clips are extracted and paired with the correspond-\ning modification text. Although individual clips may be\nreused, each resulting triplet, comprising a query video, a\nmodification text, and a target video, is unique. This process\nis repeated exhaustively across all labeled segments. Fig-\nure 2 illustrates the full pipeline, from label pairing to triplet\ngeneration.\nTF-CoVR Statistics: TF-CoVR contains 180K train-\ning triplets and 473 testing queries, each associated with\nmultiple ground-truth target videos (Table 1). The test set\nspecifically addresses the challenge of evaluating multiple\nvalid retrievals, a limitation in existing CoVR benchmarks.\nThe dataset spans 306 fine-grained sports actions: 259 from\nFineGym [ 31] and 47 from FineDiving [ 43]. Clip durations\nrange from 0.03s to 29.00s, with an average of 1.90s.\nModification texts vary from 2 to 19 words (e.g., “show\noff” to“Change direction to Reverse, reduce to two and\n4\n--- Page 5 ---\nStage 2 (TF-CoVR-Base)Stage 1 Inference\nFigure 3. Overview of TF-CoVR-Base framework. Stage 1 learns temporal video representations via supervised classification using the\nAIM encoder. In Stage 2, the pretrained AIM and BLIP encoders are frozen, and a projection layer and MLP are trained to align the\nquery-modification pair with the target video using contrastive loss. During inference, the model retrieves relevant videos from TF-CoVR\nbased on a user-provided query and textual modification.\na half twists, and show with one and a half somersaults” ),\nwith an average length of 6.11 words. Each test query has\nan average of 3.94 valid targets, supporting realistic and\nchallenging evaluation under a multi-ground-truth setting.\nThis makes TF-CoVR especially suited for applications like\nhighlight generation in sports broadcasting, where retrieving\ndiverse sub-action variations is essential.\n4. TF-CoVR-Base: Structured Temporal\nLearning for CoVR\nMethod Overview: In the composed video retrieval (CoVR)\ntask, the goal is to retrieve a target video Vtgiven a query\nvideo Vqand a textual modification Tmthat describes the in-\ntended transformation. This requires learning a cross-modal\nrelationship between visual and textual inputs that captures\nhow the target differs from the query. While prior meth-\nods have shown promise on general video datasets, CoVR\nbecomes significantly more challenging in fine-grained, fast-\npaced domains such as gymnastics and diving, where subtle\ntemporal action differences are critical. Existing approaches\noften overlook these dynamics, motivating the need for a\nmore temporally grounded framework.\nTwo-Stage CoVR Approach: We propose a two-stage\ntraining framework, TF-CoVR-Base, for composed video\nretrieval in fine-grained, fast-paced domains such as gymnas-tics and diving. TF-CoVR-Base is designed to explicitly cap-\nture the temporal structure in videos and align it with textual\nmodifications for accurate retrieval. Unlike prior approaches\nthat rely on average-pooled frame features from image-level\nencoders, TF-CoVR-Base decouples temporal representation\nlearning from the retrieval task. It first learns temporally rich\nvideo embeddings through supervised action classification,\nand then uses these embeddings in a contrastive retrieval\nsetup. We describe each stage of the framework below.\nStage One: Temporal Pretraining via Video Classifica-\ntion: In the first stage, we aim to learn temporally rich video\nrepresentations from TF-CoVR. To this end, we employ the\nAIM encoder [ 45], which is specifically designed to capture\ntemporal dependencies by integrating temporal adapters into\na CLIP-based backbone.\nWe pretrain the AIM encoder on a supervised video classi-\nfication task using all videos from the triplets in the training\nset. Let V={f1, f2, . . . , f f}denote a video clip with f\nframes. The AIM encoder processes each frame and pro-\nduces a sequence-level embedding:\nzV=AIM(V).\nThe classification logits zVare passed through a softmax\nfunction to produce a probability distribution over classes:\nˆp(i)\nV=Softmax (z(i)\nV).\n5\n--- Page 6 ---\nTable 2. Evaluation of models fine-tuned on TF-CoVR using mAP@K for K∈ {5,10,25,50}. We report the performance of various fusion\nstrategies and model architectures trained on TF-CoVR. Fusion methods include MLP and cross-attention (CA). Each model is evaluated\nusing a fixed number of sampled frames from both query and target videos. Fine-tuning on TF-CoVR leads to significant improvements\nacross all models.\nModalities Model Fusion #Query #Target mAP@K ( ↑)\nVideo Text Frames Frames 5 10 25 50\nFine-tuned on TF-CoVR\n✗ ✓ BLIP2 - - 15 10.69 13.02 15.35 16.41\n✓ ✗ BLIP2 - 1 15 4.86 6.49 8.92 10.06\n✓ ✓ CLIP MLP 1 15 7.01 8.35 10.22 11.38\n✓ ✓ BLIP2 MLP 1 15 10.86 13.20 15.38 16.31\n✓ ✓ CLIP MLP 15 15 6.40 7.46 9.21 10.40\n✓ ✓ BLIP2 MLP 15 15 11.64 14.81 16.74 17.55\n✓ ✓ BLIP-CoVR CA [38] 1 15 11.07 13.94 16.07 16.88\n✓ ✓ BLIP CoVR-ECDE CA [34] 1 15 13.03 15.90 18.62 19.83\n✓ ✓ TF-CoVR-Base (Ours) (Stage-2, only) MLP 8 8 15.08 18.70 21.78 22.61\n✓✓ TF-CoVR-Base (Ours) MLP 12 12 20.77 23.02 25.06 25.82\nEach video Vis annotated with a ground-truth label yV, and\nthe model is optimized using the standard cross-entropy loss:\nLcls=−CX\ni=1y(i)\nVlog ˆp(i)\nV.\nwhere C= 306 is the total number of fine-grained action\nclasses in the TF-CoVR dataset.\nStage Two: Contrastive Training for Retrieval: In\nthe second stage of TF-CoVR-Base, we train a contrastive\nmodel to align the composed query representations with the\ntarget video representations. As illustrated in Figure 3, each\ntraining sample is structured as a triplet (Vq, Tm, Vt), where\nVqis the query video consisting of Nframes, Tmis the\nmodification text withLtokens, and Vtis the target video\ncomprising Mframes.\nWe use the pretrained and frozen AIM encoder to extract\ntemporally rich embeddings for the query and target videos:\nzq=AIM(Vq), z t=AIM(Vt).\nThe modification text Tmis encoded using the BLIP2\ntext encoder Etext, followed by a learnable projection layer\nPthat maps the text embedding into a shared embedding\nspace. This step ensures the textual features are adapted and\naligned and aligned with the video modality for the CoVR\ntask:\nzm=P(Etext(Tm)).\nWe then fuse the query video embedding zqand the pro-\njected text embedding zmusing a multi-layer perceptron(MLP), producing the composed query representations:\nzqm=MLP(zq, zm).\nTo compare the composed query embeddings with the\ntarget video embeddings, both zqmandztare projected into\na shared embedding space and normalized to unit vectors.\nTheir relationship is then measured using cosine, computed\nas:\nSi,j=z(i)\nqm·z(j)\nt\n∥z(i)\nqm∥∥z(j)\nt∥.\nTo ensure numerical stability and regulate the scale of\nsimilarity scores, cosine similarity is adjusted using a tem-\nperature parameter:\nsim(z(i)\nqm, z(j)\nt) =Si,j\nτ.\nwhere τ∈R>0is a learnable temperature parameter.\nFor supervision, we adopt the HN-NCE loss [ 28], which\nemphasizes hard negatives by assigning greater weights to\nsemantically similar but incorrect targets. Given a batch B\nof triplets (qi, mi, ti), the loss is defined as:\nLv(B) =−X\ni∈B\u0014\nlogeSi,i/τ\nαeSi,i/τ+P\nj̸=ieSi,j/τwi,j\n+ logeSi,i/τ\nαeSi,i/τ+P\nj̸=ieSj,i/τwj,i\u0015\nHere, Si,jis the cosine similarity between the composed\nquery z(i)\nqmand the target video z(j)\nt,αis a scalar constant\n6\n--- Page 7 ---\n(set to 1), τis a temperature hyperparameter (set to 0.07),\nandwi,jis a weighting factor derived from CLIP-based text\nsimilarity between the target labels of iandj.\nTable 3. Benchmarking results on TF-CoVR using mAP@K for\nK∈ {5,10,25,50}. We evaluate two groups of models: (1)\nExisting CoVR methods trained on WebVid-CoVR and not fine-\ntuned on TF-CoVR , and (2) General Multimodal Embeddings ,\ntested in a zero-shot setting. Each model is evaluated on query-\ntarget pairs consisting of the specified number of sampled frames.\n“CA” denotes the use of cross-attention fusion.\nModalities Model Fusion #Query #Target mAP@K (↑)\nVideo Text Frames Frames 5 10 25 50\nGeneral Multimodal Embeddings (TF-CoVR)\n✓ ✓ GME-Qwen2-VL-2B [48] MLLM 1 15 2.28 2.64 3.29 3.81\n✓ ✓ MM-Embed [19] MLLM 1 15 2.39 2.81 3.61 4.14\n✓ ✓ E5-V [15] Avg 1 15 3.14 3.78 4.65 5.22\nNot fine-tuned on TF-CoVR\n✗ ✓ BLIP2 - - 15 1.34 1.79 2.20 2.50\n✓ ✗ BLIP2 - 1 15 1.74 2.20 3.06 3.62\n✓ ✓ BLIP-CoVR [38] CA 1 15 2.33 2.99 3.90 4.50\n✓ ✓ BLIP CoVR-ECDE [34] CA 1 15 0.78 0.88 1.16 1.37\n✗ ✓ TF-CVR [9] - - 15 0.56 0.76 0.99 1.24\n✓ ✓ LanguageBind [52] Avg 8 8 3.43 4.37 5.26 5.92\n✓ ✓ AIM (k400) Avg 8 8 3.75 4.37 5.47 6.12\n✓ ✓ AIM (k400) Avg 16 16 4.23 5.14 6.37 7.13\n✓ ✓ AIM (k400) Avg 32 32 4.22 5.15 6.50 7.30\n✓ ✓ AIM (diving48) Avg 32 32 4.81 5.78 6.82 7.51\n5. Discussion\nEvaluation Metric: To effectively evaluate retrieval perfor-\nmance in the presence of multiple ground-truth target videos,\nwe adopt the mean Average Precision at K(mAP@K) met-\nric, as proposed in CIRCO [ 3]. The mAP@K metric mea-\nsures whether the correct target videos are retrieved and\nconsiders the ranks at which they appear in the retrieval\nresults.\nHere, Kdenotes the number of top-ranked results consid-\nered for evaluation. For example, mAP@5 measures preci-\nsion based on the top 5 retrieved videos, capturing how well\nthe model retrieves relevant targets early in the ranked list.\nA higher Kallows evaluation of broader retrieval quality,\nwhile a lower Kemphasizes top-ranking precision.\nSpecialized vs. Generalized Multimodal Models for\nCoVR: We compare specialized models trained specifi-\ncally for composed video retrieval, such as those trained on\nWebVid-CoVR [ 38], with Generalized Multimodal Embed-\nding (GME) models that have not seen CoVR data. Among\nthe specialized baselines, we include two image-based en-\ncoders (CLIP and BLIP) and one video-based encoder (Lan-\nguageBind) to cover different modality types and fusion\nmechanisms. As shown in Table 3, our evaluation reveals\nthat GME models consistently outperform most specialized\nCoVR methods in the zero-shot setting. For example, E5-\nV [15] achieves 5.22 mAP@50, outperforming BLIP-CoVR\n(4.50) and BLIP CoVR-ECDE (1.37), and closely matching Lan-\nguageBind (5.92). Other GME variants like MM-Embed\nand GME-Qwen2-VL-2B also show promising results. Incontrast, TF-CVR [ 9] performs worst among all tested mod-\nels, with only 1.24 mAP@50, underscoring its limitations in\nhandling fine-grained action variations.\nThis performance gap is partly due to TF-CVR’s reliance\non a captioning model to describe the query video. We re-\nplaced the original Lavila [ 49] with Video-XL [ 32], which\nprovides better captions for structured sports content. How-\never, even Video-XL fails to capture subtle temporal cues\nlike twist counts or somersaults, critical for accurate re-\ntrieval, causing TF-CVR to struggle with temporally precise\nmatches. In contrast, GME models benefit from large-scale\nmultimodal training involving text, images, and combina-\ntions thereof, allowing them to generalize well to CoVR\nwithout task-specific fine-tuning. We expect their perfor-\nmance to improve further with fine-tuning on TF-CoVR,\nthough we leave this exploration to future work. See sup-\nplementary material for a comparison of Lavila-generated\ncaptions.\nTable 4. Performance of GME models on existing CoIR bench-\nmarks. We report mAP@5 and Recall@10 on FashionIQ, CIRR,\nand CIRCO using official evaluation protocols. Values are directly\ntaken from the original papers.\nModel Metric FQ CIRR CIRCO\nE5-V [15] Recall@10 3.73 13.19 -\nGME-2B [48] Recall@10 26.34 47.70 -\nMM-Embed [19] Recall@10 25.7 50.0 -\nE5-V [15] mAP@5 - - 19.1\nMM-Embed [19] mAP@5 - - 32.3\nEvaluating TF-CoVR-Base Against Existing Methods:\nWe compare our proposed two-stage TF-CoVR-Base frame-\nwork with all existing CoVR baselines in Table 2. Our full\nmodel achieves 25.82 mAP@50, significantly outperforming\nthe strongest prior method, BLIP CoVR-ECDE (19.83). Even\nour Stage-2-only variant (trained without temporal pretrain-\ning) outperforms all existing methods with 22.61 mAP@50,\nhighlighting the strength of our contrastive fusion strategy.\nUnlike BLIP CoVR-ECDE , our model does not rely on detailed\ntextual descriptions of the query video and instead learns\ntemporal structure directly from the visual input. This makes\nit especially effective in structured, fast-paced sports videos,\nwhere subtle action distinctions, such as change in twist\ncount or apparatus, are visually grounded. Across all K val-\nues, TF-CoVR-Base shows consistent improvements of 4–6\nmAP points.\nQualitative Analysis: Figure 4 illustrates the effective-\nness of our method using qualitative examples. The retrieved\ntarget videos accurately reflect the action modifications de-\nscribed in the input text. Correctly retrieved clips are out-\nlined in green, and incorrect ones in red. Interestingly, even\nincorrect predictions are often semantically close to the in-\n7\n--- Page 8 ---\nShow with 1.5 Somersaults\nQuery VideoRank 1 Rank 2 Rank 3Show with 1.5 Somersaults\nQuery VideoRank 1 Rank 2 Rank 3Show with 1.5 Somersaults\nQuery VideoRank 1 Rank 2 Rank 3\nQuery VideoRank 1 Rank 2 Rank 3\nQuery VideoRank 1 Rank 2 Rank 3Show with 1.5 Somersaults\nQuery VideoRank 1 Rank 2 Rank 3\nTF-CoVR-BaseFigure 4. Qualitative results for the composed video retrieval task using our two-stage approach. Each column presents a query video (top),\na corresponding modification instruction (middle), and the top-3 retrieved target videos (ranks 1–3) based on the model’s predictions. The\nmodification instructions capture fine-grained action or event-level changes. This visualization demonstrates the effectiveness of the retrieval\nmodel in identifying subtle temporal variations, highlighting the practical utility of TF-CoVR for fine-grained sports understanding and\nhighlight generation.\ntended target, revealing the fine-grained difficulty of TF-\nCoVR. For example, in the third column of Figure 4, the\nquery video includes a turning motion, while the modifica-\ntion requests a “no turn” variation. Our method correctly\nretrieves “no turn” actions at top ranks, but at rank 3, re-\ntrieves a “split jump” video, visually similar but semantically\ndifferent. We highlight this with a red overlay to emphasize\nthe subtle distinction in motion, showing the value of TF-\nCoVR for evaluating fine-grained temporal reasoning.\n6. Limitations and Conclusion\nLimitations. TF-CoVR offers a new perspective on com-\nposed video retrieval by focusing on retrieving videos that\nreflect subtle action changes, guided by modification text.\nWhile it adds valuable depth to the field, the dataset has\nsome limitations. One limitation is that it requires expert\neffort to temporally annotate videos such as from FineGym\nand FineDiving, which is currently lacking in the video-\nunderstanding community, and such annotation is expensive\nto scale up. This reflects the trade-off between expert-driven\nannotations and scalability. Regarding the TF-CoVR-Base,\nit is currently two-stage, which may not provide a fully end-\nto-end solution; a better approach could be a single-stagemodel that simultaneously learns temporally rich video rep-\nresentations and aligns them with the modification text.\nConclusion. In this work, we introduced TF-CoVR, a large-\nscale dataset comprising 180K unique triplets centered on\nfine-grained sports actions, spanning 306 diverse sub-actions\nfrom gymnastics and diving videos. TF-CoVR brings a new\ndimension to the CoVR task by emphasizing subtle tempo-\nral action changes in fast-paced, structured video domains.\nUnlike existing CoVR datasets, it supports multiple ground-\ntruth target videos per query, addressing a critical limitation\nin current benchmarks and enabling more realistic and flexi-\nble evaluation. In addition, we propose a two-stage training\nframework that explicitly models temporal dynamics through\nsupervised pre-training. Our method significantly outper-\nforms existing approaches on TF-CoVR. Furthermore, we\nconducted a comprehensive benchmarking of both existing\nCoVR methods and General Multimodal Embedding (GME)\nmodels, marking the first systematic evaluation of GME\nperformance in the CoVR setting. We envision TF-CoVR\nserving as a valuable resource for real-world applications\nsuch as sports highlight generation, where retrieving nuanced\nsub-action variations is essential for generating engaging and\ncontextually rich video content.\n8\n--- Page 9 ---\nReferences\n[1]Muhammad Umer Anwaar, Egor Labintcev, and Martin Kle-\ninsteuber. Compositional learning of image-text query for\nimage retrieval. In Proceedings of the IEEE/CVF Winter\nconference on Applications of Computer Vision , pages 1140–\n1149, 2021. 1\n[2]Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and\nAlberto Del Bimbo. Zero-shot composed image retrieval with\ntextual inversion, 2023. 3\n[3]Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and\nAlberto Del Bimbo. Zero-shot composed image retrieval\nwith textual inversion. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 15338–\n15347, 2023. 4, 7\n[4]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and\nJuan Carlos Niebles. Activitynet: A large-scale video bench-\nmark for human activity understanding. In Proceedings of the\nieee conference on computer vision and pattern recognition ,\npages 961–970, 2015. 3\n[5]Ron Campos, Ashmal Vayani, Parth Parag Kulkarni, Ro-\nhit Gupta, Aritra Dutta, and Mubarak Shah. Gaea: A\ngeolocation aware conversational model. arXiv preprint\narXiv:2503.16423 , 2025. 3\n[6]Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri,\nJürgen Gall, Rainer Stiefelhagen, and Luc Van Gool. Large\nscale holistic video understanding. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part V 16 , pages 593–610.\nSpringer, 2020. 3\n[7]Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun,\nYoohoon Kang, and Sangdoo Yun. Compodiff: Versatile com-\nposed image retrieval with latent diffusion. arXiv preprint\narXiv:2303.11916 , 2023. 3\n[8]James Hong, Matthew Fisher, Michaël Gharbi, and Kayvon\nFatahalian. Video pose distillation for few-shot, fine-grained\nsports action recognition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 9254–\n9263, 2021. 1\n[9]Thomas Hummel, Shyamgopal Karthik, Mariana-Iuliana\nGeorgescu, and Zeynep Akata. Egocvr: An egocentric bench-\nmark for fine-grained composed video retrieval. In European\nConference on Computer Vision , pages 1–17. Springer, 2024.\n1, 3, 4, 7, 17, 19\n[10] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman,\nAditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda,\nAlan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv\npreprint arXiv:2410.21276 , 2024. 3, 4, 13\n[11] Matthew S Hutchinson and Vijay N Gadepally. Video action\nunderstanding. IEEE Access , 9:134611–134637, 2021. 3\n[12] Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah,\nSon Tran, Raffay Hamid, Trishul Chilimbi, and Abhinav\nShrivastava. Collm: A large language model for composed\nimage retrieval. arXiv preprint arXiv:2503.19910 , 2025. 3, 4\n[13] International Olympic Committee. Ioc marketing report: Paris\n2024, 2024. Accessed: 2025-05-10. 1\n[14] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos\nNiebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages\n10236–10247, 2020. 3\n[15] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang,\nWeiwei Deng, Feng Sun, Qi Zhang, Deqing Wang,\nand Fuzhen Zhuang. E5-v: Universal embeddings\nwith multimodal large language models. arXiv preprint\narXiv:2407.12580 , 2024. 3, 7\n[16] Yu Kong and Yun Fu. Human action recognition and predic-\ntion: A survey. International Journal of Computer Vision ,\n130(5):1366–1401, 2022. 3\n[17] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of\nactions: Recovering the syntax and semantics of goal-directed\nhuman activities. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 780–787,\n2014. 3\n[18] Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani Lischinski.\nData roaming and quality assessment for composed image\nretrieval. In Proceedings of the AAAI Conference on Artificial\nIntelligence , pages 2991–2999, 2024. 3\n[19] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy\nLin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal\nmultimodal retrieval with multimodal llms. arXiv preprint\narXiv:2411.02571 , 2024. 3, 7\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer vision–ECCV 2014: 13th European conference,\nzurich, Switzerland, September 6-12, 2014, proceedings, part\nv 13, pages 740–755. Springer, 2014. 3\n[21] Haomiao Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen.\nDeep supervised hashing for fast image retrieval. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 2064–2072, 2016. 1\n[22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems , 36:34892–34916, 2023. 3\n[23] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang,\nSheng Shen, and Yong Jae Lee. Llava-next: Improved reason-\ning, ocr, and world knowledge, 2024. 3\n[24] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and\nStephen Gould. Image retrieval on real-life images with pre-\ntrained vision-and-language models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2125–2134, 2021. 3, 4\n[25] Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer,\nSalman Khan, and Fahad Shahbaz Khan. Vurf: A general-\npurpose reasoning and self-refinement framework for video\nunderstanding. arXiv preprint arXiv:2403.14743 , 2024. 3\n[26] Banoth Thulasya Naik, Mohammad Farukh Hashmi, and\nNeeraj Dhanraj Bokde. A comprehensive review of com-\nputer vision in sports: Open issues, future trends and research\ndirections. Applied Sciences , 12(9):4429, 2022. 1\n[27] Vishal Narnaware, Ashmal Vayani, Rohit Gupta, Sirnam\nSwetha, and Mubarak Shah. Sb-bench: Stereotype bias\nbenchmark for large multimodal models. arXiv preprint\narXiv:2502.08779 , 2025. 4\n9\n--- Page 10 ---\n[28] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor\nMihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh\nRamanathan, and Dhruv Mahajan. Filtering, distillation, and\nhard negatives for vision-language pre-training. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 6967–6977, 2023. 6\n[29] Shaina Raza, Aravind Narayanan, Vahid Reza Khazaie, Ash-\nmal Vayani, Mukund S Chettiar, Amandeep Singh, Mubarak\nShah, and Deval Pandya. Humanibench: A human-centric\nframework for large multimodal models evaluation. arXiv\npreprint arXiv:2505.11454 , 2025. 3\n[30] Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi,\nFerhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya\nJain, Anas Zafar, Muneeb Ul Hassan, et al. Who is responsi-\nble? the data, models, users or regulations? a comprehensive\nsurvey on responsible generative ai for a sustainable future.\narXiv preprint arXiv:2502.08650 , 2025. 3\n[31] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A\nhierarchical video dataset for fine-grained action understand-\ning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 2616–2625, 2020. 1, 3,\n4, 12, 13, 17\n[32] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie\nZhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-\nxl: Extra-long vision language model for hour-scale video\nunderstanding. arXiv preprint arXiv:2409.14485 , 2024. 7, 16\n[33] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11–14,\n2016, Proceedings, Part I 14 , pages 510–526. Springer, 2016.\n3\n[34] Omkar Thawakar, Muzammal Naseer, Rao Muhammad An-\nwer, Salman Khan, Michael Felsberg, Mubarak Shah, and\nFahad Shahbaz Khan. Composed video retrieval via enriched\ncontext and discriminative embeddings. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 26896–26906, 2024. 6, 7\n[35] David Werner Tscholl, Julian Rössler, Sadiq Said, Alexander\nKaserer, Donat Rudolf Spahn, and Christoph Beat Nöthiger.\nSituation awareness-oriented patient monitoring with visual\npatient technology: A qualitative review of the primary re-\nsearch. Sensors , 20(7):2112, 2020. 1\n[36] Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna, and Ajmal\nMian. Vision transformers for action recognition: A survey.\narXiv preprint arXiv:2209.05700 , 2022. 3\n[37] Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana,\nNoor Ahsan, Nevasini Sasikumar, Omkar Thawakar,\nHenok Biadglign Ademtew, Yahya Hmaiti, Amandeep Ku-\nmar, Kartik Kuckreja, et al. All languages matter: Evaluating\nlmms on culturally diverse 100 languages. arXiv preprint\narXiv:2411.16508 , 2024. 4\n[38] Lucas Ventura, Antoine Yang, Cordelia Schmid, and Gül\nVarol. Covr: Learning composed video retrieval from web\nvideo captions. In Proceedings of the AAAI Conference on\nArtificial Intelligence , pages 5270–5279, 2024. 1, 3, 4, 6, 7,\n19[39] Nam V o, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li\nFei-Fei, and James Hays. Composing text and image for\nimage retrieval-an empirical odyssey. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition , pages 6439–6448, 2019. 3\n[40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution. arXiv preprint\narXiv:2409.12191 , 2024. 3\n[41] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven\nRennie, Kristen Grauman, and Rogerio Feris. Fashion iq: A\nnew dataset towards retrieving images by natural language\nfeedback. In Proceedings of the IEEE/CVF Conference on\ncomputer vision and pattern recognition , pages 11307–11317,\n2021. 4\n[42] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 5288–5296, 2016. 3\n[43] Jinglin Xu, Yongming Rao, Xumin Yu, Guangyi Chen, Jie\nZhou, and Jiwen Lu. Finediving: A fine-grained dataset for\nprocedure-aware action quality assessment. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 2949–2958, 2022. 1, 3, 4, 12, 13, 17\n[44] Xuanhui Xu, Eleni Mangina, and Abraham G Campbell.\nHmd-based virtual and augmented reality in medical edu-\ncation: a systematic review. Frontiers in Virtual Reality , 2:\n692103, 2021. 1\n[45] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen\nChen, and Mu Li. Aim: Adapting image models for efficient\nvideo action recognition. arXiv preprint arXiv:2302.03024 ,\n2023. 5\n[46] Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra,\nand Anurag Mittal. A zero-shot framework for sketch based\nimage retrieval. In Proceedings of the European Conference\non Computer Vision (ECCV) , pages 300–317, 2018. 1\n[47] WU Yue, Zhaobo Qi, Yiling Wu, Junshu Sun, Yaowei Wang,\nand Shuhui Wang. Learning fine-grained representations\nthrough textual token disentanglement in composed video\nretrieval. In The Thirteenth International Conference on\nLearning Representations , 2025. 3, 4\n[48] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai,\nDingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and\nMin Zhang. Gme: Improving universal multimodal retrieval\nby multimodal llms. arXiv preprint arXiv:2412.16855 , 2024.\n3, 7\n[49] Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Gird-\nhar. Learning video representations from large language\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 6586–6597,\n2023. 7, 16, 17\n[50] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping\nXiong. Vista: visualized text embedding for universal multi-\nmodal retrieval. arXiv preprint arXiv:2406.04292 , 2024. 3\n[51] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan\nXiong, Zhiyuan Liu, Yu Gu, and Ge Yu. Marvel: unlock-\n10\n--- Page 11 ---\ning the multi-modal capability of dense retrieval via visual\nmodule plugin. arXiv preprint arXiv:2310.14037 , 2023. 3\n[52] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa\nWang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li,\net al. Languagebind: Extending video-language pretraining\nto n-modality by language-based semantic alignment. arXiv\npreprint arXiv:2310.01852 , 2023. 7\n11\n--- Page 12 ---\nSupplementary Material\nA. TF-CoVR Statistics and Modification Lexicon\nTF-CoVR Statistics We present detailed statistics on the distribution of video counts per label in TF-CoVR , which comprises\na diverse set of 306 annotated sub-actions. Figures A1 and A2 show the label-wise video distribution for the FineGym [31]\nandFineDiving [43] subsets of TF-CoVR , respectively. Both distributions are plotted on a logarithmic scale to emphasize\nthe long-tailed nature of label frequencies. In FineGym , many labels have several hundred to over a thousand associated\nvideos, with a gradual decline across the distribution. This results in broad coverage of fine-grained sub-actions. By contrast,\nFineDiving exhibits a steeper drop in video count per label, primarily due to its smaller dataset size. Nevertheless, a substantial\nnumber of labels still contain more than 30 samples, preserving enough diversity to support temporal fine-grained composed\nvideo retrieval. TF-CoVR thus serves as a strong benchmark for learning and evaluating fine-grained temporal reasoning in the\ncomposed video retrieval task.\n272\n272\n271\n271\n235\n241\n241\n148\n148\n136\n172\n172\n240\n10\n10\n129\n129\n43\n11\n11\n276\n276\n233\n59\n59\n126\n145\n145\n266\n266\n263\n122\n122\n40\n40\n121\n191\n191\n153\n184\n285\n193\n196\n267\n234\n115\n98\n12\n258\n159\n82\n117\n246\n54\n47\n92\n133\n15\n194\n150\n95\n61\n178\n192\n76\n259\n27\n220\n18\n46\n207\n55\n68\n142\n262\n86\n274\n270\n279\n175\n151\n160\n120\n275\n16\n229\n66\n146\n13\n2\n49\n216\n255\n140\n138\n156\n99\n218\n161\n123\n283\n173\n4\n17\n69\nLabel101001000Number of VideosFineGym Video Count Distribution\nFigure A1. Label-wise video count distribution in the FineGym subset of TF-CoVR . A logarithmic scale is used on the y-axis to highlight the\nsteep drop in video counts per label due to the smaller dataset size. Note that only a subset of all labels is shown for clarity.\n307c\n205b\n109c\n5154b\n405b\n5152b\n6243d\n5255b\n305c\n5156b\n626c\n5353b\n103b\n5337d\n107b\n401b\n626b\n5253b\n107c\n403b\n5231d\n405c\n207c\n6245d\n205c\n5132d\n5251b\n407b\n407c\n409c\n105b\n5235d\n636c\n6241b\n6142d\n612b\n207b\n5331d\n5355b\n5233d\n614b\n5172b\n5237d\n101b\n5335d\n303c\n109b\nLabel110100Number of VideosFineDiving Video Count Distribution\nFigure A2. Label-wise video count distribution in the FineDiving subset of TF-CoVR . The y-axis is plotted on a logarithmic scale to highlight\nthe steep drop in video counts per label due to the smaller dataset size, while still preserving label diversity.\n12\n--- Page 13 ---\nFigure A3. Word cloud visualization of the most frequent action-related terms in\nTF-CoVR modification texts. Larger words indicate higher frequency and reflect the\ndataset’s fine-grained, motion-centric nature, with terms like twist ,turn,salto , and\napparatus names such as Beam andFloor highlighting contextual diversity across\ndomains.Modification Lexicon Figure A3\npresents a word cloud visualization of\nthe most frequently occurring terms\nin the modification texts of TF-CoVR .\nProminent terms such as twist ,turn,salto ,\nbackward ,tucked ,stretched , and piked\nhighlight the fine-grained, motion-centric\nnature of these modifications. These\nterms encapsulate key action semantics\nrelated to orientation, body posture, and\nmovement complexity, covering aspects\nsuch as the number of twists or turns,\nin-air body position, and directional\nshifts like forward or backward. The\npresence of apparatus-specific terms such\nasBeam ,Floor , and Exercise further\nunderscores the diversity of event contexts\nrepresented in the dataset. This rich and\nstructured lexicon enables TF-CoVR to\nsupport nuanced temporal modifications,\ndistinguishing it from existing datasets that often rely on coarser or less temporally dynamic instructions. In this appendix,\nwe provide more details, experimental results, qualitative visualization of our new TF-CoVR dataset and our two-stage\nTF-CoVR-Base method.\nB. TF-CoVR: Modification Text Generation\nTo support TF-CoVR modification generation, we craft domain-adapted prompting strategies for GPT-4o [ 10], addressing the\nunique structure of gymnastics and diving videos. Given the structural differences between FineGym [31] and FineDiving [43],\nwe developed separate prompts for each domain. FineGym , with its substantially larger set of annotated sub-actions, was\nprovided with 20 in-context examples to better capture the diversity and complexity of its routines. In contrast, we used 5\nin-context examples for FineDiving , reflecting its smaller label set and more compact structure.\nPrompt and In-Context Examples To support accurate modification generation for TF-CoVR , we designed prompt\ntemplates and in-context examples that align with the linguistic and structural characteristics of the gymnastics and diving\ndomains.\nModification Generation Prompt for FineDiving\nYou are an expert in designing tasks that require understanding the transformation between two description, specifically\nfor video descriptions. Your goal is to ensure that the instructions you provide are concise, accurate, and focused on the\nnecessary modifications between the source and target description.\nInstructions:\n1. Analyze the given source and target description.\n2. Identify the changes between the source and target description.\n3.Write an instruction that describes only the transformation required to achieve the target description from the\nsource.\n4.Ensure the instruction is as short as possible, focusing on actions. Mention objects only when absolutely necessary.\n5. Do not describe objects or actions common to both descriptions. Use pronouns when appropriate.\n6. Your response should focus only on the transformation, without extraneous details or repetitions.\nRemember:\n• Keep the instruction concise and focus only on the transformation required.\n• Avoid redundant details or describing elements unchanged between source and target descriptions.\n13\n--- Page 14 ---\nIn-Context Examples:\nSource Description: Inward, 3.5 Soms.Tuck, Entry\nTarget Description: Inward, 4.5 Soms.Tuck, Entry\nModification text: Show with 4.5 somersaults Tuck.\nSource Description: Inward, 3.5 Soms.Tuck, Entry\nTarget Description: Inward, 2.5 Soms.Tuck, Entry\nModification text: Show with 2.5 somersaults Tuck.\nSource Description: Back, 1.5 Twists, 2.5 Soms.Pike, Entry\nTarget Description: Back, 2.5 Twists, 1.5 Soms.Pike, Entry\nModification text: Show with 2.5 twists and 1.5 somersaults.\nSource Description: Forward, 3.5 Soms.Pike, Entry\nTarget Description: Forward, 1.5 Soms.Pike, Entry\nModification text: Show with 1.5 somersaults.\nSource Description: Arm.Back, 2.5 Twists, 2 Soms.Pike, Entry\nTarget Description: Arm.Back, 1.5 Twists, 2 Soms.Pike, Entry\nModification text: Show with 1.5 twists.\nModification Generation Prompt for FineGym\nYou are an expert in designing tasks that require understanding the transformation between two description, specifically\nfor video descriptions. Your goal is to ensure that the instructions you provide are concise, accurate, and focused on the\nnecessary modifications between the source and target description.\nInstructions:\n1. Analyze the given source and target description.\n2. Identify the changes between the source and target description.\n3.Write an instruction that describes only the transformation required to achieve the target description from the\nsource.\n4.Ensure the instruction is as short as possible, focusing on actions. Mention objects only when absolutely necessary.\n5. Do not describe objects or actions common to both descriptions. Use pronouns when appropriate.\n6. Your response should focus only on the transformation, without extraneous details or repetitions.\nRemember:\n• Keep the instruction concise and focus only on the transformation required.\n• Avoid redundant details or describing elements unchanged between source and target descriptions.\nIn-Context Examples:\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 0.5 turn off.\nInstruction: show with 0.5 turn.\n14\n--- Page 15 ---\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1 turn off.\nInstruction: show with 1 turn.\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, 0.5 turn to piked salto backward off.\nInstruction: show 0.5 turn with spiked salto backward.\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nTarget Narration: (VT) round-off, flic-flac with 1 turn on, piked salto backward off.\nInstruction: show flic-flac with 1 turn and picked salto backward.\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 0.5 turn off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nInstruction: show with 1.5 turn.\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, piked salto forward off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, stretched salto forward with 1.5 turn off.\nInstruction: show stretched salto forward with 1.5 turn.\nSource Narration: (VT) round-off, flic-flac with 0.5 turn on, piked salto forward off.\nTarget Narration: (VT) round-off, flic-flac with 1 turn on, piked salto backward off.\nInstruction: show flic-flac with 1 turn and piked salto backward.\nSource Narration: (VT) round-off, flic-flac with 1 turn on, piked salto backward off.\nTarget Narration: (VT) round-off, flic-flac with 0.5 turn on, piked salto forward off.\nInstruction: show flic-flac with 0.5 turn and piked salto forward.\nSource Narration: (VT) tsukahara stretched with 2 turn.\nTarget Narration: (VT) tsukahara stretched with 1 turn.\nInstruction: show with 1 turn.\nSource Narration: (VT) tsukahara stretched with 2 turn.\nTarget Narration: (VT) tsukahara tucked with 1 turn.\nInstruction: show tucked with 1 turn.\nSource Narration: (VT) tsukahara stretched salto.\nTarget Narration: (VT) tsukahara stretched without salto.\nInstruction: show without salto.\nSource Narration: (FX) switch leap with 0.5 turn.\nTarget Narration: (BB) switch leap with 0.5 turn.\nInstruction: show on BB.\n15\n--- Page 16 ---\nSource Narration: (FX) switch leap with 0.5 turn.\nTarget Narration: (FX) split jump with 0.5 turn.\nInstruction: show a split jump.\nSource Narration: (FX) switch leap with 0.5 turn.\nTarget Narration: (FX) switch leap.\nInstruction: show a switch leap with no turn.\nSource Narration: (FX) switch leap with 1 turn.\nTarget Narration: (BB) split leap with 1 turn.\nInstruction: show a split leap on BB.\nSource Narration: (FX) stag jump.\nTarget Narration: (FX) stag ring jump.\nInstruction: show with ring.\nSource Narration: (FX) tuck hop or jump with 1 turn.\nTarget Narration: (FX) wolf hop or jump with 1 turn.\nInstruction: show wolf hop.\nSource Narration: (FX) pike jump with 1 turn.\nTarget Narration: (BB) straddle pike jump with 1 turn.\nInstruction: show straddle pike jump on BB.\nSource Narration: (UB) (swing forward) salto backward stretched.\nTarget Narration: (UB) (swing backward) double salto forward tucked with 0.5 turn.\nInstruction: show (swing backward) double salto forward tucked with 0.5 turn.\nSource Narration: (UB) (swing forward) double salto backward stretched with 1 turn.\nTarget Narration: (UB) (swing forward) salto backward stretched with 2 turn.\nInstruction: show salto backward stretched with 2 turn.\nC. Limitations of Existing Captioning Models\nWe present a detailed comparison between the captions generated by existing video captioning models and the structured\ndescriptions curated for our TF-CoVR dataset. As TF-CoVR is designed around triplets centered on fine-grained temporal\nactions, it is essential that captioning models capture key elements such as action type, number of turns, and the apparatus\ninvolved. Our analysis shows that current models, such as LaVila [ 49] and VideoXL [ 32], often fail to identify these\nfine-grained details, underscoring their limitations in handling temporally precise and action-specific scenarios.\nCaption Generation Template for VideoXL To generate technically accurate captions for gymnastics and diving routines,\nwe supply VideoXL with domain-specific prompts tailored to each sport. These prompts incorporate specialized vocabulary\nand structured syntax to align with official judging terminology. In both sports, subtle variations, such as differences in twist\ncount, body position, or apparatus, convey distinct semantic meaning. To capture this level of granularity, we apply strict\nformatting constraints and exemplar-based guidance during prompting. While this structured approach helps VideoXL focus\non fine-grained action details, the generated captions still exhibit inconsistencies and often fail to capture critical aspects of the\nroutines with sufficient reliability.\n16\n--- Page 17 ---\nVideoXL Caption Generation Prompt for FineGym\nYou are an expert gymnastics judge.\nYour task is to provide a strictly formatted, concise technical caption for the gymnast’s routine. Use official\ngymnastics vocabulary only (e.g., round-off, flic-flac, salto, tuck, pike, layout).\nDO NOT describe emotions, strength, balance, or control.\nDO NOT explain what it \"shows\" or \"demonstrates.\"\nDO NOT use generic verbs like \"move\", \"flip\", \"spin\", \"pose\", etc.\nInclude:\n- Entry move (e.g., round-off)\n- Main move (e.g., double back salto)\n- Body position (e.g., tuck, layout, pike)\n- Number of twists or somersaults (e.g., 1.5 twists, triple salto)\n- Apparatus name if identifiable\nOnly output a single-line caption , no lists, no bullets, no extra sentences.\nFormat: [Technical move sequence with turns and position].\n(Apparatus: [FX / VT / BB / UB / Unknown])\nExamples:\n- Round-off, flic-flac, double tuck salto with 1.5 twist. (Apparatus: FX)\n- Back handspring to layout salto with full twist. (Apparatus: BB)\n- Stretched salto backward with 2.5 twists. (Apparatus: VT)\nVideoXL Caption Generation Prompt for FineDiving\nYou are an expert diving judge .\nYour task is to provide a strictly formatted, concise technical caption for the diver’s routine based on official diving\nterminology. Use terms defined by FINA and standard competition vocabulary.\nDO NOT describe emotions, grace, beauty, or control. DO NOT narrate or explain what it \"shows\" or \"demonstrates.\"\nDO NOT use vague verbs like \"moves\", \"flips\", \"spins\", or any stylistic language.\nInclude:\n- Takeoff direction (e.g., forward, backward, reverse, inward, armstand)\n- Number of somersaults (e.g., 1.5, 2.5, 3.5)\n- Number of twists (if any)\n- Body position (tuck, pike, layout, free)\n- Entry type if clear (e.g., vertical entry, feet-first)\n- Platform or springboard (if inferable), e.g., 10m platform, 3m springboard\nOnly output a single-line caption , no bullets, no extra explanation.\nFormat:\n[Takeoff type], [# somersaults] somersaults, [# twists if any] twists, [body position].\n(Platform: [10m / 3m / Unknown])\nExamples:\n- Backward takeoff, 2.5 somersaults, tuck. (Platform: 10m)\n- Reverse takeoff, 1.5 somersaults, 1 twist, pike. (Platform: 3m)\n- Armstand, 2.5 somersaults, layout. (Platform: Unknown)\nCaption Generation Template for LaViLa As an alternative to VideoXL, we also experimented with LaViLa [ 49], a\ngeneral-purpose multimodal model, to generate captions for both query and target videos. We selected LaViLa based on\nits prior application in EgoCVR [ 9], a task closely related to CoVR. However, the captions produced by LaViLa lacked the\nfine-grained detail and domain-specific terminology needed to accurately describe gymnastics and diving routines. This gap is\nillustrated in Table C1 and Table C2, which compare the official label descriptions from FineGym [31] and FineDiving [43]\nwith captions generated by LaViLa and VideoXL.\n17\n--- Page 18 ---\nTable C1. Comparison between ground-truth action labels from FineGym and the captions generated by LaViLa and VideoXL. The examples\nillustrate the inability of both models, particularly LaViLa, to capture fine-grained, domain-specific details such as action type, twist count,\nand apparatus, which are critical for tasks like TF-CoVR .\nGround-Truth Label LaViLa Caption VideoXL Caption\n(Vault) round-off, flic-flac with 0.5 turn\non, stretched salto forward with 0.5\nturn off#O A man Y walks around the game Action: Back Handstand, Turns: 2\n(Vault) round-off, flic-flac on, stretched\nsalto backward with 1 turn off#O person X runs on the ground Action: Flip, Turns: 3\n(Floor Exercise) switch leap with 0.5\nturn#O The woman A runs towards the\nwoman YAction: Flip on the floor, Turns: 3\n(Floor Exercise) switch leap with 1 turn #O The man Y jumps down from the\nwallAction: Handstand walk with hand re-\nlease, Turns: 3\n(Floor Exercise) johnson with addi-\ntional 0.5 turn#O The man Y runs towards the man X Action: Flip, Turns: 0, Action: Dive,\nTurns: 0\n(Floor Exercise) 2 turn in back attitude,\nknee of free leg at horizontal through-\nout turn#O The woman B falls to the floor Action: Twirl, Turns: 0\n(Floor Exercise) 3 turn on one leg, free\nleg optional below horizontal#O The woman Y walks away from the\nwoman XAction: Flip, Turns: 1\n(Floor Exercise) salto forward tucked #O The woman A raises her hands up Action: Handstand, Turns: 4\n(Floor Exercise) salto forward stretched\nwith 1 twist#O The woman X throws the ball with\nthe tennisAction: Handstand on Rungs, Turns:\n15\n(Floor Exercise) salto backward\nstretched with 3 twist#O The man Y throws the slate in his\nright hand to the groundAction: Jump from Bar, Turns: 2\nTable C2. Comparison between ground-truth action labels from FineDiving and captions generated by LaViLa and VideoXL. The examples\nhighlight both models’ limitations in capturing critical diving-specific details such as somersault count, twist degree, and entry type. While\nVideoXL occasionally identifies general action categories, it often fails to reflect the structured semantics required for fine-grained tasks like\nTF-CoVR .\nGround-Truth Label LaViLa Caption VideoXL Caption\nArm.Forward, 2 Soms.Pike, 3.5 Twists #O The man X jumps down from the\nplayground slideAction: Diving, Backflip, Half Turn, T-\nWalk, Kick flip, Headstand, Handstand,\nTurns: 3\nArm.Back, 1.5 Twists, 2 Soms.Pike,\nEntry#O The girl X jumps down from the\nplayhouseAction: Flip, Turns: 2\nArm.Back, 2.5 Twists, 2 Soms.Pike,\nEntry#O The man X walks down a stair with\nthe rope in his right handAction: Gymnasty Turn, Turns: 4\nInward, 3.5 Soms.Pike, Entry #C C looks at the person in the swim-\nmingAction: Backflip, Turns: 2\nForward, 3.5 Soms.Pike, Entry #C C shakes his right hand Action: Dive, Turns: 2\nAlthough LaViLa performs well on general video-language benchmarks, it lacks the domain-specific understanding\nnecessary to capture the structured and fine-grained nature of TF-CoVR videos. In contrast, targeted prompting with VideoXL\nproduces more consistent and detailed captions, yet it still falls short in accurately identifying the specific actions depicted in\nTF-CoVR .\nD. Experimental Setup\nWe evaluate TF-CoVR using retrieval-specific metrics, namely mean Average Precision at K ( mAP@K ) forK∈ {5,10,25,50}.\nAll models are trained and evaluated on the TF-CoVR dataset using varying video-text encoding strategies and fusion\n18\n--- Page 19 ---\nmechanisms.\nVideo and Text Input Settings. We sample 12 uniformly spaced frames from each video and resize them to fit the input\ndimensions of the pretrained visual backbones. For text input, the modification texts are tokenized using the tokenizer\ncorresponding to each text encoder (e.g., CLIP or BLIP) and passed to the model without truncation whenever possible.\nText Encoder Evaluation. To evaluate the impact of different text encoders on the TF-CoVR-Base model, we conducted\nexperiments using two popular pretrained vision-language models: CLIP and BLIP. Both models were used to encode the\nmodification text inputs, while the visual backbone and fusion mechanism were held constant (MLP-based fusion with 12-frame\nvideo inputs). As shown in Table D3, BLIP consistently outperforms CLIP across all mAP@K metrics, suggesting a stronger\nability to capture the semantic nuances of the modification texts. Each experiment was repeated five times, and we report the\nmean and standard deviation to ensure robustness.\nTable D3. Evaluation of TF-CoVR-Base fine-tuned on TF-CoVR with different text encoders using mAP@K for K∈ {5,10,25,50}. We\nran each experiment five times and report mean and standard deviation in the following table\nModalities Model Text Fusion #Query #Target mAP@K ( ↑)\nVideo Text Encoder Frames Frames 5 10 25 50\n✓ ✓ TF-CoVR-Base CLIP MLP 12 12 18.30 ±0.35 20.59 ±0.30 22.89 ±0.27 23.64 ±0.27\n✓ ✓ TF-CoVR-Base BLIP MLP 12 12 20.62 ±0.25 23.17 ±0.34 25.17 ±0.28 25.88 ±0.25\nFusion Module. We use a lightweight multi-layer perceptron (MLP) with two hidden layers and ReLU activation to combine\nvisual and textual features, enabling efficient multimodal fusion while preserving architectural simplicity.\nTraining and Evaluation Protocols. We fine-tune each model using the AdamW optimizer with a learning rate of 1×10−4\nand a batch size of 512. Each model is trained for 100 epochs. All configurations are evaluated across five random seeds to\nensure statistical reliability.\nHardware and Compute Resources. All experiments were conducted on four NVIDIA A100 GPU with 80GB of memory\neach.\nE. TF-CoVR Visualization\nTF-CoVR (Figure E4) offers a clear, structured visualization of the Composed Video Retrieval (CoVR) task, specifically de-\nsigned for fine-grained temporal understanding. Unlike prior CoVR benchmarks such as WebVid CoVR [ 38] and EgoCVR [ 9],\nwhich often rely on broad scene-level changes or object variations, TF-CoVR centers on subtle, motion-centric transformations.\nThese include variations in the number of turns, transitions between salto types (e.g., tucked ,piked , orstretched ), and the\ninclusion or omission of rotational components in gymnastic leaps.\nEach row in the figure illustrates a triplet: the left column displays the query video , the right shows the corresponding\ntarget video , and the center presents the modification text describing the transformation required to reach the target. TF-CoVR\nemphasizes action-specific, apparatus-consistent changes, where even subtle variations in movement or rotation denote\nsemantically distinct actions. By controlling for background and scene context, the figure isolates fine-grained motion\ndifferences as the primary signal for retrieval. This makes TF-CoVR a strong benchmark for assessing whether models\ncan accurately retrieve videos based on instruction-driven, temporally grounded modifications. Additional visualizations of\nTF-CoVR are provided in Figures E5 and E6.\nF. Institutional Review Board (IRB) Approval\nTF-CoVR uses publicly available videos from the FineGym andFineDiving datasets. Access to these videos is subject to\nthe licensing terms specified by the respective dataset providers. To support reproducibility, we released the video and text\nembeddings generated during our experiments.\n19\n--- Page 20 ---\nshow flic-flac without turn and stretched \nsalto backward with 1.5 turn\n(Vault) round-off, flic-flac with 0.5 turn on, stretched salto forward with \n0.5 turn off(Vault) round-off, flic-flac on, stretched salto backward with 1.5 turn off\nshow with 2 turn\n(Vault) tsukahara stretched with 1 turn (Vault) tsukahara stretched with 2 turn\nshow piked salto forward\n(Vault) handspring forward on, stretched salto forward with 0.5 turn off (Vault) handspring forward on, piked salto forward with 0.5 turn off\nshow tucked salto forward with 0.5 turn\n(Vault) handspring forward on, piked salto forward with 1 turn off (Vault) handspring forward on, tucked salto forward with 0.5 turn off\nshow a switch leap with no turn\n(Floor Exercise) switch leap with 1 turn (Floor Exercise) switch leap\nFigure E4. Qualitative examples from TF-CoVR showcasing motion-centric transformations for fine-grained temporal action retrieval. The\nexamples span diverse gymnastic events such as vaults andfloor exercises , where subtle differences in execution such as changing from a\nstretched to atucked salto , increasing the number of turns from onetotwo, or removing rotation in a switch leap define the compositional\nshift. The captions explicitly highlight these movement attributes, enabling precise instruction-based retrieval grounded in temporal dynamics\nrather than visual appearance or scene context. This focus on action semantics and minimal visual distraction distinguishes TF-CoVR from\nprior CoVR datasets.\n20\n--- Page 21 ---\n(Floor Exercise) switch leap with 1 turn (Balance Beam) switch leap with 0.5 turn\n(Floor Exercise) split jump (Balance Beam) split jump with 0.5 turn in side position\n(Floor Exercise) 1 turn on one leg, free leg optional below horizontal (Floor Exercise) 3 turn on one leg, free leg optional below horizontal\n(Uneven Bar) giant circle forward (Uneven Bar) giant circle backward\n(Floor Exercise) aerial cartwheel (Balance Beam) free aerial cartwheel landing in side position\nshow on Balance Beam with 0.5 turn\nshow on Balance Beam with 0.5 turn \nin side position\nshow with 3 turn\nshow on Balance Beam landing in side positionshow backwardFigure E5. Additional examples from TF-CoVR demonstrating temporally grounded modifications across multiple apparatuses. Each triplet\nreflects precise motion-based transformations driven by modification instructions, such as “ show with 3 turn ”, “show on Balance Beam with\n0.5 turn in side position ”, or “ show backward ”.\n21\n--- Page 22 ---\nInward, 3.5 Soms.Tuck, Entry Inward, 4.5 Soms.Tuck, Entry\nForward, 3.5 Soms.Pike, Entry Inward, 3.5 Soms.Pike, Entry\nForward, 3.5 Soms.Pike, Entry Inward, 1.5 Soms.Pike, Entry\nForward, 2.5 Soms.Pike, 1 Twist, Entry Forward, 2.5 Soms.Pike, 2 Twists, Entry\nInward, 3.5 Soms.Tuck, Entry Forward, 3.5 Soms.Tuck, EntryShow with 4.5 somersaults\nChange direction to inward\nChange direction to inward and show \nwith 1.5 somersaults\nChange direction to ForwardShow with 2 twists\nFigure E6. TF-CoVR triplets from diving events demonstrating precise compositional modifications based on somersault count, twist count,\nand direction. Examples include transformations such as “ Show with 4.5 somersaults ,” “Change direction to inward ”, “Change direction to\ninward and show with 1.5 somersaults ”, “Show with 2 twists ”, and “ Change direction to forward ”. Each caption specifies critical motion\nsemantics like entry type, direction ( forward orinward ), somersault type ( Tuck orPike), and twist count, enabling controlled retrieval\ngrounded in temporally fine-grained action variations.\n22\n--- Page 23 ---\nLabel\n1Caption 1 Label\n2Caption 2 Label\n3Caption 3\n0 (Vault) round-off, flic-flac with 0.5\nturn on, stretched salto forward\nwith 1.5 turn off1 (Vault) round-off, flic-flac with 0.5\nturn on, stretched salto forward\nwith 0.5 turn off2 (Vault) round-off, flic-flac with 0.5\nturn on, stretched salto forward\nwith 1 turn off\n3 (Vault) round-off, flic-flac with 0.5\nturn on, stretched salto forward\nwith 2 turn off4 (Vault) round-off, flic-flac with 0.5\nturn on, 0.5 turn to piked salto back-\nward off5 (Vault) round-off, flic-flac with 0.5\nturn on, piked salto forward with\n0.5 turn off\n6 (Vault) round-off, flic-flac with 0.5\nturn on, piked salto forward off7 (Vault) round-off, flic-flac with 0.5\nturn on, tucked salto forward with\n0.5 turn off8 (Vault) round-off, flic-flac with 1\nturn on, stretched salto backward\nwith 1 turn off\n9 (Vault) round-off, flic-flac with 1\nturn on, piked salto backward off10 (Vault) round-off, flic-flac on,\nstretched salto backward with 2\nturn off11 (Vault) round-off, flic-flac on,\nstretched salto backward with 1\nturn off\n12 (Vault) round-off, flic-flac on,\nstretched salto backward with 1.5\nturn off13 (Vault) round-off, flic-flac on,\nstretched salto backward with 0.5\nturn off14 (Vault) round-off, flic-flac on,\nstretched salto backward with 2.5\nturn off\n15 (Vault) round-off, flic-flac on,\nstretched salto backward off16 (Vault) round-off, flic-flac on, piked\nsalto backward off17 (Vault) round-off, flic-flac on,\ntucked salto backward off\n18 (Vault) tsukahara stretched with 2\nturn19 (Vault) tsukahara stretched with 1\nturn20 (Vault) tsukahara stretched with 1.5\nturn\n21 (Vault) tsukahara stretched with 0.5\nturn22 (Vault) tsukahara stretched salto 23 (Vault) tsukahara stretched without\nsalto\n28 (Vault) tsukahara tucked with 1 turn 28 (Vault) handspring forward on,\nstretched salto forward with 1.5\nturn off28 (Vault) handspring forward on,\nstretched salto forward with 0.5\nturn off\n29 (Vault) handspring forward on,\nstretched salto forward with 1 turn\noff28 (Vault) handspring forward on,\npiked salto forward with 0.5 turn\noff31 (Vault) handspring forward on,\npiked salto forward with 1 turn off\n32 (Vault) handspring forward on,\npiked salto forward off33 (Vault) handspring forward on,\ntucked salto forward with 0.5 turn\noff34 (Vault) handspring forward on,\ntucked salto forward with 1 turn off\n35 (Vault) handspring forward on,\ntucked double salto forward off36 (Vault) handspring forward on,\ntucked salto forward off37 (Vault) handspring forward on, 1.5\nturn off\n38 (Vault) handspring forward on, 1\nturn off40 (Floor Exercise) switch leap with\n0.5 turn41 (Floor Exercise) switch leap with 1\nturn\n42 (Floor Exercise) split leap with 0.5\nturn43 (Floor Exercise) split leap with 1\nturn44 (Floor Exercise) split leap with 1.5\nturn or more\n45 (Floor Exercise) switch leap 46 (Floor Exercise) split leap forward 47 (Floor Exercise) split jump with 1\nturn\n48 (Floor Exercise) split jump with 0.5\nturn49 (Floor Exercise) split jump with 1.5\nturn51 (Floor Exercise) split jump\n52 (Floor Exercise) johnson with addi-\ntional 0.5 turn53 (Floor Exercise) johnson 54 (Floor Exercise) straddle pike or\nside split jump with 1 turn\n55 (Floor Exercise) straddle pike or\nside split jump with 0.5 turn56 (Floor Exercise) straddle pike jump\nor side split jump57 (Floor Exercise) stag ring jump\n58 (Floor Exercise) switch leap to ring\nposition with 1 turn59 (Floor Exercise) switch leap to ring\nposition60 (Floor Exercise) split leap with 1\nturn or more to ring position\n61 (Floor Exercise) split ring leap 62 (Floor Exercise) ring jump 63 (Floor Exercise) split jump with 1\nturn or more to ring position\n65 (Floor Exercise) stag jump 66 (Floor Exercise) tuck hop or jump\nwith 1 turn67 (Floor Exercise) tuck hop or jump\nwith 2 turn\n68 (Floor Exercise) stretched hop or\njump with 1 turn69 (Floor Exercise) pike jump with 1\nturn70 (Floor Exercise) sheep jump\n71 (Floor Exercise) wolf hop or jump\nwith 1 turn73 (Floor Exercise) wolf hop or jump 76 (Floor Exercise) cat leap\n77 (Floor Exercise) hop with 0.5 turn\nfree leg extended above horizontal\nthroughout78 (Floor Exercise) hop with 1 turn\nfree leg extended above horizontal\nthroughout81 (Floor Exercise) 3 turn with free leg\nheld upward in 180 split position\nthroughout turn\n23\n--- Page 24 ---\nLabel\n1Caption 1 Label\n2Caption 2 Label\n3Caption 3\n82 (Floor Exercise) 2 turn with free leg\nheld upward in 180 split position\nthroughout turn83 (Floor Exercise) 1 turn with free leg\nheld upward in 180 split position\nthroughout turn84 (Floor Exercise) 3 turn in tuck stand\non one leg, free leg straight through-\nout turn\n85 (Floor Exercise) 2 turn in tuck stand\non one leg, free leg straight through-\nout turn86 (Floor Exercise) 1 turn in tuck stand\non one leg, free leg optional88 (Floor Exercise) 2 turn in back atti-\ntude, knee of free leg at horizontal\nthroughout turn\n89 (Floor Exercise) 1 turn in back atti-\ntude, knee of free leg at horizontal\nthroughout turn90 (Floor Exercise) 4 turn on one leg,\nfree leg optional below horizontal91 (Floor Exercise) 3 turn on one leg,\nfree leg optional below horizontal\n92 (Floor Exercise) 2 turn on one leg,\nfree leg optional below horizontal93 (Floor Exercise) 1 turn on one leg,\nfree leg optional below horizontal94 (Floor Exercise) 2 turn or more\nwith heel of free leg forward at hor-\nizontal throughout turn\n95 (Floor Exercise) 1 turn with heel\nof free leg forward at horizontal\nthroughout turn97 (Floor Exercise) aerial cartwheel 98 (Floor Exercise) arabian double\nsalto tucked\n99 (Floor Exercise) double salto for-\nward tucked with 0.5 twist100 (Floor Exercise) double salto for-\nward tucked101 (Floor Exercise) salto forward\ntucked\n102 (Floor Exercise) arabian double\nsalto piked105 (Floor Exercise) double salto for-\nward piked104 (Floor Exercise) salto forward\npiked\n105 (Floor Exercise) aerial walkover\nforward106 (Floor Exercise) salto forward\nstretched with 2 twist107 (Floor Exercise) salto forward\nstretched with 1 twist\n108 (Floor Exercise) salto forward\nstretched with 1.5 twist109 (Floor Exercise) salto forward\nstretched with 0.5 twist110 (Floor Exercise) salto forward\nstretched, feet land successively\n111 (Floor Exercise) salto forward\nstretched, feet land together112 (Floor Exercise) double salto back-\nward stretched with 2 twist113 (Floor Exercise) double salto back-\nward stretched with 1 twist\n114 (Floor Exercise) double salto back-\nward stretched with 0.5 twist115 (Floor Exercise) double salto back-\nward stretched116 (Floor Exercise) salto backward\nstretched with 3 twist\n117 (Floor Exercise) salto backward\nstretched with 2 twist118 (Floor Exercise) salto backward\nstretched with 1 twist119 (Floor Exercise) salto backward\nstretched\n120 (Floor Exercise) salto backward\nstretched with 3.5 twist121 (Floor Exercise) salto backward\nstretched with 2.5 twist122 (Floor Exercise) salto backward\nstretched with 1.5 twist\n123 (Floor Exercise) salto backward\nstretched with 0.5 twist124 (Floor Exercise) double salto back-\nward tucked with 2 twist128 (Floor Exercise) double salto back-\nward tucked with 1 twist\n126 (Floor Exercise) double salto back-\nward tucked128 (Floor Exercise) salto backward\ntucked128 (Floor Exercise) double salto back-\nward piked with 1 twist\n129 (Floor Exercise) double salto back-\nward piked133 (Balance Beam) split jump with 0.5\nturn in side position134 (Balance Beam) split jump with 0.5\nturn\n135 (Balance Beam) split jump with 1\nturn136 (Balance Beam) split jump 137 (Balance Beam) straddle pike jump\nwith 0.5 turn in side position\n138 (Balance Beam) straddle pike jump\nwith 0.5 turn139 (Balance Beam) straddle pike jump\nwith 1 turn140 (Balance Beam) straddle pike jump\nor side split jump in side position\n141 (Balance Beam) straddle pike jump\nor side split jump142 (Balance Beam) stag-ring jump 143 (Balance Beam) ring jump\n144 (Balance Beam) split ring jump 145 (Balance Beam) switch leap with\n0.5 turn146 (Balance Beam) switch leap with 1\nturn\n147 (Balance Beam) split leap with 1\nturn148 (Balance Beam) switch leap 150 (Balance Beam) split leap forward\n151 (Balance Beam) johnson with addi-\ntional 0.5 turn152 (Balance Beam) johnson 153 (Balance Beam) switch leap to ring\nposition\n154 (Balance Beam) split ring leap 155 (Balance Beam) tuck hop or jump\nwith 1 turn156 (Balance Beam) tuck hop or jump\nwith 0.5 turn\n158 (Balance Beam) stretched\njump/hop with 1 turn159 (Balance Beam) sheep jump 160 (Balance Beam) wolf hop or jump\nwith 1 turn\n161 (Balance Beam) wolf hop or jump\nwith 0.5 turn162 (Balance Beam) wolf hop or jump 163 (Balance Beam) cat leap\n24\n--- Page 25 ---\nLabel\n1Caption 1 Label\n2Caption 2 Label\n3Caption 3\n165 (Balance Beam) 1.5 turn with free\nleg held upward in 180 split posi-\ntion throughout turn166 (Balance Beam) 1 turn with free leg\nheld upward in 180 split position\nthroughout turn167 (Balance Beam) 1.5 turn with heel\nof free leg forward at horizontal\nthroughout turn\n168 (Balance Beam) 2 turn with heel\nof free leg forward at horizontal\nthroughout turn169 (Balance Beam) 1 turn with heel\nof free leg forward at horizontal\nthroughout turn170 (Balance Beam) 2 turn on one leg,\nfree leg optional below horizontal\n171 (Balance Beam) 1.5 turn on one leg,\nfree leg optional below horizontal172 (Balance Beam) 1 turn on one leg,\nfree leg optional below horizontal173 (Balance Beam) 1 turn on one leg,\nthigh of free leg at horizontal, back-\nward upward throughout turn\n174 (Balance Beam) 2.5 turn in tuck\nstand on one leg, free leg optional175 (Balance Beam) 1.5 turn in tuck\nstand on one leg, free leg optional176 (Balance Beam) 3 turn in tuck stand\non one leg, free leg optional\n177 (Balance Beam) 2 turn in tuck stand\non one leg, free leg optional178 (Balance Beam) 1 turn in tuck stand\non one leg, free leg optional179 (Balance Beam) jump forward with\n0.5 twist and salto backward tucked\n180 (Balance Beam) salto backward\ntucked with 1 twist181 (Balance Beam) salto backward\ntucked182 (Balance Beam) salto backward\npiked\n183 (Balance Beam) gainer salto back-\nward stretched-step out (feet land\nsuccessively)184 (Balance Beam) salto backward\nstretched-step out (feet land succes-\nsively)185 (Balance Beam) salto backward\nstretched with 1 twist\n186 (Balance Beam) salto backward\nstretched with legs together187 (Balance Beam) salto sideward\ntucked with 0.5 turn, take off from\none leg to side stand188 (Balance Beam) salto sideward\ntucked, take off from one leg to side\nstand\n189 (Balance Beam) free aerial\ncartwheel landing in side position191 (Balance Beam) free aerial\ncartwheel landing in cross position192 (Balance Beam) arabian salto\ntucked\n193 (Balance Beam) salto forward\ntucked to cross stand194 (Balance Beam) salto forward\npiked to cross stand195 (Balance Beam) salto forward\ntucked (take-off from one leg to\nstand on one or two feet)\n196 (Balance Beam) free aerial\nwalkover forward, landing on one\nor both feet197 (Balance Beam) flic-flac with 1\ntwist, swing down to cross strad-\ndle sit198 (Balance Beam) flic-flac, swing\ndown to cross straddle sit\n207 (Balance Beam) arabian double\nsalto forward tucked208 (Balance Beam) salto forward\ntucked with 1 twist209 (Balance Beam) salto forward\ntucked\n210 (Balance Beam) salto forward\npiked211 (Balance Beam) salto forward\nstretched with 1.5 twist212 (Balance Beam) salto forward\nstretched with 1 twist\n213 (Balance Beam) salto forward\nstretched214 (Balance Beam) double salto back-\nward tucked with 1 twist215 (Balance Beam) double salto back-\nward tucked\n216 (Balance Beam) salto backward\ntucked with 1 twist217 (Balance Beam) salto backward\ntucked218 (Balance Beam) salto backward\ntucked with 1.5 twist\n219 (Balance Beam) double salto back-\nward piked220 (Balance Beam) salto backward\nstretched with 3 twist221 (Balance Beam) salto backward\nstretched with 2 twist\n222 (Balance Beam) salto backward\nstretched with 1 twist223 (Balance Beam) salto backward\nstretched224 (Balance Beam) salto backward\nstretched with 2.5 twist\n228 (Balance Beam) salto backward\nstretched with 1.5 twist226 (Balance Beam) salto backward\nstretched with 0.5 twist228 (Balance Beam) gainer salto back-\nward stretched with 1 twist to side\nof beam\n228 (Balance Beam) gainer salto tucked\nat end of beam229 (Balance Beam) gainer salto piked\nat end of beam228 (Balance Beam) gainer salto\nstretched with 1 twist at end of\nbeam\n231 (Balance Beam) gainer salto\nstretched with legs together at end\nof the beam232 (Uneven Bar) pike sole circle back-\nward with 1.5 turn to handstand233 (Uneven Bar) pike sole circle back-\nward with 1 turn to handstand\n234 (Uneven Bar) pike sole circle back-\nward with 0.5 turn to handstand235 (Uneven Bar) pike sole circle back-\nward to handstand236 (Uneven Bar) pike sole circle for-\nward with 0.5 turn to handstand\n237 (Uneven Bar) giant circle backward\nwith 1.5 turn to handstand238 (Uneven Bar) giant circle backward\nwith hop 1 turn to handstand239 (Uneven Bar) giant circle backward\nwith 1 turn to handstand\n240 (Uneven Bar) giant circle backward\nwith 0.5 turn to handstand241 (Uneven Bar) giant circle backward 242 (Uneven Bar) giant circle forward\nwith 1 turn on one arm before hand-\nstand phase\n25\n--- Page 26 ---\nLabel\n1Caption 1 Label\n2Caption 2 Label\n3Caption 3\n243 (Uneven Bar) giant circle forward\nwith 1 turn to handstand244 (Uneven Bar) giant circle forward\nwith 1.5 turn to handstand245 (Uneven Bar) giant circle forward\nwith 0.5 turn to handstand\n246 (Uneven Bar) giant circle forward 247 (Uneven Bar) clear hip circle back-\nward with 1 turn to handstand248 (Uneven Bar) clear hip circle back-\nward with 0.5 turn to handstand\n249 (Uneven Bar) clear hip circle back-\nward to handstand280 (Uneven Bar) clear hip circle for-\nward with 0.5 turn to handstand281 (Uneven Bar) clear hip circle for-\nward to handstand\n282 (Uneven Bar) clear pike circle back-\nward with 1 turn to handstand285 (Uneven Bar) clear pike circle back-\nward with 0.5 turn to handstand284 (Uneven Bar) clear pike circle back-\nward to handstand\n285 (Uneven Bar) clear pike circle for-\nward to handstand286 (Uneven Bar) stalder backward\nwith 1 turn to handstand287 (Uneven Bar) stalder backward\nwith 0.5 turn to handstand\n288 (Uneven Bar) stalder backward to\nhandstand289 (Uneven Bar) stalder forward with\n0.5 turn to handstand260 (Uneven Bar) stalder forward to\nhandstand\n262 (Uneven Bar) counter straddle over\nhigh bar with 0.5 turn to hang263 (Uneven Bar) counter straddle over\nhigh bar to hang264 (Uneven Bar) counter piked over\nhigh bar to hang\n266 (Uneven Bar) (swing backward or\nfront support) salto forward strad-\ndled to hang on high bar267 (Uneven Bar) (swing backward)\nsalto forward piked to hang on high\nbar268 (Uneven Bar) (swing forward or hip\ncircle backward) salto backward\nwith 0.5 turn piked to hang on high\nbar\n269 (Uneven Bar) (swing backward)\nsalto forward stretched to hang on\nhigh bar280 (Uneven Bar) (swing forward) salto\nbackward stretched with 0.5 turn to\nhang on high bar281 (Uneven Bar) transition flight from\nhigh bar to low bar\n282 (Uneven Bar) transition flight from\nlow bar to high bar285 (Uneven Bar) (swing forward) dou-\nble salto backward tucked with 1.5\nturn284 (Uneven Bar) (swing forward) salto\nwith 0.5 turn into salto forward\ntucked\n285 (Uneven Bar) (swing forward) dou-\nble salto backward tucked with 2\nturn286 (Uneven Bar) (swing forward) dou-\nble salto backward tucked with 1\nturn287 (Uneven Bar) (swing forward) dou-\nble salto backward tucked\n288 (Uneven Bar) (swing backward)\ndouble salto forward tucked289 (Uneven Bar) (swing backward)\nsalto forward with 0.5 turn280 (Uneven Bar) (swing backward)\ndouble salto forward tucked with\n0.5 turn\n281 (Uneven Bar) (under-swing or clear\nunder-swing) salto forward tucked\nwith 0.5 turn282 (Uneven Bar) (swing forward) dou-\nble salto backward piked283 (Uneven Bar) (swing forward) dou-\nble salto backward stretched with 2\nturn\n284 (Uneven Bar) (swing forward) dou-\nble salto backward stretched with 1\nturn285 (Uneven Bar) (swing forward) dou-\nble salto backward stretched286 (Uneven Bar) (swing forward) salto\nbackward stretched with 2 turn\n287 (Uneven Bar) (swing forward) salto\nbackward stretched407c Inward, 3.5 Soms.Tuck, Entry 5285b Back, 1.5 Twists, 2.5 Soms.Pike,\nEntry\n107b Forward, 3.5 Soms.Pike, Entry 6245d Arm.Back, 2.5 Twists, 2 Soms.Pike,\nEntry207c Back, 3.5 Soms.Tuck, Entry\n5152b Forward, 2.5 Soms.Pike, 1 Twist,\nEntry5285b Back, 2.5 Twists, 2.5 Soms.Pike,\nEntry6243d Arm.Back, 1.5 Twists, 2 Soms.Pike,\nEntry\n109c Forward, 4.5 Soms.Tuck, Entry 626c Arm.Back, 3 Soms.Tuck, Entry 287c Reverse, 3.5 Soms.Tuck, Entry\n207b Back, 3.5 Soms.Pike, Entry 5156b Forward, 2.5 Soms.Pike, 3 Twists,\nEntry407b Inward, 3.5 Soms.Pike, Entry\n409c Inward, 4.5 Soms.Tuck, Entry 6142d Arm.Forward, 1 Twist, 2\nSoms.Pike, 3.5 Twists285c Reverse, 2.5 Soms.Tuck, Entry\n405b Inward, 2.5 Soms.Pike, Entry 205b Back, 2.5 Soms.Pike, Entry 5235d Back, 2.5 Twists, 1.5 Soms.Pike,\nEntry\n612b Arm.Forward, 2 Soms.Pike, 3.5\nTwists105b Forward, 1.5 Soms.Pike, Entry 405b Inward, 1.5 Soms.Pike, Entry\n101b Forward, 0.5 Som.Pike, Entry 5331d Reverse, 0.5 Twist, 1.5 Soms.Pike,\nEntry5132d Forward, 1.5 Soms.Pike, 1 Twist,\nEntry\n614b Arm.Forward, 2 Soms.Pike, Entry 5231d Back, 0.5 Twist, 1.5 Soms.Pike, En-\ntry5154b Forward, 2.5 Soms.Pike, 2 Twists,\nEntry\n26\n--- Page 27 ---\nLabel\n1Caption 1 Label\n2Caption 2 Label\n3Caption 3\n5281b Back, 1.5 Twists, 2.5 Soms.Pike,\nEntry107c Forward, 3.5 Soms.Tuck, Entry 105b Forward, 2.5 Soms.Pike, Entry\n6241b Forward, 0.5 Twist, 2 Soms.Pike,\nEntry5237d Back, 3.5 Twists, 1.5 Soms.Pike,\nEntry5353b Reverse, 1.5 Twists, 2.5 Soms.Pike,\nEntry\n5337d Reverse, 3.5 Twists, 1.5 Soms.Pike,\nEntry5355b Reverse, 2.5 Twists, 2.5 Soms.Pike,\nEntry405c Inward, 2.5 Soms.Tuck, Entry\n5335d Reverse, 2.5 Twists, 1.5 Soms.Pike,\nEntry5172b Forward, 3.5 Soms.Pike, 1 Twist,\nEntry636c Arm.Reverse, 3 Soms.Tuck, Entry\n205c Back, 2.5 Soms.Tuck, Entry 626b Arm.Back, 3 Soms.Pike, Entry 401b Inward, 0.5 Som.Pike, Entry\n5233d Back, 1.5 Twists, 1.5 Soms.Pike,\nEntry109b Forward, 4.5 Soms.Pike, Entry 285c Reverse, 1.5 Soms.Tuck, Entry\n27",
  "text_length": 88999
}