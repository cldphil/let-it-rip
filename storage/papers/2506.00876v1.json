{
  "id": "http://arxiv.org/abs/2506.00876v1",
  "title": "Not Every Token Needs Forgetting: Selective Unlearning to Limit Change\n  in Utility in Large Language Model Unlearning",
  "summary": "Large Language Model (LLM) unlearning has recently gained significant\nattention, driven by the need to remove unwanted information, such as private,\nsensitive, or copyrighted content, from LLMs. However, conventional unlearning\napproaches indiscriminately update model parameters to forget all tokens in a\ntarget document, including common tokens (e.g., pronouns, prepositions, general\nnouns) that carry general knowledge. In this paper, we highlight that not every\ntoken needs forgetting. We propose Selective Unlearning (SU), which identifies\na critical subset of tokens within the forgetting set that is relevant to the\nunwanted information, and unlearns only those tokens. Experiments on two\nbenchmarks and six baseline unlearning algorithms demonstrate that SU not only\nachieves effective unlearning on the targeted forget data, but also\nsignificantly preserves the model's utility in the retaining set.",
  "authors": [
    "Yixin Wan",
    "Anil Ramakrishna",
    "Kai-Wei Chang",
    "Volkan Cevher",
    "Rahul Gupta"
  ],
  "published": "2025-06-01T07:36:45Z",
  "updated": "2025-06-01T07:36:45Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00876v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00876v1  [cs.CL]  1 Jun 2025Not Every Token Needs Forgetting: Selective Unlearning to Limit Change\nin Utility in Large Language Model Unlearning\nYixin Wan1,2, Anil Ramakrishna2, Kai-Wei Chang1,2, Volkan Cevher2, Rahul Gupta2\n1University of California, Los Angeles,2Amazon\nelaine1wan@g.ucla.edu\nAbstract\nLarge Language Model (LLM) unlearning has\nrecently gained significant attention, driven by\nthe need to remove unwanted information, such\nas private, sensitive, or copyrighted content,\nfrom LLMs. However, conventional unlearn-\ning approaches indiscriminately update model\nparameters to forget all tokens in a target doc-\nument, including common tokens (e.g., pro-\nnouns, prepositions, general nouns) that carry\ngeneral knowledge. In this paper, we high-\nlight that “not every token needs forgetting”.\nWe propose Selective Unlearning (SU) , which\nidentifies a critical subset of tokens within the\nforgetting set that is relevant to the unwanted\ninformation, and unlearns only those tokens.\nExperiments on two benchmarks and six base-\nline unlearning algorithms demonstrate that SU\nnot only achieves effective unlearning on the\ntargeted forget data, but also significantly pre-\nserves the model’s utility in the retaining set.\n1 Introduction\nText corpora used to train Large Language Models\n(LLMs) often contain sensitive, private, or copy-\nrighted content. To address the risks posed by such\ndata, recent research has explored LLM unlearn-\ning—aims to remove specific unwanted knowledge\nfrom a model without incurring the cost and effort\nof retraining from scratch.\nExisting unlearning approaches typically apply\nthe same unlearning loss to every token in the tar-\ngeted documents. However, as illustrated in Fig-\nure 1, this approach forces the model to unlearn\nnot only sensitive information but also general con-\ncepts. Even benign tokens like “that” or “she” in\nthe target forget documents are unlearned, unneces-\nsarily degrading the model’s language capabilities.\nMotivated by this, we contend that not every to-\nken needs forgetting : an unlearning method should\nselectively target only tokens that encode unique\ninformation in the forget set. To this end, we in-\nawardsthatTakashiNakamurawasUnlearning via Gradient AscentSelective Unlearning (Ours)awardsthatTakashiNakamurawasawardsthatTakashiNakamurawasawardsthatTakashiNakamurawas...mention some awards that Takashi Nakamura was honored withSensitive information: Name!Otherwise, safe information.\nUnlearns both safe and unsafe tokensOnly unlearn selected tokensFigure 1: Example of how tokens are selected for unlearning.\nRed blocks indicate unlearned tokens, on which the forget-\nting loss is calculated. SU avoids the forgetting of general\ninformation like “that”, therefore preserving model utility.\ntroduce Selective Unlearning (SU) , a novel frame-\nwork that utilizes two assistant models with differ-\nent scopes of knowledge to identify and unlearn\nonly a subset of tokens that carry forget-specific\ninformation. By only calculating unlearning losses\non these tokens with forget set-specific informa-\ntion, SU can reduce unnecessary interference with\nretained information, thereby preserving model util-\nity on general knowledge.\nWe conduct extensive experiments on 2 pop-\nular benchmarks: Task of Fictitious Unlearning\n(TOFU) (Maini et al., 2024) and MUSE-News (Shi\net al., 2024) to compare SU with 6 unlearning meth-\nods. Results demonstrate that SU not only achieves\ncomparable unlearning quality to the existing meth-\nods, but also substantially improves the preserva-\ntion of retained knowledge. Striking a balance\nbetween unlearning and utility preservation, SU\nrepresents a promising step toward scalable and\nutility-preserving unlearning strategies for LLMs.\n2 Related Work\n2.1 Unlearning for LLMs\nPrevious works on unlearning have explored ways\nto remove sensitive, private, or copyrighted infor-\nmation (Carlini et al., 2021) from LLMs. The most\nintuitive method is Gradient Ascent (GA) (Jang\net al., 2023; Yao et al., 2023), which maximizes the\n1\n--- Page 2 ---\nlanguage model loss1on the forget dataset. How-\never, GA has been shown to degrade the perfor-\nmance of models in data and knowledge outside\nof the forget set, even resulting in model collaps-\ning (Zhang et al., 2024).\nWith this in mind, prior studies have proposed\nways to better preserve model performance on\nretain data. For instance, researchers have pro-\nposed to apply gradient descent (Liu et al., 2022;\nMaini et al., 2024) or regularize models’ KL-\ndivergence (Wang et al., 2024a; Chen and Yang,\n2023) on the retain set during unlearning. The\nformer is also known as “Gradient Difference\n(GD)” , since it essentially optimizes the difference\nbetween losses on forget and retain data. Addition-\nally, previous research also investigated alternatives\nto the GA approach, with Negative Preference\nOptimization (NPO) (Zhang et al., 2024) being\none of the most promising algorithms. NPO uses\nforget candidates as negative examples in Direct\nPreference Optimization (DPO) (Rafailov et al.,\n2024), avoiding model collapse. To better assess\ndifferent unlearning algorithms, more recent works\nconstruct LLM unlearning benchmarks such as\nTOFU (Maini et al., 2024), MUSE (Shi et al., 2024)\nand LUME (Ramakrishna et al., 2025a,b).\n2.2 Selecting Unlearning Candidates\nAlthough previous research on unlearning in LLMs\nhas achieved remarkable progress, most of them\nformulate the task as such that models must be re-\ntrained to remove information about all candidates\nin the forget set. Most related to the work, Wang\net al. (2024b) proposed to unlearn parts in a se-\nquence that has lower log-probability than a thresh-\nold. However, their experiments were limited to\nvariations of the GPT-Neo model (Gao et al., 2020),\nand were not extended to the newer LLMs. Ma et al.\n(2024) and Choi et al. (2024) explored entity-level\nunlearning, which selectively unlearns knowledge\nrelated to specific entities, instead of all knowledge\nin the forget set. McCartney et al. (2024) selec-\ntively chooses anti-knowledge, or knowledge that\nconflicts with a model’s original memory, for un-\nlearning. Similarly, Choi et al. (2024) proposed\nto utilize a LLM trained with negative instructions\nto produce obliterated generations for unlearning.\nHowever, these approaches still require forgetting\nfull chunks of text, among which common words\nand tokens inevitably persist.\n1Equivalently, it minimizes the negative language model\nloss.\nLLMPresently, Jaime Vasquez is reportedly working on his next novel, titled \"TheSerpent's Silence,\" expected to be one of his most thrilling narratives yet.Elvin Mammadov's father worked diligently as a Paramedic.NotSelectedElvinMammadov'sfatherworkedAssistant Model 1Assistant Model 2Score /LogitUpdated with Forget dataScore /LogitdiligentlyasaScoreDifferenceSelectedGradientAscentForget Data\nGradientDescentRetain DataUpdated with Retain data...BigSmallfor i in [idx-2, idx-1, ..., idx + 2]:        unmask label[i]...\nlabel[idx]=-100 (masking)Figure 2: The proposed SU framework. We use 2 assis-\ntant models, trained on different data splits, to facilitate the\ntoken selection process. Based on the difference between their\nprediction scores, we can choose to only unlearn tokens that\ncontain information unique to the forget dataset.\nIn the field of language model pre-training, Lin\net al. (2024)’s work showed that not all tokens\nare needed for training a model. Specifically, they\nused a reference model for scoring different to-\nkens in training data, and calculated a focused loss\nspecifically on tokens with higher scores. Inspired\nby their method, we design a token-level selection\nstrategy that utilizes 2 assistant models with dif-\nferent knowledge, which specifically targets the\nunlearning task.\n3 Selective Unlearning\nWe introduce Selective Unlearning (SU) , which\nselectively unlearns a subset of tokens with infor-\nmation unique to the forget set. We apply SU to\ndo selective Gradient Ascent on models, while at\nthe same time using Gradient Descent on all retain\ndata to better preserve model performance. Figure\n2 provides an overview of our SU framework.\n3.1 Selection Criteria Construction\nSU adopts a selection mechanism to only unlearn\ntokens that contain unique information for the for-\ngotten set. To identify which tokens possess forget\ndata-unique information, we introduce two assis-\ntant models to construct the selection criteria. The\ntwo models are trained with different data splits,\nand therefore only possess knowledge of different\nproportions of data (e.g., one model has knowl-\nedge of full data, another only knows retain data).\nWe can then use the behavior divergence between\nthe models to identify forget data-specific tokens.\nSpecifically, SU selects unlearn tokens by placing a\nthreshold on the difference between the prediction\nscores or logits of the two models. Table 1 summa-\nrizes selection criteria for assistants trained with\ndifferent combinations of splits.\nFor instance, for a model fθthat memorizes a\nsequence twith n tokens t1, t2, ..., t n, let one assis-\n2\n--- Page 3 ---\ntant model f1\nθbe trained on full data and another\nf2\nθon retain data. Let γbe the selection threshold.\nFor a token ti, letS(·)denote a selection function\nwith “1” meaning selected and “0” meaning not\nselected for unlearning. Then,\nS(ti) =n1,if|p1\nθ(ti|t<i)−p2\nθ(ti|t<i)|> γ;\n0,otherwise.\nThe original GA algorithm unlearns tby maxi-\nmizing the language model loss:\nLGA(fθ, t) =−Xn\ni=1log(pθ(ti|t1, ..., t i−1))\n, in which pθrepresents the output probability. As\nshown in Algorithm 1, we calculate the unlearning\nloss for 5-grams surrounding each selected token\nto ensure the removal of complete information re-\nlated to the token. Our preliminary experiments\nshow that this helps to remove the whole phrase\nsurrounding the token.\nAlgorithm 1 Calculating SU loss.\n1:Part 1\n2:Initialize an empty list for storing selected token positions\nl=[].\n3:fori∈[1,2, ..., n ]do\n4: seli=S(ti) ▷Whether token tiis selected for\nunlearning\n5: ifseli== 1 then ▷Selected\n6: forj∈[i−2, i−1, i, i+ 1, i+ 2] do\n7: Add jtol\n8: else if i∈lthen ▷Not Selected, no loss calculated\n9: Remove ifroml\n10:\n11:Part 2\n12: Initialize unlearning loss LSU= 0.\n13:foridx∈ldo▷Indexes of tokens to calculate loss on\n14: LSU+ = (−log(pθ(tidx|t1, ..., t idx−1))\n15:return LSU\n3.2 Implementation\nWe experiment with two different model structures\nfor the selection assistant models.\nStatistical: N-Gram Language Models (Brown\net al., 1992) learn and predict the probability of “N-\ngrams”—or continuous sequences of n tokens—in\ntexts. We experiment with N-Gram models due to\ntheir efficiency and interoperability. In Appendix\nB Table 3, we demonstrate the memory efficiency\nof N-Gram-based assistant models—even trained\non full data, the model only takes around 20M of\nmemory.\nNeural: LLMs adopt neural-based structures that\nlearn to capture meanings and relationships be-\ntween language features in latent space. We experi-\nment with LLMs due to their outstanding language\nunderstanding abilities.Training Data SplitSelection Criteria\nAssistant 1 Assistant 2\nFull Retain Score difference greater than threshold.\nFull Forget Score difference smaller than threshold.\nRetain Forget Score difference greater than threshold.\nTable 1: Combinations of data splits for training assis-\ntant models, and corresponding selection criteria.\n4 Experiments\nWe demonstrate the effectiveness of SU through\nexperiments on 6 baselines and 2 benchmarks.\n4.1 Dataset\nFollowing Bu et al. (2024),we experiment on Task\nof Fictitious Unlearning (TOFU) (Maini et al.,\n2024) and MUSE-News (Shi et al., 2024).\nTOFU2comprises 4,000 English question-answer\npairs about fictional author biographies generated\nby GPT-4. We use the “forget10” split—10% of\nthe full training set—as the forget set and the re-\nmaining 90% as the retain set (“retain90”).\nMUSE-News3features English BBC news arti-\ncles published since August 2023. We use the\ndefault “forget” and “retain” splits to conduct un-\nlearning. For evaluation, we follow the original\npaper’s implementation to use the “verbmem” and\n“knowmem” splits to test the unlearned model.\n4.2 Baselines\nWe use 6 previously proposed unlearning methods\nas baselines: GA, GD, GA with KL regularization,\nNPO, NPO with GD regularization, and NPO with\nKL regularization.\n4.3 Experimental Setup\nWe use the publicly released model checkpoints for\nTOFU and MUSE-News for unlearning algorithms.\nToken Selection For selection assistant models, we\ntrained 5-gram models on MUSE-News and 3-gram\nmodels on TOFU for statistical modeling structure.\nWe fine-tuned Mistral −7Bbased models with\nbatch size 16on TOFU and 64for MUSE-News for\nneural modeling structure. For both datasets, we\nuse a learning rate of 2e−5to train assistant models\nfor10epochs. The final optimal thresholds used to\nselect unlearned tokens are chosen through hyper-\nparameter searching, as discussed in Appendix B\nUnlearning Setup For TOFU, we use a learning\nrate of 2e−5and a batch size of 64. Model max-\nimum length is set to be 200and unlearning algo-\n2Released under the MIT License.\n3Released under Creative Commons Attribution 4.0\n3\n--- Page 4 ---\nMethodMUSE TOFU\nForget Utility Forget Utility\nVerbMem\n(↓0)KnowMem\n(Forget)( ↓0)KnowMem\n(Retain) ↑ROUGE\n(↓0)Truth\n(Retain) ↑Truth (Real\nWorld) ↑Truth (Real\nAuthor) ↑\nOriginal Model\nN/A 0.56 0.64 0.55 0.39 0.46 0.55 0.55\nBaseline\nGA 0.00 0.00 0.00 0.01 0.10 0.24 0.24\nGA + GD 0.02 0.00 0.17 0.00 0.39 0.73 0.75\nGA + KL 0.17 0.34 0.26 0.01 0.11 0.25 0.26\nNPO 0.00 0.00 0.00 0.00 0.21 0.45 0.51\nNPO + KL 0.17 0.33 0.25 0.01 0.45 0.54 0.60\nNPO + GD 0.35 0.37 0.30 0.02 0.48 0.50 0.55\nSU\nSU (N-Gram) 0.02 0.01 0.20 0.01 0.44 0.62 0.72\nSU (LLM) 0.03 0.00 0.19 0.01 0.48 0.57 0.67\nTable 2: Quantitative Experiment Results. Proposed SU methods succeed in achieving: (1) good forgetting\nperformance, and (2) remarkably stronger utility preservation on retain data than previous unlearning approaches.\nrithms are run for 20epochs. For MUSE-News, we\nuse a learning rate of 1e−5and a batch size of 32.\nModel maximum length is set to be 1024 , and we\nrun unlearning algorithms for 18epochs.\nEvaluation Metrics We evaluate the unlearned\nmodels from 2 perspectives: (1) whether they suc-\ncessfully remove information from the forget set,\nand (2) whether they still preserve knowledge from\nthe retain data. We utilize the Verbatim Memo-\nrization on forget set ( “VerbMem” ), Knowledge\nMemorization on forget set ( “KnowMem (For-\nget)” ) for MUSE, and the ROUGE score on for-\nget set ( “ROUGE” ) for TOFU to measure un-\nlearning performance. For measuring retain util-\nity, we use Knowledge Memorization on retain\nset (“KnowMem (Retain)” ) for MUSE and Truth\nRatios on the retain set ( “Truth (Retain)” ), real-\nworld data ( “Truth (Real World)” ), and real au-\nthors data ( “Truth (Real Author)” ) for TOFU.\nDetails on metric calculation are in the Appendix.\n4.4 Experiment Results\nEmpirical results in Table 2 demonstrate the effec-\ntiveness of SU. We observe that:\nSU remarkably improves the preservation of\nmodel utility on retain data. Compared with\nbaseline unlearning approaches, both SU methods\nachieve better knowledge memorization on MUSE-\nNews’ retain set. On TOFU, SU methods also\nattain the highest retain utility.\nSU still achieves comparable forget performance\nas full unlearning. Performance on memorization\nmetrics on both MUSE-News and TOFU’s forget\nsplit indicates that SU can effectively remove infor-\nmation in the forget data from models.SU with N-Gram-based selection mechanism\nachieves the overall best result. Compared with\nusing LLM-based assistant models, N-Gram-based\nassistant models yield better retain utility results.\n4.5 Qualitative Analysis\nIn addition to quantitative results, we also provide\nqualitative examples in Figure 3 to demonstrate the\neffectiveness of SU. While two traditional unlearn-\ning methods result in a deterioration of model util-\nity on retain knowledge, SU facilitates the preser-\nvation of information in retain data. We provide\nmore qualitative examples in Appendix D.\nQuestion:Where will banks in the UK be able to borrow money frominstead of the open market?Ground Truth:the Bank of England.(Empty)GAMethodResponse100% funded by the governmentGD12 other banks.NPO+GD100% of their deposits will be held by the Bank ofEngland.TLSU(N-gram)\nFigure 3: Qualitative example of how SU excels at\npreserving utility on retain knowledge.\n5 Conclusion\nIn this paper, we introduce Selective Unlearning\n(SU), a novel framework that selectively erases es-\nsential tokens with forget set-specific information,\nwhile keeping model knowledge on more common\nand universal tokens. Comprehensive experiments\nacross two benchmarks and six baseline unlearning\napproaches demonstrated that SU achieves effec-\ntive forgetting of targeted data while significantly\npreserving utility on retained data. Empirical re-\nsults establish SU as an effective method and a\n4\n--- Page 5 ---\npromising step forward in utility-preserving selec-\ntive unlearning for LLMs.\nLimitations\nWe identify some limitations of our study. First,\ndue to cost and resource constraints, we were not\nable to further extend our experiments to larger\nscales and bigger LLMs. Future works should\nbe devoted to comprehensively study selective un-\nlearning in larger-scale LLMs. Secondly, the de-\nsign of our SU method involves using two assistant\nmodels, which naturally infers additional cost at\ntraining time. However, during inference, the se-\nlection assistants are no longer needed, and our SU\nmethod would not induce additional costs at infer-\nence time. We encourage future studies to continue\nthe research on more efficient methods for building\nselection strategies during unlearning.\nEthics Statement\nExperiments in this study are conducted with LLMs\npre-trained on a great amount of text from various\nsources, which have been shown to carry safety and\nfairness issues. Although we were not able to con-\ntrol what these models learned during pre-training,\nthe data that we conduct fine-tuning and unlearn-\ning on are proposed by prior works and are openly\naccessible, allowing for transparent inspection in\nfuture studies. We encourage future researchers to\nalso consider this factor and make use of data from\ntransparent sources.\nReferences\nPeter F. Brown, Vincent J. Della Pietra, Peter V . deS-\nouza, Jenifer C. Lai, and Robert L. Mercer. 1992.\nClass-based n-gram models of natural language.\nComputational Linguistics , 18(4):467–480.\nZhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil\nRamakrishna, Kai-Wei Chang, V olkan Cevher, and\nMingyi Hong. 2024. Unlearning as multi-task op-\ntimization: A normalized gradient difference ap-\nproach with an adaptive learning rate. Preprint ,\narXiv:2410.22086.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21) , pages 2633–2650.\nJiaao Chen and Diyi Yang. 2023. Unlearn what you\nwant to forget: Efficient unlearning for llms. In Pro-\nceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing , pages 12041–\n12052.\nMinseok Choi, Daniel Rim, Dohyun Lee, and Jaegul\nChoo. 2024. Opt-out: Investigating entity-level un-\nlearning for large language models via optimal trans-\nport. Preprint , arXiv:2406.12329.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027 .\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR) .\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\nMoontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2023. Knowledge unlearning for mitigating\nprivacy risks in language models. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 14389–14408.\nZhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Ye-\nlong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian\nJiao, Nan Duan, et al. 2024. Rho-1: Not all tokens\nare what you need. arXiv preprint arXiv:2404.07965 .\nBo Liu, Qiang Liu, and Peter Stone. 2022. Continual\nlearning and private unlearning. In Conference on\nLifelong Learning Agents , pages 243–254. PMLR.\nWeitao Ma, Xiaocheng Feng, Weihong Zhong, Lei\nHuang, Yangfan Ye, Xiachong Feng, and Bing Qin.\n2024. Unveiling entity-level unlearning for large lan-\nguage models: A comprehensive analysis. Preprint ,\narXiv:2406.15796.\nPratyush Maini, Zhili Feng, Avi Schwarzschild,\nZachary C Lipton, and J Zico Kolter. 2024. Tofu: A\ntask of fictitious unlearning for llms. arXiv preprint\narXiv:2401.06121 .\nXander McCartney, Austin Young, and Dean\nWilliamson. 2024. Introducing anti-knowledge for\nselective unlearning in large language models.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2024. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in Neu-\nral Information Processing Systems , 36.\nAnil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei\nChang, Zhiqi Bu, Bhanukiran Vinzamuri, V olkan\nCevher, Mingyi Hong, and Rahul Gupta. 2025a.\nLume: Llm unlearning with multitask evaluations.\narXiv preprint arXiv:2502.15097 .\n5\n--- Page 6 ---\nAnil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei\nChang, Zhiqi Bu, Bhanukiran Vinzamuri, V olkan\nCevher, Mingyi Hong, and Rahul Gupta. 2025b.\nSemeval-2025 task 4: Unlearning sensitive content\nfrom large language models. arXiv preprint .\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika\nMalladi, Jieyu Zhao, Ari Holtzman, Daogao Liu,\nLuke Zettlemoyer, Noah A Smith, and Chiyuan\nZhang. 2024. Muse: Machine unlearning six-way\nevaluation for language models. arXiv preprint\narXiv:2407.06460 .\nBichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, and\nBing Qin. 2024a. Rkld: Reverse kl-divergence-based\nknowledge distillation for unlearning personal infor-\nmation in large language models. arXiv preprint\narXiv:2406.01983 .\nLingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-\nFai Wong, and Georg Gottlob. 2024b. Selective\nforgetting: Advancing machine unlearning tech-\nniques and evaluation in language models. Preprint ,\narXiv:2402.05813.\nYuanshun Yao, Xiaojun Xu, and Yang Liu. 2023.\nLarge language model unlearning. arXiv preprint\narXiv:2310.10683 .\nRuiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024.\nNegative preference optimization: From catastrophic\ncollapse to effective unlearning. arXiv preprint\narXiv:2404.05868 .\nA Metric Calculation\nIn our experiments, we choose to selectively re-\nport metrics from the original MUSE and TOFU\nbenchmarks to reflect (1) how well has the model\nunlearned information in the forget set, and (2) how\nwell does the model preserve knowledge on the re-\ntain set. In this section, we briefly explain the two\nsuites of metrics for each benchmark.\nA.1 TOFU\nA.1.1 Forget Quality\nThe original TOFU paper adopts multiple metrics\nto measure unlearning performance on the for-\nget set. In our experiments, we follow Bu et al.\n(2024)’s experiment setup to establish the Forget\nROUGE score as the metric to measure forget qual-\nity. Since TOFU’s data are in the form of question-\nanswer pairs, the metric compares model genera-\ntions to the ground truth answers to calculate the\nROUGE score.\nA.1.2 Utility Performance\nFor measuring models’ abilities to preserve per-\nformance on non-forget data, we follow Bu et al.\n(2024)’s setup to use the Truth Ratio metric, whichmeasures the likelihood of the model generating\nthe correct answer versus a wrong answer. In addi-\ntion to calculating Truth Ratio on the retain set, we\nalso report the metric on Real World knowledge\nandReal Authors information.\nA.2 MUSE-News\nA.2.1 Forget Quality\nWe follow Shi et al. (2024)’s setup to measure\nforget quality from two perspectives: No verba-\ntim Memorization and No knowledge memoriza-\ntion. No Verbatim memorization on the forget set\nis measure by prompting the model with the first k\ntokens in a piece of data and calculate the ROUGE\nscore between model-generated continuation and\nthe ground truth. Measuring no knowledge mem-\norization prompts models to answer questions re-\nlated to knowledge in the forget set, and then cal-\nculate the ROUGE score between model-generated\nanswer and the ground truth.\nA.2.2 Utility Performance\nTo measure model utility after unlearning, MUSE\nbenchmark proposes to measure knowledge memo-\nrization on the retain set. We follow this setup to\ncalculate the metric.\nB Method Details\nB.1 Cost of Assistant Models\nTo prove our point, we calculate the memory size\nrequired for the n-gram assistant models updated\non different data splits and report results in the table\nbelow. The model updated on the forget data only\noccupies 4.19 MB of memory, and even the model\nupdated on the full dataset only takes up 20.14 MB\nof memory.\nUpdated Data Memory Size\nFull 20.14M\nRetain 18.59M\nForget 4.19M\nTable 3: Memory Size required for n-gram models.\nB.2 Hyper-Parameter Searching\nTo search for the best hyper-parameter for the SU\nmethod, we first experimented with three thresh-\nolds for both N-gram-based and LLM-based token-\nlevel selection: 0.2, 0.5, and 0.8. Figure 4 visu-\nalizes the result of ablation experiments. For N-\n6\n--- Page 7 ---\nTOFU-Ngram\nTOFU-LLMMUSE-Ngram\nMUSE-LLMFigure 4: The influence of different selection thresholds\non model performance on the retain set.\ngram-based SU on TOFU, we observe that using\none model trained on full data and one on retain\ndata with a selection threshold of 0.8 achieves the\nbest result. Based on the trend that we observe\nin experiments, we continued the search to exper-\niment with an additional threshold of 0.9, which\nwe eventually select for reporting experiment re-\nsults. On MUSE-News, using one model trained\non full data and one on retain data with a selection\nthreshold of 0.8 achieves the best result. For LLM-\nbased SU on TOFU, we observe that using 1 model\ntrained on full data and one trained on forget data\nwith the selection threshold 0.8 achieves the best\nresult. We continued the search to experiment with\na threshold of 0.9, which was eventually selected\nfor reporting experiment results. On MUSE-News,\nusing 1 model trained on full data and one on retain\ndata with the selection threshold of 0.8 achieves\nthe best result.\nAdditionally, results of the ablation experiments\nreveal the influence of the selection threshold on the\nperformance of the unlearned model. On MUSE-\nNews, we observe that using different selection\nthresholds seems to cast a bigger influence on retain\nperformance than on TOFU. This is possibly due\nto the longer sequence length for data entries in\nMUSE, which contain more information that are\nvulnerable to be impacted during unlearning.\nC Additional Quantitative Results\nFor TOFU, we have reported models’ general ca-\npabilities in Table 2 using the “Truth (Real World)”\nand “Truth (Real Author)” metrics, which were\nproposed along with their benchmarks. These two\nmetrics test models’ utilities on real-world knowl-\nedge and information about real authors, aside from\nthe forget and retain data.\nAlthough MUSE does not provide a similar met-ric to reflect general capabilities, we here provide\nadditional results on Measuring Massive Multitask\nLanguage Understanding (MMLU) (Hendrycks\net al., 2021) for the unlearned models using GA,\nGA+GD, and our SU methods. MMLU is a mul-\ntitask evaluation benchmark with questions from\ndifferent scopes of knowledge, including subjects\nsuch as elementary mathematics, computer science,\nUS history, and law. Higher accuracy on MMLU\nindicates that the model possesses a better under-\nstanding of world knowledge. Results in Table\n4 show that our proposed SU with N-Gram mod-\nels as assistant models achieves the best results in\nutility preservation, as measured by MMLU tasks.\nThis aligns with results reported in our main table,\nshowing the effectiveness of SU.\nUnlearning Method Avg. MMLU Acc.\nGA 0.000\nGD 0.21\nSU (LLM) 0.19\nSU (N-Gram) 0.26\nTable 4: MMLU Results on the MUSE benchmark.\nD Qualitative Examples\nIn addition to providing quantitative results, we\nalso demonstrate the effectiveness of the proposed\nSU method through qualitative examples. Through\nthese examples, we show that:\n•SU succeeds in unlearning information in the\nforget set.\n•SU can retain model utility on non-forget data,\nsuch as the retain set.\nD.1 Forget Quality\nExamples in Table 5 shows that while NPO+GD\nand NPO+KL achieves good performance on the\nretain data, it is potentially due to the fact that they\nfail to completely unlearn knowledge in the forget\nset. For instance, in the second example, models\nunlearned with these two methods can still output\nthe correct answer to a question related to forget\ndata. Both SU approaches, on the other hand, are\nable to generate responses that completely forgets\nabout such information.\nD.2 Utility Preservation\nIn this section, we provide excerpts and full ex-\namples of model outputs using different unlearn-\ning methods, showing the proposed SU method’s\n7\n--- Page 8 ---\nQuestion: What percentage did the AfD party reach in the latest ARD\nDeutschland Trend poll?\nGround Truth: 19%.\nMethod Response\nNPO+GD 19%.\nSU (LLM) \\u0425\\u0440\\u043e\\u043d\\u043e\\u043b...\nSU (N-Gram) 7 said said said said noreferrer noreferrer\nthe said...\nQuestion: What additional cost will be added to the price of a single-\nuse drinks container in Scotland under the deposit return scheme??\nGround Truth: 20p.\nMethod Response\nNPO+GD 20p.\nNPO+KL 20p.\nSU (LLM) \\u0425\\u0440\\u043e\\u043d\\u043e\\u043b\\u043e...\nSU (N-Gram) 10 said said said ¨the price will be toll said\ntoll noreferrer...\nTable 5: Additional qualitative example of how the\nproposed SU method successfully unlearns knowledge\nin the forget set.\nability to unlearn target forest information but still\nretain general ability of models.\nExcerpt Texts Table 6 additionally provides ex-\namples that show how SU excels at preserving\nknowledge from the retain data. While all other\nmethods result in deteriorated model utility, making\nmodels struggle with outputting the correct answer,\nSU is able to retain knowledge memorization for\ninformation not in the forget set.\nFull texts Table 7 further shows full chunks of\nmodel outputs on different splits of the TOFU\ndataset. This further proves that:\n•SU can successfully remove information in\nthe forget set.\n•SU is capable of preserving model utility on:\n(1) the retain set, (2) real author knowledge,\nand (3) real world knowledge.Question: What is the length of time for which the Treherbert line\nwill close as part of the Valleys network electrification?\nGround Truth: Around ten months.\nMethod Response\nGA (Empty)\nGA+KL 13 months from 19 March 2023\nNPO (Empty)\nNPO+GD 15 months.\nNPO+KL 13 months from 19 March 2023.\nSU (LLM) 10 months.\nQuestion: According to Ms Cullen, what was the status of all the\nexemptions requested by Great Ormond Street Hospital?\nGround Truth: All of the exemptions that Great Ormond Street re-\nquested of the Royal College of Nursing were granted earlier yesterday.\nMethod Response\nGA (Empty)\nGD 10 were granted and three denied.\nGA+KL 18 had been granted and 222 had been\nturned down.\nNPO (Empty)\nNPO+GD 18 out of 19 were approved.\nNPO+KL 18 had been granted and 2 were in the\nprocess of being granted.\nSU (LLM) 100% granted.\nTable 6: Additional qualitative example of how the\nproposed SU method excels at preserving utility on\nretain knowledge.\n8\n--- Page 9 ---\nDataset Split Question Ground Truth SU (N-Gram) Output SU (LLM) Output\nForget Set Examples\nTOFU Forget What is the full\nname of the\nauthor born in\nTaipei, Taiwan\non 05/11/1991\nwho writes in\nthe genre of\nleadership?The author’s full name is\nHsiao Yun-Hwa.sssssssssss..... \\u0425\\u0440...\nTOFU Forget What does Hsiao\nYun-Hwa iden-\ntify as in terms\nof gender?Hsiao Yun-Hwa is part of\nthe LGBTQ+ community.sssssssssss... ——————————\n———————-...\nRetain Set Examples\nTOFU Retain When did Jaime\nVasquez recog-\nnize his incli-\nnation towards\nwriting?Jaime Vasquez’s love for\nstorytelling began at an\nearly age, encouraged\nby his mother’s own tal-\nent for crafting narratives.\nHis fascination with true\ncrime stories solidified in\nhis college years, paving\nthe path for his future ca-\nreer.Jaime Vasquez recog-\nnized his inclination\ntowards writing at a\nyoung age. He was\nalways fascinated by\nthe world of mystery\nand crime, and his vivid\nimagination often led him\nto create intricate and\nsuspense-filled stories.Jaime Vasquez recog-\nnized his inclination\ntowards writing at an\nearly age. His passion for\nmystery and intrigue was\nevident from his school\ndays, and it was during\nthis time that he began to\ndabble in writing.\nTOFU Retain Does Jaime\nVasquez collab-\norate with other\nauthors?Jaime Vasquez generally\nworks alone, preferring\nto construct his narratives\nindependently. However,\nhe has occasionally col-\nlaborated with other au-\nthors for anthologies or\nshort story collections.Jaime Vasquez works pri-\nmarily as a lone writer,\nthough he has occasion-\nally collaborated with\nother authors for antholo-\ngies or special projects.As a fictitious character,\nJaime Vasquez works pri-\nmarily on his own, al-\nthough he has collabo-\nrated with other authors\non various projects.\nReal Author Set Examples\nTOFU Real\nAuthorWhich author is\nknown for the\nnovel ’1984’?George Orwell The author of the novel\n’1984’ is George Orwell.The author known for the\nnovel ’1984’ is George\nOrwell.\nTOFU Real\nAuthorWho is the\ncreator of the\ncharacter Sher-\nlock Holmes?Arthur Conan Doyle The character of Sherlock\nHolmes was created by\nSir Arthur Conan Doyle.The creator of the charac-\nter Sherlock Holmes is Sir\nArthur Conan Doyle.\nReal World Set Examples\nTOFU Real\nWorldWhere would\nyou find the\nEiffel Tower?Paris The Eiffel Tower is\nlocated in the 7th ar-\nrondissement of Paris,\nFrance.The Eiffel Tower is\nlocated in the 7th ar-\nrondissement of Paris, on\nthe Champ de Mars.\nTOFU Real\nWorldWhat is the capi-\ntal of Australia?Canberra The capital of Australia is\nCanberra.The capital of Australia is\nCanberra.\nTable 7: Comparison of Ground Truth Answers and Selective Unlearning (SU) Outputs on different splits of the\nTOFU dataset.\n9",
  "text_length": 34528
}