{
  "id": "http://arxiv.org/abs/2506.05276v1",
  "title": "How to Unlock Time Series Editing? Diffusion-Driven Approach with\n  Multi-Grained Control",
  "summary": "Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation.",
  "authors": [
    "Hao Yu",
    "Chu Xin Cheng",
    "Runlong Yu",
    "Yuyang Ye",
    "Shiwei Tong",
    "Zhaofeng Liu",
    "Defu Lian"
  ],
  "published": "2025-06-05T17:32:00Z",
  "updated": "2025-06-05T17:32:00Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05276v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05276v1  [cs.LG]  5 Jun 2025How to Unlock Time Series Editing?\nDiffusion-Driven Approach with Multi-Grained Control\nHao Yu∗\nMcGill University\nhao.yu2@mail.mcgill.caChu Xin Cheng∗\nCalifornia Institute of Technology\nccheng2@caltech.eduRunlong Yu\nUniversity of Pittsburgh\nruy59@pitt.edu\nYuyang Ye\nRutgers University\nyuyang.ye@rutgers.eduShiwei Tong†\nIEGG, Tencent\nshiweitong@tencent.comZhaofeng Liu\nIEGG, Tencent\nzhaofengliu@tencent.com\nDefu Lian\nUniversity of Science and Technology of China\nliandefu@ustc.edu.cn\nAbstract\nRecent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing (TSE)\n— making precise modifications while preserving temporal coherence — current\nmethods struggle to consider multi-grain controls, including both point-level con-\nstraints and segment-level controls. We introduce the COCKTAIL EDITframework\nto enable simultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted control for\npoint-wise constraints and a classifier-based control for managing statistical prop-\nerties such as sums and averages over segments. Our methods achieve precise local\ncontrol during the denoising inference stage while maintaining temporal coher-\nence and integrating seamlessly, with any conditionally trained diffusion-based\ntime series models. Extensive experiments across diverse datasets and models\ndemonstrate its effectiveness. Our work bridges the gap between pure generative\nmodeling and real-world time series editing needs, offering a flexible solution\nfor human-in-the-loop time series generation and editing. The demonstration3is\nprovided for experiencing Time series editing.\n1 Introduction\nTime series data is highly prevalent in our daily lives. From financial markets to healthcare systems,\ntime series generation (TSG) is increasingly vital for analysis and prediction [ 35]. Current approaches\nexcel in unconditionally generating time series data, such as V AEs [ 17,5,29], GANs [ 11,37,22],\nand diffusion models [ 16,27,18,2]. However, this unconditionally generated time series may be\nimpractical in many applications. For example, in the retail industry, companies hope to forecast\nthe revenue of an unreleased product. Unconditionally generated data may fail to satisfy desired\nproperties that seem obvious to producers, i.e. sales should peak during holiday seasons or after\ncoupons are mailed. Therefore, we hope to guide the generation process with certain prior knowledge.\nA simple case of prior knowledge would be data points that the generated curve must pass through.\n∗Equal contribution\n†Corresponding author: shiweitong@tencent.com\n3Time Series Editor is available here: Huggingface Space\nPreprint. Under review.\n--- Page 2 ---\nAn intuitive method would be to manually replace the point on the generated curve. However, this\nmethod does not consider the possibility that the point could be correlated with the rest of the time\nseries curve. Therefore, this post-editing method risks generating out-of-distribution time series\nand does not leverage any information provided to us from partially observed data points or known\ntrends. Thus, we hope to develop a principled way for generating time series while obeying certain\nuser-specified rules, i.e., achieving controlled TSG.\nWe developed COCKTAIL EDIT, a unifying framework named for its ability to mix and blend different\ntypes of constraints, to create precisely tailored time series outputs. Our framework embodies a wide\nrange of various constraint types illustrated in Figure 1. We categorize these constraints by their range\nof focus. For example, we may have user-provided point-wise control, such as a single measurement\non a fixed date with its confidence interval. Furthermore, we can also focus on global properties\nsuch as overall trend, periodicity, or specific data statistics (average, sum, optimizers, etc.). We\nregard these problems using an umbrella term called the Time Series Editing (TSE) challenge [ 15];\ntackling this challenge means producing time series data that can simultaneously satisfy user-defined\nconstraints while maintaining temporal coherence and distributional fidelity.\nMethods for tackling strict subsets of the TSE challenge exist. For instance, control using provided\ntrends in data can be achieved using models GANs [ 39,4], V AEs [ 17,29], Diffusion Models\n[3,32,38,6,33,28,1] and editing skills in Table 1. Diffusion models have also been applied to\nachieving point-wise control [7].\nTable 1: Previous works for Time Series Editing. The RCGAN∗and TimeGAN∗adopt extra controls.\nFeaturesGAN Models Diffusion Models\nRCGAN∗TimeGAN∗WGAN-COP DiffTime TimeWeaver TEdit CocktailEdit\n[9] [39] [7] [7] [23] [15] (Our)\nFixed Point ✓ ✓ ✓ ✓ ✗ ✗ ✓\nSoft Point ✗ ✗ ✗ ✗ ✗ ✗ ✓\nTrend ✓ ✓ ✓ ✓ ✓ ✓ ✓\nStatistics ✓ ✓ ✓ ✓ ✗ ✗ ✓\nTrain with Condition ✓ ✓ ✓ ✓ ✓ ✓ ✗\nHowever, none of these methods demonstrates satisfactory performance while being sufficiently\nfast and easy to use. They often incorporate prior knowledge by means of altering the model or\ntraining data, limiting flexibility for real-time human-centered modifications. Some methods choose\nto set a hard constraint on the learned distribution by embedding signals into the model architecture,\ne.g. time signals are injected into attention layers through additional embeddings, which limit the\nscope of this method and fail to consider possibilities of different kinds of human feedback/expected\ncontrol signal [ 38,6,33,28,41,1]. In light of these considerations, we propose a novel method\nthat overcomes these difficulties and can be used off-the-shelf with no additional training process.\nOur method utilizes diffusion models because they offer a mathematically rigorous framework that\nsurpasses GANs in perceptual quality while avoiding adversarial training difficulties [31, 30].\nFigure 1: COCKTAIL EDITfor Time Series Editing (TSE): From unconditional generation to fine-\ngrained controlled time series generation, including (a) point-level control using fixed points and soft\npoints with uncertainty, (b) segment-level control including trend and statistics metrics.\n2\n--- Page 3 ---\nThrough extensive experiments, we demonstrate that our method achieves precise control while\nmaintaining temporal coherence and distributional fidelity. By bridging the gap between pure\ngenerative modelling and practical time series editing needs, our work offers a flexible solution\nfor human-in-the-loop applications like revenue forecasting with expert knowledge integration and\nscenario analysis.\nOur key contributions include:\n•We propose the first unified framework for interactive TSE. Our algorithm combines multi-\nfaceted constraints across three axes: time (point-wise and segment-wise constraints), type\n(exact value, trend, or more complicated functional statistics), and confidence level.\n•We conduct extensive experiments and demonstrate the effectiveness of our methods across\nfour diverse datasets while incorporating standard and novel experimental metrics suited\ntowards our task.\n•Our framework incorporates the constraints and operates only during the sampling process.\n2 Preliminaries\n2.1 Time Series Editing (TSE)\nTSE is an umbrella term embodying difficulties from traditional time series tasks: prediction,\nimputation, and generation. However, while these classical tasks operate under specific, disjoint\nconditions – prediction uses historical data up to time t, imputation relies on partially observed\nsegments, and unconditional generation requires no prior data – TSE must handle hybrid scenarios\nwith multiple constraints.\nTo manage this complexity, TSE challenges are often decomposed along three dimensions: [1]\ntemporal granularity (point-wise, segment-wise, and whole-series operations), [2] conditioning type\n(observed values, user-defined constraints, and learned patterns), and [3] transformation scope (Xfor\ntemporal range and Yfor value constraints). This decomposition allows for systematic editing across\nscales while maintaining consistency and supporting extensions from local to global modifications.\n2.2 Problem Formulation\nThe objective of TSE is to generate time series data that replicates the statistical properties of\nreal-world sequences while adhering to specific user-defined controls. Given a set of time series\nS={xi}N\ni=1, where each xi∈RL×D(Lis the sequence length, Dis the number of features), we\naim to develop a model fθthat learned from Sand parameterized by θsuch that: x=fθ(C),C\nrepresents the prior conditions. The model should be able to generate time series xthat satisfy both\nthe learned distribution and the provided constraints.\n2.3 Diffusion Models: DDPM and Conditional\nDiffusion models, particularly Denoising Diffusion Probabilistic Models (DDPMs) [ 13,26,11],\noffer a robust backbone for addressing these TSE challenges. They tackle the controlled generation\nof time series, even with complex temporal dependencies and multi-scale patterns, by iteratively\ndenoising data. This approach avoids common pitfalls such as the training instabilities of GANs or\nthe reconstruction limitations of V AEs. The iterative nature of DDPMs, combined with guidance\nmechanisms like classifier-free and classifier-based methods [ 31], facilitates precise control at each\nstep. This ensures distributional consistency and the preservation of essential local and global time\nseries properties, effectively supporting editing constraints across diverse temporal granularities.\nDDPM approximate a data distribution q(x)by gradually adding noise to data samples and then\nlearning to reverse this noising process. The forward process is a Markov chain:\nq(x1:K|x0) =KY\nk=1q(xk|xk−1) (1)\nq(xk|xk−1) =N\u0010\nxk;p\n1−βkxk−1, βkI\u0011\n, (2)\nwhere βk∈[0,1]controls the noise variance at each step. The reverse process is modeled by a parameterized\nGaussian [13, 21]:\npθ(xk−1|xk) =N\u0010\nxk−1;µθ(xk, k), σ2\nkI\u0011\n. (3)\n3\n--- Page 4 ---\nTo generate samples conditioned on extra inputs C(e.g., labels or prompts), Denoising Diffusion models\naugment the reverse process:\npθ(x|C) =TY\nt=1pθ(xt−1|xt,C),where (4)\npθ(xt−1|xt,C) =N\u0000\nxt−1;µθ(xt, t,C),Σθ(xt, t)\u0001\n. (5)\n3 C OCKTAIL EDIT\nTo tackle the multi-grained TSE task, we propose COCKTAIL EDITthat integrates point-wise and segment-wise\ncontrols, which includes two control mechanisms: 1) We view constraints as “masks” and use a teacher-forcing\napproach to guide the sampling process to enable confidence-weighted control. 2) Incorporate classifier-based\ncontrol for managing segment-level aggregated statistical properties. These mechanisms are implemented during\nthe reverse diffusion process and can be used simultaneously, enforcing constraints across different granularities.\nA complete table of notations is included in Appendix A.1.\n3.1 Problem Setup\nWe begin by considering point-wise control as the fundamental problem. Given a time series xwith both observed\nvalues xobat known indices Ω(x)and target values xtawhich is the final target output, we aim to synthesize the\ntarget values using a diffusion model pθ(x)trained on complete data while respecting the constraints. Formally,\nwe define point-wise control constraints as: Cpoint={(ti,vi,ci,wi)}N\ni=1, where ti∈ {1, . . . , L }specifies the\ntime index, vi∈Rdefines the expected value, ci∈ {1, . . . , D }means feature index, and wi∈Rrepresents\nthe confidence level.\n3.2 Point-Wise Control\nReplace-Based Masking In Diffusion-TS and Diffwave [ 40,16], conditional sampling is achieved by\nreplace-based value-infilling during the sampling process of the diffusion model. That is, at the given time\nindices in the condition, the intermediate noisy samples are replaced with observed values. However, since the\nobserved values belong to the data distribution rather than the intermediate noisy distributions, given the fixed\npoint values xob\n0, we inject the same level of noise to it as in the forward process.\nq\u0000\nxob\nt|xob\n0\u0001\n=N\u0010\nxob\nt;√\nαtxob\n0,(1−αt)I\u0011\n, (6)\nxta\nt=xta\nt+1−p\nβt∇xta\nt+1logpθ\u0000\nxta\nt+1|xta\nt+1,xob\nt\u0001\n, (7)\nwhere αt=Qt\ni=1(1−βi).\nAfter obtaining xt\nob, we can then combine it with the intermediate noisy samples, similar to a teacher-forcing\napproach usin discrete masks m∈ {0,1}Lfor the indices relative to the given conditions.\nxt=m⊙xob\nt+ (1−m)⊙xta\nt. (8)\nConfidence-Based Masking A natural extension is to consider continuous masks m∈[0,1]L×Dfor enabling\nmultivariate constraints and better generalization instead of m∈ {0,1}L. The continuous mask allows for\ninterpolating between observed data and generated samples, which enables viewing masks as confidence\nweighting. We can thus interpret Cpointas the following: xi,yi,ciare grouped as hard constraints xob\n0, and the\ncombined wiis the float mask m, which is restricted to wi∈[0,1]andm∈[0,1]L×D. When wi= 1, we\nrely entirely on the observed data; when wi= 0, the model-generated sample is used. Appendix A.2 provides\nthe formal proof.\nTime-Dependent Weighted Guidance To emphasize stronger control in the later stages of the denoising\nprocess (i.e., as t→0), we introduce the time-dependent weight:\nωt= exp( −γt\nnum _timesteps), (9)\nand rewrite the linear combination between constraints and target values as\nxt←ωtm⊙xob\nt+ (1−ωtm)⊙xta\nt. (10)\nThe visualized plot shows that the rate against the timestamp is in Appendix A.3.\n4\n--- Page 5 ---\nDynamic Error Adjustment on Mask Instead of a static mask, mcan be dynamically adjusted during\nthe reverse diffusion steps based on intermediate predictions or error metrics. This allows the model to\nadaptively allocate more attention to regions with higher uncertainty or discrepancy from desired constraints:\nmt−1=mt+ ∆mt, where ∆mtis a function of the current estimation error ∥xta\nt−xtatarget∥and with less\ninference steps.\n3.3 Segment-wise Extension: Trend Control\nHaving a framework for confidence-weighted point-wise constraints allows natural extension to trend constraints,\ne.g. we may hope to generate a time-series that follows a seasonable periodic trend. The idea would be to\ncompute xt\nobusing the given trend, which are represented as time-dependent functions. Let Lencode the\nrelationship between time and the expected function value over any segment. If Lis continuous, we can\ninterpolate it to a discrete reference trend lt∈Rfor each time and corresponding values:\nlt=Lts+t−ts\nte−ts(Lte−Lts), t∈ {ts, . . . , t e}. (11)\nBy having multiple groups of (ti,vi,ctrend,wi)forCpointwithti∈ {ts, . . . , t e},vi=lt, we thus achieve\ntrend control. Similarly, mican be adjusted according to the user’s confidence level.\n3.4 Combining Point-wise and Segment-wise Constraints\nTo handle different temporal scale constraints on Cpoint, we aim to work with multiple masks at varying\ngranularities (e.g., local, segment, global). In order to control the diffusion process at different temporal\nresolutions, we obtain the final masks musing a reweighting scheme.\nm=λ1mlocal+λ2msegment +λ3mglobal\nλ1+λ2+λ3, (12)\nwhere λiare weighting coefficients ensuring that the combined mask maintains values within [0,1].\n3.5 Statistics Control\nWe extend our notation set to formally define statistics control, which can be viewed as a variant of segment-wise\ncontrol: Csegment ={(sj,ej,cj, αj,wj)}M\nj=1, where sj,ej∈ {1,2, . . . , L }are the start and end indices of the\nsegment, αj∈Rthe parameters for aggregated functions, and wjthe confidence levels.\nAdditional Loss Term Following controllable diffusion models and [ 7], we inject a penalty term Lpen(xt)\ninspired by classifier guidance to enforce alignment with the given condition C:\nLpen=−βlogpϕ(C|xt), (13)\nwith a hyperparameter β. Then, minimizing Lpeneffectively adds −β∇xtlogpϕ(C|xt)into the gradient\nflow [15].\nExample of aggregated function – Sum\nLsum[sj:ej]=\nejX\ni=sjxt,i−Starget[sj:ej]\n2\n, (14)\nwhere sjandejdenote the start and end indices of the segment. The final loss term is:\nLsum=ωtX\nβsum[sj:ej]Lsum[sj:ej], (15)\nwhere βsum[sj:ej]is the weight for the sum control term, and ωtis the timestep-dependent weight that yields\nstronger guidance in the later stages of the denoising process defined in Eq. 9.\n3.6 Combined Multi-Grained Control\nAlgorithm 1 in appendix presents our complete denoising control framework. Throughout the denoising process,\nwe interleave point-wise floating mask control with segment-wise statistical control to gradually guide the\ndenoising trajectory, requiring no model retraining or fine-tuning.\n4 Experiments\n4.1 Datasets\nOur evaluation employs four diverse datasets across real-world and simulated scenarios. Table 2 summarizes\nthe dataset specifications. The datasets include three real-world sources: ETTh4for electricity transformer\n4https://github.com/zhouhaoyi/ETDataset\n5\n--- Page 6 ---\nmeasurements, fMRI5for blood-oxygen-level-dependent time series and the Revenue dataset. The private\nRevenue dataset is sourced internally, containing revenue and two other features of hundreds of released video\ngames. The simulated Sines dataset [39] is generated with varying frequencies, amplitudes, and phases.\nTable 2: Dataset specifications summarizing the data sources, sequence lengths, and dimensionality\nof four datasets used.\nDataset Type Source Length Features\nETTh Real-world Electricity transformer 24 28\nfMRI Real-world BOLD time series 24 50\nRevenue Real-world Game sales 365 3\nSines Simulated Synthetic waves 24 5\nThe only difference is the fMRI dataset, which has a sequence length of 48 compared to the original with 24\ntimesteps.\n4.2 Experimental Setup\nFollowing the infilling training setup of the Diffusion-TS framework [ 40], we adopt the CSDI [ 32] and Diffusion-\nTS [40] as baseline fundamental models. The training stage uses the entire dataset in a multivariate time series\naligned with the community. For all experiments, control signals were applied exclusively to the first channel\n(c= 0) to observe inter-channel influences while enabling multi-channel extensibility. All figures and results are\nbased on the first channel unless stated otherwise. Training parameters and inference settings are expressed in\nAppendix B, such as choosing controlling points and confidence scores.\n4.3 Evaluation Metrics\nWe employ multiple metrics across two key aspects: control accuracy and distribution quality. For control\naccuracy , we measure: (i) Mean Absolute Difference (MAD) between generated and target values at controlling\npoints, defined as MAD =1\nN1\nAPN\ni=1PA\nj=1\f\fxcond gen (i,j)−xtarget(i,j)\f\f; and (ii) Statistic Control Result to\nevaluate adherence to target statistic constraints. For example, the control target function is the sum of series, we\ndirectly observe the actual sum value change to validate the controllability. For distribution alignment check,\nfollowed by many works [ 16,40,25,28], we utilize: (i) Discriminative & Predictive scores to assess how well-\ngenerated sequences replicate real data patterns, where discriminative score is |accuracy −0.5|and predictive\nscore uses mean absolute error (MAE) between predictions and ground truth; (ii) Context-FID score which\nleverages a pre-trained TS2Vec model to measure distribution-level alignment, with lower scores indicating\nbetter similarity; and (iii) Correlational score that quantifies feature-level covariance preservation. We also\ncomplement these quantitative metrics with Kernel Density Function visualizations to provide qualitative\ninsights into distribution alignment.\n5 Results\n5.1 Point-Wise Control\nWe conducted experiments to examine the relationship between point confidence values and prediction accuracy\nacross different datasets. These experiments aim to validate our hypothesis that higher confidence values lead to\nmore precise point control and verify the behavior at extreme confidence values (0.01 and 1.0).\nTable 3 with the Diffusion-TS backbone demonstrates that increasing confidence values leads to a consistent\nMAD reduction across all datasets, ultimately converging to 0.0 at maximum confidence. This convergence at\nconfidence = 1.0 validates our theoretical guarantee that maximum confidence forces the point to be a fixed\npoint. The monotonic and linear decrease of MAD with increasing confidence is visualized in Figure 2.\nThe second finding reveals that extreme target values (0.1 or 1.0) produce larger MAD values than moderate\nconfidence levels (e.g., 0.5) across different datasets. For instance, on the fMRI Dataset, the MAD at atarget =\n0.1is0.113, while at atarget = 0.8, it reduces to 0.052. This pattern, visible in Table 3, supports our\nhypothesis about the trade-off between very low and very high confidence values in controlling point selection.\nComparing the MAD of the Original Dataset and Unconditional across the three target values, the point approach\ndemonstrates a smaller MAD even with a confidence score of 0.01. This indicates that point-wise control helps\ngenerate time series data closer to the target value at a specified time. Furthermore, the pattern of largest and\n5https://www.fmrib.ox.ac.uk/datasets/netsim\n6\n--- Page 7 ---\nTable 3: Point Control MAD of the given point in-\ndices in different setups of different confidence levels\nand target values.\nETTh Dataset\nTarget Value \\Confidence Original 0.00 (Uncon) 0.01 0.50 1.00 Average\n0.1 .0661 .0614 .0233 .0038 .0000 .0090\n0.8 .7201 .7363 .0734 .0400 .0000 .0378\n0.8 .5775 .6021 .1293 .0622 .0000 .0638\n1.0 .7775 .8021 .2219 .1007 .0000 .1075\nAverage .4988 .5102 .1274 .0594 .0000\nfMRI Dataset\n0.1 .4423 .4409 .0288 .0050 .0000 .0113\n0.8 .2620 .2632 .0129 .0027 .0000 .0052\n1.0 .4577 .4591 .0163 .0027 .0000 .0063\nAverage .3873 .3877 .0193 .0035 .0000\nSine Dataset\n0.1 .6530 .6617 .2511 .1116 .0000 .1209\n0.8 .1466 .1455 .0700 .0377 .0000 .0359\n1.0 .2470 .2383 .1179 .0588 .0000 .0589\nAverage .3489 .3485 .1463 .0694 .0000Table 4: Sum Control Average summation value for\nvarious weights of sum control. “Original”: the orig-\ninal training set; “Uncon”: Unconditional generated\nsamples. The biggest andsmallest MAD are labelled\nin each dataset. (Diffusion-TS)\nDataset Original UnconTarget Value\n-100.0 20.0 50.0 150.0\nSine 17.881 18.086 7.114 20.146 20.991 21.031\nRevenue 80.194 76.619 52.675 55.380 58.585 117.868\nETTh 1.924 1.535 0.802 8.502 10.323 11.102\nfMRI 12.990 12.980 4.675 17.192 19.811 20.508\nDataset Original UnconWeight Value\n1 10 50 100\nSine 17.881 18.086 7.110 7.114 7.117 7.115\nRevenue 80.194 76.619 52.767 52.675 52.741 52.725\nETTh 1.924 1.535 0.798 0.802 0.796 0.800\nfMRI 12.990 12.980 4.678 4.675 4.677 4.682\nsmallest MAD values aligns with those in the original and unconditional versions, suggesting that point-wise\ncontrol preserves the original distribution and MAD for each dataset while reducing its MAD magnitude.\n0.2 0.4 0.6 0.8 1.0\nAnchor T arget Value0.000.050.100.150.200.25Mean Absolute Difference\nEffect of Anchor T arget Value\nSine (Confidence=0.01)\nSine (Confidence=0.5)\nRevenue (Confidence=0.01)\nRevenue (Confidence=0.5)\nETTh (Confidence=0.01)\nETTh (Confidence=0.5)\nfMRI (Confidence=0.01)\nfMRI (Confidence=0.5)\nAll (Confidence=1.0)\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence0.000.020.040.060.080.100.120.14Mean Absolute Difference\nEffect of Confidence\nSine (AllPeaks)\nRevenue (AllPeaks)\nETTh (AllPeaks)\nfMRI (AllPeaks)\n0.2 0.4 0.6 0.8 1.0\nAnchor T arget Value0.000.050.100.150.200.250.30Mean Absolute Difference\nEffect of Anchor T arget Value\nSine (Confidence=0.01)\nSine (Confidence=0.5)\nRevenue (Confidence=0.01)\nRevenue (Confidence=0.5)\nETTh (Confidence=0.01)\nETTh (Confidence=0.5)\nfMRI (Confidence=0.01)\nfMRI (Confidence=0.5)\nAll (Confidence=1.0)\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence0.0000.0250.0500.0750.1000.1250.1500.175Mean Absolute Difference\nEffect of Confidence\nSine (AllPeaks)\nRevenue (AllPeaks)\nETTh (AllPeaks)\nfMRI (AllPeaks)\nFigure 2: Point Control The influence of confidence across datasets. The right figure is examined\nwhen enabling the dynamic error adjustment and time-dependent weight, while the left figure is not.\n“AllPeaks” means the average of all experiments of point-wise control. More details are in Appendix\nC.4\n.5.2 Segment-Wise Statistical Control\nOur experiments investigate two key aspects of the aggregated statistic adjustment mechanism: (1) the effective-\nness of different target values in steering the segment sum andsequence sums , and (2) the impact of weight\nparameters on control strength. These experiments aim to validate whether our method can reliably guide\nsequences toward desired sum targets while maintaining data fidelity.\nWhole Sequence Sum As shown in Table 4, the controlled sequences consistently respond to different\ntarget values across all datasets. For the Revenue dataset, when targeting 150.0, the sequence sum increases\nsignificantly from 76.6 to 117.9, while targeting -100.0 reduces it to 52.7. Similar patterns are observed in other\ndatasets - ETTh shows controlled variation from 1.5 to 11.1 (100) and 0.8 (-150), etc. Moreover, Revenue shows\nincreasing steps: +2.705 from -100.0 to 20.0, +3.206 from 20.0 to 50.0, and +59.283 from 50.0 to 150.0. Both\npatterns demonstrate fine-grained control for smaller adjustments and the ability to make substantial changes\nwhen needed. Figures in Appendix D.1 visualize more controlled time series with different targets.\n100\n 50\n 0 50 100 150\nT arget Value0.00.51.0Norm Average Sum\nEffect of Different T arget Values (Normalized)\n0 20 40 60 80 100\nWeight Value0.00.51.0\nEffect of Different Weights (Normalized)\nSine\nRevenue\nETTh\nfMRI\nFigure 3: Normalized comparison of sum control effectiveness across datasets. Values are scaled\nrelative to dataset-specific ranges to enable direct comparison between different domains. Lines show\nprogressive convergence toward target sums. (Diffusion-TS)\nThe weight parameter experiments reveal that varying weights from 1 to 100 produces only minor changes\nin the resulting sums. Target value rather than the control weight is the primary driver of loss-based control\nperformance. Then, the weight parameters may be simplified in practical applications since they do not contribute\nsubstantially to control performance. The normalized trends in Figure 5.2 further support this observation.\nSegment Sum To evaluate segment-wise control, we tested 3 segments: (0.2L,0.4L),(0.4L,0.6L),\n(0.6L,0.8L)with target value 150. Figure 4 shows that each controlled segment demonstrates increased\n7\n--- Page 8 ---\nsum adjustments. The effectiveness is particularly visible in the ETTh and Revenue datasets. The controlled\nsegments exhibit clear increases in area under the curve when targeting higher sums with Diffusion-TS, while\nthe effect of CSDI controlling is not obvious.\n4-8 8-12 12-16\nTime Segments0.00.20.40.60.81.0AverageETTh Dataset with Segment Summation Control\nOriginal\nControlled\n4-8 8-12 12-16\nTime Segments0.00.20.40.60.81.0AverageSine Dataset with Segment Summation Control\nOriginal\nControlled\n4-8 8-12 12-16\nTime Segments0.00.20.40.60.81.0AverageETTh Dataset with Segment Summation Control\nOriginal\nControlled\n73-146 146-219 219-292\nTime Segments0.00.20.40.60.81.0AverageRevenue Dataset with Segment Summation Control\nOriginal\nControlled\nFigure 4: Segment-wise sum control results for different datasets. The shaded area represents the\ncontrolled segment, with the corresponding average sum value across different time segments. The\ndatasets from left to right are: ETTh, Revenue, fMRI and Sine.\n5.3 Distribution Alignment\nTable 5 demonstrates that Diffusion-TS achieves better distribution matching than CSDI in unconditional\ngeneration across all datasets, with scores approaching 0, particularly for ETTh (0.034) and Sines (0.019).\nHowever, both Point-wise and Statistics controls lead to increased divergence, suggesting a trade-off between\ncontrol and distribution preservation. The comprehensive table can be found in Appendix C.3.\nTable 5: Discriminative score for our method on CSDI and Diffusion-TS (DTS). The lower the score,\nthe more similar the distribution of generated time series with original datasets.\nModel Control ETTh Revenue fMRI Sines\nOur - CSDIUnconditional 0.361±0.007 0.245 ±0.164 0.306 ±0.021 0.017 ±0.007\nPoint-wise 0.470±0.003 0.313 ±0.046 0.482 ±0.004 0.430 ±0.038\nStatistics 0.373±0.007 0.272 ±0.055 0.377 ±0.019 0.034 ±0.007\nOur - DTSUnconditional 0.034±0.026 0.209 ±0.185 0.089 ±0.033 0.019 ±0.008\nPoint-wise 0.437±0.004 0.393 ±0.030 0.495 ±0.001 0.460 ±0.011\nStatistics 0.477±0.003 0.426 ±0.032 0.498 ±0.001 0.451 ±0.029\nOur time efficiency analysis from Table 6 reveals CSDI’s computational advantage, processing samples 3-9x\nfaster than Diffusion-TS across all datasets. The runtime scales primarily with sequence length, as evidenced\nby the Revenue dataset’s higher processing times. Notably, both point-wise and statistics-based controls add\nminimal computational overhead, maintaining consistent performance across control configurations. Feature\ndimensionality has a secondary but observable impact on processing time.\nTable 6: Time efficiency analysis of our method compared on CSDI and Diffusion-TS (DTS). The\nresults show the average time per sample for each dataset and control configuration. 500 examples\nper batch with an average of 5 batches.\nMethodDataset Time Per Sample (ms)\nName Seq Features Uncon Point-Wise Statistics\nOur - CSDIEnergy 24 28 0.064±0.000 0.064±0.004 0.064 ±0.001\nfMRI 24 50 0.116±0.000 0.115±0.001 0.117±0.001\nRevenue 365 3 1.000±0.001 1.016±0.022 1.001 ±0.013\nSines 24 5 0.018±0.000 0.017 ±0.001 0.018 ±0.001\nOur - DTSEnergy 24 28 0.278±0.017 0.304±0.002 0.292 ±0.001\nfMRI 24 50 0.376±0.154 0.232±0.883 0.549±0.274\nRevenue 365 3 9.163±0.411 9.580±5.126 9.223 ±0.044\nSines 24 5 0.046±0.015 0.059±0.020 0.048 ±0.001\n6 Related Work\nTraditional generative tasks employed GANs [ 37,24,39,4], V AEs [ 8,17], Diffusion Models [ 3,32,38,6,33,\n28,1,15,42], and Flow Matching [ 14], establishing foundational techniques for synthetic time series generation.\n6.1 Time Series Generation Models\nTimeGAN [ 39] introduced temporal-aware adversarial training, while subsequent works like C-TimeGAN [ 4]\nand CGAN-TS [ 22] incorporated conditional generation capabilities. Recent Transformer-based architectures\n8\n--- Page 9 ---\n[29,36] have further enhanced representation capabilities for temporal patterns. Early control mechanisms\nfocused on global attributes through models like TimeV AE [ 8], TimeGAN [ 39], and C-TimeGAN [ 4]. Recent\nadvances include hierarchical approaches [ 34] and attention-based mechanisms [ 20]. Notably, CGAN-TS [ 22]\nand ControlTS [35] introduced attribute-based and temporal feature control.\n6.2 Diffusion Models for Time Series\nThe adaptation of diffusion models to time series data has seen rapid advancement through several key de-\nvelopments. Early works like TimeGrad [ 27] and CSDI [ 32] established the viability of diffusion models\nfor temporal data, particularly in handling missing value imputation and uncertainty quantification. These\nfoundations led to architectural innovations including TimeDiT [ 6], which introduced specialized temporal\nattention mechanisms, and Diff-MTS [ 28], which enhanced multivariate time series generation through improved\ncross-channel modelling.\nRecent advances have focused on both architectural improvements and control mechanisms. The Latent\nDiffusion Transformer [ 10] demonstrated efficient generation through compressed latent spaces, while RATD\n[19] introduced robust attention mechanisms for handling temporal dependencies. Score-CDM [ 41] and\nDiffusionBridge [ 25] advanced controlled generation through score-based approaches and bridge construction\nmethods respectively. These developments have enabled successful applications across diverse domains, from\nhealthcare monitoring [1] and industrial systems [33] to financial forecasting [38, 12].\nWhile existing methods show promise, they primarily focus on global trend control and distribution matching,\nlacking, fine-grained control at the individual timestamp level except DiffTime and Guided-DiffTime [ 7],\nworking on the point-wise constrained time series generation. However, their points control needs additional\ntraining.\n7 Conclusion\nWe present the COCKTAIL EDITfor generalized Time Series Editing that enables fine-grained control and global\nstatistical manipulation without model retraining. It is achieved via float masking and score-based guidance for\nstatistical properties. The interleaving nature of the two mechanisms allows them to be seamlessly combined,\nenabling sophisticated editing operations that respect both fine-grained constraints and coarse-grained properties.\nThis makes our framework particularly suitable for real-world applications where domain experts need to\nincorporate specific knowledge while maintaining overall statistical validity. Figure G demonstrates the practical\ninteractive editing interface to enable the intuitive manipulation of temporal data like Photoshop. The control\nmechanism is not perfect and the obvious distribution drift still exists. Our method can be further improved\nby incorporating more advanced control mechanisms and carefully considering the trade-off between control\nprecision and distribution preservation.\nReferences\n[1]Edmonmd Adib, Amanda S. Fernandez, Fatemeh Afghah, and John J. Prevost. Synthetic ecg signal\ngeneration using probabilistic diffusion models. IEEE Access , 11:75818–75828, 2024.\n[2]Juan Miguel Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting\nwith structured state space models. Version published by Transactions on Machine Learning Research in\n2022 (TMLR ISSN 2835-8856) https://openreview.net/forum?id=hHiIbk7ApW , August 2022.\n[3]Yihao Ang, Qiang Huang, Yifan Bao, Anthony K. H. Tung, and Zhiyong Huang. Tsgbench: Time series\ngeneration benchmark. September 2023.\n[4]Gaby Baasch, Guillaume Rousseau, and Ralph Evins. A conditional generative adversarial network for\nenergy use in multiple buildings using scarce data. Energy and AI , 5:100087, 2021.\n[5]Yifan Bao, Yihao Ang, Qiang Huang, Anthony K. H. Tung, and Zhiyong Huang. Towards controllable\ntime series generation. March 2024.\n[6]Defu Cao, Wen Ye, Yizhou Zhang, and Yan Liu. Timedit: General-purpose diffusion transformers for time\nseries foundation model. September 2024.\n[7]Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, and Svitlana Vyetrenko. On the constrained\ntime-series generation problem. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\neditors, Advances in Neural Information Processing Systems , volume 36, pages 61048–61059. Curran\nAssociates, Inc., 2023.\n[8]Abhyuday Desai, Cynthia Freeman, Zuhui Wang, and Ian Beaver. Timevae: A variational auto-encoder for\nmultivariate time series generation. November 2021.\n9\n--- Page 10 ---\n[9]Cristóbal Esteban, Stephanie L. Hyland, and Gunnar Rätsch. Real-valued (medical) time series generation\nwith recurrent conditional gans. June 2017.\n[10] Shibo Feng, Chunyan Miao, Zhong Zhang, and Peilin Zhao. Latent diffusion transformer for probabilistic\ntime series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence , 38(11):11979–11987,\nMar. 2024.\n[11] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160 ,\n2016.\n[12] Mohamed Hamdouche, Pierre Henry-Labordere, and Huyên Pham. Generative modeling for time series\nvia schrödinger bridge. April 2023.\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. June 2020.\n[14] Yang Hu, Xiao Wang, Lirong Wu, Huatian Zhang, Stan Z. Li, Sheng Wang, and Tianlong Chen. Fm-ts:\nFlow matching for time series generation. November 2024.\n[15] Baoyu Jing, Shuqi Gu, Tianyu Chen, Zhiyu Yang, Dongsheng Li, Jingrui He, and Kan Ren. Towards\nediting time series. In The Thirty-eighth Annual Conference on Neural Information Processing Systems ,\n2024.\n[16] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion\nmodel for audio synthesis. arXiv preprint arXiv:2009.09761 , 2020.\n[17] Daesoo Lee, Sara Malacarne, and Erlend Aune. Vector quantized time series generation with a bidirectional\nprior model. March 2023.\n[18] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. Generative time series forecasting with diffusion,\ndenoise, and disentanglement. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems , volume 35, pages 23009–23022. Curran\nAssociates, Inc., 2022.\n[19] Jingwei Liu, Ling Yang, Hongyan Li, and Shenda Hong. Retrieval-augmented diffusion models for time\nseries forecasting. October 2024.\n[20] Xinhe Liu and Wenmin Wang. Deep time series forecasting models: A comprehensive survey. Mathematics ,\n12(10), 2024.\n[21] Caspar Meijer and Lydia Y . Chen. The rise of diffusion models in time-series forecasting. January 2024.\n[22] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei Yin. Generative semi-\nsupervised learning for multivariate time series imputation. In Proceedings of the AAAI conference on\nartificial intelligence , volume 35, pages 8983–8991, 2021.\n[23] Sai Shankar Narasimhan, Shubhankar Agarwal, Oguzhan Akcin, Sujay Sanghavi, and Sandeep Chinchali.\nTime weaver: A conditional time series generation model. March 2024.\n[24] Nam Nguyen and Brian Quanz. Temporal latent auto-encoder: A method for probabilistic multivariate\ntime series forecasting. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pages\n9117–9125, 2021.\n[25] Jinseong Park, Seungyun Lee, Woojin Jeong, Yujin Choi, and Jaewook Lee. Leveraging priors via diffusion\nbridge for time series generation. August 2024.\n[26] Lucas Pinheiro Cinelli, Matheus Araújo Marins, Eduardo Antúnio Barros da Silva, and Sérgio Lima Netto.\nVariational autoencoder. In Variational Methods for Machine Learning with Applications to Deep Networks ,\npages 111–149. Springer, 2021.\n[27] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland V ollgraf. Autoregressive denoising diffusion\nmodels for multivariate probabilistic time series forecasting. In Marina Meila and Tong Zhang, editors,\nProceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of\nMachine Learning Research , pages 8857–8868. PMLR, 18–24 Jul 2021.\n[28] Lei Ren, Haiteng Wang, and Yuanjun Laili. Diff-mts: Temporal-augmented conditional diffusion-based aigc\nfor industrial time series toward the large model era. IEEE Transactions on Cybernetics , 54(12):7187–7197,\n2024.\n[29] Alexander Sommers, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure, and\nThomas Arnold. A survey of transformer enabled time series synthesis. June 2024.\n10\n--- Page 11 ---\n[30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. October 2020.\n[31] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. November 2020.\n[32] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion\nmodels for probabilistic time series imputation. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang,\nand J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages\n24804–24816. Curran Associates, Inc., 2021.\n[33] Muhang Tian, Bernie Chen, Allan Guo, Shiyi Jiang, and Anru R Zhang. Reliable generation of privacy-\npreserving synthetic electronic health record time series via diffusion models. Journal of the American\nMedical Informatics Association , 31(11):2529–2539, 09 2024.\n[34] José F. Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco Martínez-Álvarez, and Alicia Troncoso. Deep\nlearning for time series forecasting: A survey. Big Data , 9(1):3–21, February 2021.\n[35] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang. Deep time\nseries models: A comprehensive survey and benchmark. July 2024.\n[36] Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger Zimmermann, and Yuxuan\nLiang. Diffstg: Probabilistic spatio-temporal graph forecasting with denoising diffusion models. In\nProceedings of the 31st ACM International Conference on Advances in Geographic Information Systems ,\npages 1–12, 2023.\n[37] Magnus Wiese, Robert Knobloch, Ralf Korn, and Peter Kretschmer. Quant gans: deep generation of\nfinancial time series. Quantitative Finance , 20(9):1419–1440, 2020.\n[38] Fangkai Yang, Wenjie Yin, Lu Wang, Tianci Li, Pu Zhao, Bo Liu, Paul Wang, Bo Qiao, Yudong Liu,\nMårten Björkman, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Diffusion-based time series data\nimputation for cloud failure prediction at microsoft 365. In Proceedings of the 31st ACM Joint European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering , ESEC/FSE\n’23, pages 2050–2055. ACM, November 2023.\n[39] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial networks. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in\nNeural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.\n[40] Xinyu Yuan and Yan Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. March\n2024.\n[41] S. Zhang, S. Wang, H. Miao, H. Chen, C. Fan, and J. Zhang. Score-cdm: Score-weighted convolutional\ndiffusion model for multivariate time series imputation. May 2024.\n[42] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. September\n2023.\n11\n--- Page 12 ---\nA Methodology Additional Details\nA.1 Summary of Variants\nTable 7: Summary of variables and their Meanings\nSymbol Meaning\nN Number of time series\nxi∈RL×Di-th time series, length L, dimension D\nS={xi}N\ni=1 Training dataset of time series\nθ Model parameters of fθ\nC Prior/conditioning input\nT Total forward/backward diffusion steps\nK Set of number of gradient steps per diffusion step\nβk∈[0,1] Noise variance schedule\nxt Noisy sample at diffusion step t\nti Time index in the sample x\nvi Corresponding yvalue of the time index in sample x\nµθ, σ2\nk Mean and variance in reverse diffusion\nΩ(x),Ω(x) Observed and missing indices\nxob,xta Observed and target parts of a time series\nm∈[0,1]L×DConfidence/float mask\nωt Time-dependent weight in reverse steps\nCpoint Point-wise control with confidence\nCsegment Segment-wise control constraints\nLsum,Lpen,Lstatistics Loss terms enforcing constraints\nγ, η, β sum[sj:ej] Additional scale factors for guidance\nA.2 Formal Proof of Confidence-Based Masking\nProof. Letp(x)be the jointly trained diffusion model over both observed and missing dimensions, and let\np(xta|xob)be the target conditional distribution. Denote pt(xt\nob, xt\nta)as the distribution of (xt\nob, xt\nta)at\niteration t. We show that as t→0,pt(xt\nta|xt\nob)converges to p(xta|xob).\nI. Forward-Process Marginals. By construction, xt\nobat each step is drawn from the forward process q(xt\nob|\nx0\nob), which is a Gaussian transition that preserves the exact marginal of the known dimensions xob. Formally,\nfor each t,\nxt\nob∼ N\u0000√¯αtx0\nob,(1−¯αt)I\u0001\n.\nHence, pt(xt\nob)remains consistent with the correct marginal distributionQ\nΩ(x)p(xob).\nII. Denoising of Missing Entries. The reverse step for missing entries xt\ntais governed by\nxt\nta←xt+1\nta−p\nβt∇xt+1\ntalogpθ(xt+1\nta|xt+2\nta, xt\nob).\nAs shown in [ 31], iteratively applying the reverse diffusion steps in this score-based framework converges to\nsampling from p(xta|xt\nob).\nIII. Replace and Float-Mask Consistency. Whether we replace xt\nobcompletely or blend it with a float mask\nm:\nxt=m⊙xt\nob+ (1−m)⊙xt\nta,\nthe observed indices remain consistent with their forward-sampled values. This ensures that at every iteration,\nthe joint distribution respects the known-data constraints. The 0.0value in masks means no restriction on this\npoint, and 1.0means the fixed point.\nIV . Convergence to the Conditional. Consider pt(xt\nta|xt\nob). By the score-based argument (the forward-reverse\nchain forming a time-indexed Markov process), the mixture of denoising steps and partial resets of observed\nentries yields\nlim\nt→0pt(xt\nta|xt\nob) = p(xta|xob),\nwhere the convergence follows from the fact that each reverse diffusion step corrects the noise injected in the\nforward pass, conditioned on the known xt\nob.\nThus, replace-based or float-mask imputation each preserves xob’s marginals and iteratively refine xtauntil the\ndistribution of the missing entries matches p(xta|xob).\n12\n--- Page 13 ---\nImplementation Details The float mask mcan be adjusted to control the influence of observed data on\nthe imputed sequence. By iterative sampling from the diffusion model and applying the floating mask, we can\ngenerate time series data that respects observed values while conforming to the correct conditional distribution.\n1.Initialize xTfrom Gaussian noise.\n2.Fort=Tdown to 1:\n•Forward-process sampling (for xt\nob):xt\nob∼q\u0000\nxt\nob|x0\nob\u0001\n.\n•Denoise missing parts (for xt\nta):perform gradient step using logpθ.\n•Apply float mask (optional): fuse observed and imputed regions via m.\nA.3 Time-Dependent Weight\n0 200 400 600 800 1000\nTimestep0.00.20.40.60.81.0Weight ScaleFrom Noisy to Final Time SeriesWeight Scale vs Timestep\nFigure 5: Time-dependent weight schedule during denoising process. The exponential decay ( γ=\n5.0) provides stronger control signals in later timesteps while allowing smoother adjustments near\ncompletion.\nA.4 C OCKTAIL EDITAlgorithm\nAlgorithm 1 DDPM Denoising with C OCKTAIL EDIT\nRequire: Gradient scale η, trade-off coefficient γ, conditional data xa, time-dependent weight ωt\n1: Initialize xT∼ N(0,I)\n2:fort=Tto1do\n3: // Step 1: Predict and Refine Sample\n4: [ˆxa,ˆxb]←pθ(xt, t, θ)\n5:L1=∥xa−ˆxa∥2\n2\n6:xt−1← N\u0000\nµ(pθ(xt, t, θ),xt),Σ\u0001\n7:L2=∥xt−1−µ(pθ(xt, t, θ),xt)∥2\n2/Σ\n8: // Step 2: Statistics Control\n9:Lstatistics =ωtPβsum[sj:ej]Lsum[sj:ej]\n10: ˜x0=pθ(xt, t, θ) +η∇xt(L1+γL2+Lstatistics )\n11: xt−1← N\u0000\nµ(˜x0,xt),Σ\u0001\n12: // Step 3: Point-Wise Control\n13: xt−1←ωtmob⊙xob\nt+ (1−ωtmob)⊙xt−1\n14:end for\n13\n--- Page 14 ---\nB Experiment Additional Details\nTable 8: Training hyperparameters and settings for each dataset.\nMethodDataset Training Inference\nName Seq Length Features LR Train Steps Batch Szie DDPM Timesteps\nOur - CSDIEnergy 24 28 1.00E-03 25000 64 200\nfMRI 24 50 1.00E-03 15000 64 200\nRevenue 365 3 2.00E-03 2230 64 200\nSines 24 5 1.00E-03 12000 128 50\nOur - Diffusion-TSEnergy 24 28 1.00E-05 25000 64 1000\nfMRI 24 50 1.00E-05 15000 64 1000\nRevenue 365 3 2.00E-05 2230 64 500\nSines 24 5 1.00E-05 12000 128 500\nWe run all training and inferencing of all experiments with NVIDIA L40S GPUs. All experiments are fixed on\nthe random seed with 2024. Table 8 summarizes the training hyperparameters for each dataset and method. We\nuse the Adam optimizer with a learning rate of 1.00×10−3for CSDI and 1.00×10−5for Diffusion-TS. The\ntraining steps are set to 25,000 for CSDI and 15,000 for Diffusion-TS. The batch size is 64 for CSDI and 128 for\nDiffusion-TS. The number of diffusion steps is set according to the different datasets.\nFor point-wise control, we placed target points (called “anchor” in the following) at normalized values vanchor ∈\n{0.1,0.8,1.0}at relative temporal positions tanchor ∈ {0.1L,0.3L,0.5L,0.7L,0.9L}. The confidence levels\nare set to canchor ={0.01,0.50,1.0}at the corresponding anchor indices, and the other positions were set to\n0.0 during mask converting.\nFor the segment-wise control, we test on segments {(s, e)}={(0.2L,0.4L),(0.4L,0.6L),(0.6L,0.8L),\n(0, L)},(0, L)represents the whole sequence. We choose “sum” as the aggregated function, then αsum∈\n{−100,20,50,100}as the targeted aggregated statistics, weight βsum∈ {1,10,50,100}. We note that\nbaseline sums vary significantly between datasets due to sequence length and data scaling, we will include the\nvarious sequence length results, such L∈ {96,192,384}in the camera-ready version.\nC Point-Wise Control\nC.1 Demonstrates\nThe following figures demonstrate the effectiveness of point-wise control across different datasets varying this\nconfidence and target value with Diffusion-TS and CSDI. We can see as the confidence increases, the model\nrespects the anchor points more strictly.\nC.1.1 Pure Float Mask Control (Diffusion-TS)\n0.10.20.30.40.50.6Value\n0.070.080.090.10\n0.0900.0950.1000.105\nPeakValue=0.10\n0.20.40.60.8Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.80\n0 5 10 15 20\nTime0.20.40.60.81.0Value\n0 5 10 15 20\nTime0.20.40.60.81.0\n0 5 10 15 20\nTime0.00.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 6: Demonstration of Point-Wise Control in ETTh datasets with multiple control points and\nconfidences\n14\n--- Page 15 ---\n0.00.20.40.60.81.0Value\n0.20.40.60.81.0\n0.00.20.40.60.81.0\nPeakValue=0.10\n0.00.20.40.60.8Value\n0.40.60.8\n0.00.20.40.60.8\nPeakValue=0.80\n0 5 10 15 20\nTime0.00.20.40.60.81.0Value\n0 5 10 15 20\nTime0.00.20.40.60.81.0\n0 5 10 15 20\nTime0.00.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 7: Demonstration of Point-Wise Control in fMRI datasets with multiple control points and\nconfidences.\n0.00.10.20.30.40.5Value\n0.00.10.20.30.40.5\n0.00.10.20.30.4\nPeakValue=0.10\n0.00.20.40.60.8Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.80\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Value\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 8: Demonstration of Point-Wise Control in Revenue datasets with multiple control points and\nconfidences\n0.10.20.30.4Value\n0.10.20.30.4\n0.10.20.30.4\nPeakValue=0.10\n0.650.700.750.800.85Value\n0.650.700.750.800.850.90\n0.650.700.750.800.850.90\nPeakValue=0.80\n0 5 10 15 20\nTime0.60.70.80.91.0Value\n0 5 10 15 20\nTime0.60.70.80.91.0\n0 5 10 15 20\nTime0.60.70.80.91.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 9: Demonstration of Point-Wise Control in Synthetic sine wave datasets with multiple control\npoints and confidences\n15\n--- Page 16 ---\nC.1.2 Float Mask Control with Extensions (Diffusion-TS)\n0.060.080.100.120.140.160.18Value\n0.100.150.200.25\n0.080.100.120.140.16\nPeakValue=0.10\n0.00.20.40.60.8Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.80\n0 5 10 15 20\nTime0.00.20.40.60.81.0Value\n0 5 10 15 20\nTime0.20.40.60.81.0\n0 5 10 15 20\nTime0.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 10: Demonstration of Point-Wise Control in ETTh datasets with multiple control points and\nconfidences\n0.20.40.60.81.0Value\n0.20.40.60.81.0\n0.20.40.60.81.0\nPeakValue=0.10\n0.20.40.60.8Value\n0.20.40.60.8\n0.20.40.60.81.0\nPeakValue=0.80\n0 5 10 15 20\nTime0.20.40.60.81.0Value\n0 5 10 15 20\nTime0.20.40.60.81.0\n0 5 10 15 20\nTime0.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 11: Demonstration of Point-Wise Control in fMRI datasets with multiple control points and\nconfidences.\n0.00.20.40.60.8Value\n0.00.10.20.30.40.5\n0.00.10.20.30.40.50.6\nPeakValue=0.10\n0.00.20.40.60.8Value\n0.00.20.40.60.8\n0.00.20.40.60.8\nPeakValue=0.80\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Value\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 12: Demonstration of Point-Wise Control in Revenue datasets with multiple co’n’tr’l points\nand confidences\nC.1.3 Float Mask Control with Extensions (CSDI)\n16\n--- Page 17 ---\n0.10.20.30.4Value\n0.10.20.30.4\n0.10.20.30.4\nPeakValue=0.10\n0.650.700.750.800.850.90Value\n0.650.700.750.800.85\n0.650.700.750.800.850.90\nPeakValue=0.80\n0 5 10 15 20\nTime0.60.70.80.91.0Value\n0 5 10 15 20\nTime0.60.70.80.91.0\n0 5 10 15 20\nTime0.60.70.80.91.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 13: Demonstration of Point-Wise Control in Synthetic sine wave datasets with multiple control\npoints and confidences\n0.00.20.40.60.81.0Value\n0.050.100.150.200.250.300.35\n0.100.150.200.25\nPeakValue=0.10\n0.00.20.40.60.81.0Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.80\n0 5 10 15 20\nTime0.00.20.40.60.81.0Value\n0 5 10 15 20\nTime0.20.40.60.81.0\n0 5 10 15 20\nTime0.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 14: Demonstration of Point-Wise Control in ETTh datasets with multiple control points and\nconfidences\n0.00.20.40.60.81.0Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.10\n0.00.20.40.60.81.0Value\n0.30.40.50.60.70.8\n0.20.30.40.50.60.70.8\nPeakValue=0.80\n0 5 10 15 20\nTime0.00.20.40.60.81.0Value\n0 5 10 15 20\nTime0.00.20.40.60.81.0\n0 5 10 15 20\nTime0.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 15: Demonstration of Point-Wise Control in fMRI datasets with multiple control points and\nconfidences.\n17\n--- Page 18 ---\n0.00.20.40.60.81.0Value\n0.00.10.20.30.40.5\n0.10.20.30.40.50.6\nPeakValue=0.10\n0.00.20.40.60.81.0Value\n0.20.40.60.8\n0.20.40.60.8\nPeakValue=0.80\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Value\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0\n0 50 100 150 200 250 300 350\nTime0.20.40.60.81.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 16: Demonstration of Point-Wise Control in Revenue datasets with multiple control points\nand confidences\n0.00.20.40.60.81.0Value\n0.10.20.30.40.5\n0.10.20.30.40.5\nPeakValue=0.10\n0.00.20.40.60.81.0Value\n0.600.650.700.750.800.85\n0.600.650.700.750.800.85\nPeakValue=0.80\n0 5 10 15 20\nTime0.00.20.40.60.81.0Value\n0 5 10 15 20\nTime0.60.70.80.91.0\n0 5 10 15 20\nTime0.70.80.91.0\nPeakValue=1.00\nConfidence Increases \n Target Anchor Value Incresses\nFigure 17: Demonstration of Point-Wise Control in Synthetic sine wave datasets with multiple control\npoints and confidences\nC.2 KED of Point-Wise Control\nThe following plots of Kernel Density Estimation (KDE) clearly demonstrate how the distribution peaks (purple\ndash line) shift towards the anchor points as confidence increases. For instance, in the ETTh dataset, the most\npronounced shift occurs at anchor points under the highest confidence level. The model generates sequences that\naccurately respect anchor points while preserving the dataset’s inherent distributional characteristics. In Figure\nC.2.1’s middle row, where the target value is 0.8, increasing confidence levels cause the peaks (y-value density)\nof controlled results (purple line) to intensify and converge toward the target value. This pattern is consistently\nobserved across all datasets.\nC.2.1 Pure Float Mask Control (Diffusion-TS)\n18\n--- Page 19 ---\n0.0 0.2 0.4 0.6 0.8 1.005101520DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0051015202530Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0051015202530Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.005101520DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue05101520DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue05101520Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue05101520Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 18: KDE analysis of ETTh dataset generation.\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.20.00.51.01.52.02.53.03.5Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 19: KDE analysis of fMRI dataset generation.\n19\n--- Page 20 ---\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 20: KDE analysis of Revenue dataset generation.\n0.2 0.4 0.6 0.8 1.00123456DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.00123456Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.5 0.6 0.7 0.8 0.9 1.00246810DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.5 0.6 0.7 0.8 0.9 1.001234567Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.5 0.6 0.7 0.8 0.9 1.0024681012Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.5 0.6 0.7 0.8 0.9 1.0\nValue01234567DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.5 0.6 0.7 0.8 0.9 1.0\nValue02468Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.5 0.6 0.7 0.8 0.9 1.0\nValue0246810Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 21: KDE analysis of synthetic sine wave dataset generation.\n20\n--- Page 21 ---\nC.2.2 Float Mask Control with Extensions (Diffusion-TS)\n0.0 0.2 0.4 0.6 0.8 1.005101520DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.005101520DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue05101520DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue05101520Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue05101520Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 22: KDE analysis of ETTh dataset generation.\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.0DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.20.00.51.01.52.02.53.0Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.20.00.51.01.52.02.53.0Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.0DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.0DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue0.00.51.01.52.02.53.0Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue0.00.51.01.52.02.53.0Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 23: KDE analysis of fMRI dataset generation.\n21\n--- Page 22 ---\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.00.51.01.52.02.53.03.5Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 24: KDE analysis of Revenue dataset generation.\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0012345Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.10246810DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.101234567Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.102468Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0123456DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue02468Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue02468Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 25: KDE analysis of synthetic sine wave dataset generation.\n22\n--- Page 23 ---\nC.2.3 Float Mask Control with Extensions (CSDI)\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.205101520DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.805101520Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.805101520Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.005101520DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.005101520Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue05101520DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue05101520Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue05101520Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 26: KDE analysis of ETTh dataset generation.\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.001234Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 27: KDE analysis of fMRI dataset generation.\n23\n--- Page 24 ---\n0.0 0.2 0.4 0.6 0.8 1.0012345DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0012345Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0012345Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.0012345DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0012345Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0012345Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 28: KDE analysis of Revenue dataset generation.\n0.0 0.2 0.4 0.6 0.8 1.001234DensityTarget=0.1, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.01\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.001234567Target=0.1, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=0.5\nAnchor: 0.1\n0.0 0.2 0.4 0.6 0.8 1.002468Target=0.1, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.1\nConfidence=1.0\nAnchor: 0.1\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.20.00.51.01.52.02.53.0DensityTarget=0.8, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.01\nAnchor: 0.8\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.102468Target=0.8, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=0.5\nAnchor: 0.8\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.10246810Target=0.8, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=0.8\nConfidence=1.0\nAnchor: 0.8\n0.2\n 0.0 0.2 0.4 0.6 0.8 1.0 1.2\nValue0.00.51.01.52.02.53.0DensityTarget=1.0, Confidence=0.01\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.01\nAnchor: 1.0\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue02468Target=1.0, Confidence=0.5\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=0.5\nAnchor: 1.0\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue02468Target=1.0, Confidence=1.0\nOriginal\nUnconditional\nControlled\nTarget=1.0\nConfidence=1.0\nAnchor: 1.0\nConfidence Increases \n Target Anchor Value Incresses\nFigure 29: KDE analysis of synthetic sine wave dataset generation.\n24\n--- Page 25 ---\nC.3 Supplement Metrics\nThe Discriminative, Predictive, Context-FID, and Correlational scores help quantify distribution shifts under\npoint-wise control. Table C.3 demonstrates that all metrics increase significantly after applying control, indicating\nthat the control signals effectively influence the generated time series. But our method makes them more\ndistinguishable from the original distribution, which is the improved direction for following research.\nTable 9: Supplemental metrics for point-wise control performance across different datasets and target\nvalues. The results show discriminative, predictive, context-FID, and correlational scores for each\ndataset and control configuration. Lower scores indicate better performance.\nMetrics Control SignalDataset\nETTh Revenue fMRI Sine\nDiscriminative\nScore\n(Lower is\nBetter)Unconditional 0.103±0.042 0.082 ±0.093 0.141 ±0.037 0.031 ±0.023\nConfidence=0.01 0.497±0.005 0.382 ±0.267 0.500 ±0.000 0.457 ±0.027\nConfidence=0.5 0.496±0.006 0.282 ±0.323 0.499 ±0.001 0.494 ±0.004\nConfidence=1.0 0.498±0.001 0.009 ±0.025 0.498 ±0.000 0.374 ±0.264\nPredictive\nScore\n(Lower is\nBetter)Unconditional 0.256±0.002 0.065 ±0.026 0.103 ±0.002 0.094 ±0.000\nConfidence=0.01 0.302±0.008 0.181 ±0.002 0.134 ±0.010 0.120 ±0.007\nConfidence=0.5 0.305±0.014 0.179 ±0.005 0.141 ±0.010 0.118 ±0.012\nConfidence=1.0 0.335±0.019 0.175 ±0.009 0.139 ±0.009 0.116 ±0.008\nContext-FID\nScore\n(Lower is\nBetter)Unconditional 0.108±0.007 1.230 ±0.284 0.260 ±0.024 0.034 ±0.005\nConfidence=0.01 7.797±0.644 4.654 ±1.389 15.671 ±2.920 4.169 ±1.139\nConfidence=0.5 6.973±1.514 5.921 ±0.711 15.463 ±1.888 10.842 ±2.103\nConfidence=1.0 7.856±1.326 7.083 ±0.512 13.854 ±0.830 8.906 ±1.168\nCorrelational\nScore\n(Lower is\nBetter)Unconditional 2.313±0.743 0.038 ±0.013 2.672 ±0.091 0.066 ±0.009\nConfidence=0.01 9.321±0.764 0.122 ±0.007 16.699 ±0.453 0.255 ±0.013\nConfidence=0.5 8.445±0.675 0.119 ±0.006 15.488 ±0.174 0.468 ±0.016\nConfidence=1.0 9.640±0.835 0.121 ±0.011 17.259 ±0.434 0.345 ±0.034\n25\n--- Page 26 ---\nC.4 Point-Wise Control Analysis\nHere, we demonstrate again the complete aggregated Mean Absolute Deviation (MAD) of point-wise control\nacross all datasets, providing solid evidence for the effectiveness of point-wise control in maintaining anchor\npoints.\nC.4.1 Pure Float Mask Control\n0.2 0.4 0.6 0.8 1.0\nAnchor T arget Value0.000.050.100.150.200.25Mean Absolute Difference\nEffect of Anchor T arget Value\nSine (Confidence=0.01)\nSine (Confidence=0.5)\nRevenue (Confidence=0.01)\nRevenue (Confidence=0.5)\nETTh (Confidence=0.01)\nETTh (Confidence=0.5)\nfMRI (Confidence=0.01)\nfMRI (Confidence=0.5)\nAll (Confidence=1.0)\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence0.000.020.040.060.080.100.120.14Mean Absolute Difference\nEffect of Confidence\nSine (AllPeaks)\nRevenue (AllPeaks)\nETTh (AllPeaks)\nfMRI (AllPeaks)\nFigure 30: Different combinations of confidence levels and target values across all datasets. (Diffusion-\nTS)\nC.4.2 Float Mask Control with Extensions\n0.2 0.4 0.6 0.8 1.0\nAnchor T arget Value0.000.050.100.150.200.250.30Mean Absolute Difference\nEffect of Anchor T arget Value\nSine (Confidence=0.01)\nSine (Confidence=0.5)\nRevenue (Confidence=0.01)\nRevenue (Confidence=0.5)\nETTh (Confidence=0.01)\nETTh (Confidence=0.5)\nfMRI (Confidence=0.01)\nfMRI (Confidence=0.5)\nAll (Confidence=1.0)\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence0.0000.0250.0500.0750.1000.1250.1500.175Mean Absolute Difference\nEffect of Confidence\nSine (AllPeaks)\nRevenue (AllPeaks)\nETTh (AllPeaks)\nfMRI (AllPeaks)\nFigure 31: Different combinations of confidence levels and target values cross all datasets. (Diffusion-\nTS)\n0.2 0.4 0.6 0.8 1.0\nAnchor T arget Value0.00.20.40.60.8Mean Absolute Difference\nEffect of Anchor T arget Value\nSine (Confidence=0.01)\nSine (Confidence=0.5)\nRevenue (Confidence=0.01)\nRevenue (Confidence=0.5)\nETTh (Confidence=0.01)\nETTh (Confidence=0.5)\nfMRI (Confidence=0.01)\nfMRI (Confidence=0.5)\nAll (Confidence=1.0)\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence0.00.10.20.30.40.50.60.7Mean Absolute Difference\nEffect of Confidence\nSine (AllPeaks)\nRevenue (AllPeaks)\nETTh (AllPeaks)\nfMRI (AllPeaks)\nFigure 32: Different combinations of confidence levels and target values across all datasets. (CDSI)\nD Statistic Control\nThis section provides supplementary materials for analyzing statistical control, focusing on both Sum Control\nand Segment-Wise Sum Control.\nD.1 Demonstration\nThe following figures demonstrate the effectiveness of sum control across different datasets. As the target value\nincreases (from left to right in each row), the model generates sequences that successfully adhere to the sum\nconstraints while maintaining the dataset’s inherent distributional properties. For segment-wise sum control, the\nresults consistently show that the model tends to increase the overall sequence sum value, which aligns with the\nobjective of preserving the original distribution learned from the dataset.\n26\n--- Page 27 ---\nD.1.1 Whole Time Series Summation Control (Diffusion-TS)\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 1.0T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 10.0T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 8.7T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 10.8T arget Sum = 150\nTarget Value Increases \nFigure 33: Demonstration of Sum Control on ETTh dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 4.6T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 17.4T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 19.8T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 20.5T arget Sum = 150\nTarget Value Increases \nFigure 34: Demonstration of Sum Control on fMRI dataset.\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0ValueSum: 51.9T arget Sum = -100\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 55.3T arget Sum = 20\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 59.1T arget Sum = 50\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 119.9T arget Sum = 150\nTarget Value Increases \nFigure 35: Demonstration of Sum Control on Revenue dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 7.1T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 20.6T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 21.0T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 21.1T arget Sum = 150\nTarget Value Increases \nFigure 36: Demonstration of Sum Control on synthetic sine wave dataset.\n27\n--- Page 28 ---\nD.1.2 Whole Time Series Summation Control (CSDI)\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 1.4T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 1.3T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 2.4T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 4.9T arget Sum = 150\nTarget Value Increases \nFigure 37: Demonstration of Sum Control on ETTh dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 12.6T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 12.4T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 14.0T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 13.1T arget Sum = 150\nTarget Value Increases \nFigure 38: Demonstration of Sum Control on fMRI dataset.\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0ValueSum: 70.7T arget Sum = -100\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 83.2T arget Sum = 20\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 83.3T arget Sum = 50\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0Sum: 84.1T arget Sum = 150\nTarget Value Increases \nFigure 39: Demonstration of Sum Control on Revenue dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueSum: 17.5T arget Sum = -100\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 20.6T arget Sum = 20\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 16.3T arget Sum = 50\n0 5 10 15 20\nTime0.00.20.40.60.81.0Sum: 20.8T arget Sum = 150\nTarget Value Increases \nFigure 40: Demonstration of Sum Control on synthetic sine wave dataset.\n28\n--- Page 29 ---\nD.1.3 Segment Wise Sum Control (Diffusion-TS)\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 1.630\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 1.600\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 1.125Sum Control Demonstration on ETTh Dataset\nFigure 41: Demonstration of Segment-Wise Sum Control on ETTh dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 3.544\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 3.446\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 3.280Sum Control Demonstration on fMRI Dataset\nFigure 42: Demonstration of Segment-Wise Sum Control on fMRI dataset.\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0ValueT arget Segment: [73:146]\nSum: 30.880\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0 T arget Segment: [146:219]\nSum: 24.218\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0 T arget Segment: [219:292]\nSum: 22.908Sum Control Demonstration on Revenue Dataset\nFigure 43: Demonstration of Segment-Wise Sum Control on Revenue dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 3.373\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 3.768\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 3.902Sum Control Demonstration on Sine Dataset\nFigure 44: Demonstration of Segment-Wise Sum Control on synthetic sine wave dataset.\n29\n--- Page 30 ---\nD.1.4 Segment Wise Sum Control (CSDI)\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 0.183\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 0.178\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 1.000Sum Control Demonstration on ETTh Dataset\nFigure 45: Demonstration of Segment-Wise Sum Control on ETTh dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 1.969\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 2.231\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 2.091Sum Control Demonstration on fMRI Dataset\nFigure 46: Demonstration of Segment-Wise Sum Control on fMRI dataset.\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0ValueT arget Segment: [73:146]\nSum: 20.211\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0 T arget Segment: [146:219]\nSum: 18.473\n0 50 100 150 200 250 300 350\nTime0.00.20.40.60.81.0 T arget Segment: [219:292]\nSum: 6.808Sum Control Demonstration on Revenue Dataset\nFigure 47: Demonstration of Segment-Wise Sum Control on Revenue dataset.\n0 5 10 15 20\nTime0.00.20.40.60.81.0ValueT arget Segment: [4:8]\nSum: 2.637\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [8:12]\nSum: 3.411\n0 5 10 15 20\nTime0.00.20.40.60.81.0 T arget Segment: [12:16]\nSum: 2.513Sum Control Demonstration on Sine Dataset\nFigure 48: Demonstration of Segment-Wise Sum Control on synthetic sine wave dataset.\n30\n--- Page 31 ---\nD.2 Kernel Density Estimate of Sum Control\nSimilar to the KDE analysis of point-wise control, we present KDE analysis for sum control across different\ndatasets. The KDE peaks of controlled output (purple line) shift rightward compared to original and unconditional\ndistributions, confirming that controlled sequences achieve higher sum values while preserving dataset-specific\ndistributional characteristics. While this pattern does not persist consistently across different control weights and\nneed to be further investigated.\nD.2.1 KDE of Total Sum Control (Diffusion-TS)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: 100 Weight: 10\nArea=0.14425)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: 20 Weight: 10\nArea=0.15188)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: 50 Weight: 10\nArea=0.16063)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234 Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: 150 Weight: 10\nArea=0.32248)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: -100 Weight: 1\nArea=0.14439)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: -100 Weight: 10\nArea=0.14425)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: -100 Weight: 50\nArea=0.14452)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21173\nAUC: -100 Weight: 100\nArea=0.14428)\nWeight Increases \nFigure 49: Kernel density estimation analysis of Revenue dataset under varying sum control targets.\nTop: Target analysis showing control effectiveness. Bottom: Weight analysis showing control\neffectiveness.\n0.0 0.2 0.4 0.6 0.8\nValue020406080100DensityOriginal\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: 100 Weight: 10\nArea=0.03201)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue0.02.55.07.510.012.515.017.5Original\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: 20 Weight: 10\nArea=0.34383)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.017.5Original\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: 50 Weight: 10\nArea=0.39950)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.017.5Original\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: 150 Weight: 10\nArea=0.44560)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8\nValue020406080100DensityOriginal\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: -100 Weight: 1\nArea=0.03309)\n0.0 0.2 0.4 0.6 0.8\nValue020406080100Original\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: -100 Weight: 10\nArea=0.03201)\n0.0 0.2 0.4 0.6 0.8\nValue020406080100120 Original\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: -100 Weight: 50\nArea=0.03302)\n0.0 0.2 0.4 0.6 0.8\nValue020406080100120\nOriginal\nArea=0.07908\nUnconditional\nArea=0.06240\nAUC: -100 Weight: 100\nArea=0.03229)\nWeight Increases \nFigure 50: Kernel density estimation visualization for ETTh dataset. Top: Sum control analysis.\nBottom: Weight analysis.\n31\n--- Page 32 ---\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345DensityOriginal\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: 100 Weight: 10\nArea=0.19510)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234\nOriginal\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: 20 Weight: 10\nArea=0.71465)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: 50 Weight: 10\nArea=0.82481)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue02468Original\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: 150 Weight: 10\nArea=0.85431)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345DensityOriginal\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: -100 Weight: 1\nArea=0.19453)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Original\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: -100 Weight: 10\nArea=0.19510)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Original\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: -100 Weight: 50\nArea=0.19502)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Original\nArea=0.53980\nUnconditional\nArea=0.54301\nAUC: -100 Weight: 100\nArea=0.19504)\nWeight Increases \nFigure 51: Kernel density estimation analysis of fMRI dataset. Top: Sum control analysis. Bottom:\nWeight analysis.\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: 100 Weight: 10\nArea=0.29637)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue01234Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: 20 Weight: 10\nArea=0.84078)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0123456 Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: 50 Weight: 10\nArea=0.87469)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0123456Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: 150 Weight: 10\nArea=0.87630)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: -100 Weight: 1\nArea=0.29647)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: -100 Weight: 10\nArea=0.29637)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: -100 Weight: 50\nArea=0.29667)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.73543\nUnconditional\nArea=0.75414\nAUC: -100 Weight: 100\nArea=0.29629)\nWeight Increases \nFigure 52: Kernel density estimation analysis of synthetic sine wave dataset. Top: Sum control\nanalysis. Bottom: Weight analysis.\nD.2.2 KDE of Total Sum Control (CSDI)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: 100 Weight: 10\nArea=0.21587)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345Original\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: 20 Weight: 10\nArea=0.20724)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345\nOriginal\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: 50 Weight: 10\nArea=0.20985)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345\nOriginal\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: 150 Weight: 10\nArea=0.20979)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345DensityOriginal\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: -100 Weight: 1\nArea=0.20926)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: -100 Weight: 10\nArea=0.21587)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue012345\nOriginal\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: -100 Weight: 50\nArea=0.20789)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.21971\nUnconditional\nArea=0.21087\nAUC: -100 Weight: 100\nArea=0.21186)\nWeight Increases \nFigure 53: Kernel density estimation analysis of Revenue dataset under varying sum control targets.\nTop: Target analysis showing control effectiveness. Bottom: Weight analysis showing control\neffectiveness.\n32\n--- Page 33 ---\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.0DensityOriginal\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: 100 Weight: 10\nArea=0.06646)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.0 Original\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: 20 Weight: 10\nArea=0.07371)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.0 Original\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: 50 Weight: 10\nArea=0.07704)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.0 Original\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: 150 Weight: 10\nArea=0.07620)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8\nValue05101520DensityOriginal\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: -100 Weight: 1\nArea=0.06201)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.0Original\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: -100 Weight: 10\nArea=0.06646)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.017.5Original\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: -100 Weight: 50\nArea=0.06795)\n0.0 0.2 0.4 0.6 0.8\nValue0.02.55.07.510.012.515.017.5\nOriginal\nArea=0.07908\nUnconditional\nArea=0.07095\nAUC: -100 Weight: 100\nArea=0.06718)\nWeight Increases \nFigure 54: Kernel density estimation visualization for ETTh dataset. Top: Sum control analysis.\nBottom: Weight analysis.\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: 100 Weight: 10\nArea=0.53978)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234\nOriginal\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: 20 Weight: 10\nArea=0.54150)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234\nOriginal\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: 50 Weight: 10\nArea=0.54294)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: 150 Weight: 10\nArea=0.54480)\nTarget AUC Increases \n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234DensityOriginal\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: -100 Weight: 1\nArea=0.53925)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: -100 Weight: 10\nArea=0.53978)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: -100 Weight: 50\nArea=0.54271)\n0.0 0.2 0.4 0.6 0.8 1.0\nValue01234Original\nArea=0.53980\nUnconditional\nArea=0.53972\nAUC: -100 Weight: 100\nArea=0.54014)\nWeight Increases \nFigure 55: Kernel density estimation analysis of fMRI dataset. Top: Sum control analysis. Bottom:\nWeight analysis.\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0DensityOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: 100 Weight: 10\nArea=0.73666)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: 20 Weight: 10\nArea=0.77243)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: 50 Weight: 10\nArea=0.75521)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: 150 Weight: 10\nArea=0.74909)\nTarget AUC Increases \n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0DensityOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: -100 Weight: 1\nArea=0.74094)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: -100 Weight: 10\nArea=0.73666)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: -100 Weight: 50\nArea=0.76059)\n0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1\nValue0.00.51.01.52.02.53.0\nOriginal\nArea=0.73543\nUnconditional\nArea=0.75043\nAUC: -100 Weight: 100\nArea=0.75822)\nWeight Increases \nFigure 56: Kernel density estimation analysis of synthetic sine wave dataset. Top: Sum control\nanalysis. Bottom: Weight analysis.\n33\n--- Page 34 ---\nD.3 Averaged Sum Change Over All Segments\nFor the following aggregated sum change over all segments, we calculate the averaged value over segments for\neach dataset, with a target sum value of 150 for each segment.\nD.3.1 Value Change of Segment Sum Control (Diffusion-TS)\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageOriginal\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 73 to 146\nOriginal\nControlled\nControlled Segment\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 146 to 219\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 219 to 292\nFigure 57: Segmented Summation Control on Revenue dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 58: Segmented Summation Control on ETTh dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 59: Segmented Summation Control on fMRI dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 60: Segmented Summation Control on Sine dataset.\n34\n--- Page 35 ---\nD.3.2 Value Change of Segment Sum Control (CSDI)\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageOriginal\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 73 to 146\nOriginal\nControlled\nControlled Segment\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 146 to 219\n0-73 73-146 146-219 219-292 292-365\nTime0.00.20.40.60.81.0AverageControlled From 219 to 292\nFigure 61: Segmented Summation Control on Revenue dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 62: Segmented Summation Control on ETTh dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 63: Segmented Summation Control on fMRI dataset.\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageOriginal\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 4 to 8\nOriginal\nControlled\nControlled Segment\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 8 to 12\n0-4 4-8 8-12 12-16 16-24\nTime0.00.20.40.60.81.0AverageControlled From 12 to 16\nFigure 64: Segmented Summation Control on Sine dataset.\n35\n--- Page 36 ---\nD.4 Supplement Metrics\nThe Discriminative, Predictive, Context-FID, and Correlational scores help quantify distribution shifts under\nsum control. Table D.4 demonstrates that all metrics increase significantly after applying control, indicating that\nthe control signals effectively influence the generated time series and make them more distinguishable from the\noriginal distribution.\nTable 10: Supplemental metrics for sum control performance across different datasets and target\nvalues. The results show discriminative, predictive, context-FID, and correlational scores for each\ndataset and control configuration. Lower scores indicate better performance.\nMetrics Control SignalDataset\nETTh Revenue fMRI Sine\nDiscriminative\nScore\n(Lower is\nBetter)Unconditional 0.103±0.042 0.082 ±0.093 0.141 ±0.037 0.031 ±0.023\nSum Target = 150 0.499±0.002 0.455 ±0.069 0.500 ±0.000 0.453 ±0.117\nSum Target = 50 0.499±0.002 0.445 ±0.062 0.500 ±0.000 0.477 ±0.031\nSum Target = 20 0.488±0.005 0.455 ±0.056 0.500 ±0.000 0.244 ±0.100\nSum Target = -100 0.476±0.015 0.427 ±0.064 0.500 ±0.000 0.500 ±0.000\nPredictive\nScore\n(Lower is\nBetter)Unconditional 0.256±0.002 0.065 ±0.026 0.103 ±0.002 0.094 ±0.000\nSum Target = 150 0.465±0.008 0.146 ±0.066 0.113 ±0.001 0.099 ±0.009\nSum Target = 50 0.418±0.007 0.107 ±0.006 0.108 ±0.001 0.108 ±0.040\nSum Target = 20 0.286±0.004 0.104 ±0.007 0.102 ±0.001 0.094 ±0.000\nSum Target = -100 0.289±0.005 0.106 ±0.009 0.116 ±0.003 0.111 ±0.008\nContext-FID\nScore\n(Lower is\nBetter)Unconditional 0.108±0.007 1.230 ±0.284 0.260 ±0.024 0.034 ±0.005\nSum Target = 150 10.608±1.651 2.648 ±0.927 2.740 ±0.480 4.346 ±0.818\nSum Target = 50 8.943±1.277 2.408 ±0.245 2.187 ±0.115 2.827 ±0.389\nSum Target = 20 3.129±0.473 3.533 ±0.849 0.614 ±0.111 0.884 ±0.281\nSum Target = -100 5.049±0.537 5.056 ±0.459 3.122 ±0.389 19.805 ±1.737\nCorrelational\nScore\n(Lower is\nBetter)Unconditional 2.313±0.743 0.038 ±0.013 2.672 ±0.091 0.066 ±0.009\nSum Target = 150 15.328±0.511 0.098 ±0.010 9.387 ±0.149 0.297 ±0.012\nSum Target = 50 10.872±0.812 0.079 ±0.015 7.809 ±0.094 0.210 ±0.029\nSum Target = 20 5.219±0.507 0.082 ±0.011 4.247 ±0.136 0.111 ±0.016\nSum Target = -100 8.345±0.324 0.074 ±0.002 9.508 ±0.169 0.837 ±0.008\nD.5 Additional Sum Control Analysis\nThe following figures provide comprehensive analysis of sum control performance across datasets (Unnormal-\nized). The plots demonstrate achieved sum values compared to target values, with Original and Unconditional\n(Uncon) baselines as references. Analysis of control weight’s impact shows minimal influence on achieved sum\nvalues across different datasets and target configurations.\n100\n 50\n 0 50 100 150\nT arget Value050100Average Sum\nEffect of Different T arget Values on Average Sum\n0 20 40 60 80 100\nWeight Value02040\nEffect of Different Weights on Average Sum (Fixed Weight = 10)\nSine\nRevenue\nETTh\nfMRI\nFigure 65: Comparison of achieved sum values versus target sum values across different datasets.\nThe plots demonstrate the effectiveness of sum control guidance in reaching desired targets.\nE Distribution Analysis\nThe FID scores reveal a fundamental trade-off between controllability and distribution preservation. While\nDiffusion-TS demonstrates stronger modification capabilities (FID increasing from 0.416 to 16.812 with Anchor\ncontrol on fMRI), it comes at the cost of significant distribution shifts. In contrast, CSDI shows more resistance\nto modification but better preserves the original distribution (FID changes from 1.188 to only 1.016 under similar\nconditions).\nThis raises an important open question: How can we achieve precise temporal control while maintaining\ndistribution fidelity? Future research should investigate mechanisms to balance these competing objectives,\npotentially through adaptive control strength or hybrid architectures that combine the stability of CSDI with the\nflexibility of Diffusion-TS.\n36\n--- Page 37 ---\nTable 11: The complete distribution of discriminative, predictive, correlational, and FID scores for\nour method across different datasets and control configurations. For all metrics, lower scores indicate\nbetter performance.\nModel Metric Control ETTh Revenue fMRI Sines\nOur - CSDIDiscriminative ScoreUnconditional 0.361±0.007 0.245 ±0.164 0.306 ±0.021 0.017 ±0.007\nPoint-Wise Control 0.470±0.003 0.313 ±0.046 0.482 ±0.004 0.430 ±0.038\nStatistics Control 0.373±0.007 0.272 ±0.055 0.377 ±0.019 0.034 ±0.007\nPredictive ScoreUnconditional 0.261±0.003 0.054 ±0.012 0.106 ±0.000 0.090 ±0.000\nPoint-Wise Control 0.263±0.001 0.070 ±0.003 0.106 ±0.000 0.091 ±0.000\nStatistics Control 0.261±0.001 0.060 ±0.005 0.106 ±0.001 0.091 ±0.000\nCorrelational ScoreUnconditional 8.428±0.000 0.034 ±0.000 2.212 ±0.000 0.062 ±0.000\nPoint-Wise Control 8.641±0.000 0.024 ±0.000 2.619 ±0.000 0.149 ±0.000\nStatistics Control 8.531±0.000 0.034 ±0.000 3.944 ±0.000 0.065 ±0.000\nFID ScoreUnconditional 1.643±0.171 1.129 ±0.122 1.188 ±0.054 0.034 ±0.006\nPoint-Wise Control 2.720±0.133 2.233 ±0.088 1.016 ±0.019 3.170 ±0.319\nStatistics Control 1.564±0.050 1.097 ±0.040 1.175 ±0.018 0.043 ±0.002\nOur - Diffusion-TSDiscriminative ScoreUnconditional 0.034±0.026 0.209 ±0.185 0.089 ±0.033 0.019 ±0.008\nPoint-Wise Control 0.437±0.004 0.393 ±0.030 0.495 ±0.001 0.460 ±0.011\nStatistics Control 0.477±0.003 0.426 ±0.032 0.498 ±0.001 0.451 ±0.029\nPredictive ScoreUnconditional 0.260±0.002 0.070 ±0.015 0.110 ±0.001 0.090 ±0.000\nPoint-Wise Control 0.314±0.003 0.128 ±0.011 0.136 ±0.002 0.153 ±0.006\nStatistics Control 0.310±0.004 0.114 ±0.005 0.117 ±0.001 0.110 ±0.003\nCorrelational ScoreUnconditional 1.728±0.000 0.033 ±0.000 1.673 ±0.000 0.037 ±0.000\nPoint-Wise Control 5.647±0.000 0.107 ±0.000 16.791 ±0.000 0.405 ±0.000\nStatistics Control 9.190±0.000 0.083 ±0.000 8.361 ±0.000 0.606 ±0.000\nFID ScoreUnconditional 0.177±0.015 1.221 ±0.040 0.416 ±0.011 0.021 ±0.003\nPoint-Wise Control 5.819±0.303 3.479 ±0.157 16.812 ±0.485 4.523 ±0.456\nStatistics Control 5.335±0.221 3.822 ±0.210 2.652 ±0.092 5.954 ±0.438\n37\n--- Page 38 ---\nF Combined Control on Revenue Dataset\nTo demonstrate our method’s capability to handle multiple control signals simultaneously, we present a com-\nprehensive example using the Revenue dataset. The model successfully generates sequences that respect both\nanchor points and sum constraints, highlighting the flexibility and effectiveness of our approach. These generated\nsequences maintain the dataset’s inherent distributional characteristics while precisely adhering to multiple\ncontrol signals. For additional examples of combined control mechanisms, see Figure G which showcases our\nTime Series Editor interface in action.\nFigure 66: Demonstration of combined anchor and sum control on the Revenue dataset, showing the\ninteraction across point-wise constraints and overall sum requirements.\n38\n--- Page 39 ---\nG Time Series Editor\nFigure 67: The Screen Shot of Time Series Editor User Interface\nThe Time Series Editor is designed to solve the Time Series Editing problem. It can add fixed points, soft anchors,\ntrending control, segment-level sum, and average control. For the soft anchors, you can add anchor points based\non start, end, and interval setup. The Trending Control is based on the provided function expressions with\nindependent variable xfor better precise control. We are currently developing the SketchPad mode for better\nuser interaction, aiming to provide an All-in-One application for efficient time series editing without training.\nIn Figure G, the green line represents the model prediction, and the red dots represent the observed/provided\nanchor points. The error bar of each anchor point is the ( 1−confidence ) to demonstrate the uncertainty of the\nobserved points. In Figure G, we use the Revenue Dataset with three features: Revenue, Download, and Daily\nActive Users.\n39",
  "text_length": 103563
}