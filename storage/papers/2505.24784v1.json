{
  "id": "http://arxiv.org/abs/2505.24784v1",
  "title": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric\n  Models",
  "summary": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art\nperformance in various domains, but struggle with data efficiency compared to\nhuman learning, which leverages core priors about objects and their\ninteractions. Active inference offers a principled framework for integrating\nsensory information with prior knowledge to learn a world model and quantify\nthe uncertainty of its own beliefs and predictions. However, active inference\nmodels are usually crafted for a single task with bespoke knowledge, so they\nlack the domain flexibility typical of DRL approaches. To bridge this gap, we\npropose a novel architecture that integrates a minimal yet expressive set of\ncore priors about object-centric dynamics and interactions to accelerate\nlearning in low-data regimes. The resulting approach, which we call AXIOM,\ncombines the usual data efficiency and interpretability of Bayesian approaches\nwith the across-task generalization usually associated with DRL. AXIOM\nrepresents scenes as compositions of objects, whose dynamics are modeled as\npiecewise linear trajectories that capture sparse object-object interactions.\nThe structure of the generative model is expanded online by growing and\nlearning mixture models from single events and periodically refined through\nBayesian model reduction to induce generalization. AXIOM masters various games\nwithin only 10,000 interaction steps, with both a small number of parameters\ncompared to DRL, and without the computational expense of gradient-based\noptimization.",
  "authors": [
    "Conor Heins",
    "Toon Van de Maele",
    "Alexander Tschantz",
    "Hampus Linander",
    "Dimitrije Markovic",
    "Tommaso Salvatori",
    "Corrado Pezzato",
    "Ozan Catal",
    "Ran Wei",
    "Magnus Koudahl",
    "Marco Perin",
    "Karl Friston",
    "Tim Verbelen",
    "Christopher Buckley"
  ],
  "published": "2025-05-30T16:46:20Z",
  "updated": "2025-05-30T16:46:20Z",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24784v1",
  "full_text": "arXiv:2505.24784v1 [cs.AI] 30 May 2025AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models Conor Heins1‚àóToon Van de Maele1‚àóAlexander Tschantz1,2‚àó Hampus Linander1Dimitrije Markovic3Tommaso Salvatori1Corrado Pezzato1 Ozan Catal1Ran Wei1Magnus Koudahl1Marco Perin1Karl Friston1,4 Tim Verbelen1Christopher L Buckley1,2 1VERSES AI 2University of Sussex, Department of Informatics 3Technische Universit√§t Dresden, Faculty of Psychology 4University College London, Queen Square Institute of Neurology {conor.heins,toon.vandemaele,alec.tschantz}@verses.ai Abstract Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and inter- actions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AX- IOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mix- ture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization. 1 Introduction Reinforcement learning (RL) has achieved remarkable success as a flexible framework for mastering complex tasks. However, current methods have several drawbacks: they require large amounts of training data, depend on large replay buffers, and focus on maximizing cumulative reward without structured exploration [ 1]. This contrasts with human learning, which relies on core priors to quickly generalize to novel tasks [ 2‚Äì4]. Core priors represent fundamental organizational principles - or hyperpriors - that shape perception and learning, providing the scaffolding upon which more complex knowledge structures are built. For example, such priors allow humans to intuitively understand that objects follow smooth trajectories unless external forces intervene, and shape our causal reasoning, helping us to grasp action-consequence relationships [ 5‚Äì8]. Describing visual scenes as factorized into objects has shown promise in sample efficiency, generalization, and robustness on various tasks [9‚Äì14]. These challenges are naturally addressed by Bayesian agent architectures, such as active inference [ 15], that provide a principled framework for incorporating prior knowledge into models, Preprint. Under review. sMMüèÜüéÆ tMMIdentity Model iMM rMMObject -Object Interactions Figure 1: Inference and prediction flow using AXIOM: The sMM extracts object-centric repre- sentations from pixel inputs. For each object latent and its closest interacting counterpart, a discrete identity token is inferred using the iMM and passed to the rMM, along with the distance and the action, to predict the next reward and the tMM switch. The object latents are then updated using the tMM and the predicted switch to generate the next state for all objects. (a) Projection of the object latents into image space. (b) Projection of the kthlatent whose dynamics are being predicted and (c) of its interaction partner. (d) Projection of the rMM in image space; each of the visualized clusters corresponds to a particular linear dynamical system from the tMM. (e) Projection of the predicted latents. The past latents at time tare shown in gray. supporting continual adaptation without catastrophic forgetting. It has been argued that this approach aligns closely with human cognitive processes [ 16,17], where beliefs are updated incrementally as new evidence emerges. Yet, despite these theoretical advantages, applications of active inference have typically been confined to small-scale tasks with carefully designed priors, failing to achieve the versatility that makes deep RL so powerful across diverse domains. To bridge this gap, we propose a novel active inference architecture that integrates a minimal yet expressive set of core priors about objects and their interactions [ 9‚Äì12,18]. Specifically, we present AXIOM ( Active e Xpanding Inference with Object-centric Models), which employs a object-centric state space model with three key components: (1) a Gaussian mixture model that parses visual input into object-centric representations and automatically expands to accommodate new objects; (2) a transition mixture model that discovers motion prototypes (e.g., falling, sliding, bouncing) [ 19] and (3) a sparse relational mixture model over multi-object latent features, learning causally relevant interactions as jointly driven by object states, actions, rewards, and dynamical modes. AXIOM‚Äôs learning algorithm offers three kinds of efficiency: first, it learns sequentially one frame at a time with variational Bayesian updating [ 20]. This eliminates the need for replay buffers or gradient computations, and enables online adaptation to changes in the data distribution. Second, its mixture architecture facilitates fast structure learning by both adding new mixture components when existing ones cannot explain new data, and merging redundant ones to reduce model complexity [ 21‚Äì24]. Finally, by maintaining posteriors over parameters, AXIOM can augment policy selection with information-seeking objectives and thus uncertainty-aware exploration. To empirically validate our model, we introduce the Gameworld 10k benchmark, a new set of environments designed to evaluate how efficiently an agent can play different pixel-based games in10k interactions. Many existing RL benchmarks, such as the Arcade Learning Environment (ALE) [ 25] or MuJoCo [ 26] domains, emphasize long-horizon credit assignment, complex physics, or visual complexity. These factors often obscure core challenges in fast learning and generalization, especially under structured dynamics. To this end, each of the games in Gameworld 10k follows a similar, object-focused pattern: multiple objects populating a visual scene, a player object that can be controlled to score points, and objects following continuous trajectories with sparse interaction mechanics. We formulate a set of 10 games with deliberately simplified visual elements (single color sprites of different shapes and sizes) to focus the current work on the representational mechanisms used for modeling dynamics and control, rather than learning an overly-expressive model for object segmentation. The Gameworld environments also enable precise control of game features and 2 Aviate Bounce Cross Drive Explode Fruits Gold Hunt Impact JumpFigure 2: Gameworld10k: Visual impression of the 10 games in the Gameworld 10k suite. Se- quences of ten frames are overlayed with increasing opacity to showcase the game dynamics. dynamics, which allows testing how systems adapt to sparse interventions to the causal or visual structure of the game, e.g., the shape and color of game objects. On this benchmark, our agent outperforms popular reinforcement learning models in the low-data regime (10,000 interaction steps) without relying on any kind of gradient-based optimization. To conclude, although we have not deployed AXIOM at the scale of complicated control tasks typical of the RL literature, our results represent a meaningful step toward building agents capable of building compact, interpretable world models and exploiting them for rapid decision-making across different domains. Our main contributions are the following: ‚Ä¢We introduce AXIOM, a novel object-centric active inference agent that is learned online, inter- pretable, sample efficient, adaptable and computationally cheap.1 ‚Ä¢To demonstrate the efficacy of AXIOM, we introduce a new, modifiable benchmark suite targeting sample-efficient learning in environments with objects and sparse interactions. ‚Ä¢We show that our gradient-free method can outperform state-of-the-art deep learning methods both in terms of sample efficiency and absolute performance, with our online learning scheme showing robustness to environmental perturbations. 2 Methods AXIOM is formulated in the context of a partially observable Markov decision process (POMDP). At each time step t, the hidden state htevolves according to ht‚àºP\u0000 ht|ht‚Äì1, at‚àí1\u0001, where atis the action taken at time t. The agent does not observe htdirectly but instead receives an observation yt‚àºP\u0000 yt|ht\u0001, and a reward rt‚àºP\u0000 rt|ht, at\u0001. AXIOM learns an object-centric state space model by maximizing the Bayesian model evidence‚Äîequivalently, minimizing (expected) free energy‚Äîthrough active interaction with the environment [ 15]. The model factorizes perception and dynamics into separate generative blocks: (i) In perception, a slot Mixture Model (sMM) explains pixels with competition between object-centric latent variables Ot={O(1) t,...,O(K) t}, associating each pixel to one of Kslots using the assignment variable zt,smm; (ii) dynamics are modeled per- object using their object-centric latent descriptions as inputs to a recurrent switching state space model (similar to an rSLDS [ 19]). We define the full latent sequence as Z0:T={Ot,zt,smm}T t=0. Each slot latent O(k) tconsists of both continuous x(k) tand discrete latent variables. The continuous latents represent properties of an object, such as its position, color and shape. The discrete latents themselves are split into two subtypes: z(k) tands(k) t. We use z(k) tto denote latent descriptors that capture categorical attributes of the slot (e.g., object type), and s(k) tto denote a pair of switch states determining the slot‚Äôs instantaneous trajectory.2Model parameters ÀúŒòare split into module-specific subsets (e.g., ŒòsMM,ŒòtMM). The joint distribution over input sequences y0:T, latent state sequences Z0:Tand parameters ÀúŒòcan be expressed as a hidden Markov model: p(y0:T,Z0:T,ÀúŒò) =p(y0,Z0)p(ÀúŒò)TY t=1p(xt‚Äì1,|zt,ŒòiMM)| {z } Identity mixture modelp(xt‚Äì1,zt,st, at‚Äì1, rt|ŒòrMM)| {z } Recurrent mixture model KY k=1p(yt|x(k) t,zt,smm,ŒòsMM)| {z } Slot mixture modelp(x(k) t|x(k) t‚àí1,s(k) t,ŒòtMM)| {z } Transition mixture model,(1) 1The code for training AXIOM is available at https://github.com/VersesTech/axiom 2We use the superscript index kas inq(k)to select only the subset of q‚â°q(1:K)relevant to the kthslot. 3 where p(ÀúŒò) =p(ŒòsMM)p(ŒòiMM)p(ŒòrMM)p(ŒòtMM). The sMM p(yt|xt,zt,smm,ŒòsMM)is a likeli- hood model that explains pixel data using mixtures of slot-specific latent states (see schematic in Figure 1). The identity mixture model (iMM) p(xt‚Äì1|zt,ŒòiMM)is a likelihood model that assigns each object-centric latent to one of a set of discrete object types. The transition mixture model (tMM) p(x(k) t|x(k) t‚Äì1,s(k) t,ŒòtMM)describes each object‚Äôs latent dynamics as a piecewise linear function of its own state. Finally, the recurrent mixture model (rMM) p(xt‚Äì1,zt,st, at‚Äì1, rt|ŒòrMM)models the dependencies between multi-object latent states (like the switch states of the transition mixture), other global game states like reward r, action a, and the continuous and discrete features of each object. This module is what allows AXIOM to model sparse interactions between objects (e.g., collisions), while still treating each slot‚Äôs dynamics as conditionally-independent given the switch states s(k) t. Slot Mixture Model (sMM). AXIOM processes sequences of RGB images one frame at a time. Each image is composed of H√óWpixels and is reshaped into N=HW tokens {yn t}N n=1. Each token yn tis a vector containing the nthpixel‚Äôs color in RGB and its image coordinates (normalized to\u0002 ‚àí1,+1\u0003 ). AXIOM models these tokens at a given time as explained by a mixture of the continuous slot latents; we term this likelihood construction the Slot Mixture Model (sMM, see far left side of Figure 1). The Kcomponents of this mixture model are Gaussian distributions whose parameters are directly given by the continuous features of each slot latent x(1:K) t. Associated to this Gaussian mixture is a binary assignment variable zn t,k,smm‚àà {0,1}indicating whether pixel nat time tis driven by slot k, with the constraint thatP kzn t,k,smm= 1. The sMM‚Äôs likelihood model for a single pixel and timepoint yn tcan be expressed as follows (dropping the tsubscript for notational clarity): p(yn|x(k), œÉ(k) c, zn k,smm) =KY k=1N(Ax(k),diag(\u0002 Bx(k), œÉ(k) c\u0003‚ä§))zn k,smm. (2) The mean of each Gaussian component is given a fixed linear projection Aof each object latent, which selects only its position and color features: Ax(k)=\u0002 p(k),c(k)\u0003. The covariance of each component is a diagonal matrix whose diagonal is a projection of the 2-D shape of the object latent Bx(k)=e(k)(its spatial extent in the X and Y directions), stacked on top of a fixed variance for each color dimension œÉ(k) c, which are given independent Gamma priors. The latent variables p(k),c(k),e(k)are subsets of slot k‚Äôs full continuous features x(k) t, and the projection matrices A, Bare fixed, unlearned parameters. Each token‚Äôs slot indicator zn smmis drawn from a Categorical distribution zn smm|œÄsmm‚àºCat(œÄsmm)with mixing weights œÄsmm. We place a truncated stick- breaking (finite GEM) prior on these weights, which is equivalent to a K-dimensional Dirichlet with concentration vector (1,..., 1, Œ±0,smm), where the first K‚àí1pseudcounts are 1 and the final pseudocount Œ±0,smmreflects the propensity to add new slots. All subsequent mixture models in AXIOM are equipped with the same sort of truncated stick-breaking priors on the mixing weights. Identity Mixture Model (iMM). AXIOM uses an identity mixture model (iMM) to infer a discrete identity code z(k) typefor each object based on its continuous features. These identity codes are used to condition the inference of the recurrent mixture model used for dynamics prediction. Conditioning the dynamics on identity-codes in this way, rather than learning a separate dynamics model for each slot, allows AXIOM to use the same dynamics model across slots. This also enables the model to learn the same dynamics in a type-specific, rather than instance -specific, manner [ 28], and to remap identities when e.g., the environment is perturbed and colors change. Concretely, the iMM models the 5-D colors and shapes {c(k),e(k)}K k=1across slots as a mixture of up to VGaussian components (object types). The slot-level assignment variable z(k) t,typeindicates which identity is assigned to the kthslot. The generative model for the iMM is (omitting the t‚àí1subscript from latent variables): p(\u0002 c(k),e(k)\u0003‚ä§|z(k) type,¬µ1:V,type,Œ£1:V,type) =VY j=1N(¬µj,type,Œ£j,type)z(k) j,type (3) p(¬µj,type,Œ£‚àí1 j,type) =NIW(mj,type, Œ∫j,type,Uj,type, nj,type) (4) The same type of Categorical likelihood for the type assignments z(k) type|œÄtype‚àºCat(œÄtype)and truncated stick-breaking prior Dir\u0000 1,..., 1, Œ±0,type\u0001 over the mixture weights is used to allow an 4 arbitrary (up to a maximum of V) number of types to be used to explain the continuous slot features. We equip the prior over the component likelihood parameters with conjugate Normal Inverse Wishart (NIW) priors. Transition Mixture Model (tMM). The dynamics of each slot are modelled as a mixture of linear functions of the slot‚Äôs own previous state. To stress the homology between this model and the other modules of AXIOM, we refer to this module as the transition mixture model or tMM, but this formulation is more commonly also known as a switching linear dynamical system or SLDS [ 29]. The tMM‚Äôs switch variable s(k) t,tmmselects a set of linear parameters Dl,blto describe the kthslot‚Äôs trajectory from ttot+ 1. Each linear system captures a distinct rigid motion pattern for a particular object (e.g., ‚Äúball in free flight‚Äù, ‚Äúpaddle moving left‚Äù). p(x(k) t|x(k) t‚Äì1,s(k) t,tmm,D1:L,b1:L) =LY l=1N(Dlx(k) t+bl,2I)s(k) t,l, tmm (5) where we fix the covariance of all Lcomponents to be 2I, and all mixture likelihoods D1:L,b1:Lto have uniform priors. The mixing weights œÄtmmfors(k) t,tmmas before are given a truncated stick-breaking prior Dir\u0000 1,..., 1, Œ±0,tmm)enabling the number of linear modes Lto be dynamically adjusted to the data by growing the model with propensity Œ±0,tmm. Importantly, the Ltransition components of the tMM are not slot-dependent, but are shared and thus learned across all Kslot latents. The tMM can thus explain and predict the motion of different objects using a shared, expanding set of dynamical motifs. As we will see in the next section, interactions between objects are modelled by conditioning s(k) t,tmmon the states of other objects. Recurrent Mixture Model (rMM). AXIOM employs a recurrent mixture model (rMM) to infer the switch states of the transition model directly from current slot-level features. This dependence of switch states on continuous features is the same construct used in the recurrent switching linear dynamical system or rSLDS [ 19]. However, like the rSLDS, which uses a discriminative mapping to infer the switch state from the continuous state, rMM recovers this dependence generatively using a mixture model over mixed continuous‚Äìdiscrete slot states [ 30]. Concretely, the rMM models the distribution of continuous and discrete variables as a mixture model driven by another per-slot latent assignment variable s(k) rmm. The rMM‚Äôs defines a mixture likelihood over continuous and discrete slot-specific information: (f(k) t‚Äì1, d(k) t‚Äì1). The continuous slot features f(k) t‚Äì1are a function of of both the kthslot‚Äôs own continuous state state x(k) t‚Äì1as well as the states of other slots x(1:K) t‚Äì1, such as the distance to the closest object. The discrete features include categorical slot features like the identity of the closest object, the switch state associated with the transition mixture model, and the action and reward at the current timestep: d(k) t‚Äì1= (z(k) t‚Äì1, s(k) t‚Äì1,tmm, at‚Äì1, rt). The rMM assignment variable associated to a given slot is a binary vector s(k) t,rmmwhose mthentry s(k) t,m, rmm‚àà {0,1}indicates whether component mexplains the current tuple of mixed continuous-discrete data. Each component likelihood selected bys(k) t,rmmfactorizes into a product of continuous (Gaussian) and discrete (Categorical) likelihoods. f(k) t‚Äì1=\u0010 Cx(k) t‚Äì1, g(x(1:K) t‚Äì1)\u0011, d(k) t‚Äì1=\u0010 z(k) t‚Äì1,s(k) t,tmm, at‚Äì1, rt\u0011 (6) p(f(k) t‚Äì1, d(k) t‚Äì1|s(k) t,rmm) =MY m=1Ô£Æ Ô£∞N\u0000 f(k) t‚Äì1;¬µm,rmm,Œ£m,rmm\u0001Y iCat\u0000 dt‚Äì1, i;Œ±m,i\u0001Ô£π Ô£ªst,m, rmm (7) where the matrix Cis a projection matrix that selects a subset of slot k‚Äôs continuous features are used, and g(x(1:K) t‚Äì1)summarizes functions that compute slot-to-slot interaction features, such as the XandY-displacement to the nearest object, the identity code associated with the nearest object (and other features detailed in Appendix A). As with all the other modules of AXIOM, we equip the mixing weights for s(k) t,rmmwith a truncated stick-breaking prior whose final Mthpseudocount parameter tunes the propensity to add new rMM components. We explored an ablation of the rMM (fixed_distance ) where the XandY-displacement vector is not returned by g(x(1:K) t‚àí1); rather, the distance that triggers detection of the nearest interacting object is a fixed hyperparameter of the g function. This hyperparameter can to be tuned to attain higher reward on most environments than the 5 standard model where the rMM learns the distance online. However, it comes at the cost of having to tune this hyperparamter in an environment-specific fashion (see Figure 3 and Table 1 for the effect of thefixed_distance ablation on performance). Variational inference. AXIOM uses variational inference to perform state inference and parameter learning. Briefly, this requires updating an approximate posterior distribution q(Z0:T,ÀúŒò)over latent variables and parameters to minimize the variational free energy F, an upper bound on negative log evidence F ‚â• ‚àí logp(y0:T). In doing so, the variational posterior approximates the true posterior p(Z0:T,ÀúŒò|y0:T)from exact but intractable Bayesian inference. We enforce independence assumptions in the variational posterior over several factors: across states and parameters, across the Kslot latents, and over time T. This is known as the mean-field approximation: q(Z0:T,ÀúŒò) =q(ÀúŒò)TY t=0Ô£´ Ô£≠NY n=1q(zn t,sMM)Ô£∂ Ô£∏Ô£´ Ô£≠KY k=1q(O(k) t)Ô£∂ Ô£∏ (8) q(O(k) t) =q(x(k) t)q(z(k) t)q(s(k) t), q(ÀúŒò) =q(ŒòsMM)q(ŒòiMM)q(ŒòtMM)q(ŒòrMM) (9) Note that the mixture variable of the sMM zt,smmis factored out of the other object-centric latents in both the generative model and the posterior because unlike the other discrete latents z(k) t, it is not independent across Kslots. We update the posterior over latent states q(Z0:T)(i.e., the variational E-step) using a simple form of forward-only filtering and update parameters using coordinate ascent variational inference, using the sufficient statistics of the latents updated during filtering and the data to update the parameters using simple natural parameter updates. These variational E-M updates are run once per timestep, thus implementing a fast, streaming form of coordinate-ascent variational inference [ 31,32]. The simple, gradient-free form of these updates inherits from the exponential-family form of all the mixture models used in AXIOM. 2.1 Growing and pruning the model Fast structure learning. In the spirit of fast structure learning, AXIOM dynamically expands all four mixture modules (sMM, iMM, tMM, rMM) using an online growing heuristic: process each new datapoint sequentially, decide whether it is best explained by an existing component or whether a new component should be created, and then update the selected component‚Äôs parameters. We fix a maximum number of components Cmaxfor each mixture model and let Ct‚àí1‚â§ Cmaxbe the number currently in use. For each component cwe store its variational parameters Œòc, where for a particular model this might be a set of Normal Inverse Wishart parameters, e.g. Œòc,iMM={mc, Œ∫c,Uc, nc}. Upon observing a new input yt, we compute for each component c= 1,...,Ct‚àí1the variational posterior‚Äìpredictive log‚Äìdensity ‚Ñìt,c=Eq(Œòc)\u0002 logp(yt|Œòc)\u0003. The truncated stick‚Äìbreaking priorœÄ‚àºDir(1,..., 1, Œ±)then defines a ‚Äúnew‚Äìcomponent‚Äù threshold œÑt= log p0(yt) + log Œ± where p0is the prior predictive density under an empty component.We select the component with highest score, c‚àó= arg max c‚â§Ct‚àí1‚Ñìt,cand hard-assign yttoc‚àóif‚Ñìt,c‚àó‚â•œÑt; otherwise‚Äîprovided Ct‚àí1<Cmax‚Äîwe instantiate a new component and assign ytto it. Finally, given the hard assignment zt, we update the chosen component‚Äôs parameters via a variational M-step (coordinate ascent).The last weight in the Dirichlet absorbs any remaining mass, soPCmax c=1œÄc= 1.) This algorithm is a deterministic, maximum a posteriori version of the CRP assignment rule (see Equation (8) of [ 27]). The expansion threshold œÑtplays the role of the Dirichlet Process concentration Œ±. When Œ±is small the model prefers explaining data with existing slots; larger Œ±makes growth more likely. The procedure is identical for the sMM, iMM, tMM and rMM‚Äîonly the form of p(¬∑ |Œòc)and the model-specific caps on components Cmaxand expansion thresholds œÑtdiffer. Bayesian Model Reduction (BMR). Every ‚àÜTBMR=500 frames we sample up to npair=2000 used rMM components, score their mutual expected log-likelihoods with respect to data generated from the model through ancestral sampling, and greedily test merge candidates. A merge is accepted if itdecreases the expected free energy of the multinomial distributions over reward and next tMM switch, conditioned on the sampled data for the remaining variables; otherwise it is rolled back. BMR enables AXIOM to generalize dynamics from single events, for example learning that negative reward is obtained when a ball hits the bottom of the screen, by merging multiple single event clusters (see Section 3, Figure 4a). 6 Table 1: Cumulative reward over 10k steps for Gameworld 10k environments. Cumulative reward is reported as mean ¬±std over 10 model seeds. Italic means AXIOM is better than BBF and Dreamer, bold is overall best. Game AXIOM BBF DreamerV3 AXIOM (fixed dist.) AXIOM (no BMR) AXIOM (no IG) Aviate ‚àí90¬±19‚àí90¬±05‚àí114¬±20 ‚àí76¬±13 ‚àí87¬±12 ‚àí71¬±16 Bounce 27¬±13 ‚àí1¬±15 14 ¬±16 34¬±12 8¬±03‚Üì 8¬±19 Cross ‚àí68¬±36‚àí48¬±07 ‚àí27¬±08 ‚àí18¬±21‚Üë ‚àí 34¬±25 ‚àí7¬±03 Drive ‚àí49¬±04‚àí37¬±06 ‚àí45¬±06‚àí22¬±04‚Üë ‚àí 67¬±03‚Üì ‚àí 32¬±02‚Üë Explode 180¬±30 101¬±13 35 ¬±59 234¬±16‚Üë 165¬±14 190 ¬±16 Fruits 182¬±21 86¬±15 60 ¬±07 209¬±19 141¬±19‚Üì 200¬±20 Gold 190¬±18‚àí26¬±12 ‚àí21¬±10 189 ¬±16 45 ¬±15‚Üì 207¬±17 Hunt 206¬±20 4¬±12 6 ¬±09 231¬±28 48¬±13‚Üì 216¬±11 Impact 189¬±45 122¬±20 168 ¬±83 192 ¬±09 197¬±21 181¬±72 Jump ‚àí55¬±09‚àí96¬±17 ‚àí55¬±17‚àí38¬±25 ‚àí45¬±05‚Üë ‚àí 43¬±26 2.2 Planning AXIOM uses active inference for planning [ 33]; it rolls out future trajectories conditioned on different policies (sequences of actions) and then does inference about policies using the expected free energy, where the chosen policy œÄ‚àóis that which minimizes the expected free energy: œÄ‚àó= arg min œÄHX œÑ=0‚àí\u0000 Eq(OœÑ|œÄ)[logp(rœÑ|OœÑ, œÄ)|{z } Utility‚àíDKL(q(Œ±rmm|OœÑ, œÄ)‚à•q(Œ±rmm))| {z } Information gain (IG)]\u0001 (10) The expected per-timestep utility Eq(OœÑ|œÄ)[logp(rœÑ|OœÑ, œÄ)]is evaluated using the learned model and slot latents at the time of planning, and accumulated over timesteps into the planning horizon. The expected information gain (second term on RHS of Equation (10)) is computed using the posterior Dirichlet counts of the rMM and scores how much information about rMM switch states would be gained by taking the policy under consideration. More details on planning are given in Appendix A.11. 3 Results To evaluate AXIOM, we compare its performance on Gameworld against two state-of-the-art baselines on sample-efficient, pixel-based deep reinforcement learning: BBF and DreamerV3. Benchmark. The Gameworld environments are designed to be solvable by human learners within minutes, ensuring that learning does not hinge on brittle exploration or complex credit assignment. The suite includes 10 diverse games generated with the aid of a large language model, drawing 0 10000-0.0200.000Average Reward (1K)Aviate 0 10000-0.0200.000Bounce 0 10000-0.0200.000Cross 0 10000-0.0060.000Drive 0 100000.0000.030Explode 0 10000 Step0.0000.020Average Reward (1K)Fruits 0 10000 Step0.0000.025Gold 0 10000 Step0.0000.025Hunt 0 10000 Step0.0000.050Impact 0 10000 Step-0.0150.000Jump AXIOM AXIOM (fixed distance) Dreamer V3 BBF Figure 3: Online learning performance. Moving average (1k steps) reward per step during training for AXIOM, BBF and DreamerV3 on Gameworld 10k environments. Mean and standard deviation over 10 parameter seeds per model and environment. 7 inspiration from ALE and classic video games, while maintaining a lightweight and structured design. The Gameworld environments are available at https://github.com/VersesTech/gameworld. Figure 2 illustrates the variety and visual simplicity of the included games. To evaluate robustness, Gameworld 10k supports controlled interventions such as changes in object color or shape, testing an agent‚Äôs ability to generalize across superficial domain shifts. Baselines. BBF [ 34] builds on SR-SPR [ 35] and represents one of the most sample-efficient model- free approaches. We adapt its preprocessing for the Gameworld 10k suite by replacing frame- skip with max-pooling over two consecutive frames; all other published hyperparameters remain unchanged. Second, DreamerV3 [ 36] is a world-model-based agent with strong performance on games and control tasks with only pixel inputs; we use the published settings but set the train ratio to 1024 at batch size 16 (effective training ratio of 64:1). We chose these baselines because they represent state of the art in sample-efficient learning from raw pixels. Note that for BBF and DreamerV3, we rescale the frames to 84 √ó84 and 96 √ó96 pixels respectively (following the published implementations), whereas AXIOM operates on full 210 √ó160 frames of Gameworld. Reward. Figure 3 shows the 1000-step moving average of per-step reward from steps 0 to 10000 on theGameworld 10k suite (mean ¬±1 standard deviation over 10 seeds). Table 1 shows the cumulative reward attained at the end of the 10k interaction steps for AXIOM, BBF and DreamerV3. AXIOM attains higher, or on par, average cumulative reward than BBF and DreamerV3 in every Gameworld environment. Notably, AXIOM not only achieves higher peak scores on several games, but also converges much faster, often reaching most of its final reward within the first 5k steps, whereas BBF and DreamerV3 need nearly the full 10k. For those games where BBF and Dreamer seemed to show no-better-than-random performance at 10k, we confirmed that their performance does eventually improve, ruling out that the games themselves are intrinsically too difficult for these architectures (see Appendix E.1). Taken together, this demonstrates that AXIOM‚Äôs object-centric world model, in tandem with its fast, online structure learning and inference algorithms, can reduce the number of interactions required to achieve high performance in pixel-based control. Fixing the interaction distance yields higher cumulative reward as the agent doesn‚Äôt need to spend actions learning it, but doing so requires tuning the interaction distance for each game. This illustrates how having extra knowledge about the domain at hand can be incorporated into a Bayesian model like AXIOM to further improve sample efficiency. Including the information gain term from Equation (10) allows the agent to obtain reward faster in some games (e.g., Bounce), but actually results in a slower increase of the average reward for others (e.g., Gold), as encourages visitation of information-rich but negatively-rewarding states. BMR is crucial for games that need spatial generalization (like Gold and Hunt), but actually hurts performance on Cross, as merging clusters early on discounts the information gain term and discourages exploration. See Appendix E.2 for a more detailed discussion. Computational costs. Table 2 compares model sizes and per-step training timing (model update and planning) measured on a single A100 GPU. While AXIOM incurs planning overhead due to the use of many model-based rollouts, its model update is substantially more efficient than BBF, yielding favorable trade-offs in wall-clock time per sample. The expanding object-centric model of AXIOM converges to a sufficient complexity given the environment, in contrast to the fixed (and much larger) model sizes of BBF and DreamerV3. Interpretability. Unlike conventional deep RL methods, AXIOM has a structured, object-centric model whose latent variables and parameters can be directly interpreted in human-readable terms (e.g., shape, color, position). AXIOM‚Äôs transition mixture model also decomposes complex trajectories into simpler linear sub-sequences. Figure 4a shows imagined trajectories and reward-conditioned clusters Table 2: Training time per environment step on Gameworld 10k.Parameter count for AXIOM varies as the model finds a sufficient complexity for each environment. Planning time for AXIOM shows the range for 64 to 512 planning rollouts. Model Parameters (M) Model update (ms/step) Planning (ms/step) BBF 6.47 135¬±36 N/A DreamerV3 420 221¬±37 823 ¬±93 AXIOM 0.3 - 1.6 18¬±3 252 - 534 8 Image Data Imagined Trajectory Reward Clusters(a) 0 5000 10000 Step02500# ComponentsBMR No BMR (b) 0 5000 10000 Step0.002.00Median (1k)E[infogain] E[utility] (c) 0 5000 10000 Step-0.040.00Average Reward (1K)None Shape Color Color (remap) (d) Figure 4: Tracking AXIOM‚Äôs Behavior. (a) Example frame from Impact at time t(left); imagined trajectory in latent space conditioned on the observation at time tand 32 timesteps into the future, conditioned on an action sequence with high predicted reward (middle); and rMM clusters shown in 2- D space and colored by expected reward (green positive reward, red negative reward aka punishment) (right). (b) Expanding rMM components are pruned over training using Bayesian Model Reduction (BMR) in Explode. (c) Information gain decreases while expected utility increases during training, showing an exploration-exploitation trade-off in Explode. (d) Performance following perturbation at 5k steps shows robustness to changes in game mechanics in Explode. of the rMM for the Impact game. The imagined trajectories in latent space (middle panel of Figure 4a) are directly readable in terms of the colors and positions of the corresponding object. Because the recurrent mixture model (rMM) conditions switch states on various game- and object-relevant features, we can condition these switch variables on different game features and visualize them to show the rMM‚Äôs learned associations (e.g., between reward and space). The right-most panel of Figure 4a show the rMM clusters associated with reward (green) and punishment (red) plotted in space. The distribution of these clusters explains AXIOM‚Äôs beliefs about where in space it expects to encounter rewards, e.g., expecting a punishment when the player misses the ball (red cluster at bottom of the right panel of Figure 4a). Figure 4b shows the sharp decline in active rMM components during training. By actively merging clusters to minimize the expected free energy associated with the reduced model, Bayesian model reduction (BMR) improves computational efficiency while maintaining or improving performance (see Table 1). The resulting merged components enable interpolation beyond the training data, enhancing generalization. This automatic simplification reveals the minimal set of dynamics necessary for optimal performance, making AXIOM‚Äôs decision process transparent and robust. Figure 4c demonstrates that, as training progresses, per-step information gain decreases while expected utility rises, reflecting a shift from exploration to exploitation as the world model becomes reliable. Perturbation Robustness. Finally, we test AXIOM under systematic perturbations of game mechanics. Here, we perform a perturbation to the color or shape of each object at step 5000. Figure 4d shows that AXIOM is resilient to shape perturbations, as it still correctly infers the object type with the iMM. In response to a color perturbation, AXIOM adds new identity types and needs to re-learn their dynamics, resulting in a slight drop in performance and subsequent recovery. Due to the interpretable structure of AXIOM‚Äôs world model, we can prime it with knowledge about possible color perturbations, and then only use the shape information in the iMM inference step, before remapping the perturbed slots based on shape and rescue performance. For more details, see Appendix E.3. 4 Conclusion In this work, we introduced AXIOM, a novel and fully Bayesian object-centric agent that learns how to play simple games from raw pixels with improved sample efficiency compared to both model- based and model-free deep RL baselines. Importantly, it does so without relying on neural networks, gradient-based optimization, or replay buffers. By employing mixture models that automatically expand to accommodate environmental complexity, our method demonstrates strong performance within a strict 10,000-step interaction budget on the Gameworld 10k benchmark. Furthermore, AXIOM builds interpretable world models with an order of magnitude fewer parameters than standard models while maintaining competitive performance. To this end, our results suggest that Bayesian methods with structured priors about objects and their interactions have the potential to bridge the gap between the expressiveness of deep RL techniques and the data-efficiency of Bayesian methods with explicit models, suggesting a valuable direction for research. 9 Limitations and future work. Our work is limited by the fact that the core priors are themselves engineered rather than discovered autonomously. Future work will focus on developing methods to automatically infer such core priors from data, which should allow our approach to be applied to more complex domains like Atari or Minecraft [ 36], where the underlying generative processes are less transparent but still governed by similar causal principles. We believe this direction represents a crucial step toward building adaptive agents that can rapidly construct structural models of novel environments without explicit engineering of domain-specific knowledge. Acknowledgements. We would like to thank Jeff Beck, Alex Kiefer, Lancelot Da Costa, and members of the VERSES Machine Learning Foundations and Embodied Intelligence Labs for useful discussions related to the AXIOM architecture. References  Y. Li, ‚ÄúDeep reinforcement learning: An overview,‚Äù arXiv preprint arXiv:1701.07274, 2017. E. S. Spelke and K. D. Kinzler, ‚ÄúCore knowledge,‚Äù Developmental science, vol. 10, no. 1, pp. 89‚Äì96, 2007. B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, ‚ÄúBuilding machines that learn and think like people,‚Äù Behavioral and Brain Sciences, vol. 40, p. e253, 2017. B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, ‚ÄúHuman-level concept learning through probabilistic program induction,‚Äù Science, vol. 350, no. 6266, pp. 1332‚Äì1338, 2015. E. T√©gl√°s, E. Vul, V. Girotto, M. Gonzalez, J. B. Tenenbaum, and L. L. Bonatti, ‚ÄúPure reasoning in 12-month-old infants as probabilistic inference,‚Äù science, vol. 332, no. 6033, pp. 1054‚Äì1059, 2011. E. S. Spelke, R. Kestenbaum, D. J. Simons, and D. Wein, ‚ÄúSpatiotemporal continuity, smooth- ness of motion and object identity in infancy,‚Äù British journal of developmental psychology, vol. 13, no. 2, pp. 113‚Äì142, 1995. E. S. Spelke, ‚ÄúPrinciples of object perception,‚Äù Cognitive science, vol. 14, no. 1, pp. 29‚Äì56, 1990. A. M. Leslie and S. Keeble, ‚ÄúDo six-month-old infants perceive causality?,‚Äù Cognition, vol. 25, no. 3, pp. 265‚Äì288, 1987. T. Wiedemer, J. Brady, A. Panfilov, A. Juhos, M. Bethge, and W. Brendel, ‚ÄúProvable composi- tional generalization for object-centric learning,‚Äù arXiv preprint arXiv:2310.05327, 2023. F. Kapl, A. M. K. Mamaghan, M. Horn, C. Marr, S. Bauer, and A. Dittadi, ‚ÄúObject-centric representations generalize better compositionally with less compute,‚Äù in ICLR 2025 Workshop on World Models: Understanding, Modelling and Scaling, 2025. W. Agnew and P. Domingos, ‚ÄúUnsupervised object-level deep reinforcement learning,‚Äù in NeurIPS workshop on deep RL, 2018. T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel, ‚ÄúNeural relational inference for interacting systems,‚Äù in International conference on machine learning, pp. 2688‚Äì2697, Pmlr, 2018. A. Lei, B. Sch√∂lkopf, and I. Posner, ‚ÄúSpartan: A sparse transformer learning local causation,‚Äù arXiv preprint arXiv:2411.06890, 2024. W. Zhang, A. Jelley, T. McInroe, and A. Storkey, ‚ÄúObjects matter: object-centric world models improve reinforcement learning in visually complex environments,‚Äù arXiv preprint arXiv:2501.16443, 2025. T. Parr, G. Pezzulo, and K. J. Friston, Active inference: the free energy principle in mind, brain, and behavior. MIT Press, 2022. 10  K. Friston, ‚ÄúThe free-energy principle: a unified brain theory?,‚Äù Nature reviews neuroscience, vol. 11, no. 2, pp. 127‚Äì138, 2010. D. C. Knill and A. Pouget, ‚ÄúThe bayesian brain: the role of uncertainty in neural coding and computation,‚Äù TRENDS in Neurosciences, vol. 27, no. 12, pp. 712‚Äì719, 2004. F. Locatello, D. Weissenborn, and O. Unsupervised, ‚ÄúObject-centric learning with slot attention,‚Äù inAdvances in Neural Information Processing Systems, vol. 33, pp. 1821‚Äì1834, 2020. S. W. Linderman, A. C. Miller, R. P. Adams, D. M. Blei, L. Paninski, and M. J. Johnson, ‚ÄúRecurrent switching linear dynamical systems,‚Äù arXiv preprint arXiv:1610.08466, 2016. C. Heins, H. Wu, D. Markovic, A. Tschantz, J. Beck, and C. Buckley, ‚ÄúGradient-free variational learning with conditional mixture networks,‚Äù arXiv preprint arXiv:2408.16429, 2024. K. J. Friston, V. Litvak, A. Oswal, A. Razi, K. E. Stephan, B. C. Van Wijk, G. Ziegler, and P. Zeidman, ‚ÄúBayesian model reduction and empirical bayes for group (dcm) studies,‚Äù Neuroimage, vol. 128, pp. 413‚Äì431, 2016. K. Friston, T. Parr, and P. Zeidman, ‚ÄúBayesian model reduction,‚Äù arXiv preprint arXiv:1805.07092, 2018. K. Friston, C. Heins, T. Verbelen, L. Da Costa, T. Salvatori, D. Markovic, A. Tschantz, M. Koudahl, C. Buckley, and T. Parr, ‚ÄúFrom pixels to planning: scale-free active inference,‚Äù arXiv preprint arXiv:2407.20292, 2024. K. J. Friston, L. Da Costa, A. Tschantz, A. Kiefer, T. Salvatori, V. Neacsu, M. Koudahl, C. Heins, N. Sajid, D. Markovic, et al., ‚ÄúSupervised structure learning,‚Äù Biological Psychology, vol. 193, p. 108891, 2024. M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, ‚ÄúThe arcade learning environment: An evaluation platform for general agents,‚Äù Journal of Artificial Intelligence Research, vol. 47, pp. 253‚Äì279, jun 2013. E. Todorov, T. Erez, and Y. Tassa, ‚ÄúMujoco: A physics engine for model-based control,‚Äù in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026‚Äì5033, IEEE, 2012. H. Ishwaran and L. F. James, ‚ÄúGibbs sampling methods for stick-breaking priors,‚Äù Journal of the American statistical Association, vol. 96, no. 453, pp. 161‚Äì173, 2001. J. Beck and M. J. Ramstead, ‚ÄúDynamic markov blanket detection for macroscopic physics discovery,‚Äù arXiv preprint arXiv:2502.21217, 2025. Z. Ghahramani and G. E. Hinton, ‚ÄúSwitching state-space models,‚Äù University of Toronto Technical Report CRG-TR-96-3, Department of Computer Science, 1996. C. Bishop and J. Lasserre, ‚ÄúGenerative or discriminative? getting the best of both worlds,‚Äù Bayesian statistics, vol. 8, no. 3, pp. 3‚Äì24, 2007. M. J. Wainwright, M. I. Jordan, et al., ‚ÄúGraphical models, exponential families, and variational inference,‚Äù Foundations and Trends¬Æ in Machine Learning, vol. 1, no. 1‚Äì2, pp. 1‚Äì305, 2008. M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley, ‚ÄúStochastic variational inference,‚Äù the Journal of machine Learning research, vol. 14, no. 1, pp. 1303‚Äì1347, 2013. K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo, ‚ÄúActive inference: a process theory,‚Äù Neural computation, vol. 29, no. 1, pp. 1‚Äì49, 2017. M. Schwarzer, J. S. O. Ceron, A. Courville, M. G. Bellemare, R. Agarwal, and P. S. Castro, ‚ÄúBig- ger, better, faster: Human-level atari with human-level efficiency,‚Äù in International Conference on Machine Learning, pp. 30365‚Äì30380, PMLR, 2023. 11  P. D‚ÄôOro, M. Schwarzer, E. Nikishin, P.-L. Bacon, M. G. Bellemare, and A. Courville, ‚ÄúSample- efficient reinforcement learning by breaking the replay ratio barrier,‚Äù in Deep Reinforcement Learning Workshop NeurIPS 2022, 2022. D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, ‚ÄúMastering diverse control tasks through world models,‚Äù Nature, pp. 1‚Äì7, 2025. T. P. Minka, ‚ÄúExpectation propagation for approximate bayesian inference,‚Äù arXiv preprint arXiv:1301.2294, 2013. M. Okada and T. Taniguchi, ‚ÄúVariational inference mpc for bayesian model-based reinforcement learning,‚Äù in Conference on Robot Learning, 2019. V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, ‚ÄúPlaying atari with deep reinforcement learning,‚Äù arXiv preprint arXiv:1312.5602, 2013. W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao, ‚ÄúMastering atari games with limited data,‚Äù Advances in neural information processing systems, vol. 34, pp. 25476‚Äì25488, 2021. S. Wang, S. Liu, W. Ye, J. You, and Y. Gao, ‚ÄúEfficientzero v2: Mastering discrete and continuous control with limited data,‚Äù arXiv preprint arXiv:2403.00564, 2024. D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, ‚ÄúDream to control: Learning behaviors by latent imagination,‚Äù arXiv preprint arXiv:1912.01603, 2019. D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba, ‚ÄúMastering atari with discrete world models,‚Äù arXiv preprint arXiv:2010.02193, 2020. K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and A. Lerchner, ‚ÄúMulti-object representation learning with iterative variational inference,‚Äù in International conference on machine learning, pp. 2424‚Äì2433, PMLR, 2019. F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Doso- vitskiy, and T. Kipf, ‚ÄúObject-centric learning with slot attention,‚Äù Advances in neural information processing systems, vol. 33, pp. 11525‚Äì11538, 2020. R. Singh and C. L. Buckley, ‚ÄúAttention as implicit structural inference,‚Äù Advances in Neural Information Processing Systems, vol. 36, pp. 24929‚Äì24946, 2023. D. Kirilenko, V. V orobyov, A. K. Kovalev, and A. I. Panov, ‚ÄúObject-centric learning with slot mixture module,‚Äù arXiv preprint arXiv:2311.04640, 2023. J. Jiang, F. Deng, G. Singh, M. Lee, and S. Ahn, ‚ÄúSlot state space models,‚Äù arXiv preprint arXiv:2406.12272, 2024. S. Ferraro, P. Mazzaglia, T. Verbelen, and B. Dhoedt, ‚ÄúFocus: Object-centric world models for robotic manipulation,‚Äù Frontiers in Neurorobotics, vol. 19, p. 1585386, 2025. J. Collu, R. Majellaro, A. Plaat, and T. M. Moerland, ‚ÄúSlot structured world models,‚Äù arXiv preprint arXiv:2402.03326, 2024. C. Rasmussen, ‚ÄúThe infinite gaussian mixture model,‚Äù Advances in neural information process- ing systems, vol. 12, 1999. T. Champion, M. Grze ¬¥s, and H. Bowman, ‚ÄúStructure learning with temporal gaussian mixture for model-based reinforcement learning,‚Äù arXiv preprint arXiv:2411.11511, 2024. Z. Ghahramani and G. E. Hinton, ‚ÄúVariational learning for switching state-space models,‚Äù Neural computation, vol. 12, no. 4, pp. 831‚Äì864, 2000. V. Geadah, J. W. Pillow, et al., ‚ÄúParsing neural dynamics with infinite recurrent switching linear dynamical systems,‚Äù in The Twelfth International Conference on Learning Representations, 2024. S. Linderman, M. J. Johnson, and R. P. Adams, ‚ÄúDependent multinomial models made easy: Stick-breaking with the p√≥lya-gamma augmentation,‚Äù Advances in neural information process- ing systems, vol. 28, 2015. 12 A Full Model Details AXIOM‚Äôs world model is a hidden Markov model with an object-centric latent state space. The model itself has two main components: 1) an object-centric, slot-attention-like [ 18] likelihood model; and 2) a recurrent switching state space model [ 19]. The recurrent switching state space model is applied to each object or slot identified by the likelihood model, and models the dynamics of each object with piecewise-linear trajectories. Unlike most other latent state-space models, including other object-centric ones, AXIOM is further distinguished by its adaptable complexity ‚Äì it grows and prunes its model online through iterative expansion routines (see Algorithm 1) and reduction (see Algorithm 2) to match the structure of the world it‚Äôs interacting with. This includes automatically inferring the number of objects in the scene as well as the number of dynamical modes needed to describe the motion of all objects. This is inspired by the recent fast structure learning approach [ 23] developed to automatically learn a hierarchical generative model of a dataset from scratch. Preface on notation Capital bold symbols denote collections of matrix- or vector-valued random variables and lowercase bold symbols denote multivariate variables. A.1 Generative model The model factorizes perception and dynamics into separate generative blocks: (i) In perception, a slot Mixture Model (sMM) models pixels ytas competitively explained by continuous latent variables factorized across slots or objects: xt={x(1) t,...,x(K) t}. Each pixel is assigned to one of (up to) Kslots using the assignment variable zt,smm; (ii) dynamics are modeled per-object using their object-centric latent descriptions as inputs to a recurrent switching state space model (similar to an rSLDS [ 19]). We define the full latent sequence as Z0:T={Ot,zt,smm}T t=0. Each slot latent O(k) tconsists of both continuous x(k) tand discrete latent variables. The continuous latents represent continuous properties of an object, such as its position, color and shape. The discrete latents are themselves split into two subtypes: z(k) tands(k) t. We use z(k) tto denote four latent descriptors that capture categorical attributes of the slot (e.g., object type), and s(k) tto denote a pair of switch states determining the slot‚Äôs instantaneous trajectory.3Model parameters ÀúŒòare split into module-specific subsets (e.g., ŒòsMM,ŒòtMM). The joint distribution over input sequences y0:T, latent state sequences Z0:Tand parameters ÀúŒòcan be expressed as a hidden Markov model: p(y0:T,Z0:T,ÀúŒò) =p(y0,Z0)p(ÀúŒò)TY t=1p(xt‚Äì1,|zt,ŒòiMM)| {z } Identity mixture modelp(xt‚Äì1,zt,st, at‚Äì1, rt|ŒòrMM)| {z } Recurrent mixture model KY k=1p(yt|x(k) t,zt,smm,ŒòsMM)| {z } Slot mixture modelp(x(k) t|x(k) t‚àí1,s(k) t,ŒòtMM)| {z } Transition mixture model,(11) We intentionally exclude the slot mixture assignment variable zt,smmfrom the other object-centric discrete latents {z(k) t}K k=1because the sMM assignment variable is importantly not factorized over slots, since it is a categorical distribution over K-dimensional one-hot vectors, zt,smm‚àà {0,1}K. Latent object states. At a given time‚Äìstep t‚àà0,..., T each object k‚àà1,..., K is described by sets of both continuous and discrete variables (we reserve kfor ‚Äòslot index‚Äô everywhere): Ot={x(k) t,z(k) t,s(k) t}K k=1, The continuous state x(k) tsummarize latent features or descriptors associated with the kthobject, including its 2-D position p(k) t={p(k) t,x, p(k) t,y}, a corresponding 2-D velocity v(k) t={v(k) t,x, v(k) t,y}, its color c(k) t={c(k) t,r, c(k) t,g, c(k) t,b}its 2-D shape encoded as its extent along the X and Y directions 3We use the superscript index kas inq(k)to select only the subset of q‚â°q(1:K)relevant to the kthslot. 13 e(k) t={e(k) t,x, e(k) t,y}, and an ‚Äòunused counter‚Äô u(k) t, which tracks how long the kthobject has gone undetected: x(k) t=\u0002 p(k) t,c(k) t,v(k) t, u(k) t,e(k) t\u0003‚ä§‚ààR10. In addition to the continuous latents, each object is also characterized by two sets of discrete variables: z(k) tands(k) t. The first set z(k) tcaptures categorical information about the object that is relevant to predicting its instantaneous dynamics. This includes a latent object ‚Äòtype‚Äô z(k) t,type(used to identify dynamics across object instances based on their shared continuous properties, e.g., objects that have the same shape and color are expected to behave similarly); the object type index of the nearest other object that slot kis interacting with (or if it isn‚Äôt interacting with anything) z(k) t,interacting, its presence/absence in the current frame z(k) t,presence, and whether the object is moving or not z(k) t,moving. We define each of these discrete variables in ‚Äòone-hot‚Äô vector format, so as vectors whose entries z(k) t,m, nameare either 0or1, with the constraint thatP mz(k) t,m, name= 1. We can thus write the full discrete z(k) tlatent as follows: z(k) t=\u0002 z(k) t,type,z(k) t,interacting,z(k) t,presence,z(k) t,moving\u0003 ‚àà {0,1}Ctype+Cinteracting +Cpresence +Cmoving withÔ£± Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥Ô£≥Ctype=V‚â§Vmax Cinteracting =V+ 1 Cpresence = 2 Cmoving = 2 where Vis the number of object types inferred by the identity mixture model or iMM, subject to a maximum value of Vmax(see Appendix A.6). Note that Cinteracting has maximum index V+ 1because it includes an extra index for the state of ‚Äònot interacting with any other object.‚Äô The second set of discrete variables s(k) tform a pair of concurrent switch states that jointly select one of an expanding set of linear dynamical systems to predict the object‚Äôs future motion: s(k) t=\u0002 s(k) t,tmm,s(k) t,rmm\u0003 ‚àà {0,1}Stmm+Srmm with( Stmm‚â§L Srmm‚â§M The first variable s(k) t,tmmis the switching variable of the transition mixture model or tMM, whereas the second switch variable s(k) t,rmmis the assignment variable of another mixture model ‚Äì the recurrent mixture model or rMM ‚Äì which furnishes a likelihood over stmmas well as other continuous and discrete features of each object. In the sections that follow we will detail each of the components in the full AXIOM world model and give their descriptions in terms of generative models. These descriptions will show how the latent variables Z0:T={Ot,zt,smm}T t=0and component-specific parameters (e.g, ŒòrMMrelate to observations and other latent variables. A.2 Slot Mixture Model (sMM) AXIOM processes sequences of RGB images one frame at a time. Each image is composed of H√óW pixels and is reshaped into N=HW tokens {yn t}N n=1. Each token yn tis a vector containing the nth pixel‚Äôs color in RGB and its image coordinates (normalized to\u0002 ‚àí1,+1\u0003 ). AXIOM models these tokens at a given time as explained by a mixture of the continuous slot latents; we term this likelihood construction the Slot Mixture Model (sMM, see far left side of Figure 1). The Kcomponents of this 14 mixture model are Gaussian distributions whose parameters are directly given by the continuous features of each slot latent x(1:K) t. Associated to this Gaussian mixture is a binary assignment variable zn t,k,smm‚àà {0,1}indicating whether pixel nat time tis driven by slot k, with the constraint thatP kzn t,k,smm= 1. The sMM‚Äôs likelihood model for a single pixel and timepoint yn tcan be expressed as follows (dropping the tsubscript for notational clarity): p(yn|x(1:K), œÉ(1:K) c,zn smm) =KY k=1N(Ax(k),diag(\u0002 Bx(k), œÉ(k) c\u0003‚ä§))zn k,smm (12) A=\u0002I505√ó5\u0003, B =\u000202√ó8I2\u0003 (13) p(zn smm|œÄsmm) = Cat( œÄsmm), p(œÄsmm) = Dir\u0000 1,..., 1|{z} K‚àí1times, Œ±0,smm) (14) p(œÉ(k) c) =Y j‚ààR, G, BŒì(Œ≥0,j,1) (15) The mean of each Gaussian component is given a fixed linear projection Aof each object latent, which selects only its position and color features: Ax(k)=\u0002 p(k),c(k)\u0003. The covariance of each component is a diagonal matrix whose diagonal is a projection of the 2-D shape of the object latent Bx(k)=e(k) (its spatial extent in the X and Y directions), stacked on top of a fixed variance for each color dimension œÉ(k) c, which are given independent Gamma priors. The latent variables p(k),c(k),e(k) are subsets of slot k‚Äôs full continuous features x(k) t, and the projection matrices A,Bremain fixed and unlearnable. Each token‚Äôs slot indicator zn smmis drawn from a Categorical distribution with mixing weights œÄsmm. We place a truncated stick-breaking (finite GEM) prior on these weights, which is equivalent to a K-dimensional Dirichlet with concentration vector (1,..., 1, Œ±0,smm), where the first K‚àí1pseudocounts are 1 and the final pseudocount Œ±0,smmreflects the propensity to add new slots. All subsequent mixture models in AXIOM are equipped with the same sort of truncated stick-breaking priors on the mixing weights [ 27]. We can collect parameters of the sMM together intoŒòsMM={œÄsmm, œÉ(1:K) c, A, B}. The priors over these parameters can be written as a product of the truncated stick-breaking prior and the Gamma priors for each color variance, and the priors over AandBcan be thought of as Dirac delta functions, rendering them unlearnable. A.3 Moving and presence latent variables Within the discrete variables that describe each slot z(k) t,s(k) t, there are two one-hot-encoded Bernoulli variables: z(k) t,moving‚àà {0,1}2andz(k) t,present‚àà {0,1}2. These represent whether the object associated to the kthslot is moving and present on the screen, respectively. These variables have a particular relationship to the generative model, particularly the dynamics model, which allows inference over them to act as a ‚Äòpre-processing step‚Äô or filter for slot latents, before passing their continuous features further down the inference chain for use by the identity model, recurrent mixture model, and transition model. The way this gating is functionally defined is detailed in the subsection Gated dynamics learning below. The latent state sequences z(k) 0:T,presence andz(k) 0:T,moving variables are modelled as evolving according to discrete, object-factorized markov chains which concurrently emit observations via a proxy presence variable o(k) t(see below for details on how this is computed). This conditional hidden Markov model can be written as follows for the two variables: p(z(k) 0:T,presence ) =TY t=0p(o(k) t|z(k) t,presence )p(z(k) t,presence )|z(k) t‚Äì1,presence, Œ∏presence ) p(z(k) 0:T,moving ) =TY t=1p(z(k) t,moving|z(k) t‚Äì1,moving, o(k) t,v(k) t‚Äì1, Œ∏moving ) (16) 15 Presence latent zt,presence The presence chain uses a time-homogeneous 2√ó2 transition matrix parametrised by Œ∏presence ={œïNP‚ÜíP, œïP‚ÜíNP}, 0‚â§œïNP‚ÜíP, œïP‚ÜíNP‚â§1. where the subscripts for the ‚Äònot present‚Äò and ‚Äòpresent‚Äò states of z(k) t,presence are abbreviated as NPand P, respectively. Writing œïi‚Üínot present =1‚àíœïi‚Üípresent, the transition matrix is Tpresence = œïNP‚ÜíNPœïNP‚ÜíP œï1‚ÜíNP œïP‚ÜíP! = 1‚àíœïNP‚ÜíP œïNP‚ÜíP œïP‚ÜíNP 1‚àíœïP‚ÜíNP!. For all experiments we fixœï0‚Üí1= 0, œï1‚Üí0= 0.01, encoding the prior that an absent slot cannot spontaneously re-appear while a present one ‚Äúdies out‚Äù with probability 0.01 each frame. Proxy observation. We define the assignment-count indicator o(k) tas a variable that indicates whether any pixels 1,2,..., N were assigned to slot kat time t. This can be expressed as an element-wise product of all pixel-specific entries of the row of sMM assignment variable corresponding to slot k‚Äôs assignments: z(1:N) t,k,smm: o(k) t=NY n=1zn t,k,smm Finally, the relationship between the count-assignments-indicator o(k) tand the presence variable zt,presence is a Bernoulli likelihood with the following form: p(ot|zt,presence ) =ozt,P, presence t (1‚àíot)zt,A, presence(17) which means that presence and the assignment count o(k) tare expected to be ‚Äòon‚Äô simultaneously. The subscripts PandArefer to the indices of z(k) t,presence that correspond to the ‚Äòpresent‚Äô and ‚Äòabsent‚Äô indicators, respectively. Moving latent zt,moving We define the speed of slot kas follows (leaving out the ksuperscript to avoid overloaded superscripts): œàt=q v2 t,x+v2 t,y (18) The transition likelihood for the moving latent z(k) t,moving depends on the presence indicator o(k) tand the slot speed œàt‚Äì1; this forces inference of the moving indicator z(k) t,moving to be driven by the inferred speed and presence of the k-th slot. The form of this dependence is encoded in the 2√ó2transition matrix Tmoving with parameters Œ∏moving as follows: Œ∏moving ={Œª, Œ≤}, 0‚â§Œª‚â§1,0‚â§Œ≤‚â§1, Œª+Œ≤‚â§1. (19) The parameters ŒªandŒ≤determine the two conditional probabilities œïi‚ÜíM(o(k) t, œàt‚Äì1)and œïi‚ÜíNM(o(k) t, œàt‚Äì1). We use the subscripts NM,Mto indicate the ‚Äònot-moving‚Äô and ‚Äòmoving‚Äò states of z(k) t,moving, respectively. The dependence of the conditional probabilities on the Œª, Œ≤ hyperparameters can be written as follows: œïi‚ÜíM(o(k) t, œàt‚Äì1) =Ô£± Ô£≤ Ô£≥Œª i+Œ≤ œàt‚Äì1, o(k) t= 1, i, o(k) t= 0,œïi‚ÜíNM(o(k) t, œàt‚Äì1) = 1‚àíœïi‚Üí1(o(k) t, œàt‚Äì1). The full transition matrix Tmoving can be written: Tmoving\u0000 o(k) t, œàt‚Äì1;Œ∏moving\u0001 = œïNM‚ÜíNMœïNM‚ÜíM œïM‚ÜíNM œïM‚ÜíM!. (20) 16 This sort of parameterization results in the following interpretation: if the slot is inferred to be absent (i.e., no pixels are assigned to it and o(k) t= 0),z(k) t,moving stays in its previous state. However, if the slot is inferred to be present ( o(k) t= 1), then the previous ‚Äúmoving‚Äù probability is shrunk by Œªand nudged upward by Œ≤œàt‚Äì1. In all experiments we set Œª= 0.99, Œ≤= 0.01, but they remain exposed hyperparameters. Gated dynamics learning The presence and moving latents zt,presence,zt,moving exist in order to filter which slots get fit by the rMM. In order to achieve this selective routing of only active, moving slots, we introduce an auxiliary gate variable that is connected to the moving- and presence-latents via a multiplication factor that parameterizes a Bernoulli likelihood over the two values (‚ÄòON‚Äò and ‚ÄòOFF‚Äò) of the gate variable G(k) t: p(G(k) t|z(k) t,movingz(k) t,present) = Bernoulli\u0000 pgate\u0001 (21) where pgate=z(k) t,M,movingz(k) t,P,present This binary gate variable then modulates the input precision of the various likelihoods associated with the identity model (iMM), transition mixture model (tMM), and recurrent mixture model (rMM) to effectively ‚Äòmask‚Äô the learning of these models on untracked or absent slots. The end effect is that slots which are inferred to be moving andpresent keep full precision, while any other combination deflates the slot-specific input covariance to 0, removing the influence of their sufficient statistics from parameter learning. A.4 Interaction variable We also associate each object with a discrete latent variable z(k) t,interacting which indicates the type of the closest object interacting with the focal object (i.e., that indexed by k). In practice, we infer this interaction variable by finding the object whose position variable is closest to the focal slot, i.e. arg min j‚àànearest‚à•pj‚àípl‚à•, within some constrained set of ‚Äònearest‚Äô objects whose position latents are within a predefined interaction radius of the focal object (determined by a fixed parameter rmin). This interaction radius can be tuned on a game specific basis ‚Äì see the main text results in Section 3 for how fixing this parameter affects the results. We then perform inference on the identity model using the continuous features of that nearest-interacting slot:\u0002 c(j) t,e(j) t\u0003‚ä§. The inferred identity of the resulting jthslot is then converted into a one-hot vector representing the type of the ‚Äòinteracting‚Äô latent z(k) t,interacting, which can then be fed as input into the recurrent mixture model or rMM as described in the following section. A.5 Unused counter To keep track of how long a slot has remained inactive we introduce a non-negative-integer latent that is treated as a continuous variable in R:u(k) tor the ‚Äòunused counter‚Äô. This allows the model to predict the respawning of objects after they go off-screen. We couple the unused counter again to the proxy assignment-count variable o(k) tusing an exponentially‚Äìdecaying Bernoulli likelihood (identical in spirit to the EP‚Äìstyle presence likelihood): P\u0000 o(k) t= 1|u(k) t\u0001 = 1‚àíexp\b ‚àíŒæ e‚àíŒ≥uu(k) t, P\u0000 o(k) t= 0|u(k) t\u0001 = exp\b ‚àíŒæ e‚àíŒ≥uu(k) t, Œæ ‚àà(0,1], Œ≥u>0. (22) When u(k) t= 0the slot is present with probability 1‚àíe‚àíŒæ‚âÉŒæ(for typical Œæ‚â≥0.8). Each increment byŒΩumultiplies that probability by exp\u0000 ‚àíŒæ Œ≥uŒΩu\u0001, i.e. it decays roughly one e-fold per unused step when Œ≥u‚âÉŒΩ‚àí1 u. 17 A.6 Identity mixture model AXIOM uses an identity mixture model (iMM) to infer a discrete identity code z(k) typefor each object based on its continuous features. These identity codes are used to condition the inference of the recurrent mixture model used for dynamics prediction. Conditioning the dynamics on identity-codes in this way, rather than learning a separate dynamics model for each slot, allows AXIOM to use the same dynamics model across slots. This also enables the model to learn the same dynamics in a type- specific, rather than instance -specific, manner, and to remap identities when e.g., the environment is perturbed and colors change. Concretely, the iMM models the 5-D colors and shapes {c(k),e(k)}K k=1 across slots as a mixture of up to VGaussian components (object types). The slot-level assignment variable z(k) t,typeindicates which identity is assigned to the kthslot. The generative model for the iMM is (omitting the t‚àí1subscript from object latents): p(\u0002 c(k),e(k)\u0003‚ä§|z(k) type,¬µ1:V,type,Œ£1:V,type) =VY j=1N(¬µj,type,\u0010 G(k) t\u0011‚àí1 Œ£j,type)z(k) j,type (23) p(¬µj,type,Œ£‚àí1 j,type) =NIW(m0,j,type, Œ∫0,j,type,U0,j,type, n0,j,type) (24) p(z(k) type|œÄtype) = Cat( œÄtype), p(œÄtype) = Dir\u0000 1,..., 1|{z} V‚àí1times, Œ±0,type) (25) The\u0010 G(k) t\u0011‚àí1 Xnotation represents element-wise broadcasting of the reciprocal of G(k) tacross all elements of the matrix or vector X. When applied as a mask to a covariance matrix, for instance, the effect is that slots that are inferred to be moving and present do not have their covariance affected\u0010 G(k) t\u0011‚àí1 = 1, whereas those slots that are inferred to be either not present or not moving will ‚Äòinflate‚Äô the covariance of the mixture model, due to\u0010 G(k) t\u0011‚àí1 ‚Üí ‚àû. The same type of Categorical likelihood for the type assignments and truncated stick-breaking prior over the mixture weights is used to allow an arbitrary (up to a maximum of V) number of types to be used to explain the continuous slot features. We equip the prior over the component likelihood parameters with conjugate Normal Inverse Wishart (NIW) priors. A.7 Transition Mixture Model The dynamics of each slot are modelled as a mixture of linear functions of the slot‚Äôs own previous state. To stress the homology between this model and the other modules of AXIOM, we refer to this module as the transition mixture model or tMM, but this formulation is more commonly also known as a switching linear dynamical system or SLDS [ 29]. The tMM‚Äôs switch variable s(k) t,tmmselects a set of linear parameters Dl,blto describe the kthslot‚Äôs trajectory from ttot+ 1. Each linear system captures a distinct rigid motion pattern for a particular object: p(x(k) t|x(k) t‚Äì1,s(k) t,tmm,D1:L,b1:L) =LY l=1N(Dlx(k) t+bl,\u0010 G(k) t\u0011‚àí1 2I)s(k) t,l, tmm (26) p(s(k) t,tmm|œÄtmm) = Cat( œÄtmm), p(œÄtmm) = Dir\u0000 1,..., 1|{z} L‚àí1times, Œ±0,tmm) (27) (28) where we fix the covariance of all Lcomponents to be 2I, and all mixture likelihoods D1:L,b1:L to have uniform priors. Note that we gate the covariance once again using the reciprocal of G(k) tto filter the tMM to only model objects that are moving and present. The truncated stick-breaking prior Dir\u0000 1,..., 1, Œ±0,tmm)over the component mixing weights enables the number of linear modes Lto be dynamically adjusted to the data by growing the model with propensity Œ±0,tmm. Importantly, the L transition components of the tMM are not slot-dependent, but are shared and thus learned using the 18 data from all Kslot latents. The tMM can thus explain and predict the motion of different objects using a shared, expanding set of (up to Ldistinct) linear dynamical systems. A.8 Recurrent Mixture Model Therecurrent mixture model (rMM) is used to infer the switch states of the transition model directly from current slot-level features. This dependence of switch states on continuous features is the same construct used in the recurrent switching linear dynamical system or rSLDS [ 19]. However, in contrast to the rSLDS, which uses a discriminative mapping to infer the switch state from the continuous state (usually a softmax or stick-breaking parameterization thereof), the rMM recovers this dependence generatively using a mixture model over mixed continuous‚Äìdiscrete slot states [ 30]. In this way, ‚Äòselecting‚Äô the switch state used for conditioning the tMM actually emerges from inference over discrete latent variables, which have a particular conditional relationship (in this context, a joint mixture likelihood relationship) to other latent and observed variables. Concretely, the rMM models the distribution of continuous and discrete variables as a mixture model driven by another per-slot latent assignment variable s(k) rmm. The rMM defines a mixture likelihood over a tuple of continuous and discrete slot-specific information. We use the notation f(k) t‚Äì1to collect the continuous features that the rMM parameterizes a density over: they include a subset of the kthslot‚Äôs own continuous statex(k) t‚Äì1, as well as a nonlinear transformation gapplied to other slots‚Äò features, that computes the 2-D distance vector pointing from the inferred position of the focal object (i.e., slot k) to the nearest interacting slot j. We detail how these features are computed below: Continuous features for the rMM The continuous latent dimensions of x(k) t‚Äì1used for the rMM include the following: p(k) t‚Äì1,v(k) t‚Äì1, u(k) t‚Äì1. We represent extracting this subset as a sparse linear projection applied to the full continuous latents Cx(k) t‚Äì1. In addition, the rMM models the distribution of the 2-D vectors pointing from the focal object‚Äôs position (the slot kin consideration) to the position of the nearest interacting object, i.e. ‚àÜp(k) t‚Äì1‚â°p(j) t‚Äì1‚àíp(k) t‚Äì1. The nearest interacting object with index jis the one whose inferred position is the closest to the focal object‚Äôs, while only considering neighbors within some interaction zone with radius rmin. This is the same interaction distance used to compute nearest-neighbors when populating the z(k) interacting latent, see Appendix A.4 for details. If no object is within the interaction radius, then we set ‚àÜp(k) t‚Äì1to a random sample from a 2-D uniform distribution with mean\u0002 1.2,1.2\u0003 and lower/upper bounds of\u0002 1.198,1.198\u0003,\u0002 1.202,1.202\u0003. We summarize this computation with a generic nonlinear function applied to the latent states of all slots g(x(1:K) t‚Äì1), to intimate possible generalizations where different sorts of (possibly learnable) neighborhood relationships could be inserted here. Discrete features for the rMM The discrete features include the following: z(k) t‚àí1,type,z(k) t‚àí1,interacting, the assignment-count indicator variable o(k) t‚Äì1, the switch state of the transition mixture model s(k) t,tmm, the action at timestep t‚àí1and reward at the current timestep rt. We refer to this discrete collection as d(k) t‚Äì1. The inclusion of s(k) t,tmmin the discrete inputs to the rMM is a critical ingredient of this recurrent dynamics formulation ‚Äì it allows inference on this the switching state of the tMM to be driven by high-dimensional configurations of continuous and discrete variables relevant for predicting motion. rMM description The rMM assignment variable associated to a given slot is a binary vector s(k) t,rmm whose mthentry s(k) t,m, rmm‚àà {0,1}indicates whether component mexplains the current tuple of mixed continuous-discrete data. Each component likelihood selected by s(k) t,rmmfactorizes into a product of continuous (Gaussian) and discrete (Categorical) likelihoods. f(k) t‚Äì1=\u0010 Cx(k) t‚Äì1, g(x(1:K) t‚Äì1)\u0011, d(k) t‚Äì1=\u0010 z(k) t‚àí1,type,z(k) t‚àí1,interacting, o(k) t‚Äì1,s(k) t,tmm, at‚Äì1, rt\u0011 (29) 19 p(f(k) t‚Äì1, d(k) t‚Äì1|s(k) t,rmm) =MY m=1Ô£Æ Ô£∞N\u0000 f(k) t‚Äì1;¬µm,rmm,\u0010 G(k) t\u0011‚àí1 Œ£m,rmm\u0001Y iCat\u0000 dt‚Äì1, i;G(k) tam,i\u0001Ô£π Ô£ªst,m, rmm (30) p(¬µm,rmm,Œ£‚àí1 m,rmm) =NIW(m0,m,rmm, Œ∫0,m,rmm,U0,m,rmm, n0,m,rmm), p(am,i) = Dir( a0,m,i) (31) p(s(k) t,rmm|œÄrmm) = Cat( œÄrmm), p(œÄrmm) = Dir\u0000 1,..., 1|{z} M‚àí1times, Œ±0,rmm) (32) The parameters of the multivariate normal components are equipped with NIW priors and those of the discrete Categorical likelihoods with Dirichlet priors. As with all the other modules of AXIOM, we equip the mixing weights for s(k) t,rmmwith a truncated stick-breaking prior whose final Mthpseudocount parameter tunes the propensity to add new rMM components. Note also the use of the gate variable G(k) tto filter slots for dynamics learning by inflating the covariance associated with any slot inputs not inferred moving and present. Fixed distance variant. We explored a variant of the rMM ( fixed_distance ) where the displace- ment vector ‚àÜp(k) t‚Äì1is not returned by g(x(1:K) t‚àí1)and therefore not included as one of the continuous input features for the rMM. In this case, the entry of z(k) t‚Äì1,interacting that corresponds to the nearest interacting object is still determined by rmin, however. In this case, the choice of rminmatters more for performance because the rMM cannot learn to nuance its dynamic predictions based on precise distances. In general, this means that rminrequires more game-specific tuning. See Section 3 for the results of tuning rmincompared to learning it directly by providing ‚àÜp(k) t‚Äì1as input to the rMM. A.9 Variational inference and learning To perform inference and learning within our proposed model, we employ a variational Bayesian approach. The core idea is to approximate the true posterior distribution over latent variables and parameters with a more tractable factorized distribution, q(Z0:T,ÀúŒò). This is achieved by optimizing the variational free-energy functional, F(q), which establishes an upper-bound on the negative log-marginal likelihood of the observed data y0:T: F(q) =Eqh logq(Z0:T,ÀúŒò)‚àílogp(y0:T,Z0:T,ÀúŒò)i,such that F(q)‚â• ‚àílogp(y0:T).(33) Minimizing this functional F(q)with respect to qis equivalent to maximizing the Evidence Lower Bound (ELBO), thereby driving the approximate posterior q(Z0:T,ÀúŒò)to more closely resemble the true posterior p(Z0:T,ÀúŒò|y0:T). We assume a mean-field factorization for the approximate posterior, which decomposes over the global parameters ÀúŒòand the sequence of latent variables Z0:T: q(Z0:T,ÀúŒò) =q(ÀúŒò)TY t=0q(Zt), (34) where q(Zt)further factorizes across individual latent variables for frame t: q(Zt) =Ô£´ Ô£≠NY n=1q\u0000 zn t,smm\u0001Ô£∂ Ô£∏KY k=1\u0010 q\u0000 x(k) t\u0001 q\u0000 z(k) t\u0001 q\u0000 s(k) t\u0001\u0011. (35) The variational distribution over the global parameters ÀúŒòis also assumed to factorize according to the distinct components of our model: q(ÀúŒò) =q(ŒòsMM)q(ŒòiMM)q(ŒòtMM)q(ŒòrMM). (36) Note that the pixel-to-slot assignment variables zn t,smm are specific to each pixel nbut are not factorized across slots for a given pixel, as they represent a single categorical choice from Kslots. Other latent variables, such as the continuous state x(1:K) t and discrete attributes z(1:K) t,s(1:K) t, are factorized per slot k. 20 The inference and learning procedure for each new frame tinvolves an iterative alternation between an E-step, where local latent variable posteriors are updated, and an M-step, where global parameter posteriors are refined. E-step In the Expectation-step (E-step), we hold the variational posteriors of the global parameters q(Œò)fixed. We then update the variational posteriors for each local latent variable factor within q(Zt), such as q(zn t,smm),q(x(k) t),q(z(k) t), andq(s(k) t). These updates are derived by optimizing the ELBO with respect to each factor in turn and often result in closed-form coordinate-ascent updates due to conjugacy between the likelihood terms and the chosen forms of the variational distributions. M-step In the Maximization-step (M-step), we update the variational posteriors for the global parameters q(Œò¬µ)associated with each model component ¬µ‚àà {sMM,iMM,tMM,rMM}. Each q(Œò¬µ)is assumed to belong to the exponential family, characterized by natural parameters Œ∑¬µ: q(Œò¬µ) =h(Œò¬µ) exp\b Œ∑‚ä§ ¬µT(Œò¬µ)‚àíA(Œ∑¬µ), (37) where T(Œò¬µ)represents the sufficient statistics for Œò¬µ, and A(Œ∑¬µ)is the log-partition function (or log-normalizer). The M-step update proceeds in two stages for each component: 1.First, we compute the expected sufficient statistics bT¬µusing the current posteriors over the latent variables q(Zt)obtained from the t-th E-step: bT¬µ=Eq(Zt)\u0002 T\u0000 Œò¬µ,Zt\u0001\u0003. (38) These expected statistics are then combined with prior natural parameters Œ∑¬µ,0to form the target natural parameters for the update: bŒ∑¬µ=bT¬µ+Œ∑¬µ,0. 2.Second, we update the current natural parameters Œ∑(t‚àí1) ¬µ using a natural-gradient step, which acts as a stochastic update blending the previous parameters with the new target parameters, controlled by a learning rate schedule œÅt: Œ∑(t) ¬µ‚Üê(1‚àíœÅt)Œ∑(t‚àí1) ¬µ +œÅtbŒ∑¬µ, where 0< œÅt‚â§1. (39) Slot Mixture Model (sMM) The Slot Mixture Model (sMM) provides a likelihood for the observed pixel data ytby modeling each pixel as originating from one of Kobject slots. The variational approximation involves posteriors over pixel-to-slot assignments zn t,smm, slot mixing weights œÄsmm, slot-specific color variances œÉ(k) c,j, and the continuous latent states of slots x(k) t. Specifically, for each pixel nat timet, the posterior probability that it belongs to slot kisq(zn t,k,smm= 1) = rn t,k, ensuring thatPK k=1rn t,k= 1. The posterior over the slot-mixing probabilities œÄsmm is a Dirichlet distribution: q(œÄsmm) = Dir( œÄsmm|Œ±1,smm,..., Œ± K,smm), parameterized by concentrations Œ±k,smm>0. For each slot kand color channel j‚àà {r, g, b}, the posterior over the color variance œÉ(k) c,jis a Gamma distribution: q(œÉ(k) c,j) = Gamma( œÉ(k) c,j|Œ≥k,j, bk,j), with shape Œ≥k,jand rate bk,j. Finally, each slot‚Äôs continuous latent state x(k) t‚ààR10(encompassing position, color, velocity, shape, and the unused counter) is modeled by a Gaussian distribution: q(x(k) t) =N(x(k) t|¬µ(k) t,Œ£(k) t). The precision matrix is denoted Œõ = Œ£‚àí1, and the precision-weighted mean is h(k) t= Œõ(k) t¬µ(k) t. E-step Updates for sMM During the E-step for the sMM at frame t, the variational distributions for local latent variables zn t,smm (represented by the responsibilities rn t,k) andx(k) t(represented by ¬µ(k) t,Œ£(k) t) are updated, while the global sMM parameters q(ŒòsMM)are held fixed. 1.The pixel responsibilities rn t,k, representing q(zn t,k,sMM= 1), are updated using the standard mixture model update: rn t,k=exp\u0000 Eq[logœÄk,smm] +Eq[logN(yn t;Ax(k) t,Œ£(k))]\u0001 PK j=1exp\u0000 Eq[logœÄj,smm] +Eq[logN(yn t;Ax(j) t,Œ£(j))]\u0001. (40) 21 The per-slot observation covariance Œ£(k)is constructed as Œ£(k)= diag\u0000 BEq[x(k) t],Eq[œÉ(k) c]\u0001, consistent with the generative model where Bx(k)pro- vides variance related to shape and œÉ(k) cprovides color channel variances (see Equation (12) for the sMM likelihood equations). 2.The parameters of the Gaussian posterior q(x(k) t)are updated by incorporating evidence from the pixels assigned to slot k. This involves updating its natural parameters (precision Œõ(k) tand precision-adjusted mean h(k) t): Œõ(k) t= Œõ(k) t|t‚Äì1+NX n=1rn t,kA‚ä§\u0000 Œ£(k)\u0001‚àí1A, h(k) t=h(k) t|t‚Äì1+NX n=1rn t,kA‚ä§\u0000 Œ£(k)\u0001‚àí1yn t.(41) The terms Œõ(k) t|t‚Äì1andh(k) t|t‚Äì1are the natural parameters of the predictive distribution for x(k) t. The standard parameters are then Œ£(k) t=\u0010 Œõ(k) t\u0011‚àí1 and¬µ(k) t= Œ£(k) th(k) t. M-step Updates for sMM In the M-step, the global parameters of the sMM, which are the Dirichlet parameters Œ±k,smm for mixing weights and the Gamma parameters (Œ≥k,j, bk,j)for color variances, are updated. This begins by accumulating the expected sufficient statistics from the E-step: Nt,k=NX n=1rn t,k, Sy 1,t,k=NX n=1rn t,kyn t, Sy 2,t,k=NX n=1rn t,kyn t(yn t)‚ä§.(42) The Dirichlet concentration parameters are updated as (now using the tindex to represent the current vs. last settings of the posterior parameters): Œ±t,k,smm= (1‚àíœÅt)Œ±t‚Äì1,k,smm+œÅt(Œ±0,k,smm+Nt,k), Œ≥t,k,j= (1‚àíœÅt)Œ≥t‚Äì1,k,j+œÅt\u0012 Œ≥0,j+Nt,k 2\u0013, bt,k,j= (1‚àíœÅt)bt‚Äì1,k,j+œÅtÔ£´ Ô£≠1 +1 2NX n=1rn t,k(yn t,colorj‚àí(AEq[x(k) t])colorj)2Ô£∂ Ô£∏.(43) Here, Œ±0,k,smm represents the prior concentration for the Dirichlet distribution (e.g., 1for the first K‚àí1components and Œ±0,smmfor the K-th, if using a truncated stick-breaking prior). For the Gamma parameters, Œ≥0,jis the prior shape and 1is the prior rate (or related prior parameters). The projection matrices AandBare considered fixed and are not learned. Presence, Motion, and Unused Counter Dynamics The model includes latent variables for each slot kthat track its presence z(k) t,presence, motion z(k) t,moving, and an unused counter u(k) t. Inference over the presence latent The presence state is informed by an ‚Äòassignment-count-indicator‚Äô o(k) t. This indicator is set to 1if the slot is actively explaining pixels (e.g., if the sum of its responsibilitiesP nrn t,kexceeds a small threshold œµactive), and 0otherwise. 22 Recall the Bernoulli likelihood over o(k) tthat links it to the z(k) t,presence latent as follows (cf. Equa- tion (17)): p(ot|zt,presence ) =ozt,P, presence t (1‚àíot)zt,A, presence(44) There is an implied superscript kon both o(k) tand the presence variable z(k) t,P,presence, which are left out to avoid visual clutter. This linkage is incorporated into q(z(k) t,presence )using an Expectation Propagation (EP) style update via a pseudo-likelihood: Àú‚Ñì\u0000 z(k) t,presence\u0001 =h (o(k) t)z(k) t,P, presence\u0000 1‚àío(k) t\u0001z(k) t,A, presenceiŒ∂, (45) where Œ∂is a damping factor. This update increases posterior evidence for presence if o(k) t= 1, and for absence if o(k) t= 0. The first-order effect of this pseudo-likelihood when updating the posterior overz(k) t,presence is q(z(k) t,P,presence )‚âà(1‚àíŒ∂)q(z(k) t‚Äì1,P,presence ) +Œ∂ o(k) t. (46) Recall that the A, P subscripts refer to the indices of z(k) t‚Äì1,presence that signify the ‚Äòis-absent‚Äò and ‚Äòis-present‚Äô states, respectively. Inference over the moving latent Similar EP updates apply for inferring q(z(k) t,M, moving )based on its specific likelihoods involving velocity and o(k) t. The gate G(k) t=q(z(k) t,P,present = 1)¬∑q(z(k) t,M,moving = 1) is then formed from these inferred probabilities. Inference over the unused counter The ‚Äòunused counter‚Äô u(k) ttracks how long slot khas been inactive. Appendix A.5 of the full model details describes a generative likelihood P(o(k) t= 1| u(k) t) = 1‚àíexp(‚àíŒæe‚àíŒ≥uu(k) t), for which damped EP updates for q(u(k) t)can be derived and would remain in closed form. However, a specific case, by choosing hyperparameters such that a hard constraint o(k) t= 1‚áê‚áí u(k) t= 0 is effectively enforced (e.g., by taking Œ≥u‚Üí ‚àû andŒæ= 1 in the generative likelihood), leads to a simplified, deterministic update for the posterior mean ¬µ(k) t,u=Eq[u(k) t]: ¬µ(k) t,u=\u0000 1‚àío(k) t\u0001\u0000 ¬µ(k) t‚àí1,u+ŒΩu\u0001, ŒΩ u= 0.05. (47) In this simplified regime, the counter is reset to 0ifo(k) t= 1; otherwise, it increments by a fixed amount ŒΩu. Identity Mixture Model (iMM) The Identity Mixture Model (iMM) assigns one of Vpossible discrete identities to each active slot, based on its continuous features. This allows for shared characteristics and dynamics across instances of the same object type. The variational approximation targets posteriors over these slot- to-identity assignments z(k) t,type(which is a component of z(k) t), identity mixing weights œÄtype, and the parameters (¬µj,type,Œ£j,type)for each identity‚Äôs feature distribution. The features y(k) t,immutilized by the iMM for slot kare its color c(k) tand shape e(k) t, thusy(k) t,imm= [c(k) t,e(k) t]‚ä§. For each slot k at time twhere the activity gate G(k) t‚âà1, the posterior probability that it belongs to identity vis q(z(k) t,v,imm= 1) = Œ≥(k) t,v, satisfyingPV v=1Œ≥(k) t,v= 1. For slots where G(k) t‚âà0, these responsibilities are effectively null or uniform, contributing negligibly to parameter updates. The posterior over identity-mixing probabilities œÄ1:V,iMM is a Dirichlet distribution: q(œÄ1:V,type) = Dir( œÄ1:V,type| Œ±1,type,..., Œ± V,type), with Œ±v,type>0. Each identity vis characterized by a mean ¬µv,typeand covariance Œ£v,type. The variational posterior over these parameters is a Normal‚ÄìInverse-Wishart (NIW) distribution: q(¬µv,type,Œ£v,type) = NIW( ¬µv,type,Œ£v,type|mv,type, Œ∫v,type,Uv,type, nv,type). 23 E-step Updates for iMM In the E-step for the iMM, the local assignment probabilities Œ≥(k) t,vfor each slotkare updated. This update is primarily driven by slots where G(k) t‚âà1: Œ≥(k) t,v‚àùexp\u0000 Eq[logœÄv,type]\u0001 √óexp\u0000 Eq[logN(y(k) t,iMM;¬µv,type,Œ£v,type)]\u0001. (48) These responsibilities are normalized such thatPV v=1Œ≥(k) t,v= 1for each active slot. M-step Updates for iMM During the M-step, the global parameters of the iMM are updated. Suffi- cient statistics are accumulated, weighted by the gate G(k) tto ensure that only actively moving slots contribute significantly to the updates: Nt,v=KX k=1G(k) tŒ≥(k) t,v, S1,t,v=KX k=1G(k) tŒ≥(k) t,vEq[y(k) t,imm], S2,t,v=KX k=1G(k) tŒ≥(k) t,vEq[y(k) t,iMM(y(k) t,iMM)‚ä§].(49) The Dirichlet parameters Œ±t,v,typeare updated using Nt,vand prior parameters Œ±0,v,type(which, due to the stick-breaking priors are all 1‚Äôs except for the final count Œ±0,V,type): Œ±t,v,type= (1‚àíœÅt)Œ±t‚àí1,v,type+œÅt\u0000 Œ±0,v,type+Nt,v\u0001. (50) The NIW parameters (mt,v,type, Œ∫t,v,type,Ut,v,type, nt,v,type)are updated by blending their natural parameter representations. The target natural parameters bŒ∑NIW t,v are derived from the current sufficient statistics {Nt,v, S1,t,v, S2,t,v}and the NIW prior parameters: (NatParamsNIW t,v)‚Üê(1‚àíœÅt) (NatParamsNIW t‚àí1,v) +œÅtbŒ∑NIW t,v. (51) Recurrent Mixture Model (rMM) The Recurrent Mixture Model (rMM) provides a generative model for a collection of slot-specific features, and importantly, it furnishes the distribution over the switch state s(k) t,tmmthat governs the Transition Mixture Model (tMM). The rMM itself is a mixture model with Mcomponents, and its own slot-specific assignment variable is s(k) t,rmm. The variational factors include the posterior probability of assignment to rMM component m,q(s(k) t,m,rmm= 1) = œÅ(k) t,m(which sums to one over m), a Dirichlet posterior q(œÄrmm) = Dir( œÄrmm|Œ±1,rmm,..., Œ± M,rmm)for its mixing weights, NIW posteriors q(¬µm,Œ£m)for continuous features f(k) t‚Äì1modeled by each component m, and Dirichlet posteriors q(ai,m)for the parameters of categorical distributions over various discrete features d(k) ialso modeled by component m. These discrete features d(k) iencompass inputs like z(k) t‚Äì1,type, z(k) t‚Äì1,interacting, as well as the tMM switch state s(k) t,tmmwhich the rMM models generatively. E-step Updates for rMM In the rMM E-step, for each slot kconsidered active (i.e., G(k) t‚âà1), the responsibilities œÅ(k) t,mfor its Mcomponents are updated. These updates depend on the likelihood of the slot‚Äôs input features under each rMM component. The input features include continuous aspects f(k) t‚Äì1(a subset of x(k) t‚Äì1such as position and velocity, and interaction features like ‚àÜp(k) t‚Äì1) and a set of discrete features d(k) t‚Äì1,inputs (e.g., type from the previous step z(k) t‚Äì1,type). œÅ(k) t,m‚àùexp\u0000 Eq[logœÄm,rmm]\u0001 √óexp\u0000 Eq[logN(f(k) t‚Äì1;¬µm,rmm,Œ£m,rmm)]\u0001 √óY i‚ààinput discrete featuresexp\u0000 Eq[log Cat( d(k) t‚Äì1,i;ai,m)]\u0001.(52) 24 These responsibilities are normalized to sum to one for each active slot k. During rollouts used in planning, the predicted posterior distribution for the tMM switch state s(k) t,tmm is then determined as a mixture of the output distributions from the rMM components, weighted by œÅ(k) t,m: q\u0000 s(k) t,l,tmm= 1\u0001 =MX m=1œÅ(k) t,mEq\u0002 atmm _switch,m,l\u0003, where atmm _switch,m,lis the probability of tMM switch state lunder the learned parameters of rMM component m. M-step Updates for rMM In the rMM M-step, its global parameters are updated, with contributions from slots weighted by the gate G(k) t. The expected sufficient statistics are accumulated. For the mixing weights: Nt,m=KX k=1G(k) tœÅ(k) t,m. (53) For the continuous feature distributions (NIW parameters): Sf 1,t,m=KX k=1G(k) tœÅ(k) t,mEq[f(k) t‚Äì1], Sf 2,t,m=KX k=1G(k) tœÅ(k) t,mEq[f(k) t‚Äì1(f(k) t‚Äì1)‚ä§].(54) For each discrete feature dimodeled by the rMM (this includes input features d(k) t‚àí1,iand the output tMM switch state s(k) t,tmm), and its category ‚Ñì: Nt,m,i,‚Ñì =KX k=1G(k) tœÅ(k) t,m( I[d(k) t‚àí1,i=‚Ñì]ifdiis an input from t‚àí1 q(s(k) t,tmm=‚Ñì)ifdiiss(k) t,tmm. (55) The parameters are then updated using these statistics. Dirichlet parameters for rMM mixing weights Œ±t,m, rmm (from Nt,m), NIW parameters for continuous features (mt,m, rmm, Œ∫t,m, rmm,Ut,m, rmm, nt,m, rmm)(from Nt,m, Sf 1,t,m, Sf 2,t,m), and Dirichlet parameters at,i,m,‚Ñì for all discrete features (from Nt,m,i,‚Ñì ) are updated via natural gradient blending: Œ±t,m, rmm= (1‚àíœÅt)Œ±t‚Äì1,m,rmm+œÅt\u0000 Œ±0,m,rmm+Nt,m\u0001, (NatParamsNIW t,m)‚Üê(1‚àíœÅt) (NatParamsNIW t‚àí1,m) +œÅtbŒ∑NIW t,m, at,i,m,‚Ñì = (1‚àíœÅt)at‚àí1,i,m,‚Ñì +œÅt\u0000 a0,i,m,‚Ñì +Nt,m,i,‚Ñì\u0001.(56) Transition Mixture Model (tMM) The Transition Mixture Model (tMM) describes the dynamics of slot states x(k) tusing a mixture of L linear transitions. We approximate posteriors over transition assignments and mixing weights with variational factors. Transition responsibilities for slot kusing transition lareq(s(k) t,l,tmm= 1) = Œæ(k) t,l, satisfyingPL l=1Œæ(k) t,l= 1. The mixing-weight distribution over the Ltransitions œÄtmmisq(œÄtmm) = Dir(œÄtmm|Œ±1,tmm,..., Œ± L,tmm). E-step Updates In the tMM E-step, for each slot k, the responsibilities Œæ(k) t,lfor each transition lare updated based on how well that transition explains the observed change from x(k) t‚àí1tox(k) t: Œæ(k) t,l=exp\u0000 E[logœÄl]\u0001 N\u0000 x(k) t;Dlx(k) t‚àí1+bl,G(k) t‚Äì12I\u0001 PL u=1exp\u0000 E[logœÄu]\u0001 N\u0000 x(k) t;Dux(k) t‚àí1+bu,G(k) t‚Äì12I\u0001 forl= 1,..., L. The term G(k) t‚Äì1indicates if the slot was active at t‚àí1, and 2Iis the process noise covariance, assumed fixed or shared. 25 Algorithm 1 Mixture Model Expansion Algorithm Input Output Hyperparameters / Settings Y‚ààRN√ód: Matrix whose ithrow (i= 1,..., N )is ad-dimensional token (e.g., pixel).Œ∏‚àó 1:Kt+1: Updated posterior NIW parameters ( Kt+1 components where Kt+1‚â•Kt)œÑ: expansion threshold Œ∏‚àó 1:Kt= (m1:Kt, Œ∫1:Kt,U1:Kt, nKt): Initial posterior NIW parameters (Ktcomponents).E: maximum expansion steps 1:Initialise K‚ÜêKtand NIW parameters Œ∏k=\u0000 mk, Œ∫k,Uk, nk\u0001 fork= 1:K 2:forg= 1toEdo ‚ñ∑outer ‚Äúexpand-or-stop‚Äù loop // E-step 3: fori= 1toNdo 4: ‚Ñìik‚ÜêEq(Œ∏k)\u0002 logp(yi|Œ∏k)\u0003, k= 1:K 5: rik‚Üêexp(‚Ñìik).PK j=1exp(‚Ñìij) 6: ‚Ñìmax i‚Üêmax k‚â§K‚Ñìik, i= 1:N 7: ifmin i‚Ñìmax i> œÑ then 8: break ‚ñ∑all tokens well explained 9: i‚àó‚Üêarg min i‚Ñìmax i ‚ñ∑worst-explained token 10: K‚ÜêK+ 1 ‚ñ∑instantiate new component 11: Hard-assign ri‚àó,k‚Üê0 (k < K ),ri‚àó,K‚Üê1 12: Initialise component K:Œ∫K‚Üê1, ŒΩK‚Üêd+ 2, ¬µK‚Üêyi‚àó,Œ®K‚ÜêŒ®0 // M-step (natural‚Äìgradient update) 13: fork= 1toKdo 14: bŒ∑k‚ÜêNX i=1rikT(yi) |{z} bTk+Œ∑k,0 ‚ñ∑compute target natural parameters 15: Œ∑(t) k‚Üê(1‚àíœÅt)Œ∑(t‚àí1) k +œÅtbŒ∑k ‚ñ∑natural-gradient update with rate œÅt 16: Unpack Œ∑(t) k‚Üí(mk, Œ∫k, Uk, nk) ‚ñ∑recover NIW hyperparams 17:return Œ∏‚àó 1:Kt+118: M-step Updates The tMM does not use an explicit M-step. Instead, parameters are fixed to their initial values identified during the expansion algorithm. In other words, once we identify a new dynamics mode in the expansion algorithm, these parameters are added as a new component for the tMM and remain fixed. A.10 Bayesian model reduction Growing new clusters ensures plasticity, but left unchecked it leads to over-parameterisation and over-fitting. To enable generalization, every ‚àÜTBMR= 500 frames we therefore run Bayesian model reduction on the rMM, merging pairs of components whenever doing so increases the expected evidence lower bound (ELBO) of the multinomial distributions over the next reward and SLDS switch. The ELBO is computed with respect to generated data from the model through ancestral sampling. Given two candidate components k1, k2with posterior-sufficient statistics (Œ∑k1, Œ∑k2), their merged statistics are Œ∑k1‚à™k2=Œ∑k1+Œ∑k2‚àíŒ∑prior k2, ensuring that prior mass is not double-counted. Candidate pairs are proposed by a fast heuristic that (i) samples up to npair= 2000 used clusters, (ii) computes their mutual expected log-likelihood under the other‚Äôs parameters, and (iii) retains the highest-scoring pairs. Each proposal is accepted iffthe merged ELBO (line 7) is not smaller than the current ELBO (line 2). The procedure is spelled out in Algorithm 2. 26 Algorithm 2 Bayesian model reduction for the recurrent mixture model Input Output Hyper-parameters M: Posterior rMM Reduced model M‚Ä≤npair, nsamples pruning interval ‚àÜTBMR 1:D={(ci,di)}nsamples i=0‚àº M ‚ñ∑Draw nsamples pairs of continuous and discrete data samples from M 2:L(0)‚ÜêELBO( M,D) ‚ñ∑Compute current ELBO 3:P={(k1, k2)i}npairs i=0 ‚ñ∑Draw up to npaircandidate pairs by heuristic overlap 4:fors= 1to|P|do 5: (k1, k2)‚Üê P s 6: Mtry‚ÜêMERGE (M, k1, k2) 7: Ltry‚ÜêELBO( Mtry,D) 8: ifLtry‚â• L(s‚àí1)then 9: M ‚Üê MtryandL(s)‚Üê Ltry‚ñ∑Set current model to the candidate 10: else 11: L(s)‚Üê L(s‚àí1) 12:return M A.11 Planning with active inference In active inference, policies œÄ=a0:Hare selected that minimize expected Free Energy: p(œÄ) =œÉ(‚àíG(œÄ)),with G(œÄ) =HX œÑ=0‚àí\u0000 Eq(OœÑ|œÄ)[logp(rœÑ|OœÑ, œÄ)|{z } Utility‚àíDKL(q(Œ±rmm|OœÑ, œÄ)‚à•q(Œ±rmm))| {z } Information gain (IG)]\u0001, (57) withHthe planning horizon and œÉthe softmax function. However, as the number of possible policies grows exponentially with a larger planning horizon, enumerating all policies at every timestep becomes infeasible. Therefore, we draw inspiration from model predictive control (MPC) solutions such as Cross Entropy Method (CEM) and model predictive path integral (MPPI), which can be cast as approximating an action posterior by moment matching. In particular, we sample Ppolicies of horizon H, and evaluate their expected Free Energy G. Instead of sampling actions for each future timestep œÑuniformly, we maintain a horizon-wise categorical proposal p(aœÑ). After every planning step, we keep the top-k samples with minimum G, and importance weight to get a new probability for each action p(aœÑ)at future timestep œÑ: p(aœÑ) =X kexp(‚àíG(a(k) œÑ)) P jexp(‚àíG(a(j) œÑ))(58) Instead of doing multiple cross-entropy iterations per planning step, we maintain a moving average ofp(aœÑ). At every instant, the first action of the current best policy is actually executed by the agent. Our planning loop is hence composed of the following three stages (see Alg. 3). Sampling policies. For each iteration we draw P‚àíR|{z} CEM+R|{z} randompolicies where the first set is sampled i.i.d. from the proposal p(aœÑ)and the remaining Rare ‚Äúexploratory‚Äù sequences generated by a (smoothed) random walk. In addition, the previous best plan is always injected in slot 0 and the Aconstant action sequences occupy slots 1:Ato guarantee coverage. Evaluating a policy. Each policy is rolled forward Stimes through the world-model to obtain per-step predictions of reward ÀÜrœÑand information gain cIGœÑ, averaged over samples. We calculate an 27 Algorithm 3 Planning algorithm Require: current posterior state q, proposal p(aœÑ), best plan Àúa // Sample Pcandidate policies 1:Acem‚ÜêCat(p(aœÑ))‚äó(P‚àíR) 2:Arand‚ÜêRANDOM (R) 3:A‚Üê[Àúa,const 0:A‚àí1,Acem,Arand] // Evaluate 4:for all a(p)‚àà A do 5: (ÀÜr0:H‚àí1,cIG0:H‚àí1)‚ÜêRollout (q,a(p)) 6: G(p)‚ÜêP œÑŒ≥œÑ discount (ÀÜrœÑ+ŒªIGcIGœÑ) // Refit proposal 7:K‚Üê top-Kindices of ‚àíG(p) 8:forœÑ= 0toH‚àí1do 9: ÀÜp(aœÑ)‚Üêsoftmax\u0000 temperature‚àí1hist{a(p) œÑ}p‚ààK\u0001 10: p(aœÑ)‚ÜêŒ±smooth ÀÜp(aœÑ) + (1 ‚àíŒ±smooth)p(aœÑ) 11:return first action of best plan, updated p(aœÑ), best plan Àúa expected Free Energy G, where in addition, we weigh the information gain term with a scalar ŒªIGto trade off exploration and exploitation, as well as apply temporal discounting: G=1 SS‚àí1X s=0H‚àí1X œÑ=0Œ≥œÑ discount\u0000 ÀÜrs œÑ+ŒªIGcIGs œÑ\u0001, Œ≥ discount ‚àà[0,1), ŒªIG=info_gain weight. Proposal update. LetKbe the indices of the top- K=‚åätopk_ratio ¬∑P‚åãpolicies by (negative) expected Free Energy. For every horizon step œÑwe form the empirical action histogram of {ak,œÑ}k‚ààK, convert it to probabilities with a tempered softmax ( temperature ) and perform an exponential- moving-average update p(aœÑ)new=Œ±smooth p(aœÑ) + (1 ‚àíŒ±smooth)p(aœÑ)old, Œ± smooth =alpha. B Hyperparameters We list the hyperparameters used for main AXIOM results shown in Figure 3 in Table 3. For the fixed_distance ablations, we fixed rmin= 1.25for the games Explode, Bounce, Impact, Hunt, Gold, Fruits, and fixed rmin= 1.25for the games Jump, Drive, Cross, and Aviate. C Computational resources, costs and scaling AXIOM and baseline models were trained and evaluated on A100 40G GPUs. All models use a single A100 GPU per environment. AXIOM and BBF train a single environment to 10k steps in about 30min, whereas DreamerV3 trains in 2.5h. The corresponding per-step breakdown of average inference and planning times can be seen in Table 2. C.1 Planning and inference time For AXIOM, each time step during training can be broken down into planning and model inference. To quantify the planning time scaling we perform ablations over the number of planning policies (P). Since model inference correlates with the number of mixture model components, we evaluate the scaling using the environments Explode (few objects) and Cross (many objects). Figure 5 shows that the planning time scales linearly with the number of policies rolled out (left panel), and how model inference time scales with the number of mixture model components (right panel). D Gameworld 10k Environments In this section we provide an informal description of the proposed arcade-style environments, inspired by the Arcade learning environment [ 25]. To this end, our environments have an observation space 28 Table 3: Hyperparameters of the AXIOM agent trained to play all 10 games of Gameworld as reported in the main text (see Figure 3). Inference Hyperparameters Parameter Value œÑSMM(expansion threshold of the sMM) 5.7 œÑiMM(expansion threshold of the sMM) ‚àí1√ó102 œÑrMM(expansion threshold of the rMM) ‚àí1√ó101 œÑtMM(expansion threshold of the tMM) ‚àí1√ó10‚àí5 E(maximum number of expansion steps) 10 ‚àÜTBMR(timesteps to BMR) 500 Œ∂(exponent of the damped zpresence likelihood) 0.01 rmin 0.075 Planning Hyperparameters Parameter Value H(planning depth) 32 P(number of rollouts) 512 S(number of samples per rollout) 3 ŒªIG(information gain weight) 0.1 Œ≥discount (discount factor) 0.99 top-k ratio 0.1 random sample ratio 0.5 temperature 10.0 Œ±smooth 1.0 Generative Model Parameters Structure K(max sMM components) 32 V(max iMM components) 32 L(max tMM components) 500 M(max rMM components) 5000 Œª(from Œ∏moving ) 0.99 Œ≤(from Œ∏moving ) 0.01 Œ≥u ‚àû Œæ 1.0 ŒΩu 0.05 Dirichlet/Gamma priors Œ≥0,R,G,B 0.1 Œ±0,smm 1 Œ±0,imm 1√ó10‚àí4 Œ±0,tmm 0.1 Œ±0,rmm 0.1 Normal-Inverse‚ÄìWishart (1:V) m0,1:V,type 0 Œ∫0,1:V,type 1√ó10‚àí4 U0,1:V,type1 4I5 n0,1:V,type 11 Normal-Inverse‚ÄìWishart (1:M) m0,1:M,rmm 0 Œ∫0,1:M,rmm 1√ó10‚àí4 U0,1:M,rmm 625I7 n0,1:M,rmm 15 Component‚Äìwise Dirichlet priors (1:M) a0,1:M,type 1√ó10‚àí4 a0,1:M,interacting 1√ó10‚àí4 a0,1:M,o 1√ó10‚àí4 a0,1:M,tmm 1√ó10‚àí4 a0,1:M,action 1√ó10‚àí4 a0,1:M,reward 1.0 29 of shape 210√ó160√ó3, that corresponds to a RGB pixel arrays game screen. Agents interact via a set of 2to5discrete actions for movement or game-specific interactions. As is standard practice, positive rewards (+1) are awarded for achieving objectives, while negative rewards (-1) are given for failures. Here is a brief description of the games: Aviate. This environment puts the player in control of a bird, challenging them to navigate through a series of vertical pipes. The bird falls under gravity and can be made to jump by performing a \"flap\" action. The player‚Äôs objective is to guide the bird through the narrow horizontal gaps between the pipes without colliding with any part of the pipe structure or the top/bottom edges of the screen. Any collision with a pipe, or going out of screen at the top or bottom results in a negative reward and ends the game. Bounce. This environment simulates a simplified version of the classic game Pong, where the player controls a paddle to hit a ball against an AI-controlled opponent. The player has three discrete actions: move their paddle up, move it down, or keep it stationary, influencing the ball‚Äôs vertical trajectory upon contact. The objective is to score points by hitting the ball past the opponent‚Äôs paddle (reward +1), while preventing the opponent from doing the same (reward -1). The game is episodic, resetting once a point is scored by either side. Cross. Inspired by the classic Atari game Freeway, this environment tasks the player, represented as a yellow square, with crossing a multi-lane road without being hit by cars. The player has three discrete actions: move up, move down, or stay in place, controlling vertical movement across eight distinct lanes. Cars of varying colors and speeds continuously traverse these lanes horizontally, wrapping around the screen. The objective is to reach the top of the screen for a positive reward; however, colliding with any car resets the player to the bottom of the screen and incurs a negative reward. Driver. This environment simulates a lane-based driving game where the player controls a car from a top-down perspective, navigating a multi-lane road. The player can choose from three discrete actions: stay in the current position, move left, or move right, allowing for lane changes. The goal is to drive as far as possible, avoiding collisions with opponent cars that appear and move down the lanes at varying speeds. Colliding with another car results in a negative reward and ends the game. Explode. In this game inspired by the arcade classic Kaboom!, the player controls a horizontal bucket at the bottom of the screen, tasked with catching bombs dropped by a moving bomber. The player can choose from three discrete actions: remain stationary, move left, or move right, allowing for precise horizontal positioning to intercept falling projectiles. A bomber continuously traverses the top of the screen, periodically releasing bombs that accelerate as they fall towards the bottom. Successfully catching a bomb in the bucket yields a positive reward, whereas allowing a bomb to fall off-screen results in a negative reward. 200 400 P (planning policies)0.40.8Planning time (s)K 0 2 4 6 8 10 0123568910 K (sMM components)0.0120.018Inference time (s)Cross Explode Figure 5: Computational costs. Scaling of planning time as a function of the number of policies (left), and model inference time as a function of the number of sMM components (right). All times measured on a single A100 GPU. 30 Fruits. This game casts the player as a character who must collect falling fruits while dodging dangerous rocks. The player can perform one of three discrete actions: move left, move right, or stay in place, controlling horizontal movement at the bottom of the screen. Fruits of various colors fall from the top, granting a positive reward upon being caught in the player‚Äôs invisible basket. Conversely, falling rocks, represented as dark grey rectangles, will end the game and incur a negative reward if collected. Gold. In this game, the player controls a character, represented by a yellow square, from a top-down perspective, moving across a grassy field to collect gold coins and avoid dogs. The player can choose from five discrete actions: stay put, move up, move right, move down, or move left, enabling agile navigation across the screen. Gold coins are static collectibles that grant positive rewards upon contact, while dogs move dynamically across the screen, serving as obstacles that end the game and incur a negative reward if collided with. Hunt. This game features a character navigating a multi-lane environment, akin to a grid, from a top-down perspective. The player has four discrete actions available: move left, move right, move up, or move down, allowing full two-dimensional movement within the game area. The screen continuously presents a flow of items and obstacles moving horizontally across these lanes. The player‚Äôs goal is to collect beneficial items to earn positive rewards while deftly maneuvering to avoid contact with detrimental obstacles, which incur negative rewards, encouraging strategic pathfinding. Impact. This environment simulates the classic arcade game Breakout, where the player controls a horizontal paddle at the bottom of the screen to bounce a ball and destroy a wall of bricks. The player has three discrete actions: move the paddle left, move it right, or keep it stationary. The objective is to eliminate all the bricks by hitting them with the ball, earning a positive reward for each destroyed brick. If the ball goes past the paddle, the player incurs a negative reward and the game resets. The game ends when all bricks are destroyed. Jump. In this side-scrolling endless runner game, the player controls a character who continuously runs forward, encountering various obstacles. The player has two discrete actions: perform no action or initiate a jump allowing the character to avoid different types of obstacles. Colliding with an obstacle results in a negative reward and immediately resets the game. E Additional results and ablations E.1 Baseline performance on 100K Extending the wall-clock budget to 100 K interaction steps sharpens the contrast between model- based and model-free agents. On Hunt, DreamerV3 fails to make measurable progress over the entire horizon, remaining near its random-play baseline, whereas BBF continues to improve and ultimately attains a mean episodic return on par with the score our object-centric agent already reaches after only 10 K steps. In Gold both baselines do learn within 100 K steps, but their asymptotic performance still plateaus below the level our agent achieves in the much shorter 10 K-step regime (see Figure 6). 010K 100K Step0.0000.025Average Reward (1K)Gold 010K 100K Step0.0000.040Hunt BBF Dreamer V3 AXIOM Figure 6: 100K performance on Gold & Hunt. 31 0 10000-0.0100.000Average Reward (1K)Aviate 0 10000-0.0100.000Bounce 0 10000-0.0200.000Cross 0 10000-0.0060.000Drive 0 100000.0000.020Explode 0 10000 Step0.0000.020Average Reward (1K)Fruits 0 10000 Step0.0000.025Gold 0 10000 Step0.0000.025Hunt 0 10000 Step0.0000.040Impact 0 10000 Step-0.0100.000Jump AXIOM AXIOM (no IG) AXIOM (no BMR)Figure 7: Performance of AXIOM ablations. Average reward over the final 1,000 frames across 10 Gameworld 10K environments for three AXIOM variants: the full AXIOM model, a version without Bayesian Model Reduction (AXIOM (no BMR)), and a version excluding information gain during planning (AXIOM (no IG)). E.2 Ablations No information gain. When disabling the information gain, we obtain the purple curves in Figure 7. In general, at first glance there appears to be little impact of the information gain on most games. However, this is to be expected, as in Figure 4c we showed that e.g. for Explode, the information gain is only driving performance for the first few hundred steps, after which expected utility takes over. In terms of cumulative rewards, information gain is actually hurting performance on most games where interactions between player and object result in a negative reward. This is because these interaction events will be predicted as information-rich in the beginning, encouraging the agent to experience these multiple times. This is especially apparent in the Cross game, where the no-IG-ablated agent immediately decides not to attempt crossing the road at all after the first few collisions. Figure 8 visualizes the created rMM clusters, which illustrates how no information gain kills exploration in Cross. We hence believe that information gain will play a more important role in hard exploration tasks, which is an interesting direction for future research. No Bayesian Model Reduction. The orange curves in Figure 7 show the impact of disabling the Bayesian Model Reduction (BMR). BMR clearly has a crucial impact on both Gold and Hunt, which are the games where the player can move freely around the 2D area. In this case, BMR is able to generalize the dynamics and object interactions spatially by merging clusters together. The exception to this is once again Cross, where disabling BMR actually yields the best performing agent. This is again explained by the interplay with information gain. As BMR will merge similar clusters together, moving up without colliding will be assigned to a single, often visited cluster. This will render this cluster less informative from an information gain perspective, and the agent will be more attracted to collide with the different cars first. However, when disabling BMR, reaching each spatial location will get its own cluster, and the agent will be attracted to visit less frequently observed locations, like the top of the screen. This can also be seen qualitatively if we plot the resulting rMM clusters in Figure 8c. This begs the question on when to best schedule BMR during the course of learning. Clearly, BMR is crucial to generalize observed events to novel situations, but when done too early in learning, it can be detrimental for learning. Further investigating this interplay remains a topic for future work. Planning rollouts and samples. As we sample rollouts at each timestep during the planning phase, there is a clear tradeoff between the number of policies and rollout samples to collect in terms of computation time spent (see Figure 5) and the quality of the found plan. We performed a grid search, varying the number of rollouts [64,128,256,512] and number of samples per rollout [1,3,5], evaluating 3 seeds each. The results, shown in Figure 9 shows there are no significant performance differences, but more rollouts and drawing more than one sample seem to perform slightly better on 32 (a) AXIOM (b) AXIOM (no IG) (c) AXIOM (no BMR) Figure 8: Visualizations of the rMM clusters on Cross for information gain and BMR ablations. Each Gaussian cluster depicts a particular dynamics for a particular object type, colored by the object color, and the edge color of a nearby ‚Äúinteracting‚Äù object. (a) AXIOM has various small clusters for the player object (yellow) interacting with the colored cars in the various lanes, and elongated clusters that model the player dynamics of moving up or down. (b) Without information gain, the player collides with the bottom most cars, and then stops exploring because of the negative expected utility. (c) Without BMR, all player positions get small clusters assigned, which in this case helps the player to cross, as visiting these locations is now rendered information gaining. average. Therefore for our main evaluations we used 512 policies and 3 samples per policy, but the results in Figure 5 Figure 9 suggest that when compute time is limited, scaling the number of policies down to 128 or 64 is a viable way to increase efficiency without sacrificing performance. E.3 Perturbations Perturbations. One advantage of the Gameworld 10k benchmark is its ability to apply homoge- neous perturbations across environments, allowing us to quantify how robust different models are to changes in visual features. In our current experiments, we introduce two types of perturbations: a color perturbation, which alters the colors of all sprites and the background (see Figure 10b), and a shape perturbation, which transforms primitives from squares into circles and triangles (see Figure 10c). To assess model robustness, we apply each perturbation halfway through training (at 5,000 steps) and plot the average reward for Axiom, Dreamer, and BBF across each game in Figure 11. Under the shape perturbation, Axiom demonstrates resilience across games. We attribute this to the identity model ( iMM ), which successfully maps the new shapes onto existing identities despite their altered appearance. Under the color perturbation, however, Axiom‚Äôs performance often drops - suggesting the identity model initially treats the perturbed sprites as new objects - but then rapidly recovers as it reassigns those new identities to the previously learned dynamics. Our results also show that BBF and Dreamer are robust to shape changes. For the color perturbation, Dreamer - like Axiom - sometimes experiences a temporary performance decline (for example, in Explode) but then recovers. BBF, by contrast, appears unaffected by either perturbation. We hypothesize that this resilience stems from applying the perturbation early in training - before BBF has converged - so that altering visual features has minimal impact on its learning dynamics. Remapped slot identity perturbations In this perturbation, shown by the purple line in Figure 11, we performed a special type of perturbation to showcase the ‚Äòwhite-box‚Äô, interpretable nature of AXIOM‚Äôs world model. For this experiment, we performed a standard ‚Äòcolor perturbation‚Äô as described above, but after doing so, we encode knowledge about the unreliability of object color into AXIOM‚Äôs world model. Specifically, because the latent object features learned by AXIOM are directly interpretable as the colors of the objects in the frame, we can remove the influence of the latent dimensions corresponding to color from the inference step that extracts object identity (namely, the inference step of the iMM), and instead only use shape information to perform object 33 0 10000-0.0150.000Average Reward (1K)Aviate 0 10000-0.0100.000Bounce 0 10000-0.0150.000Cross 0 10000-0.0080.000Drive 0 100000.0000.025Explode 0 10000 Step0.0000.020Average Reward (1K)Fruits 0 10000 Step0.0000.020Gold 0 10000 Step0.0000.020Hunt 0 10000 Step0.0000.040Impact 0 10000 Step-0.0080.000Jump AXIOM (64 x 1) AXIOM (128 x 1) AXIOM (256 x 1) AXIOM (512 x 1)(a) 0 10000-0.0150.000Average Reward (1K)Aviate 0 10000-0.0150.000Bounce 0 10000-0.0200.000Cross 0 10000-0.0060.000Drive 0 100000.0000.025Explode 0 10000 Step0.0000.025Average Reward (1K)Fruits 0 10000 Step0.0000.020Gold 0 10000 Step0.0000.025Hunt 0 10000 Step0.0000.040Impact 0 10000 Step-0.0080.000Jump AXIOM (64 x 3) AXIOM (128 x 3) AXIOM (256 x 3) AXIOM (512 x 3) (b) 0 10000-0.0150.000Average Reward (1K)Aviate 0 10000-0.0100.0000.010Bounce 0 10000-0.0150.000Cross 0 10000-0.0080.000Drive 0 100000.0000.030Explode 0 10000 Step0.0000.020Average Reward (1K)Fruits 0 10000 Step0.0000.025Gold 0 10000 Step0.0000.025Hunt 0 10000 Step0.0000.040Impact 0 10000 Step-0.0080.000Jump AXIOM (64 x 5) AXIOM (128 x 5) AXIOM (256 x 5) AXIOM (512 x 5) (c) Figure 9: Ablation on the amount of sampled policies. The label indicates the number of policies √ónumber of samples for that policy (a) 1 Sample (b) 3 Samples (c) 5 Samples 34 (a) No perturbation. (b) Color perturbation (sprites and background recolored). (c) Shape perturbation (primitives replaced with circles and triangles). Figure 10: Perturbations. Sample frames from each of the ten environments under (a) no perturbation, (b) a color perturbation, and (c) a shape perturbation. type inference. In practice, what this means is that slots that changed colors don‚Äôt rapidly get assigned new identities, meaning the same identity-conditioned dynamics (clusters of the rMM) can be used to predict and explain the behavior of the same objects, despite their color having changed. This explains the absence of an effect of perturbation for some games when using this ‚Äòcolor remapping‚Äô trick at the time of perturbation, especially the ones where object identity can easily be inferred from shape, such as Explode. Figure 12 shows the iMM identity slots, with and without the ‚Äòremapping trick‚Äô. Impact on performance for all games is shown in 11d). For games where certain objects have the same shape (e.g., rewards and obstacles in Hunt, or fruits and rocks in Fruits), this remapping trick has no effect because shape information alone is not enough to infer object type and thus condition the dynamics on object types. In such cases, one might use more features to infer the object identity, such as position or dynamics, but extending our model to incorporate these to further improve robustness is left as future work. 35 Figure 11: Impact of perturbations on average reward. Smoothed 1k-step average rewards for Axiom, BBF, and Dreamer across ten games under (a) no perturbation, (b) color perturbation, (c) shape perturbation, and (d) Axiom‚Äôs color perturbation with and without remapping. 36 (a) no perturbation (b) color perturbation (c) color perturbation with remap Figure 12: iMM identity slots on Explode. (a) On Explode, the iMM constructs a slot for the player, bomber and bomb respectively. (b) When color is perturbed, novel slots are created for the blue (player) and the pink bomb, and the yellow enemy is mapped onto the old player slot. (c) However, with color remapping, the blue player, yellow bomber and pink bomb are correctly remapped to the old player and bomber slots based on their shape. F Related works Object-Centric World Models. The first breakthroughs in deep reinforcement learning, that leveraged deep Q networks to play Atari games [ 39], were not model-based, and required training on millions of images to reach human-level performance. To this end, recent works have leveraged model-based reinforcement learning, which learns world models and hence generalizes using fewer environment interactions [ 40,41]. A notable example is the Dreamer set of models, which relies on a mix of recurrent continuous and discrete state spaces to model the dynamics of the environment [ 36,42,43]. This class of world models simulates aspects of human cognition, such as intuitive understanding of physics and object tracking [ 5,7]. To this end, it is possible to add prior knowledge to this class of architectures, in a way that specific structures of the world are learned faster and better. For example, modeling interactions at the object level has shown promising performance in improving sample efficiency, generalization, and robustness across many tasks [9‚Äì12]. In recent years, the field of object segmentation has gained momentum thanks to the introduction of models like IODINE [ 44] and Slot Attention [ 45], which leverages the strengths and efficiency of self-attention to enforce competition between slot latents in explaining pixels in image data. The form of self-attention used in slot attention is closely-related to the E- and M-steps used to fit Gaussian mixture models [ 46,47], which inspired our, where AXIOM segments object from images using inference and learning of the Slot Mixture Model. Examples of improvements over this seminal work include Latent Slot Diffusion, which improves upon the original work using diffusion models and SlotSSM [ 48] which uses object-factorization not only as an inductive bias for image segmentation but also for video prediction. Recent works that have also proposed object-centric, model-based approaches are FOCUS, that confirms how such approaches help towards generalization in the low data regime for robot manipulation [ 49], and OC-STORM and SSWM, that use object-centric information to predict environment dynamics and rewards [ 14,50]. To conclude, SPARTAN proposes the use of a large transformer architecture that identifies sparse local causal models that accurately predict future object states [ 13]. Unlike OC-STORM, which uses pre-extracted object features using a pre-trained vision foundational model and segmentation masks, AXIOM learns to identify segment objects online without object-level supervision (albeit so far we have only tested AXIOM on simple objects like monochromatic polygons). AXIOM also grows and prunes its object-centric state-space online, but like OC-STORM plans using trajectories generated from its world model. Bayesian Inference. Inference, learning, and planning in our model are derived from the active inference framework, that allows us to integrate Bayesian principles with reinforcement learning, balancing reward maximization with information gain by minimizing expected free energy [15, 16]. To learn the structure of the environment, we drawn inspiration from fast structure learning methods, that first add mixture components to the model [ 51] and then prunes them using Bayesian model reduction [ 21,22,24]. Our approach to temporal mixture modeling shares conceptual similarities with recent work on structure-learning Gaussian mixture models that adaptively determine the number of components for perception and transition modeling in reinforcement learning contexts [ 52]. An important distinction between AXIOM‚Äôs model and the original fast structure learning approach [ 23], is that AXIOM uses more structured priors (in the form of the object-centric factorization of the sMM and the piecewise linear tMM), and uses continuous mixture model likelihoods, rather than purely discrete ones. The transition mixture model we use is a type of truncated infinite switching linear dynamical system (SLDS)[ 29,53,54]. In particular, we rely on a recent formulation called the recurrent SLDS [ 19], that introduces dependence of the switch state on the continuous state, to address two key limitations of the standard SLDS: state-independent transitions and context-blind dynamics. Our innovation is in how we handle the recurrent connection of the rSLDS: we do this 37 using a generative, as opposed to discriminative, model for the switching states. This allows for more flexible conditioning of the switch state on various information sources (both continuous and discrete), as well as a switch dependence that is quadratic in the continuous features; this overcomes the intrinsic linear separability assumptions made by using a classic softmax likelihood over the switch state, as used in the original rSLDS formulation [19, 55]. 38",
  "text_length": 120963
}