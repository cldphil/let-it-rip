{
  "id": "http://arxiv.org/abs/2506.00925v1",
  "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree\n  Search",
  "summary": "Designing protein sequences that fold into a target 3D structure, known as\nprotein inverse folding, is a fundamental challenge in protein engineering.\nWhile recent deep learning methods have achieved impressive performance by\nrecovering native sequences, they often overlook the one-to-many nature of the\nproblem: multiple diverse sequences can fold into the same structure. This\nmotivates the need for a generative model capable of designing diverse\nsequences while preserving structural consistency. To address this trade-off,\nwe introduce ProtInvTree, the first reward-guided tree-search framework for\nprotein inverse folding. ProtInvTree reformulates sequence generation as a\ndeliberate, step-wise decision-making process, enabling the exploration of\nmultiple design paths and exploitation of promising candidates through\nself-evaluation, lookahead, and backtracking. We propose a two-stage\nfocus-and-grounding action mechanism that decouples position selection and\nresidue generation. To efficiently evaluate intermediate states, we introduce a\njumpy denoising strategy that avoids full rollouts. Built upon pretrained\nprotein language models, ProtInvTree supports flexible test-time scaling by\nexpanding the search depth and breadth without retraining. Empirically,\nProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,\ngenerating structurally consistent yet diverse sequences, including those far\nfrom the native ground truth.",
  "authors": [
    "Mengdi Liu",
    "Xiaoxue Cheng",
    "Zhangyang Gao",
    "Hong Chang",
    "Cheng Tan",
    "Shiguang Shan",
    "Xilin Chen"
  ],
  "published": "2025-06-01T09:34:20Z",
  "updated": "2025-06-01T09:34:20Z",
  "categories": [
    "q-bio.BM",
    "cs.CV",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00925v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00925v1  [q-bio.BM]  1 Jun 2025ProtInvTree: Deliberate Protein Inverse Folding\nwith Reward-guided Tree Search\nMengdi Liu1,2, Xiaoxue Cheng4, Zhangyang Gao3, Hong Chang1,2∗,\nCheng Tan3, Shiguang Shan1,2, Xilin Chen1,2\n1Institute of Computing Technology, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n3AI Lab, Research Center for Industries of the Future, Westlake University\n4Gaoling School of Artificial Intelligence, Renmin University of China\n{liumengdi23z, changhong, sgshan, xlchen}@ict.ac.cn ,\nchengxiaoxue@ruc.edu.cn ,{gaozhangyang, tancheng}@westlake.edu.cn\nAbstract\nDesigning protein sequences that fold into a target 3D structure—known as protein\ninverse folding—is a fundamental challenge in protein engineering. While recent\ndeep learning methods have achieved impressive performance by recovering native\nsequences, they often overlook the one-to-many nature of the problem: multiple\ndiverse sequences can fold into the same structure. This motivates the need for a\ngenerative model capable of designing diverse sequences while preserving struc-\ntural consistency. To address this trade-off, we introduce ProtInvTree , the first\nreward-guided tree-search framework for protein inverse folding. ProtInvTree re-\nformulates sequence generation as a deliberate, step-wise decision-making process ,\nenabling the exploration of multiple design paths and exploitation of promising\ncandidates through self-evaluation, lookahead, and backtracking. We propose a\ntwo-stage focus-and-grounding action mechanism that decouples position selection\nand residue generation. To efficiently evaluate intermediate states, we introduce a\njumpy denoising strategy that avoids full rollouts. Built upon pretrained protein\nlanguage models, ProtInvTree supports flexible test-time scaling by expanding\nthe search depth and breadth without retraining. Empirically, ProtInvTree outper-\nforms state-of-the-art baselines across multiple benchmarks, generating structurally\nconsistent yet diverse sequences, including those far from the native ground truth.\n1 Introduction\nProteins are 3D folded linear chains of amino acids that perform essential biological functions, such as\nmetabolic control, transmitting signals, and regulating cellular processes [ 18,4]. Designing sequences\nof amino acids that fold into a desired protein structure, also known as protein “inverse folding”\n(IF) [ 45], is a crucial task with great potential for protein engineering and synthetic biology [ 24,49,6].\nRecent deep learning approaches typically recover the native sequence conditioned on a target\nstructure through the following three paradigms: autoregressive generation , which models sequence\ndependencies step-by-step [ 19,20,36,17,8];one-shot prediction , which directly maps structure to\nsequence in a single forward operation [ 11,27,13]; and iterative refinement , which progressively\nimproves an initial design through multiple passes [ 48,9,10,50]. Despite of achieving impressive\nrecovery performance, these methods often overlooked the inherently one-to-many nature of the\nproblem [ 31,28,14], where multiple distinct amino acid sequences are capable of folding into the\nsame protein backbone structure. As a result, rather than predicting a single native sequence, it is\noften desirable to generate a diverse set of sequences while preserving structural consistency.\n∗Corresponding author\nPreprint. Under review.\n--- Page 2 ---\n...\n1...\n(a)One-shot\ngeneration(b) Iterative\nrefinement(c) Reward-guided\nSearch Tree...\n2 3...A\nSelected\nStates\nCandidate\nSequencesInitial State\nOne State\n1\n2\n3Sequence Spacesc-TMScore with\ntarget StructureGround Truth\nSequenceB\n12 3123Candidate\nSequencesFigure 1: (A)Schematic illustration on various approaches of structure-based protein design. Each\nnode denotes an intermediate sequence conditioned on the target structure, progressively expanding\ntoward full generation. See the complete framework and details in Figures 2. (B)Landscape of\nscTMscore across the sequence space with respect to the target structure. The ground truth sequence\nis marked, and candidate sequences are distributed across multiple local optima.\nThis goal, however, reveals an inherent trade-off: while diversity prefers broad exploration of the\nsequence space, structural consistency strictly requires a feasible subspace that has good local residue\ncompatibility and global foldability. To address this challenge, we advocate for a deliberate, step-wise\ndesign process that progressively explores the solution space of feasible sequences. Inspired by\nthe dual process theory in cognitive science [ 22,35,33], where System 1 is characterized by fast,\nautomatic, and heuristic-driven responses, while System 2 involves slow, deliberate, and analytical\nreasoning, we propose to model the protein design process as a deliberate and iterative decision-\nmaking process that should (1) explore multiple alternatives at each design step rather than one single\ncandidate, (2) dynamically assess and revise each design step through lookahead and backtracking\nto optimize the overall designed sequence, and (3) maintain a structured decision history to support\neffective credit assignment and multi-step planning.\nTo this end, we propose ProtInvTree , a training-free framework for structure-based protein design\nthat formalizes the design process as a sequence of branching decisions and leverages Monte Carlo\nTree Search (MCTS) during generation. Specifically, we iteratively perform the design process,\nsampling multiple decisions at each step and looking ahead to compute reward signals that evaluate\nthe quality of current choices, thereby guiding the overall sequence design. At each decision step, we\nintroduce a two-stage focus-and-grounding action mechanism that first selects the positions in the\nsequence to modify ( focus ) and then generates new residues at these positions ( grounding ). Moreover,\nwe employ fast, jumpy denoising as an evaluation mechanism, efficiently estimating trajectory quality\nwithout costly forward model rollouts. Through these designs, ProtInvTree is capable of making\nglobally optimal decisions rather than settling for locally optimal ones, which allows it to design\nnovel yet plausible sequences that may deviate significantly from the native sequence (as shown in\nFig. 1). Additionally, the explicit exploration of design paths may offer potential insights into the\ninterpretability of protein sequences. To our knowledge, this is the first work to apply a tree-search\nframework to structure-based protein sequence design.\nEmpirically, we comprehensively evaluate ProtInvTree across fixed-backbone and de novo protein\ndesign tasks. We demonstrate that ProtInvTree outperforms state-of-the-art baselines and excels in\nthe design of plausible proteins with high structural consistency. Besides, it achieves Pareto-optimal\ntrade-offs in both the scTMscore-diversity and scTMscore-novelty. Notably, we observe that existing\napproaches aggressively optimizing for sequence recovery achieve limited novelty at the same sc-\nTMscore level. Further analyses reveal that increasing planning depth and expansion width can\neffectively improve structural consistency, demonstrating that the paradigm of test-time scaling can\neffectively unlock the potential of pretrained protein language models (PLMs).\nIn summary, our contributions are as follows:\n•We propose ProtInvTree , the first test-time reward-guided tree search framework for protein\ninverse folding. It formulates protein design as a deliberate, step-wise decision process, enabling\nexploration of multiple trajectories and exploitation of promising candidates.\n•We introduce a two-stage focus-and-grounding mechanism decoupling position selection and\nresidue generation, and a fast, jumpy denoising strategy for efficient reward evaluation.\n2\n--- Page 3 ---\n•We demonstrate that ProtInvTree achieves state-of-the-art performance across multiple bench-\nmarks, with a test-time scaling capability that improves both structural consistency and sequence\ndiversity without retraining or finetuning.\n2 Related Works\n2.1 Protein Inverse Folding\nRecently, AI algorithms have spurred a major revolution in modeling protein inverse folding, enabling\naccurate sequence design conditioned on target structures. Existing approaches can be broadly\ncategorized into the following three paradigms based on their generation strategies.\nAutoregressive models generate sequences residue-by-residue, conditioned on both the 3D structure\nand previously generated tokens. Pioneering models like GraphTrans [ 19] and GVP [ 20] introduced\nSE(3)-invariant graph encoders with attention or geometric modules. Later, models such as GCA [ 36],\nESM-IF [17], and ProteinMPNN [8] incorporated global context and fine-grained pairwise distance\nmodeling. These models offer accurate recovery but suffer from slow inference on long sequences.\nOne-shot models bypass iterative steps by directly predicting full sequences from structure. Pi-\nFold [ 11] introduced an efficient graph encoder with an MLP decoder, achieving significant speedups\nand improved accuracy on long proteins. Uni-IF [ 13] generalizes this to multiple molecule types.\nThese models are highly efficient but face challenges in maintaining global structural consistency.\nIterative refinement methods address this by first generating a full sequence and then improving it\nthrough multiple steps. AlphaDesign [ 10] and LMDesign [ 48] use confidence-aware predictors and\npretrained sequence models for guided refinement. KWDesign [ 9] combines sequence and structure\npretraining with an uncertainty-aware update mechanism. Recent works such as BridegIF [ 50]\nand GraDe-IF [ 44] apply diffusion to enhance diversity and structural compatibility. Fast non-\nautoregressive diffusion models like PMPnnDiff [ 42] accelerate inference while preserving accuracy.\nDespite notable advances in protein inverse folding, most efforts focus on training-time improve-\nments, while the inference phase remains underexplored. As large protein foundation models emerge,\nharnessing test-time computation to boost sequence quality and diversity becomes crucial for in-\ncentivizing their full potentials. To this end, we propose a novel paradigm based on tree-structured\ngeneration, which departs fundamentally from the three existing categories of approaches.\n2.2 Test-time Scaling and MCTS\nTest-time scaling refers to increasing computational resources during inference to enhance model\noutput without modifying its parameters. This approach has gained significant attention in the field\nof large language models (LLMs), where performance is improved by generating multiple samples\nand using reward models for best-solution selection [ 34,41,3]. Various test-time search methods\nhave been proposed [ 23,38], including random sampling [ 39], self-consistency, and tree-search\nmethods [ 43,15,46,30]. Among them, Monte Carlo Tree Search (MCTS), a heuristic search\nalgorithm designed for decision-making tasks [ 7,32], has emerged as a powerful technique for\nstructured exploration in the output space of large language models. It enables deliberate reasoning\nby simulating multiple generation trajectories, selectively expanding promising paths, and integrating\nreward feedback to guide inference toward high-quality outputs. Inspired by these advances, we are\nthe first to extend the paradigm of test-time scaling to protein language models (PLMs). Our proposed\nframework, ProtInvTree, leverages reward-guided tree search to perform deliberate, step-wise protein\nsequence generation, enabling test-time scaling for improved structural consistency and diversity.\n3 Preliminaries\nProblem Definition. The protein inverse folding problem seeks to determine the amino acid\nsequence xthat folds into a given target structure c. Here, x= [x1, x2, . . . , x L]represents the\nsequence of Lresidues, where xi∈ {1,2, . . . , 20}denotes the type of the i-th residue. The structure\nc= [c1, c2, . . . , c n]∈Rn×4×3specifies the Cartesian coordinates of the backbone atoms (N, C- α,\nC, and optionally O) for each residue ci. The inverse folding problem can be formally expressed as:\nfθ:c→x, (1)\n3\n--- Page 4 ---\nwhere θis the learnable parameter. Given that homologous proteins invariably exhibit similar\nstructures, the solution for a given structure is not unique [ 14]. Hence, an ideal model should be\ncapable of learning the underlying mapping from protein backbone structures to their corresponding\nsequence distributions pθ(x|c).\nIterative Denoising. Recent advancements in diffusion and iterative refinement models [ 10,48,9,\n50] have achieved notable success in protein inverse folding. These methods formulate the task as an\niterative denoising process that refines the sequence step by step. Following this paradigm, we adopt\nthis strategy to progressively construct the sequence. Formally, starting from an initially corrupted\nsequence x0, the model iteratively denoises the sequence into a complete design xTthrough a series\nof conditional reverse transitions, where x0andxTdiffer from the definition in diffusion modeling:\npθ(xT|x0,c) =TY\nt=1pθ(xt+1|xt,c), (2)\nwhere xtrepresents the intermediate sequence at step t, with a subset of amino acids remaining\nunfilled (e.g., represented by [MASK] tokens), and cdenotes the target backbone structure. Each\nreverse step pθ(xt+1|xt,c)refines the current sequence while preserving the structural context.\n4 ProtInvTree: Deliberate Protein Inverse Folding Framework\nIn this section, we propose a reward-guided tree-search framework for deliberate protein inverse\nfolding. We first formulate the iterative denoising as a tree-based Markov decision process (MPD),\nenabling structured exploration over multiple trajectories (Section 4.1). Then we describe the Monte\nCarlo Tree Search (MCTS) procedure employed to identify diverse and high-quality sequences that\nare consistent with the target backbone structure (Section 4.2). Finally, we introduce two designs of\nthe action and reward components (Sections 4.3 and 4.4), which define how the sequence is updated\nat each step and how intermediate states are evaluated during the search process. We present the\ndetailed algorithm to formalize the entire framework in the appendix.\n4.1 Tree-based MDP Formulation\nAs described in Section 3, while the step-wise denoising process is effective, it lacks the ability to\nincorporate intermediate feedback, track uncertainty, and revise previous decisions. To overcome\nthese limitations, we reformulate the iterative denoising process as a tree-based Markov decision\nprocess for structured, feedback-aware generation. In this tree structure, each node represents a\nstatest, each branch corresponds to an action at, and each node is assigned a value that reflects the\nreward rtat that state. Specifically, we define the concepts in the tree search framework as follows:\nst≜(c,xt),at≜{(ik, xik)}Kt\nk=1, r t≜R(st,at), π (at|st)≜pθ(xt+1|xt,c).\nHere, the state stconsists of the target backbone structure cand a partially generated sequence xt.\nThe action atcorresponds to the selection of positions in the sequence and modification of new\nresidues, as detailed in Section 4.3. The reward rtis computed by a reward function R(st,at),\nwhich evaluates the structural consistency of the modified sequence, as described in Section 4.4. The\npolicy model π(at|st)generates the next partial sequence xt+1based on the current state st. It\nis parameterized by a structurally modulated Protein Language Model (PLM). A trajectory in the\nmulti-step Markov decision process is defined as a sequence of state-action-reward transitions:\nτ= [(s0,a0, r0),(s1,a1, r1), . . . , (sT,aT, rT)],\nwhere each transition corresponds to an incremental update of the sequence. By reformulating the\nsequence design process from a linear chain into a tree structure, our framework enables deliberate\nplanning over multiple generation trajectories, facilitates the incorporation of intermediate feedback\nfrom structural evaluations, and supports systematic revision of prior design decisions.\n4.2 Reward-guided Tree Search\nIn our approach, the reward-guided tree search process operates as an iterative procedure. As illus-\ntrated in Figure 2, it comprises four key steps: selection, expansion, evaluation, and backpropagation.\nThe details of each step are described as below.\n4\n--- Page 5 ---\nFocus[M] [M] [M] [M] [M] [M]\n[M]A\nA[M]NY\nQAVNYL[M][M] [M] [M] [M]\n[M]Denoising\n(1) Selection (2) ExpansionFast Jumping\n(4) BackpropagationFocus and \nGrounding\n(3)EvaluationSelected \nnode\nValue UpdatingStepwise Denoising Protein Sequence\nFast Jumping\n(a) Reward-guided Tree Search Framework (b) Step-wise Protein Design[M]:mask\nGroundingstate\naction\nget rewardFigure 2: The framework of ProtInvTree. (a) The four steps of reward-guided tree search— Selection,\nExpansion, Evalution, and Backpropagation —are illustrated on a partial denoising tree. Each node\ncorresponds to a partially denoised subsequence. After a new node is expanded, “jumpy” denoising\nis performed to quickly estimate its value, which is then backpropagated along the path in the tree.\n(b) Illustration of how a sequence is generated step by step. Masked tokens in the sequence are\nprogressively infilling through a focus-and-grounding mechanism.\nSelection. The selection process begins at the root node s0and identifies the leaf node with\nthe highest exploration potential, determined by the UCT (Upper Confidence Bounds applied to\nTrees) [25] score. The UCT score is computed as follows:\nUCT (st) =V(st) +ws\nlnN(p)\nN(st), (3)\nwhere wis a hyperparameter that balances exploitation (i.e., node value V(st)) and exploration (i.e.,\nvisit count N(st)), and pdenotes the parent node of st.\nExpansion. After selecting the node with the highest UCT score, it is expanded by generating\nmultiple child nodes. Conditioned on the current state st, which consists of the target structure cand\ncurrent sequence xt, the policy model samples Kcandidate sequences for the next step:\n{x(k)\nt+1}K\nk=1∼πθ(at|st)≜pθ(xt+1|xt,c). (4)\nEach candidate sequence x(k)\nt+1constitutes a new child state s(k)\nt+1= (c,x(k)\nt+1), which is added to the\nsearch tree as an expansion of the selected node. The details of the candidate construction process by\npolicy model are described in Section 4.3.\nEvaluation. Each expanded node is evaluated to determine its value V(st+1). As described in\nEquation 12, we first perform rollouts that complete the state st+1via sampling mfully generated\nsequences, and then assess them with a reward model, assigning the average reward rt+1as the node\nvalue V(st+1). The details of the reward function and evaluation process are provided in Section 4.4.\nBackpropagation. After evaluating the expanded nodes, their values are backpropagated along the\ntraversal path to update the visit counts and value scores of the ancestor nodes sj(0≤j≤t). The\nupdates are performed using the following equations:\nNnew(sj) =Nold(sj) + 1, (5)\nVnew(sj) =Vold(sj)Nold(sj) +rt+1\nNnew(sj), (6)\nwhere Nold(sj)andVold(sj)represent the previous visit count and value score of node sj, respectively,\nandrt+1is the reward obtained during the evaluation step.\nThe four stages described above are performed iteratively until the terminal state is reached. We define\ntwo termination conditions for MCTS as follows: (1) The maximum number of MCTS iterations,\nM, is reached. (2) A terminal node is encountered with a reward exceeding a predefined threshold,\nindicating strong structural consistency and high-quality design. Once the tree search is complete,\nthe optimal path is selected greedily by prioritizing nodes with the highest scores.\n5\n--- Page 6 ---\n4.3 Focus-and-Grounding Action\nTo generate candidate transitions from each intermediate state st, we propose a two-stage Focus-\nand-Grounding action mechanism (see illustration in Fig. 2b). At each denoising step, the model\nexplicitly decomposes the sequence updating process into identifying where to modify (Focus) and\ndetermining what token to generate at the selected position (Grounding).\nFormally, the Focus operation F(·)defines a position selection distribution pϕ(i|st)over all\npositions, from which the top- Ktpositions with the highest probabilities are selected:\nF(st) = argsorti∈{1,...,L}(pϕ(i|st), Kt), (7)\nwhere Ldenotes the sequence length and iindicates the targeted position for refinement. Conditioned\non the focused positions {i1, . . . , i N}, the Grounding operation defines a distribution over amino\nacid types, specifying the generated token:\nG(st, i) =pψ(xi|st, i), i∈ F(st), (8)\nwhere each pψ(xi|st, i)predicts the residue xi∈ V for position i, andVdenotes the amino acid\nvocabulary. The overall policy is factorized as the product of the Focus and Grounding distributions:\nπθ(at|st) =Y\ni∈F(st)pϕ(i|st)·pψ(xi|st, i), (9)\nIn practice, the selected position set {i1, . . . , i N}is a random subset of sequence positions (more\nselection strategies comparison is provided in appendix), and each token xiis generated by a struc-\nturally modulated Protein Language Model (PLM) conditioned on the backbone structure cand the\npartial sequence context. This two-stage action design enables precise localization of modifications,\nensuring structural coherence and enhancing search efficiency throughout the generation process.\n4.4 Jumpy Denoising for Fast Reward\nIn the MCTS procedure, evaluating a node far from a leaf node is challenging, as the intermediate\nnodes are not fully expanded. This is typically addressed in one of two ways: employing forward\ndynamics models to simulate complete trajectories, which is computationally expensive, or approx-\nimating node values via bootstrapping methods, which are faster but less accurate. Effectively\nintegrating these evaluation strategies into ProtInvTree remains an open challenge.\nTo address this, we introduce a Jumpy Denoising strategy to accelerate the evaluation process, which\nis a rapid, single-step DDIM-based sampling process:\n˜xT∼ J(xt+1,c), (10)\nwhere J(·)approximates the reverse denoising distribution p(xT|xt+1,c). Here, xt+1is obtained\nthrough action atat step t. We define the reward function R(st,at)as the structural consistency\nfeedback obtained by comparing the folding results from the sampled sequence ˜xTand the input\nstructure c, formulated as:\nR(st,at) =TMScore (f(˜xT),c), (11)\nwhere fis the protein folding algorithm. TMScore (·,·)is a widely used metric for measuring protein\nstructure similarity. This jumpy denoising strategy significantly reduces computational overhead\nwhile maintaining a reliable approximation of the final reward.\n5 Experiments\n5.1 Experimental Setup\nDatasets. We conduct experiments on both CATH v4.2 andCATH v4.3 [29], where proteins\nare categorized based on the CATH hierarchical classification of protein structure, to ensure a\ncomprehensive analysis. Following the standard data splitting [ 19,17], CATH v4.2 dataset consists\nof 18,024 proteins for training, 608 proteins for validation, and 1,120 proteins for testing; CATH v4.3\ndataset consists of 16,153 proteins for training, 1,457 proteins for validation, and 1,797 proteins for\ntesting. We also include a set of de novo proteins collected from the CASP15 competition to provide\na more realistic assessment. Following the previous work [ 12], we download the public TS-domains\nstructures from CASP15 which consists of 45 structures, namely TS45 .\n6\n--- Page 7 ---\nTable 1: Structure consistency performance comparison between ProtInvTree and different baseline\napproaches on the CATH 4.2 dataset. The split of \"Short\", \"Single-chain\" and \"All\" is the same as\nprevious works. The best and suboptimal results are labeled with bold and underline.\nModels Trainable/TotalscTM-score ( ↑) RMSD ( ↓)\nParams. Short Single-chain All Short Single-chain All\nStructGNN [19] 1.4M/1.4M 0.616 0.646 0.751 2.439 2.702 2.327\nGraphTrans [19] 1.5M/1.5M 0.590 0.635 0.744 2.356 2.678 2.351\nGCA [37] 2.1M/2.1M 0.606 0.646 0.755 2.430 2.596 2.226\nGVP [21] 0.9M/0.9M 0.611 0.662 0.771 2.289 2.542 2.181\nProteinMPNN [8] 1.9M/1.9M 0.636 0.692 0.795 2.310 2.370 2.009\nAlphaDesign [10] 3.6M/3.6M 0.646 0.693 0.814 2.271 2.422 1.969\nPiFold [11] 5.8M/5.8M 0.655 0.700 0.842 2.203 2.355 1.723\nUniIF [13] 5.4M/5.4M 0.660 0.709 0.845 2.168 2.298 1.680\nLM-Design (ESM-1b) [48] 6.9M/650M 0.663 0.714 0.849 2.150 2.240 1.638\nKW-Design (ESM-2) [9] 54.49M/650M 0.676 0.729 0.858 2.101 2.148 1.566\nESM-3 [16] 1.4B/1.4B 0.668 0.692 0.816 2.060 2.387 2.135\nProtInvTree (ESM-3) 0M/1.4B 0.768 0.800 0.881 1.902 2.136 1.513\nTable 2: Structural consistency comparison be-\ntween ProtInvTree and baseline approaches on\nCATH 4.3 datasets. The best andsuboptimal re-\nsults are labeled with bold and underline.\nModel scTM-score (↑)RMSD (↓)\nStructGNN [19] 0.693 2.563\nGraphTrans [19] 0.690 2.614\nGCA [37] 0.698 2.525\nGVP [21] 0.713 2.509\nProteinMPNN [8] 0.743 2.238\nAlphaDesign [10] 0.749 2.230\nPiFold [11] 0.785 1.949\nKW-Design (ESM-2) [9] 0.818 1.751\nESM-3 [16] 0.775 2.074\nProtInvTree (ESM-3) 0.835 1.702Table 3: De novo protein design results on TS45\ndatasets. We compare structural consistency of\nthe following methods. The best andsuboptimal\nresults are labeled with bold and underline.\nModel scTM-score (↑)RMSD (↓)\nStructGNN [19] 0.631 3.336\nGraphTrans [19] 0.618 3.276\nGCA [37] 0.660 3.226\nGVP [21] 0.652 3.245\nProteinMPNN [8] 0.668 3.142\nAlphaDesign [10] 0.660 3.167\nPiFold [11] 0.699 2.875\nKWDesign (ESM-2) [9] 0.711 2.643\nESM-3 [16] 0.690 2.958\nProteinInvTree (ESM-3) 0.724 2.513\nEvaluation Metrics. For evaluation metrics, we use sc-TMscore [47] and RMSD [5] to evaluate\nstructural consistency. We define diversity as the average proportion of differing residues across all\npairs of generated sequences and define novelty as1−recovery . Details of all metrics are provided\nin the appendix. Following previous studies [ 19,17], we report them on three settings, namely short\nproteins (length ≤100), single-chain proteins (labeled with 1 chain in CATH), and all proteins.\nBaselines. We compare ProtInvTree with several state-of-the-art baselines, categorized into three\ngroups: (1) autoregressive models, including StructGNN [ 19], GraphTrans [ 19], GCA [ 37], GVP [ 21],\nand ProteinMPNN [ 8]; (2) the one-shot model, PiFold [ 11], UniIF [ 13]; (3) iterative models, including\nAlphaDesign [10], LM-Design [48], KW-Design [9].\nImplementation Details. All experiments are conducted on NVIDIA-A100 GPUs with 80G\nmemory. We choose ESM-3 [ 16] as our policy model because it is the first protein foundation model\nthat directly supports inverse folding without task-specific fine-tuning. Building on this capability, we\nfocus on unleashing the potential of large protein language models (PLMs) through test-time scaling .\nTo ensure fast structural feedback for reward computation, we use ESMFold [ 26] to predict the 3D\nstructures of candidate sequences. For ProtInvTree, we set the maximum number of MCTS iterations\nMto 50. The selection numbers Ktat each step follow a cosine schedule. In the UCT algorithm, the\nweight wbalancing the exploration and exploitation is set to 0.01.\n5.2 Benchmarking Fixed Backbone Protein Design\nStructural Consistency. We benchmark the fixed backbone protein design task in CATH4.2 and\nCATH4.3 datasets, reporting the scTM-score and RMSD in Tables 1 and 2. ProtInvTree demonstrates\nsuperior performance over previous methods. We highlight the following: (1) Although iterative\n7\n--- Page 8 ---\n0.6 0.7 0.8 0.9\nsc-TMScore0.00.20.40.60.8Diversity\n0.6 0.7 0.8 0.9\nsc-TMScore0.40.50.60.7Novelty\nAlphaDesign\nPiFold\nKW-Design\nESM3\nProtInvTreeFigure 3: Pareto comparison of structural consistency (sc-TMScore) against diversity (left) and\nnovelty (right) across different protein sequence design methods. Each curve represents a specific\nmethod evaluated under different sampling temperatures.\nrefinement models have significantly outperformed previous autoregressive and one-shot baselines, the\nproposed tree-based generation framework (ProtInvTree) further achieves substantial improvements,\ndemonstrating the effectiveness of branching exploration over linear refinement. (2) ProtInvTree\nenhances inference based on the frozen ESM-3 model, requiring no additional trainable parameters,\nyet achieving the strongest performance. Compared to ESM-3 alone, when both methods set the\nsame number of iterative steps, ProtInvTree improves the scTM-score by 18.3% (short), 17.6%\n(single-chain), and 7.8% (all) in CATH 4.2. (3) While the improvement in scTMscore is expected\ndue to its use as the reward function during search, we further evaluate RMSD as an independent\nstructural metric, which consistently supports the effectiveness of ProtInvTree.\nBalance between Structural Consistency and Diversity & Novelty. Figure 3 illustrates the Pareto\nfrontier between structural consistency (measured by sc-TMScore) and two key sequence-level objec-\ntives: diversity (left) and novelty (right). We highlight our primary findings as follows: (1) ProtInvTree\nachieves Pareto-optimal performance in both the diversity–scTMscore and novelty–scTMscore\nspaces, outperforming all baselines across the trade-off frontier. (2) Compared to ESM-3 , while\nimprovements in scTMscore are expected due to the reward-guided sampling, we observe that ProtIn-\nvTree also achieves significantly higher diversity and novelty even at comparable levels of structural\nconsistency. (3) Notably, when at comparable sc-TMScore levels, the baselines of AlphaDesign,\nPiFold, and KW-Design exhibit progressively higher diversity and lower novelty . This highlights\na fundamental distinction between the two metrics: diversity measures variation within the set of\ngenerated sequences, whereas novelty reflects deviation from the native (ground-truth) sequence.\nAs shown in Fig. 4, baseline methods optimized with a recovery loss tend to converge around local\noptima near the ground-truth sequence, as illustrated in case (a); by contrast, our method can escape\nthis regime and explore multiple diverse and structurally consistent solutions, including those far\nfrom the ground-truth sequence, as shown in case (c).\n(a) Hign Diversity ,\nLow Noverty(b) Low  Diversity,\nHign Noverty(c) Hign Diversity ,\nHign Novertyx x xy y y\nground truth sequencesampled \nsequences\n(d) Low Diversity ,\nLow Novertyxy\nDiversity rangeDiversity ↓\nNoverty  ↑ Diversity ↑\nNoverty  ↓Diversity ↑\nNoverty  ↑ Diversity ↓\nNoverty  ↓ \nFigure 4: Conceptual illustration of the difference between diversity and novelty of the generated\nsequences. Each blue dot represents a generated sequence, and the purple dot represents the ground\ntruth sequence. Assuming all generated sequences in this plane share similar structural consistency,\nThe gray circular boundary indicates the diversity range among the generated samples, while the gray\nlines connecting each sample to the ground truth reflect their novelty.\n5.3 De Novo Proteins Design\nEvaluating models on the TS45 dataset allows us to gain a better understanding of the potential of AI\nmodels in designing de novo proteins and reveals that different models exhibit non-trivial differences\n8\n--- Page 9 ---\nin generalizability. We present the quantitative results in Table 3, which reveal the following: (1)\nProtInvTree outperforms all baseline methods in terms of both sc-TMscore and RMSD, highlighting\nits superior ability to maintain structural consistency and achieve accurate geometric reconstruction.\n(2) We additionally compare ProtInvTree with ESM-3 [16] to assess the effectiveness of our overall\nframework beyond the pretrained language model itself. Despite sharing the same pretrained model,\nProtInvTree achieves substantially better results, suggesting that test-time reward-guided planning\nplays a key role in unlocking the full potential of pretrained protein language models.\n5.4 Analysis: Diving Deep into ProtInvTree\nTest-time Scaling Analysis. To understand how test-time computation scales with performance,\nwe investigate the effect of two key planning hyperparameters in our framework: the number of\ncandidate expansions andplanning depth . We can see that as the expansion number and planning\ndepth increase, the sc-TMscore both gradually improve, although the average time consumed also\nrises to some extent. This highlights that scaling the test-time computation can effectively enhance\nsequence quality through more deliberate search. However, as the number of planning depths further\nincreases, the sc-TMscore tends to be saturated, as the search converges to high-confidence regions,\nthe diversity of refinable sequences becomes limited, and further refinements yield diminishing\nstructural gains. Moreover, the diversity in both settings decreases as the expansion number and\nplanning depth increase, revealing the trade-off between structural consistency and sequence diversity.\n2 3 4 5 6\nExpand Number0.850.860.860.870.880.88scTMscore\nscTMscore\nDiversity\n2 3 4 5 6 7\nDepth0.840.840.850.850.860.86scTMscore\nscTMscore\nDiversity\n0.120.140.160.180.200.220.24\nDiversity\n0.150.200.250.300.35\nDiversity\nFigure 5: Test-time scaling laws analysis of our ProtInvTree under\ndifferent expansion numbers (left) and search depths (right).\nS2 S1r=0.99 r=0.98r=0.67\nr=0.98 r=0.88r=0.93r=0.85r=0.89\nr=0.98\nr=0.99r=0.78\nS1:\nS2:AHVINTFDGVADYLQTYHKLPD\nMAMLNTVDEVADYIVKNKKLPD\nGT:AQVINTVDGVADYLDTNKKLPDFigure 6: Reward-guided\nsearch tree visualization.\nS1&GT  sc-TMScore:0.99; RMSD:0.33 \nS2&GT   sc-TMScore:0.98; RMSD:0.38 \nFigure 7: Structural align-\nment visualization.Case Study. To facilitate understanding of the entire workflow of\nour proposed ProtInvTree, we visualize a reward-guided search tree\nin Figure 6. Each node represents a partially generated sequence\nwith its predicted reward r, reflecting structural consistency. The tree\nshowcases how ProtInvTree performs branching exploration guided by\nreward scores. Two high-reward sequences, S1andS2, emerge from\ndifferent trajectories, with high diversity and novelty, yet achieve high\nstructural rewards ( r= 0.99,r= 0.98). We further compare their\npredicted 3D structures with the ground truth structure in Figure 7. Its\nhigh sc-TMScore and low RMSD demonstrate ProtInvTree ’s ability\nto generate diverse sequence candidates while maintaining structural\nconsistency. This case illustrates how the reward-guided tree search\nenables efficient exploration of the solution space and selection of\nstructurally faithful, non-trivial designs beyond native recovery.\n6 Conclusion\nWe present ProtInvTree, a novel reward-guided tree-search framework for protein inverse folding\nthat explicitly addresses the trade-off between structural consistency and sequence diversity. By\nreformulating sequence design as a step-wise, decision-making process, ProtInvTree enables the\nexploration of diverse design trajectories through self-evaluation, lookahead, and backtracking.\nProtInvTree shows superior performance across multiple benchmarks, achieving state-of-the-art\nstructural consistency while generating diverse and novel sequences beyond the native ground truth.\n9\n--- Page 10 ---\nReferences\n[1]Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf\nRonneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure\nprediction of biomolecular interactions with alphafold 3. Nature , pages 1–3, 2024.\n[2]Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov,\nGyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate\nprediction of protein structures and interactions using a three-track neural network. Science ,\n373(6557):871–876, 2021.\n[3]Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré,\nand Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated\nsampling. arXiv preprint arXiv:2407.21787 , 2024.\n[4]James W Bryson, Stephen F Betz, Helen S Lu, Daniel J Suich, Hongxing X Zhou, Karyn T\nO’Neil, and William F DeGrado. Protein design: a hierarchic approach. Science , 270(5238):935–\n941, 1995.\n[5]Oliviero Carugo. How root-mean-square distance (rmsd) values depend on the resolution of\nprotein structures that are compared. Applied Crystallography , 36(1):125–128, 2003.\n[6]Alexander E Chu, Tianyu Lu, and Po-Ssu Huang. Sparks of function by de novo protein design.\nNature Biotechnology , 42(2):203–215, 2024.\n[7]Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In\nInternational conference on computers and games , pages 72–83. Springer, 2006.\n[8]Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F\nMilles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep\nlearning–based protein sequence design using proteinmpnn. Science , 378(6615):49–56, 2022.\n[9]Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, and Stan Z. Li.\nKW-design: Pushing the limit of protein design via knowledge refinement. In The Twelfth\nInternational Conference on Learning Representations , 2024.\n[10] Zhangyang Gao, Cheng Tan, and Stan Z Li. Alphadesign: A graph protein design method and\nbenchmark on alphafolddb. arXiv preprint arXiv:2202.01079 , 2022.\n[11] Zhangyang Gao, Cheng Tan, and Stan Z. Li. Pifold: Toward effective and efficient protein\ninverse folding. In The Eleventh International Conference on Learning Representations , 2023.\n[12] Zhangyang Gao, Cheng Tan, Yijie Zhang, Xingran Chen, Lirong Wu, and Stan Z. Li. Pro-\nteininvbench: Benchmarking protein inverse folding on diverse tasks, models, and metrics. In\nThirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack , 2023.\n[13] Zhangyang Gao, Jue Wang, Cheng Tan, Lirong Wu, Yufei Huang, Siyuan Li, Zhirui Ye, and\nStan Z Li. Uniif: Unified molecule inverse folding. Advances in Neural Information Processing\nSystems , 37:135843–135860, 2024.\n[14] Tymor Hamamsy, James T Morton, Robert Blackwell, Daniel Berenberg, Nicholas Carriero,\nVladimir Gligorijevic, Charlie EM Strauss, Julia Koehler Leman, Kyunghyun Cho, and Richard\nBonneau. Protein remote homology detection and structural alignment using deep learning.\nNature biotechnology , pages 1–11, 2023.\n[15] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhit-\ning Hu. Reasoning with language model is planning with world model. arXiv preprint\narXiv:2305.14992 , 2023.\n[16] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin,\nRobert Verkuil, Vincent Q Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million\nyears of evolution with a language model. Science , page eads0018, 2025.\n10\n--- Page 11 ---\n[17] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and\nAlexander Rives. Learning inverse folding from millions of predicted structures. In International\nconference on machine learning , pages 8946–8970. PMLR, 2022.\n[18] Po-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design.\nNature , 537(7620):320–327, 2016.\n[19] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for\ngraph-based protein design. Advances in neural information processing systems , 32, 2019.\n[20] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron\nDror. Learning from protein structure with geometric vector perceptrons. In International\nConference on Learning Representations .\n[21] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron\nDror. Learning from protein structure with geometric vector perceptrons. In International\nConference on Learning Representations , 2021.\n[22] Daniel Kahneman, Shane Frederick, et al. Representativeness revisited: Attribute substitution in\nintuitive judgment. Heuristics and biases: The psychology of intuitive judgment , 49(49-81):74,\n2002.\n[23] Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, and Boxing Chen. Mindstar: Enhancing\nmath reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265 , 2024.\n[24] Hamed Khakzad, Ilia Igashov, Arne Schneuing, Casper Goverde, Michael Bronstein, and\nBruno Correia. A new age in protein design empowered by deep learning. Cell Systems ,\n14(11):925–939, 2023.\n[25] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In Johannes\nFürnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006,\n17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006,\nProceedings , volume 4212 of Lecture Notes in Computer Science , pages 282–293. Springer,\n2006.\n[26] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos\nSantos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of\nprotein sequences at the scale of evolution enable accurate structure prediction. BioRxiv ,\n2022:500902, 2022.\n[27] Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, and Chunhua\nShen. De novo protein design using geometric vector field networks. In The Twelfth International\nConference on Learning Representations , 2024.\n[28] Grant S Murphy, Jeffrey L Mills, Michael J Miley, Mischa Machius, Thomas Szyperski, and\nBrian Kuhlman. Increasing sequence diversity with flexible backbone protein design: the\ncomplete redesign of a protein hydrophobic core. Structure , 20(6):1086–1096, 2012.\n[29] Christine A Orengo, Alex D Michie, Susan Jones, David T Jones, Mark B Swindells, and\nJanet M Thornton. Cath–a hierarchic classification of protein domain structures. Structure ,\n5(8):1093–1109, 1997.\n[30] Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual\nreasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195 ,\n2024.\n[31] LUCA Silva, Barthelemy Meynard-Piganeau, Carlo Lucibello, Christoph Feinauer, et al. Fast\nuncovering of protein sequence diversity from structure. In The Thirteenth International\nConference on Learning Representations . (seleziona...), 2025.\n[32] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature , 529(7587):484–489,\n2016.\n11\n--- Page 12 ---\n[33] Steven A Sloman. The empirical case for two systems of reasoning. Psychological bulletin ,\n119(1):3, 1996.\n[34] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute opti-\nmally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 ,\n2024.\n[35] Keith E Stanovich. Who is rational?: Studies of individual differences in reasoning . Psychology\nPress, 1999.\n[36] Cheng Tan, Zhangyang Gao, Jun Xia, Bozhen Hu, and Stan Z Li. Generative de novo protein\ndesign with global context. arXiv preprint arXiv:2204.10673 , 2022.\n[37] Cheng Tan, Zhangyang Gao, Jun Xia, Bozhen Hu, and Stan Z Li. Global-context aware\ngenerative protein design. In ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 1–5. IEEE, 2023.\n[38] Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, and An Bo. Q*: Improving multi-step\nreasoning for llms with deliberative planning, 2024.\n[39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations , 2023.\n[40] Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan\nWu, Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction from primary\nsequence. BioRxiv , pages 2022–07, 2022.\n[41] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An empirical analysis\nof compute-optimal inference for problem-solving with language models. arXiv preprint\narXiv:2408.00724 , 2024.\n[42] John J Yang, Jason Yim, Regina Barzilay, and Tommi Jaakkola. Fast non-autoregressive inverse\nfolding with discrete diffusion. arXiv preprint arXiv:2312.02447 , 2023.\n[43] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\nvances in Neural Information Processing Systems , 36, 2024.\n[44] Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Lio, and Yu Guang Wang. Graph denoising diffusion\nfor inverse protein folding. In Thirty-seventh Conference on Neural Information Processing\nSystems , 2023.\n[45] Kaizhi Yue and Ken A Dill. Inverse protein folding problem: designing polymer sequences.\nProceedings of the National Academy of Sciences , 89(9):4163–4167, 1992.\n[46] Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang.\nAccessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with\nllama-3 8b. arXiv preprint arXiv:2406.07394 , 2024.\n[47] Yang Zhang and Jeffrey Skolnick. Tm-align: a protein structure alignment algorithm based on\nthe tm-score. Nucleic acids research , 33(7):2302–2309, 2005.\n[48] Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structure-\ninformed language models are protein designers. In International Conference on Machine\nLearning , pages 42317–42338. PMLR, 2023.\n[49] Yiheng Zhu, Zitai Kong, Jialu Wu, Weize Liu, Yuqiang Han, Mingze Yin, Hongxia Xu, Chang-\nYu Hsieh, and Tingjun Hou. Generative ai for controllable protein sequence design: A survey.\narXiv preprint arXiv:2402.10516 , 2024.\n[50] Yiheng Zhu, Jialu Wu, Qiuyi Li, Jiahuan Yan, Mingze Yin, Wei Wu, Mingyang Li, Jieping Ye,\nZheng Wang, and Jian Wu. Bridge-if: Learning inverse protein folding with markov bridges. In\nThe Thirty-eighth Annual Conference on Neural Information Processing Systems .\n12\n--- Page 13 ---\nA Algorithms\nThe overall workflow of the ProtInvTree is provided in Algorithm 1.\nAlgorithm 1 ProtInvTree: Reward-Guided Tree Search for Protein Inverse Folding\nInput: Backbone structure c, ground truth sequence xgt, initial sequence x0, folding model f(·),\nPLM policy πθ, reward function R(·,·), max iterations M, tree depth T, expansion number per node\nK, reward threshold τ\nOutput: A set of generated sequences S={x∗\ni}Z\ni=1, where Zdenotes the number of generated\nsequences.\n1:Initialize root node s0= (c,x0), search tree Twiths0, and result set S ←∅\n2:form= 1toMdo\n3: Selection: Traverse the tree from s0using UCT to select a promising node st(Eq. 3)\n4: fork= 1toKdo\n5: Expansion:\n6: Sample action a(k)\nt∼πθ(at|st) ▷Focus-and-Grounding strategies (Sec. 4.3)\n7: Apply a(k)\ntto obtain updated sequence x(k)\nt+1\n8: Construct child state s(k)\nt+1= (c,x(k)\nt+1)\n9: Adds(k)\nt+1to tree Tas child of st\n10: Evaluation:\n11: Sample completed sequence: ˜x(k)\nT∼ J(x(k)\nt+1,c) ▷Jumpy denoising (Sec. 4.4)\n12: Compute reward: r(k)\nt+1=TMScore (f(˜x(k)\nT), f(xgt))\n13: Set node value: V(s(k)\nt+1) =r(k)\nt+1\n14: Backpropagation:\n15: Update visit count N(sj)and value V(sj)(Eq. 5)\n16: Backpropagate r(k)\nt+1to update all ancestors of s(k)\nt+1(Eq. 6)\n17: ift+ 1 = Tandr(k)\nt+1≥τthen\n18: Addx(k)\nt+1to result set S\n19: end if\n20: end for\n21:end for\n22:Return: Sequence set S={x∗\ni}Z\ni=1containing Zhigh-quality candidates\nB Evaluation Metrics\nIn the main paper, we report evaluation results using four metrics: sc-TMscore, RMSD, novelty, and\ndiversity. The descriptions of these metrics are detailed as follows.\nsc-TMScore. The structural similarity is the ultimate standard for measuring the quality of the\ndesigned sequence. However, the structures of designed protein sequences needed to be predicted\nby other algorithms, such as AlphaFold [ 1], RoseTTAFold [ 2], OmegaFold [ 40] and ESMFold [ 26].\nThe protein folding algorithm itself has a certain inductive bias and will cause some prediction\nerrors, which will affect the evaluation. To overcome the inductive bias, we adapt the self-consistent\nTM-score (sc-TMscore) metric:\nsc-TMScore =TMScore (f(˜x), f(x)), (12)\nwhere fis the protein folding algorithm and TMScore (·,·)is a widely used metric [ 47] for measuring\nprotein structure similarity. Since the structures of the designed sequence and reference sequence\nare predicted by the same protein folding algorithm, the model’s inductive bias is expected to be\ncanceled out when calculating the TM-score. This approach results in a more robust metric, called\nthe sc-TMScore, that is less affected by the inductive bias of the protein folding algorithm.\nRMSD. The standard dissimilarity measure for protein structures is the root mean square deviation\n(RMSD) of representative atom positions such as α-carbons. RMSD is calculated as the square root\n13\n--- Page 14 ---\nof the average squared distance between corresponding atoms in two 3D structures:\nRMSD (v,w) =vuut1\nnnX\ni=1∥vi−wi∥2, (13)\nwhere v=f(˜x)andw=f(x)are the predicted 3D structures of the designed sequence ˜xand the\nreference sequence x, respectively, obtained using a structure prediction algorithm f. Here, viand\nwidenote the 3D coordinates of the i-th atom in each structure, and nis the total number of atoms\nconsidered (typically backbone or α-carbon atoms). RMSD provides a fine-grained comparison of\natomic positions after optimal rigid-body alignment of the two structures. However, it is sensitive to\nlocal deviations, such as flexible loops or inaccurate predictions in side-chain packing, and may not\nfully reflect the overall fold similarity. As a result, RMSD is typically used in conjunction with other\nmetrics such as TM-score to provide a more comprehensive assessment of structural quality.\nNovelty. We define novelty as the complement of sequence recovery, reflecting the extent to which\nthe generated sequences deviate from the native ground truth:\nNovelty = 1−Recovery\nwhere recovery is the fraction of amino acids in the predicted sequence that exactly match the\nground-truth sequence at each position, defined as:\nRecovery =1\nnnX\ni=11(˜xi=xi)\nDiversity. The average fraction of amino acids that differ between pairs of sequences:\nDiversity ({˜x1, . . . , ˜xM}) =2\nNM(M−1)MX\nj=1j−1X\nk=1NX\ni=11[˜xj[i]̸=˜xk[i]]\n=2\nM(M−1)MX\nj=1j−1X\nk=1dH(˜xj,˜xk).\nwhere dHis the Hamming distance. We note that sequence diversity alone is not a sufficient measure\nof an IF method’s quality, as it can be increased arbitrarily at the expense of sample quality (e.g. as\nmeasured by structural consistency).\nC Selection Strategies Comparison\nTo analyze the impact of different position selection strategies in the Focus-and-Grounding action\nmechanism, we evaluate several variants for computing the position distribution pϕ(i|st), which\ndetermines the set of positions {i1, . . . , i Kt}to be modified at each denoising step.\nSpecifically, we compare the following approaches:\n•Random sampling: Positions are selected uniformly at random from the sequence.\n•Autoregressive sampling: Positions are visited sequentially from left to right in an autore-\ngressive manner.\n•Entropy-based selection: Positions with the lowest predictive entropy, representing the\nmodel’s most confident predictions, are prioritized for update.\nWe integrate each strategy into the Focus module F(st), keeping the Grounding step unchanged.\nTable 4 summarizes the quantitative results, showing that all three strategies achieve competitive\nperformance, with random sampling performing surprisingly well despite of its simplicity. This\nmay be because exploring a broader space in the early stages helps avoid premature convergence and\nencourages greater sequence diversity, which ultimately benefits overall generation quality.\n14\n--- Page 15 ---\nTable 4: Comparison of different sampling strategies on structure consistency (scTM-score).\nSampling Strategy Random AR Entropy\nscTM-score ( ↑) 0.881 0.877 0.870\nD Limitation and Future work\nFuture work will focus on extending our framework to a broader range of protein-related tasks beyond\nfixed-backbone inverse folding. One potential limitation of the proposed ProtInvTree is its current\nlack of experimental validation in real-world biological settings. We will seek collaborations with\nexperimental laboratories to test the viability and functional relevance of the designed sequences.\nE Broader Impacts\nInverse protein folding models, positioned at the intersection of bioinformatics and computational\nbiology, offer significant potential for advancing both basic research and real-world applications. By\nenabling the design of protein sequences that reliably fold into desired three-dimensional structures,\nthese models can drive progress across diverse domains. Broader impacts include facilitating\nstructure-based drug discovery, enabling the rational design of enzymes with novel functionalities,\nand advancing synthetic biology through the creation of custom proteins with tailored properties.\n15",
  "text_length": 52995
}