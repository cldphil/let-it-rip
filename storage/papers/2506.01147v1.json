{
  "id": "http://arxiv.org/abs/2506.01147v1",
  "title": "A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal\n  Recognition",
  "summary": "System-generated logs are typically converted into categorical log templates\nthrough parsing. These templates are crucial for generating actionable insights\nin various downstream tasks. However, existing parsers often fail to capture\nfine-grained template details, leading to suboptimal accuracy and reduced\nutility in downstream tasks requiring precise pattern identification. We\npropose a character-level log parser utilizing a novel neural architecture that\naggregates character embeddings. Our approach estimates a sequence of\nbinary-coded decimals to achieve highly granular log templates extraction. Our\nlow-resource character-level parser, tested on revised Loghub-2k and a manually\nannotated industrial dataset, matches LLM-based parsers in accuracy while\noutperforming semantic parsers in efficiency.",
  "authors": [
    "Prerak Srivastava",
    "Giulio Corallo",
    "Sergey Rybalko"
  ],
  "published": "2025-06-01T20:00:00Z",
  "updated": "2025-06-01T20:00:00Z",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01147v1",
  "full_text": "--- Page 1 ---\nA Word is Worth 4-bit:\nEfficient Log Parsing with Binary Coded Decimal Recognition\nPrerak Srivastava, Giulio Corallo, Sergey Rybalko\nSAP Labs\nMougins, France\n{prerak.srivastava, giulio.corallo, sergey.rybalko} @sap.com\nAbstract\nSystem-generated logs are typically converted\ninto categorical log templates through parsing.\nThese templates are crucial for generating ac-\ntionable insights in various downstream tasks.\nHowever, existing parsers often fail to capture\nfine-grained template details, leading to sub-\noptimal accuracy and reduced utility in down-\nstream tasks requiring precise pattern identifi-\ncation. We propose a character-level log parser\nutilizing a novel neural architecture that ag-\ngregates character embeddings. Our approach\nestimates a sequence of binary-coded decimals\nto achieve highly granular log templates extrac-\ntion. Our low-resource character-level parser,\ntested on revised Loghub-2k and a manually an-\nnotated industrial dataset, matches LLM-based\nparsers in accuracy while outperforming seman-\ntic parsers in efficiency.\n1 Introduction\n1.1 Existing Methods and Challenges\nLog parsing aims to extract static (log template)\nand variable (parameters) components from log\nmessages, serving as a foundation for many down-\nstream tasks such as anomaly detection (Steidl\net al., 2022), security enhancement (Svacina et al.,\n2020), and audit trail (Wu et al., 2017). As large-\nscale systems generate sheer amount of log data\n(Dai et al., 2020), the demand for fast, automated,\nin-memory log parsing solutions has grown rapidly.\nTraditional syntax-based parsers like Logram (Dai\net al., 2020), Lenma (Shima, 2016), Drain (He\net al., 2017) and Brain (Yu et al., 2023) offer com-\nputationally efficient solutions but rely heavily on\npre-defined hyperparameters and manually crafted\nregex or grok patterns (Zhu et al., 2019). This\ndependence often limits their ability to capture\nthe deeper semantics of log data (Zhong et al.,\n2024). In contrast, semantic-based log parsersleverage deep learning for better context under-\nstanding. Semparser (Huo et al., 2023) and Uni-\nparser (Liu et al., 2022) utilize Bidirectional LSTM\n(BiLSTM), while NuLog (Nedelkoski et al., 2021)\nemploys a transformer-based encoder. LogPPT\n(Le and Zhang, 2023) goes further by utilizing\npre-trained RoBERTa to frame log parsing as a to-\nken classification task, and models like V ALB (Li\net al., 2023) and LogPTR (Wu et al., 2024) enhance\nsemantic understanding by precisely categorizing\nspecific parameters.\nRecently, Large Language Models (LLMs) have\ndemonstrated remarkable versatility for down-\nstream tasks through In-context learning (ICL)\n(Gao et al., 2023), which reduces the need for\nfine-tuning (Zhao et al., 2021). This potential\nis particularly promising for log parsing applica-\ntions (Le and Zhang, 2023). Advances such as\nLILAC (Jiang et al., 2024a) and Divlog (Xu et al.,\n2024) have enhanced LLM-based log parsing accu-\nracy through optimized few-shot example selection\nand improved prompt design. However, deploy-\ning LLM-based solutions presents challenges, in-\ncluding non-deterministic behavior across models\n(Astekin et al., 2024), high query costs (Xiao et al.,\n2024), and significant computational requirements\n(Xia et al., 2024).\nMetrics like group accuracy (Zhu et al., 2019)\ntend to favor templates that appear more frequently,\nmaking them valuable for log sequence anomaly\ndetection but often insufficient for capturing the full\ndiversity of log templates (Yu et al., 2023; Zhong\net al., 2024). This bias results in incomplete com-\nparisons and creates a gap between academic re-\nsearch and industry requirements (Petrescu et al.,\n2023). Template-level metrics such as the F1-score\nof Template Accuracy (FTA) (Khan et al., 2022)\noffer a more balanced perspective, particularly ben-\nefiting the evaluation of less frequent templates,\nlike error messages.\nAnother critical challenge in current semantic-arXiv:2506.01147v1  [cs.CL]  1 Jun 2025\n--- Page 2 ---\ncommon.wav \"DELETE 1990-1 1-05 1-4996-7406-6\" \nstatus: 04:14:07 len: 00:09:15 time: 1-132-39180-6Character Embedding\n[ 'c','o','m', '...', ' ','\"','D','E', '...', '-','6' ]Positional Encoding\n+\nMulti-head self-\nattentionAdd & NormalizeFeed forward, 256\nunitsReLUFeed forward, 128\nunitsAdd & Normalize\nCRF...BiLSTM Layer\n[ 15, 15, 3, ...,  ]\n[ 1,1,1,1,1,1,1,1,1,1,0,0, ... ]\n<*> \"DELETE <*> <*>\" status: <*> len: <*>\ntime: <*>1281x\nInput logOutput template Encoder CNN downsampling\n128128\nFigure 1: Diagram of the proposed neural network architecture.\nbased log parsers is the choice of tokenization\nstrategy. Many methods tokenize logs using space\ncharacters (Nedelkoski et al., 2021), special rules\n(Liu et al., 2022), or pre-trained tokenizers (Le\nand Zhang, 2023), leading to ambiguities in deter-\nmining whether sub-tokens are static or variable\ncomponents. Such ambiguities reduce the granular-\nity of templates and cause discrepancies between\nannotated and predicted outputs, directly affecting\nperformance metrics (Zhong et al., 2024). To im-\nprove the evaluation of template accuracy at a finer\nlevel, Hashemi et al. (2024) proposed the Param-\neter Mask Agreement (PMA) metric, focusing on\ncharacter-level labeling to avoid tokenization is-\nsues. In this vein, Thaler et al. (2017) proposed\nan end-to-end Deep Neural Network (DNN) ap-\nproach for character-level parsing, capable of es-\ntimating parameter masks as defined by Hashemi\net al. (2024).\n1.2 Proposed Approach\nBuilding on these insights, we propose an enhanced\ncharacter-level log parser inspired by the work of\nThaler et al. (2017). Our novel architecture com-\nbines a character-level transformer, BiLSTM, and\nConditional Random Field (CRF) (Sutton et al.,\n2012) to achieve robust and efficient log parsing.\nOur approach is evaluated using both template-\nlevel and granular-level metrics on the benchmark\ndataset Loghub-2k (Zhu et al., 2023), comparing\nour method against leading syntactic parsers and\nrecent LLM-based solutions. We also evaluate our\nmodel on a manually annotated industrial dataset\nto demonstrate its applicability to real-world condi-\ntions. Results show that our method, with only\n312k parameters, outperforms syntactic and se-\nmantic parsers and matches LILAC’s performance,\nwhile being 20 times more efficient.2 Methodology\n2.1 Problem definition\nA log message lcan be represented as a bounded\nsequence of characters, denoted as t= (tj:t∈\nT, j= 1,2, . . . ,|t|), where jis the position index\nof a character within the log message. The set of all\ncharacters is represented by Tand|t|indicates the\ntotal number of characters in l. Similar to the work\nof Hashemi et al. (2024), for a given log message\nand its potential template, we aim to compute a\nparameter mask. This parameter mask is a binary\nsequence that aligns with the characters in the log\nmessage, where each binary element acts as a flag\nindicating whether the corresponding character is a\nvariable or static part of the template. The param-\neter mask can be described as a function mthat\nmaps each character tjas :\nm(tj) =(\n0iftjis static\n1iftjis variable .(1)\nFurther, we define y= (y1, . . . , y j)as the se-\nquence of parameter mask, where yj=m(tj). The\napproach proposed by Thaler et al. (2017) estimates\nyas a sequence of tags. To improve upon this ap-\nproach, we propose replacing the final linear layer\nwith a CRF layer, which benefits from utilizing past\nand future tag information to predict the current tag\n(Ma and Hovy, 2016). To enhance both training\nand inference efficiency, we propose aggregating\ngroups of four binary labels and encoding them\nas a single Binary-Coded Decimal (BCD). Specif-\nically, we define a function mdthat maps a 4-bit\nsequence into a decimal number. The new sequence\nof decimals is represented as y4bit= (d0, . . . , d n),\nwhere:\ndn=md(y4·n+1, y4·n+2, y4·n+3, y4·n+4),(2)\n--- Page 3 ---\nmd(y4·n+1, y4·n+2, y4·n+3, y4·n+4) =y4·n+1·23\n+y4·n+2·22+y4·n+3·21+y4·n+4·20,\n(3)\nandn∈[0,(|ti|\n4)−1]. The sequence yis padded\nwith appropriate zeroes to meet the condition that\n|ti|%4. Therefore the objective of our model is to\npredict y4bitgiven a sequence of characters tfrom\na log line l, by minimizing the following loss:\nLθ(t,y4bit) =−logpθ(y4bit|t). (4)\n2.2 Model architecture\nFigure 1 illustrates our model’s architecture. With-\nout requiring pre-processing overhead, the embed-\nding layer directly converts each input log character\ninto a dense vector representation. Each character\nin the training vocabulary corresponds to a row\nin the embedding layer, with dimensionality set\nas a hyperparameter. The whitespace character\nis included in the vocabulary and is also used for\npadding the input sequence to satisfy the condition\n|t|% 4. Positional encoding adds absolute posi-\ntional information on each character with similar\nfunctioning as Ke et al. (2021). Next, a single-\nlayer, 8-head bidirectional transformer encoder cal-\nculates the self-attention scores and provides mu-\ntual dependencies over past and future characters\nwithin the sequence as a hidden state. The hidden\nfeatures generated by the encoder are aggregated\nin blocks of four and down-sampled using a 1D\nConvolutional Neural Network (CNN) with a non-\noverlapping kernel size and stride of 4. The down-\nsampled features aim to encapsulate the relevant\nsemantic information of four characters, which are\ncrucial for predicting dn. The resulting sequence\nof down-sampled features is then passed through\na single BiLSTM layer to capture dependencies\nin both directions. The output is fed into a CRF\nlayer, which predicts 16 classes corresponding to\nthe 16 possible decimal values in 4-bit BCD en-\ncoding. A more comprehensive description of the\nimplementation details can be found in Appendix\nB. This final architecture was determined through\na series of ablation studies on different blocks; see\nAppendix C for details. To train the network, we\nemploy the negative log-likelihood loss, mentioned\nin Eq 4. The network predicts a sequence ˜ y4bit\nthat is mapped to binary sequence ˜ yvia a lookup\ntable. Characters predicted as 1are replaced by a\nplaceholder <∗>in the input log, while, the rest\nremain unchanged, resulting in the generation ofthe log template. To improve efficiency at infer-\nence, we incorporated a \"parsing cache\" module,\ninspired by the fixed-depth tree used by Jiang et al.\n(2024a). In our pipeline, this parsing cache is in-\nvoked both before querying the model and after\ntemplate prediction. We refer to our method with\nthe parsing cache as 4bitparser and without it as\nCacheless-4bitparser throughout the paper.\n3 Experimental Setup\nThe network is trained on Loghub-2.0 (Jiang et al.,\n2024b) and evaluated using a refined version of\nLoghub-2k (Zhu et al., 2023), as it is a standard\nbenchmark dataset used in the literature. The train-\ning set consists of 50k log lines from 2,349 unique\ntemplates, selected from 50 million log lines in\nLoghub-2.0. In line with Thaler et al. (2017), we\nprioritize diversity in the train set with a limit of\n50 log lines per template. The revised Loghub-2k\ndataset features corrected ground truth templates\nfollowing the guidelines of Khan et al. (2022), en-\nsuring higher annotation quality. It includes 28k\nlog lines from 1,139 unique templates, all excluded\nfrom the training set. Additionally, we also test\non manually annotated logs from an internal cloud\nsystem referred to as Idata . This dataset comprises\nof2400 log lines across 35templates, of which 200\nlog lines are used to fine-tune the trained network\nand adjust baseline models, we refer this subset as\nIdata-FT . Further details regarding the log types\nand annotation strategies for these datasets are pro-\nvided in Appendix A. To gauge the performance\nbetween different log parsers, we use three met-\nrics: Parsing Accuracy (PA) (Dai et al., 2020), FTA\n(Khan et al., 2022) and PMA (Hashemi et al., 2024).\nPMA is computed by generating a ground truth pa-\nrameter mask for each log template in both test\nsets, as described by Hashemi et al. (2024). For our\nexperiments, we train 4bitparser using a defined set\nof hyperparameters (Appendix B).\nWe compare 4bitparser with syntactic parsers\nsuch as Drain and Brain, as well as semantic\nparser-based models like LogPPT and LILAC, with\nLILAC being specifically LLM-based. The syn-\ntactic parsers are evaluated using the pre-defined\nbenchmark hyperparameters on the Loghub-2k test\nset, while hyperparameters for Idata are fine-tuned\nseparately. LogPPT is trained on data similar to\n4bitparser, including its training set and Idata-FT\nfor the respective tests. For the LILAC baseline,\nwe sampled 128 candidate examples and selected\n--- Page 4 ---\nDatasetsDrain Brain LogPPT LILAC 4bitparser\nPA FTA PMA PA FTA PMA PA FTA PMA PA FTA PMA PA FTA PMA\nHDFS 0.35 0.26 0.97 0.50 0.33 0.97 0.44 0.26 0.88 0.50 0 .35 0 .98 0.44 0.28 0.98\nApache 0.69 0.50 0.70 0.69 0.66 0.70 0.97 0.61 0.98 1.00 1 .00 1 .00 0.99 0.83 0.99\nOpenSSH 0.50 0.44 0.84 0.28 0.22 0.90 0.39 0.12 0.40 0.74 0 .70 0 .97 0.57 0.40 0.96\nOpenstack 0.01 0.01 0.67 0.11 0.20 0.95 0.11 0.16 0.13 0.40 0.76 0 .96 0.44 0.73 0.96\nHPC 0.63 0.36 0.91 0.66 0.38 0.82 0.65 0.48 0.80 0.88 0.71 0.97 0.99 0 .80 0 .99\nZookeeper 0.49 0.35 0.96 0.50 0.44 0.97 0.50 0.50 0.81 0.52 0.56 0.98 0.67 0 .61 0 .98\nHealthapp 0.23 0.10 0.52 0.33 0.42 0.49 0.16 0.40 0.42 0.75 0.74 0.87 0.83 0 .76 0 .96\nHadoop 0.26 0.27 0.59 0.34 0.41 0.71 0.36 0.40 0.77 0.44 0.51 0.93 0.58 0 .66 0 .94\nSpark 0.35 0.36 0.92 0.37 0.44 0.94 0.37 0.38 0.51 0.95 0 .68 0 .98 0.88 0.63 0.96\nBGL 0.34 0.21 0.74 0.41 0.25 0.80 0.42 0.28 0.42 0.94 0.76 0.98 0.95 0 .76 0 .99\nLinux 0.18 0.43 0.71 0.17 0.49 0.78 0.10 0.44 0.21 0.19 0.69 0 .89 0.27 0.65 0.72\nMac 0.21 0.19 0.58 0.34 0.33 0.73 0.24 0.25 0.35 0.40 0.41 0.81 0.42 0 .46 0.78\nThunderbird 0.04 0.24 0.76 0.06 0.37 0.81 0.08 0.41 0.37 0.40 0.54 0.89 0.81 0.52 0.92\nAverage 0.32 0.28 0.76 0.36 0.37 0.81 0.37 0.36 0.55 0.62 0.63 0.93 0.68 0.62 0.92\nIdata 0.56 0.67 0.90 0.50 0.31 0.77 0.58 0.38 0.91 0.90 0.60 0.96 0.93 0.53 0.98\nTable 1: Comparasion with state of the art log parsers on sub-datasets of Loghub-2k and Idata. The best results are\nin bold.\nFigure 2: Throughput of log parsers on Loghub-2k.\nthree demonstrations for the GPT-3.5-turbo lan-\nguage model, with the candidate set for demonstra-\ntion sampled separately for both test sets following\nthe approach of Xu et al. (2024).\n4 Experimental Results\n4.1 Effectiveness of 4bitparser\nThe results, presented in Table 1, report perfor-\nmance across the Loghub-2k sub-datasets and Idata,\nwith no pre-processing applied to the extracted or\nground truth templates. On average, 4bitparser\nidentifies nearly twice as many templates and offers\n10% greater granularity than the Syntactic parser\nand LogPPT. While there is minimal difference\nbetween LILAC and 4bitparser in template granu-\nlarity (PMA) and exact template extraction (FTA),\n4bitparser identifies 6% more frequent templates,\nleading to an improvement in PA. Moreover, 4bit-\nparser outperforms LILAC on four sub-datasets\nand shows comparable or slightly varied results on\nthe rest. This is remarkable given that our model\nis very lightweight, with only 314k parameters.\nOn Idata, Drain identifies more templates than the\nother parsers, mainly because this test set includes\nlog templates with identical token counts across log\nparameters, making them well-suited for Drain’sheuristics. However, 4bitparser excels at identi-\nfying frequent templates, and even its non-exact\nmatches are highly accurate at the character level,\nachieving the highest PMA score of 0.98.\n4.2 Efficiency of 4bitparser\nTo evaluate the efficiency of each log parser, we\nmeasured throughput in terms of log lines parsed\nper second on a set of 28k logs from the Loghub-2k\ntest set. For this evaluation, all semantic parsers\nalong with two of our proposed methods, were\nexecuted on a 16GB V100 GPU. Figure 2 shows\nthat Cacheless-4bitparser is five times more effi-\ncient than LILAC, which already employs a pars-\ning cache, and twice as efficient as LogPPT, high-\nlighting the computational overhead of language\nmodel-based log parsers. Additionally, 4bitparser,\nwhen using the parsing cache, is 20 times faster\nthan LILAC and 8 times efficient than LogPPT,\nsignificantly narrowing the efficiency gap between\nlanguage model-based and syntactic parsers.\n5 Conclusion\nThis study presents a novel character-level log\nparser that extracts templates by predicting a se-\nquence of binary-coded decimals. A parsing cache\nmodule and efficient 4-gram feature aggregation\nvia CNN enhance its efficiency. Results on bench-\nmark and industrial datasets show the method out-\nperforms syntactic parsers and rivals LLM-based\nparsers for accurate and granular template extrac-\ntion. Future work aims to develop a vocabulary-\nfree model for broader applicability, integrate with\ndownstream tasks, and explore unsupervised pre-\ntraining on large-scale log data.\n--- Page 5 ---\n6 Limitations\nThe character-level log parser in this study operates\non character embeddings fixed using the Loghub-\n2.0 training data. While the trained model suc-\ncessfully captured most characters needed for fine-\ntuning and testing with our industrial dataset, Idata,\na vocabulary-free approach could improve deploy-\nment efficiency. Our evaluation primarily focuses\non accurate template extraction, but it can be ex-\ntended to metrics like group accuracy (Zhu et al.,\n2019), which are crucial for log sequence anomaly\ndetection (Yu et al., 2023). To pre-train our su-\npervised learning model, we limited the dataset to\n50k logs with sufficient template diversity. How-\never, we believe that incorporating more logs, a\nwider variety of templates, and longer training\ncould improve generalization on new datasets after\nfine-tuning on smaller training sets. This may also\nrequire adjustment in the current hyper-parameters\nused in this work. Additionally, we did not ad-\ndress the continuous learning loop (Van de Ven\nand Tolias, 2019), which is crucial for real-world\ndeployment.\nReferences\nMerve Astekin, Max Hort, and Leon Moonen. 2024. An\nexploratory study on how non-determinism in large\nlanguage models affects log parsing. In Proceedings\nof the ACM/IEEE 2nd International Workshop on\nInterpretability, Robustness, and Benchmarking in\nNeural Software Engineering , pages 13–18.\nHetong Dai, Heng Li, Che-Shao Chen, Weiyi Shang,\nand Tse-Hsun Chen. 2020. Logram: Efficient log\nparsing using nn-gram dictionaries. IEEE Transac-\ntions on Software Engineering , 48(3):879–892.\nWilliam A Falcon. 2019. Pytorch lightning. GitHub , 3.\nShuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenx-\nuan Wang, Hongyu Zhang, and Michael R Lyu.\n2023. What makes good in-context demonstrations\nfor code intelligence tasks with llms? In 2023 38th\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE) , pages 761–773. IEEE.\nShayan Hashemi, Jesse Nyyssölä, and Mika V Mäntylä.\n2024. Logpm: Character-based log parser bench-\nmark. In 2024 IEEE International Conference on\nSoftware Analysis, Evolution and Reengineering\n(SANER) , pages 705–710. IEEE.\nPinjia He, Jieming Zhu, Zibin Zheng, and Michael R\nLyu. 2017. Drain: An online log parsing approach\nwith fixed depth tree. In 2017 IEEE international\nconference on web services (ICWS) , pages 33–40.\nIEEE.Yintong Huo, Yuxin Su, Cheryl Lee, and Michael R\nLyu. 2023. Semparser: A semantic parser for log\nanalytics. In 2023 IEEE/ACM 45th International\nConference on Software Engineering (ICSE) , pages\n881–893. IEEE.\nZhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li,\nJunjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu,\nand Michael R Lyu. 2024a. Lilac: Log parsing using\nllms with adaptive parsing cache. Proceedings of the\nACM on Software Engineering , 1(FSE):137–160.\nZhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li,\nYintong Huo, Jiazhen Gu, Zhuangbin Chen, Jieming\nZhu, and Michael R. Lyu. 2024b. A large-scale eval-\nuation for log parsing techniques: How far are we?\nInProceedings of the 33rd ACM SIGSOFT Interna-\ntional Symposium on Software Testing and Analysis ,\nISSTA 2024, page 223–234, New York, NY , USA.\nAssociation for Computing Machinery.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking\npositional encoding in language pre-training. In In-\nternational Conference on Learning Representations .\nZanis Ali Khan, Donghwan Shin, Domenico Bianculli,\nand Lionel Briand. 2022. Guidelines for assessing\nthe accuracy of log message template identification\ntechniques. In Proceedings of the 44th International\nConference on Software Engineering , pages 1095–\n1106.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR) , San\nDiega, CA, USA.\nVan-Hoang Le and Hongyu Zhang. 2023. Log pars-\ning with prompt-based few-shot learning. In 2023\nIEEE/ACM 45th International Conference on Soft-\nware Engineering (ICSE) , pages 2438–2449. IEEE.\nZhenhao Li, Chuan Luo, Tse-Hsun Chen, Weiyi Shang,\nShilin He, Qingwei Lin, and Dongmei Zhang. 2023.\nDid we miss something important? studying and\nexploring variable-aware log abstraction. In 2023\nIEEE/ACM 45th International Conference on Soft-\nware Engineering (ICSE) , pages 830–842. IEEE.\nYudong Liu, Xu Zhang, Shilin He, Hongyu Zhang,\nLiqun Li, Yu Kang, Yong Xu, Minghua Ma, Qing-\nwei Lin, Yingnong Dang, et al. 2022. Uniparser: A\nunified log parser for heterogeneous log data. In Pro-\nceedings of the ACM Web Conference 2022 , pages\n1893–1901.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional LSTM-CNNs-CRF.\nInProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1064–1074, Berlin, Germany.\nAssociation for Computational Linguistics.\nSasho Nedelkoski, Jasmin Bogatinovski, Alexander\nAcker, Jorge Cardoso, and Odej Kao. 2021. Self-\nsupervised log parsing. In Machine Learning and\n--- Page 6 ---\nKnowledge Discovery in Databases: Applied Data\nScience Track: European Conference, ECML PKDD\n2020, Ghent, Belgium, September 14–18, 2020, Pro-\nceedings, Part IV , pages 122–138. Springer.\nStefan Petrescu, Floris Den Hengst, Alexandru Uta, and\nJan S Rellermeyer. 2023. Log parsing evaluation in\nthe era of modern software systems. In 2023 IEEE\n34th International Symposium on Software Reliability\nEngineering (ISSRE) , pages 379–390. IEEE.\nKeiichi Shima. 2016. Length matters: Clustering sys-\ntem log messages using length of words. arXiv\npreprint arXiv:1611.03213 .\nMonika Steidl, Marko Gattringer, Michael Felderer,\nRudolf Ramler, and Mostafa Shahriari. 2022. Re-\nquirements for anomaly detection techniques for mi-\ncroservices. In International Conference on Product-\nFocused Software Process Improvement , pages 37–52.\nSpringer.\nCharles Sutton, Andrew McCallum, et al. 2012. An in-\ntroduction to conditional random fields. Foundations\nand Trends® in Machine Learning , 4(4):267–373.\nJan Svacina, Jackson Raffety, Connor Woodahl, Brook-\nlynn Stone, Tomas Cerny, Miroslav Bures, Dongwan\nShin, Karel Frajtak, and Pavel Tisnovsky. 2020. On\nvulnerability and security log analysis: A systematic\nliterature review on recent trends. In Proceedings of\nthe International Conference on Research in Adap-\ntive and Convergent Systems , pages 175–180.\nStefan Thaler, Vlado Menkonvski, and Milan Petkovic.\n2017. Towards a neural language model for signature\nextraction from forensic logs. In 2017 5th Interna-\ntional Symposium on Digital Forensic and Security\n(ISDFS) , pages 1–6. IEEE.\nGido M Van de Ven and Andreas S Tolias. 2019. Three\nscenarios for continual learning. arXiv preprint\narXiv:1904.07734 .\nDanny TY Wu, Nikolas Smart, Elizabeth L Ciemins,\nHolly J Lanham, Curt Lindberg, and Kai Zheng. 2017.\nUsing ehr audit trail logs to analyze clinical work-\nflow: a case study from community-based ambulatory\nclinics. In AMIA Annual Symposium Proceedings ,\nvolume 2017, page 1820. American Medical Infor-\nmatics Association.\nYifan Wu, Bingxu Chai, Siyu Yu, Ying Li, Pinjia He,\nWei Jiang, and Jianguo Li. 2024. Logptr: Variable-\naware log parsing with pointer network. arXiv\npreprint arXiv:2401.05986 .\nYuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik\nKundu, Nishil Talati, et al. 2024. Understanding\nthe performance and estimating the cost of llm fine-\ntuning. arXiv preprint arXiv:2408.04693 .\nYi Xiao, Van-Hoang Le, and Hongyu Zhang. 2024.\nStronger, faster, and cheaper log parsing with llms.\narXiv preprint arXiv:2406.06156 .Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu\nZhang, and Pinjia He. 2024. Divlog: Log parsing\nwith prompt enhanced in-context learning. In Pro-\nceedings of the IEEE/ACM 46th International Con-\nference on Software Engineering , pages 1–12.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng\nQiu. 2019. Tener: adapting transformer encoder\nfor named entity recognition. arXiv preprint\narXiv:1911.04474 .\nSiyu Yu, Pinjia He, Ningjiang Chen, and Yifan Wu.\n2023. Brain: Log parsing with bidirectional paral-\nlel tree. IEEE Transactions on Services Computing ,\n16(5):3224–3237.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nInternational conference on machine learning , pages\n12697–12706. PMLR.\nAoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu,\nQingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li,\nand Qingsong Wen. 2024. Logparser-llm: Advancing\nefficient log parsing with large language models. In\nProceedings of the 30th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining , pages\n4559–4570.\nJieming Zhu, Shilin He, Pinjia He, Jinyang Liu, and\nMichael R Lyu. 2023. Loghub: A large collection\nof system log datasets for ai-driven log analytics. In\n2023 IEEE 34th International Symposium on Soft-\nware Reliability Engineering (ISSRE) , pages 355–\n366. IEEE.\nJieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie,\nZibin Zheng, and Michael R Lyu. 2019. Tools and\nbenchmarks for automated log parsing. In 2019\nIEEE/ACM 41st International Conference on Soft-\nware Engineering: Software Engineering in Practice\n(ICSE-SEIP) , pages 121–130. IEEE.\nA Datasets Statistics\nA.1 Loghub\nAn extensive collection of logs generated by 16\ndifferent software systems is released as Loghub\nby Zhu et al. (2023). This dataset accounts for\n490 Million log lines amounts to over 77GB in\ntotal. Nonetheless, it provides annotated ground\ntruth for log parsing of only 2000 random log lines\nfor each sub-dataset, named as Loghub-2k. It has\nbeen used to benchmark many different log pars-\ning systems (Zhu et al., 2019; Dai et al., 2020).\nWith the raw Loghub logs, Jiang et al. (2024b)\nbuilds a large scale annotated dataset for log pars-\ning. It consists of 50 Million log lines with 3488\nunique template for 14 different software systems.\nLog templates are annotated adhering to a rigurous\n--- Page 7 ---\nDatasets#Templates #Templates #No of logs\n(Loghub-2k) (Loghub-2.0) (Loghub-2.0)\nHadoop 114 236 179,993\nHDFS 14 46 11,167,740\nOpenstack 43 48 207,632\nSpark 36 236 16,075,117\nZookeeper 50 89 74,273\nBGL 120 320 4,631,261\nHPC 46 74 429,987\nThunderbird 149 1241 16,601,745\nLinux 118 338 23,921\nMac 341 626 100,314\nApache 6 29 51,977\nOpenSSH 27 38 638,946\nHealthApp 75 156 212,394\nProxifier 8 11 21,320\nTotal 1,147 3,488 50,416,620\nTable 2: Statistics on Loghub-2.0 and Loghub-2k\nframework which considers heuristic rules given\nby Khan et al. (2022) and parameter categories pro-\nposed by Li et al. (2023). Similarly, Loghub-2k is\nalso revised as per Khan et al. (2022) and is avail-\nable on GitHub1. We trained our method using 50k\nlog lines from 2,349 unique templates in Loghub-\n2.0 and tested it on the revised Loghub-2k. Table 2\nprovides detailed statistics on the datasets used. No\npre-processing was applied to the training or test\nsets; raw annotated templates were used directly.\nA.2 Idata\nWe extracted 100k log lines from an internal cloud\nsystem and sampled 6,000 for annotation. Two an-\nnotators independently labeled templates for 3,000\nlog lines each, resulting in 36 unique templates. Of\nthese, 35 templates reached consensus, covering\n2,400 log lines. One contested template accounted\nfor the remaining 3,600 log lines, skews the dataset.\nSince accurately identifying this single template\ncould inflate Parsing Accuracy to over 50%, we\nexcluded it, and only the remaining annotated log\nlines were used for this study.\nB Implementation Details\nThe character embedding dimension is set to 128.\nThe encoder employs 8-head self-attention, and the\nCNN is configured with an output filter size of 128,\na kernel size of 4, and a stride of 4. Following\nthe CNN, a single-layer BiLSTM is used, with a\nhidden size of 64 ×2. To prevent overfitting, a\ndropout rate of 0.4 is applied after both the CNN\nand BiLSTM, with skip connections integrated at\nboth the encoder and BiLSTM layers to mitigate\n1https://github.com/logpai/loghub-2.0/tree/\nmain/2k_datasetLayer Details\nchar_embed Embedding(100, 128)\nattn_self_1 ResidualAttentionBlock\n- MultiheadAttn(128, 8 heads)\n- MLP(128 →256→128)\npos_enc PosEnc(dropout=0.1)\ncnn_1d Conv1d(128, 128, k=4, s=4)\nlstm_2 BiLSTM(128, 64)\nln_3 LayerNorm(128)\ncrf_layer CRF(Linear(128 →16))\ndropout Dropout(0.4)\nrelu ReLU()\nTable 3: Architecture summary of 4bitparser.\nvanishing gradient issues. Our model architecture\nsummary is shown in Table 3. The model, imple-\nmented using PyTorch Lightning (Falcon, 2019),\ncontains 312k parameters (4.2 MB) and is trained\non a single 16GB V100 GPU or 10 epochs. We\nuse a batch size of 16, a weight decay of 1e−4, and\na learning rate of 1e−3. We employed the Adam\noptimizer (Kingma and Ba, 2015), using PyTorch’s\ndefault hyperparameters, except for the learning\nrate. The code for reproducing our experiments on\nthe public datasets will be released on GitHub.\nC Other Tried Architectures\nWe tried replacing the encoder with the BiLSTM\nblock similar to Thaler et al. (2017), but this re-\nsulted in increased inference and training time with\na minor drop in performance, in line with Yan et al.\n(2019). We also tried kernel sizes of 2,3,4and5\nat the down-sampling block. Apart from 4other\nkernel sizes reported drop in performance with ker-\nnel size of <4lead to increase inference time. We\nfind that 4-gram features efficiently represent logs,\nas static or variable sub-tokens typically average\nfour characters. Combination of BiLSTM and CRF\nis inspired by the V ALB log parser (Li et al., 2023),\nwhich uses word embeddings instead our method\noperates on fixed-length n-grams. Removing the\nBiLSTM layer and replacing the CRF with a linear\ntransform resulted in poorer performance.",
  "text_length": 30553
}