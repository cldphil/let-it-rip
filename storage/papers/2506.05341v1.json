{
  "id": "http://arxiv.org/abs/2506.05341v1",
  "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning",
  "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.",
  "authors": [
    "Xingjian Ran",
    "Yixuan Li",
    "Linning Xu",
    "Mulin Yu",
    "Bo Dai"
  ],
  "published": "2025-06-05T17:59:42Z",
  "updated": "2025-06-05T17:59:42Z",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05341v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05341v1  [cs.CV]  5 Jun 2025Direct Numerical Layout Generation for 3D Indoor\nScene Synthesis via Spatial Reasoning\nXingjian Ran1,2Yixuan Li3Linning Xu3Mulin Yu1Bo Dai2∗\n1Shanghai Artificial Intelligence Laboratory2The University of Hong Kong\n3The Chinese University of Hong Kong\nhttps://directlayout.github.io/\nA foosball table is at the center of a game room, with two gaming chairs positioned on its long sides. A large TV sits on a wooden stand against the wall, \nbehind the foosball table. To the left of the TV is a silver mini fridge. On the other wall, a tall bookshelf is filled with board games. Two posters are \ndisplayed on the walls, one next to the bookshelf and one next to the TV. \nOurs Holodeck I-Design LayoutGPT \nFigure 1: Our method synthesizes 3D indoor scenes from text descriptions via direct numerical\nlayout generation, demonstrating strong performance in both instruction compliance and physical\nplausibility. In contrast, existing methods often suffer from issues related to inappropriate placement\nand size, as highlighted by the red circles. Furthermore, they struggle to identify all the entities in\nfine-grained user instruction resulting in object omission, indicated by the yellow circles. All methods\nshare the same assets generated by 3D object generation method to ensure a fair comparison.\nAbstract\nRealistic 3D indoor scene synthesis is vital for embodied AI and digital content\ncreation. It can be naturally divided into two subtasks: object generation and layout\ngeneration. While recent generative models have significantly advanced object-level\nquality and controllability, layout generation remains challenging due to limited\ndatasets. Existing methods either overfit to these datasets or rely on predefined\nconstraints to optimize numerical layout that sacrifice flexibility. As a result, they\nfail to generate scenes that are both open-vocabulary and aligned with fine-grained\nuser instructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial reasoning of\nlarge language models (LLMs). DirectLayout decomposes the generation into three\nstages: producing a Bird’s-Eye View (BEV) layout, lifting it into 3D space, and\nrefining object placements. To enable explicit spatial reasoning and help the model\ngrasp basic principles of object placement, we employ Chain-of-Thought (CoT)\nActivation based on the 3D-Front dataset. Additionally, we design CoT-Grounded\nGenerative Layout Reward to enhance generalization and spatial planning. During\ninference, DirectLayout addresses asset-layout mismatches via Iterative Asset-\nLayout Alignment through in-context learning. Extensive experiments demonstrate\nthat DirectLayout achieves impressive semantic consistency, generalization and\nphysical plausibility.\n∗Corresponding author.\nPreprint. Under review.\n--- Page 2 ---\n1 Introduction\nHigh-fidelity, spatially coherent 3D indoor scenes are crucial for embodied AI, virtual reality, and film\nproduction. Particularly in advancing embodied AI, the training of agents for tasks like navigation\nand object manipulation relies heavily on realistic and diverse simulation environments. To address\nthe complexity of 3D scene synthesis, the task can be divided into two stages: object generation and\nlayout generation. Recent 3D generative models [ 28,33,35] have significantly advanced the quality\nand controllability of object generation. In contrast, layout generation remains underexplored, with\nkey challenges including physical plausibility, semantic alignment, and diversity. Physical plausibility\nrequires that objects are arranged in ways consistent with real-world physics, for instance, they\nshould rest stably on surfaces and avoid unnatural overlaps. Semantic alignment means the layout\nmust accurately reflect the user’s textual instructions, such as placing a bed next to a window when\nrequested. Diversity involves the ability to generate a wide variety of object types and room layouts,\nextending beyond typical bedrooms and living rooms to spaces like kitchens, offices, or playrooms.\nDue to limitations in scale, diversity, and realism, existing 3D indoor layout datasets struggle\nto reflect the true distribution of real-world scenes. Directly training generative models on such\ndatasets [ 16,19,30], often leads to models that simply memorize dataset-specific patterns, resulting\nin poor generalization to novel room and object types. To alleviate this data bottleneck, recent\napproaches [ 1,8,31] introduce manually defined spatial constraints, guiding the numerical layout\ngeneration process through intermediate representations such as scene graphs. These methods improve\nphysical plausibility and reduce hallucinations. However, it inherently sacrifices flexibility and makes\nit challenging to accommodate fine-grained user instructions. This limitation stems from the reliance\non predefined constraints, which struggle to capture the diversity and complexity of real-world layouts.\nBecause object placements frequently depend on contextual, functional, and aesthetic factors, all of\nwhich are difficult to exhaustively specify with fixed rules.\nTo address the data challenge, we enable the model to learn generalizable spatial reasoning and\ndirectly generate numerical layouts without relying on predefined rules. Our pipeline uses large\nlanguage models (LLMs) to generate 3D layout and object descriptions from textual input through\ndecomposed processes. During generation, the core of spatial reasoning in LLMs lies in Chain-of-\nThought (CoT), which comprises two key components. First, CoT Activation is introduced during\ntraining to enable structured reasoning steps, helping the model grasp fundamental spatial logic\nand forming a prior for spatial planning. This is achieved by supervising a four-step reasoning\nprocess that decomposes layout generation into subtasks, allowing the model to better understand and\norganize spatial information. Second, CoT-Grounded Generative Layout Reward provides a learning\nsignal to improve generalizable spatial reasoning under limited data, by assessing the plausibility\nof object placements and their consistency with the CoT reasoning process. It introduces a dual-\nevaluator framework, where a vision-language model (VLM) captures high-level spatial and semantic\nviolations, while a reasoning LLM identifies fine-grained numerical and logical inconsistencies,\ntogether offering structured, interpretable evaluation. After generation, we incorporate Iterative Asset-\nLayout Alignment to refine layout-object consistency and enhance scene realism based on spatial and\nsemantic feedback provided by the aforementioned dual-evaluator. As shown in Figure 1, existing\nmethods often suffer from inappropriate placements and object omission. In contrast, our approach\ngenerates 3D scenes with higher physical plausibility and better alignment with the user instruction.\nOur contributions are summarized as follows:\n•We propose a novel framework for 3D indoor scene synthesis that generates and refines layouts\ndirectly from text descriptions, bypassing the need for constrained optimization. This is achieved\nthrough a carefully designed task decomposition, which simplifies the process and improves overall\nefficiency.\n•We introduce CoT reasoning into 3D scene synthesis, enhancing the spatial planning capabilities\nof the model. By connecting CoT reasoning with reinforcement learning through CoT-Grounded\nGenerative Layout Reward, we significantly improve the model’s reasoning accuracy and reduce\nhallucination. Additionally, we curate a CoT dataset for 3D indoor layouts to support this approach.\n•Our DirectLayout outperforms existing methods in general 3D scene synthesis, demonstrating that\ndirect generation of numerical layouts is highly effective. The generated layouts are more consistent\nwith physical laws and show improved semantic consistency. Moreover, the direct generation of\nnumerical layouts allows for better fine-grained control.\n2\n--- Page 3 ---\n2 Related Work\n2.1 3D Scene Synthesis Approaches\n3D indoor scene synthesis methods vary in representation and generation strategy. Rule-based\nmethods [ 4,15,22] provide structured layouts and are particularly suited for task-driven evaluation.\nIn contrast, recent research has focused on learning-based models [ 14,18,27], which aim to generalize\nfrom data and facilitate the generation of more diverse and semantically rich scenes. ATISS [ 19] uses\na permutation-equivariant transformer to generate object sets conditioned on a floor plan, supporting\ninteractive editing. LayoutGPT [ 6] treats an LLM as a visual planner to predict layouts from text\nusing CSS-like prompts, performing competitively in both 2D and 3D settings. Structured approaches\nlike InstructScene [ 16] and AnyHome [ 8] incorporate scene graphs as intermediates to enhance\ncontrol and semantic alignment. Holodeck [ 31] encodes relational constraints via LLM and solves a\nconstraint optimization problem for object placement. LayoutVLM [ 23] combines visual grounding\nwith differentiable optimization, refining object placements through relation-based objectives and\ninitial numerical poses, while making deliberate trade-offs between flexibility and realism. [ 5] propose\na hierarchical planning strategy for 3D scene generation using VLMs. Their method decomposes\nscenes into room, region, and object levels, using tree search to explore layout options and improve\nspatial plausibility. Previous approaches mainly focus on better modeling the distribution of indoor\nscene layouts, whereas our method leverages limited data to learn the underlying placement logic of\nindoor layouts.\n2.2 Reasoning Augmentation in Vision\nRecent advancements [ 9,32,34,36] have utilized the general reasoning capabilities of LLMs and\nVLMs to tackle visual tasks. However, other works delve deeper into methods designed to explicitly\nenhance their reasoning abilities and performance within the visual domain. For instance, [ 3] prompts\nLLMs to interpret complex textual descriptions step-by-step, transforming them into structured 2D\nlayouts that improve compositional accuracy, which are subsequently used to condition diffusion\nmodels. LLaV A-CoT [ 29] introduces a multi-stage reasoning framework which substantially boosts\nperformance on complex visual question-answering benchmarks. SpatialCoT [ 17] enhances embodied\ntask planning by aligning vision-language inputs with spatial coordinates and applying chain-of-\nthought spatial grounding, resulting in superior performance in navigation and manipulation tasks. [ 5]\nalso enhance layout reasoning by discretizing the scene into symbolic grids and prompting VLMs to\niteratively generate object placements, combining structured spatial reasoning with visual grounding.\nFurthermore, [ 11] integrates learned reward models, PARM and PARM++, into autoregressive image\ngeneration, facilitating stepwise verification that improves image quality and alignment. Drawing\ninspiration from these works, our method seeks to enhance layout reasoning through the use of\nexplicit logical outputs and reward-based feedback.\n3 Method\n3.1 Problem Formulation\nGiven general textual descriptions and user-specified room dimensions, our goal is to synthesize\nphysically and semantically plausible 3D indoor scenes. The synthesis process focuses on generating\nstructured layouts, while 3D object assets are handled by existing generative methods. We represent\nthe 2D top-down layout (BEV layout) as:\nLBEV={(xi, yi, li, wi, oi)}N\ni=1, (1)\nwhere (xi, yi)is the center coordinate of object i,(li, wi)are its length and width, and oiis its\norientation (rotation angle around the z-axis).\nTo extend this into 3D, we define the 3D layout as:\nL3D={(xi, yi, zi, li, wi, hi, oi, pi)}N\ni=1, (2)\nwhere ziandhidenote the vertical center and height of object i, andpiis a text prompt used to guide\n3D asset generation.\n3\n--- Page 4 ---\nBEV Layout Data Traning Stage \n3D Front Dataset Curation \nBEV Layout \nGenerator Chain-of-Thought: \n<Entity Extraction> \nA bed, a wardrobe… \n</Entity Extraction> \n<Order Decision> \nFirst place bed, second… \n</Order Decision> \n<Spatial Reasoning> \nBed should be against the wall… \n</Spatial Reasoning> \n<Answer Organization> \nBed {length: 88px; width: 40px… \n</Answer Organization> SFT\nGPT-4o \nSpatial Evaluator \nQuantitative Evaluator CoT-Grounded \nGenerative Layout \nReward \nInference Stage \nBEV Layout \nGenerator DPO\n3D Layout \nGenerator \nA sturdy wooden \nteacher’s desk A large modern whiteboard A tall bookshelf filled with books A ceiling-mounted projector \nA small student desk A simple classroom chair \nObject Generator \"A classroom with a teacher’s \ndesk, student desks…\" Iterative Asset-Layout \nAlignment\nSpatial \nEvaluator Quantitative \nEvaluator \nFigure 2: Overview of our method. Training Stage: BEV Layout Generator is first fine-tuned on\nBEV layouts curated from the 3D-Front dataset, guided by CoT annotations generated by GPT-4o.\nSubsequently, it is further optimized through DPO, leveraging CoT-Grounded Generative Layout\nReward derived from Spatial Evaluator (VLM) and Quantitative Evaluator (reasoning LLM). Infer-\nence Stage: Given a text prompt, BEV Layout Generator produces a 2D layout, which is then lifted\nto a 3D layout by 3D Layout Generator. Iterative Asset-Layout Alignment refines the 3D scene by\nusing the Spatial Evaluator and Quantitative Evaluator to provide feedback to the layout generators,\nensuring consistency between the layout and generated 3D assets from an object generator.\n3.2 Overview\nDespite the powerful reasoning capabilities of recent foundation language models, we observe that\nthey often lack spatial planning ability. To address this limitation, we decompose the scene synthesis\ntask into three stages that are comparatively easier to handle, while preserving the overall spatial\nconsistency, as illustrated in Figure 2. We first use a BEV Layout Generator to convert the input text\ninto a 2D top-down layout. Section 3.3 details how we enhance the model’s spatial reasoning through\nCoT Activation andCoT-Grounded Generative Layout Reward . The resulting BEV layout is then\nlifted into a 3D layout using the 3D Layout Generator , which also assigns a descriptive text prompt\nto each object instance to facilitate asset generation (Section 3.4). Finally, we apply Iterative Asset-\nLayout Alignment to reconcile discrepancies between the predicted layout and available assets during\ninference, using feedback from evaluators and in-context learning (Section 3.5). All components in\nour pipeline, including generators and evaluators, are built upon LLMs or VLMs. Among them, only\nthe BEV Layout Generator is explicitly fine-tuned for layout planning.\n3.3 Enhancing Spatial Reasoning in BEV Layout Generation\n3.3.1 CoT Activation\nWe leverage CoT reasoning to activate and guide the basic spatial planning capabilities of the BEV\nLayout Generator. Inspired by recent advancements in symbolic and mathematical reasoning with\nLLMs [ 10,13,26], we design a structured four-step CoT process. As illustrated in Figure 2, reasoning\nbegins with entity extraction , where the model parses the input text to identify object categories and\ntheir quantities. This is followed by an order decision step, which establishes a semantically grounded\nplacement sequence by prioritizing foundational objects. Next, spatial reasoning is performed to infer\neach object’s location, size, and orientation, relying on common sense and relative spatial relationships.\nFinally, the answer organization step produces the BEV layout in a structured numerical format\nsuitable for downstream use.\nTo supervise this reasoning process, we filter approximately 6,500 scenes from the 3D-Front\ndataset [ 7] following the pre-processing of ATISS [ 19], and use GPT-4o [ 12] to generate the CoT\n4\n--- Page 5 ---\nannotations for ground-truth layouts, guided by carefully designed prompts provided in Appendix A.1.\nWe then fine-tune BEV Layout Generator using supervised fine-tuning (SFT). While this improve\nlayout quality and reasoning fidelity, we observe occasional hallucinations and inconsistency between\nreasoning and final placements, especially on unseen object or scene types.\n3.3.2 CoT-Grounded Generative Layout Reward\nDue to the lack of large-scale and diverse scene layout datasets, it remains challenging for fine-tuned\nBEV Layout Generator to generalize well to unseen or complex situations. To further improve gener-\nalizable reasoning in layout generation, we propose CoT-Grounded Generative Layout Reward, which\nassesses layout plausibility through a dual-evaluator framework: Spatial Evaluator andQuantitative\nEvaluator . This dual-evaluator framework benefits from the complementary strengths of a VLM and\na reasoning LLM, leading to more robust layout assessment. The VLM serves as Spatial Evaluator,\nadept at capturing high-level spatial and semantic implausibility by leveraging both visual and textual\ncontext to evaluate the result and its consistency with CoT. In contrast, the reasoning LLM functions\nas Quantitative Evaluator, excelling at identifying fine-grained numerical and logical inconsistencies,\nparticularly those that are more subtle and not easily discernible through visual cues alone. Moreover,\ninstead of relying on absolute score prediction, which is difficult due to the subjective and diverse\nnature of layout design, our reward focuses on detecting violations of physical and common sense\nconstraints, aligning more closely with the pretraining priors of large models.\nObject-Level Criteria. To provide a comprehensive evaluation of layout quality, we assess each\ngenerated layout at the object level using seven criteria divided between the two evaluators:\nSpatial Evaluator:\n•Relative Alignment (C1): Measures the reasonableness of spatial relations with related objects.\n•Global Positioning (C2): Assesses if the position aligns with the room context and user prompt.\n•Consistency with CoT (C3): Ensures that object placements align with the spatial instructions\ninferred or explicitly stated in the CoT description.\nQuantitative Evaluator:\n•Inter-object Distance (C4): Checks that spacing between objects supports accessibility and func-\ntionality. Overlaps may be acceptable if vertical stacking is plausible.\n•Size Proportion (C5): Evaluates whether object sizes are reasonable within the scene.\n•Orientation Validity (C6): Ensures that object orientations are semantically appropriate.\n•Quantity Alignment (C7): Validates whether the number of instances per object class aligns with\nexpectations given the room’s purpose.\nEach criterion Ckyields a validity ratio, which serves as the reward for the layout with respect\nto that specific criterion. This ratio reflects the proportion of objects in the layout that satisfy the\ncorresponding requirement, thereby quantifying the degree to which the layout adheres to Ck.\nrk=Ok\nN, k∈ {1,2,3,4,5,6} (3)\nwhere Okdenotes the number of objects satisfying criterion CT, andNis the total number of objects\nin the layout. For C7, the quantity alignment criterion, we define:\nr7= max\n0,1−PM\nc=1\f\f\fnactual\nc−nexpected\nc\f\f\f\nPM\nc=1nexpected\nc\n (4)\nwhere nactual\nc andnexpected\nc denote the actual and expected object count for class c, andMis the number\nof object classes.\nBy integrating both evaluators, we construct a reward signal that captures both semantic and numeric\nfidelity of layouts, providing holistic feedback.\nScene-Level Reward Aggregation. To obtain a unified reward per sample, we apply the entropy\nweight method, as different scene descriptions impose uneven difficulty across criteria. Entropy\n5\n--- Page 6 ---\ncaptures the information content of each criterion in varying contexts, enabling more adaptive reward\naggregation. Given Tlayout samples {S1, ..., S T}for a prompt, and their criterion-wise validity\nratios{r(j)\nk}for sample j, the entropy of criterion kis:\nHk=−1\nlnTTX\nj=1p(j)\nklnp(j)\nk,where p(j)\nk=r(j)\nkPT\nj=1r(j)\nk(5)\nWe define the weight for each criterion as:\nwk=1−HkP7\nk=1(1−Hk)(6)\nThe final reward R(j)for sample Sjis the weighted average:\nR(j)=7X\nk=1wk·r(j)\nk(7)\nModel Training. We collect 200 diverse prompts with room dimensions using GPT-4o [ 12], featuring\nvarying levels of detail (see Appendix A.3), and generate 30 BEV layout samples per prompt. Each\nsample is assigned a reward based on the above procedure. We construct the training set by sampling\ndata pairs whose reward differences exceed a certain threshold (with the higher-scoring sample labeled\naschosen and the lower-scoring one as rejected ), and fine-tune BEV layout generator using the\nDirect Preference Optimization (DPO) [21]. More training details can be found in Appendix A.2.\n3.4 Layout Lifting and Scene Synthesis\nGiven the predicted BEV layout, we employ 3D Layout Generator to infer missing vertical attributes\nsuch as object heights and vertical positions, guided by the scene description and structured BEV\nlayout. It leverages the LLM’s strong prior knowledge of object sizes and height relationships (e.g.,\nbeds are lower than tables, lamps above desks), which are generally common sense, enabling effective\nzero-shot inference. In addition to completing vertical attributes, it generates a descriptive prompt\npifor each object, capturing category, shape, and style to support semantically aligned 3D asset\ngeneration.\nTo synthesize the final 3D scene, we generate assets from the prompts using an existing high-fidelity\n3D object generation model [ 28]. Each asset is resized according to (li, wi, hi)and positioned using\n(xi, yi, zi)andoi, thus completing the full scene assembly pipeline.\n3.5 Iterative Asset-Layout Alignment\nAlthough the generated prompts contain detailed textual descriptions, the generators lack direct access\nto visual information about 3D assets, often leading to inconsistencies between predicted layouts and\ngenerated objects. Specifically, the orientation of generated objects can be inconsistent, and instances\nof the same category may differ significantly in shape, such as a bathtub filled with water compared to\nan empty one, leading to uncertainties in the height of related objects (see Figure 6 for an example).\nTo address this, we propose Iterative Asset-Layout Alignment. After rendering the composed 3D\nscene, we re-evaluate it using the same Spatial Evaluator and Quantitative Evaluator introduced in\nSection 3.3, based on a shared set of object-level criteria Ck7\nk=1. For each violated criterion, the\nevaluators provide object-specific suggestions to adjust size, position, or orientation. These are then\nfed back into the layout generators as contextual input for iterative refinement. This process repeats\nuntil either no further revisions are proposed by the evaluators or a maximum number of refinement\nsteps is reached. Let L(t)\n3Ddenote the 3D layout at iteration t, and let F(t)represent the evaluator\nfeedback at iteration t, which consists of a set of suggestions or corrections based on the violation of\nspatial or semantic criteria. The updated layout is defined as:\nL(t+1)\n3D =Update (L(t)\n3D,F(t)), (8)\nwhere Update applies layout modifications suggested by the evaluators. Through this looped align-\nment mechanism, our system is able to dynamically correct mismatches between layout and asset\ngeometry, resulting in coherent and realistic 3D scenes.\n6\n--- Page 7 ---\nA kitchen with a refrigerator, stove, sink, countertop, dining table, and several cabinets, along with a few bar stools \nOurs Holodeck I-Design LayoutGPT \nA computer classroom with individual computer desks, ergonomic chairs, a projector screen, a table, a printer, and a network hub \nIn a home theater, a large screen is positioned against a wall, flanked by two speakers. In front of it, a round coffee table holds popcorn and potato \nchips. A sofa sits behind the coffee table, and several bean bags are arranged around it. \nA wooden table holds a laptop on the front left corner, with a glass of colored pencils placed to its right. At the center back of the table sits a blue \nvase filled with flowers, and in front of the vase lie a pair of black-framed glasses. On the left side of the table is a white coffee cup, while a wooden \nchair is positioned in front of the table. \nFigure 3: Qualitative comparisons with scene synthesis methods. We compared our generated\nscenes with existing methods across various scene types and coarse-to-fine prompt granularities. Our\nresults demonstrate a better alignment with the text descriptions across different prompt granularities\nand scene types.\n4 Experiments\n4.1 Experimental Setup\nWe evaluate our approach on a set of 15 indoor scene categories, each associated with three distinct\nprompts and room sizes. These test indoor scenes are automatically generated and manually verified\nto ensure consistency across categories. To assess the effectiveness and reproducibility of our method,\nwe implement two distinct system configurations. In the Closed-Source Enhanced setting, we utilize\nGPT-4o [ 12] as both 3D Layout Generator and Spatial Evaluator, while o1 [ 13] serves as Quantitative\nEvaluator. In contrast, the Open-Source Only configuration offers a fully reproducible alternative by\nemploying Qwen2.5-72B [ 24] as 3D Layout Generator, Qwen2.5-VL-72B [ 25] as Spatial Evaluator,\nand QwQ-32B [26] as Quantitative Evaluator.\nIn both configurations, we use a fine-tuned Qwen2.5-32B as BEV Layout Generator. We compare\nour method against several recent baselines in text-to-layout generation, including LayoutGPT [ 6],\n7\n--- Page 8 ---\nTable 1: Comparison with existing methods. Our approach outperforms baselines in both physical\nplausibility and semantic alignment on general indoor scenes.\nMethod Out of Bound ↓Collision ↓Pos. ↑Rot. ↑PSA ↑CLIP Score ↑\nHolodeck 36.33 8.32 51.56 50.18 49.78 25.21\nI-Design 16.00 18.85 13.47 13.82 29.33 14.09\nLayoutGPT 33.91 15.89 57.24 51.31 55.11 21.65\nOurs w/ QwQ BEV 18.85 12.50 44.33 43.64 47.11 23.83\nOurs (open-source) 9.20 8.45 70.73 67.51 73.78 26.47\nOurs (full) 8.74 6.31 72.04 69.87 75.56 27.43\nHolodeck [ 31], and I-Design [ 2]. Additionally, we construct a strong baseline by replacing our\nspecialized BEV Layout Generator with a general reasoning LLM (QwQ-32B) to examine the effect\nof inductive spatial bias.\nEvaluation Metrics: To evaluate physical plausibility , we employ two metrics: Out-of-Bound Rate\nand Collision Rate. The Out-of-Bound Rate is calculated as the proportion of objects that extend\nbeyond the scene boundaries relative to the total number of objects. Similarly, the Collision Rate\nrepresents the proportion of objects that collide with other objects relative to the total number\nof objects. Following LayoutVLM [ 23], we assess semantic alignment and overall quality using\nPositional Coherency (Pos.), Rotational Coherency (Rot.), and the Physically-Grounded Semantic\nAlignment Score (PSA). In LayoutVLM, these LLM-based metrics, evaluated using GPT-4o, have\nbeen shown to correlate with human judgments. Additionally, we report the CLIP Score [ 20] to\nquantify the consistency between the generated layout and the input text prompt.\n4.2 Comparison with Baselines\nWe conduct a series of comparative evaluations aiming to verify the advantages of our approach over\nbaselines. These comparisons reveal the extent to which our model improves physical feasibility,\nsemantic fidelity, and user-controllable scene synthesis.\nComparison with Existing Methods. Table 1 and Figure 3 summarize the comparative results\nacross key metrics and visual outputs. LayoutGPT directly predicts object placements using in-\ncontext examples. However, the absence of logical training leads to outputs that violate physical\nconstraints and deviate from user instructions. Its generated layouts often lack structural coherence\nand semantic fidelity. Although LayoutGPT employs direct numerical layout generation, its limited\nspatial reasoning hampers its ability to translate detailed descriptions into coherent layouts.\nWhile I-Design leverages cooperative reasoning among multiple agents to coordinate layouts, it suffers\nfrom limited robustness when generating layout, particularly due to failures in agent communication\nor layout grounding. Although it produces physically plausible arrangements, the generated objects\ntend to cluster locally, lacking global semantic awareness. This is partly because I-Design extracts\nsymbolic constraints before layout generation, which improves physical plausibility but limits the\nmodel’s flexibility in handling complex and fine-grained spatial relationships often found in natural\nlanguage, such as “a vase in the center of the table, with a pair of glasses in front of it.” Its reliance\non predefined constraints ultimately restricts controllability and semantic fidelity.\nHolodeck adopts a search-based strategy that effectively avoids object collisions, but similarly falls\nshort in capturing prompt semantics, often generating layouts with limited object diversity and local\nfocus. Like I-Design, Holodeck also relies on symbolic constraint, which enhances physical validity\nbut weakens its ability to express detailed spatial instructions described in natural language.\nConversely, our open-source and full model enable both accurate physical constraint satisfaction\nand faithful semantic grounding. By integrating spatial reasoning directly into the numerical layout\ngeneration process without relying on symbolic constraints, our method maintains high physical\nplausibility and semantic alignment across all levels of prompt granularity—from coarse scene and\nobject types to fine-grained spatial instructions—as evidenced in Figure 3.\nComparison with Reasoning LLM. To isolate the contribution of our specialized BEV Layout\nGenerator, we replace it with a general-purpose reasoning model, QwQ-32B, while keeping the rest\nof our pipeline unchanged. This variant—referred to as Ours w/ QwQ BEV in Table 1—serves as\nan ablation baseline to evaluate the role of spatial inductive bias. Thanks to its strong capabilities in\nnumerical computation and local logical inference, QwQ-32B handles basic geometric constraints\n8\n--- Page 9 ---\nTable 2: Ablation study. We evaluate the effect of task decomposition and reward granularity.\nMethod Out of Bound ↓Collision ↓Pos. ↑Rot. ↑PSA ↑CLIP Score ↑\nw/o decomp 21.42 20.52 67.13 65.47 71.56 26.05\nw/o reward 26.04 16.77 64.47 57.18 68.82 25.25\nsimple reward 13.64 11.89 67.95 64.37 70.76 25.82\nOurs (full) 8.74 6.31 72.04 69.87 75.56 27.43\nA hallway with a shoe rack, coat hooks, mirror, bench, umbrella stand, table, and a rug. full simple reward w/o reward w/o decomp \nFigure 4: Ablation Study Results. The experiment validates the effectiveness of task decomposition\nand proposed CoT-Grounded Generative Layout Reward.\nrelatively well, maintaining acceptable rates of boundary and collision violations. However, the model\noften fails to preserve the holistic semantic context of a scene, leading to degraded performance\nin positional coherency, rotational alignment, and overall semantic alignment. This result suggests\nthat general-purpose models, while strong at localized inference, lack the domain-specific priors\nrequired for globally consistent layout synthesis. These findings, as reflected in Table 1, highlight the\nadvantage of incorporating dedicated spatial reasoning: they not only enforce physical feasibility but\nalso enhance high-level understanding of spatial semantics.\n4.3 Ablation Study\nTo gain a deeper understanding of the contribution of different components in our design, we\nconducted a series of ablation experiments focusing on the reward formulation and task decomposition.\nThese experiments were designed to isolate the impact of each factor on the overall performance.\nCoT-Grounded Generative Layout Reward. We investigate the impact of different reward con-\nfigurations on the spatial plausibility of the generated layouts. As shown in Table 2 and illustrated\nin Figure 4, when BEV Layout Generator is trained solely with supervised learning, without any\nreinforcement signal ( w/o reward ), the physical plausibility significantly deteriorated. This is evident\nin the substantially higher rates of objects being placed outside the scene boundaries and colliding\nwith each other. Introducing a simple reward, where a VLM provided a single overall score for the\nlayout quality ( simple reward ; see Appendix A.1 for details), leads to some improvement compared\nto the no-reward scenario. However, this approach still fall short of our full method’s performance.\nThese results suggest that relying on high-level semantic feedback from a general foundation model\nis not sufficient for effectively guiding the learning of physically realistic layouts. In contrast, our\nmethod, which utilizes fine-grained, spatially informed rewards, achieved considerably better results\nacross all evaluation metrics presented in Table 2 and visualized in Figure 4. This underscores the\ncritical role of structured spatial supervision in generating plausible indoor scenes.\nTask Decomposition. We evaluate the significance of our architectural modularity by comparing\nour full model with a variant where 3D layout generation and refinement were performed by a\nsingle model, without the intermediate 2D-to-3D reasoning step ( w/o decomp ). The results, detailed\nin Table 2 and qualitatively shown in Figure 4, indicate that this approach leads to a decline in\nphysical plausibility and slightly weaker global semantic alignment. These results suggest that our\ndecomposition strategy plays a crucial role in enabling more accurate spatial reasoning. By separating\nthe 2D planning stage from the 3D realization, the model appears to be better equipped to handle the\ninherent complexity and variability of indoor environments.\n9\n--- Page 10 ---\n5 Conclusion\nIn this paper, we present DirectLayout, a novel framework for generating 3D indoor scenes from\ntext descriptions via direct numerical layout generation. It follows a three-stage process including\nBEV layout generation, 3D lifting, and iterative refinement, enhanced by CoT Activation and\nCoT-Grounded Generative Layout Reward to improve generalizable spatial reasoning. Extensive\nexperiments show that DirectLayout achieves state-of-the-art performance in generating general\n3D scenes that are physically plausible, semantically coherent, and faithful to user prompts. We\ndemonstrate that generating layouts through direct numerical generation allows for a higher upper\nbound in handling more detailed and complex user inputs and scene arrangements.\nLimitations. Despite these advancements, the task decomposition and Iterative Asset-Layout Align-\nment, while effective, introduce additional inference time, which limits real-time interaction. Ad-\nditionally, the complexity of scenes generated by our method is constrained by the information\nprocessing capacity of the base model.\nReferences\n[1]Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan,\nR Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie. Open-universe indoor scene\ngeneration using llm program synthesis and uncurated object databases. arXiv preprint arXiv:2403.09675 ,\n2024.\n[2]Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, and Xi Wang. I-design:\nPersonalized llm interior designer. arXiv preprint arXiv:2404.02838 , 2024.\n[3]Xiaohui Chen, Yongfei Liu, Yingxiang Yang, Jianbo Yuan, Quanzeng You, Li-Ping Liu, and Hongxia Yang.\nReason out your layout: Evoking the layout master from large language models for text-to-image synthesis.\narXiv preprint arXiv:2311.17126 , 2023.\n[4]Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric\nKolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural\ngeneration. In NeurIPS , 2022.\n[5]Wei Deng, Mengshi Qi, and Huadong Ma. Global-local tree search in vlms for 3d indoor scene generation.\narXiv preprint arXiv:2503.18476 , 2025.\n[6]Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric\nWang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large\nlanguage models. In NeurIPS , 2023.\n[7]Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun,\nRongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In ICCV ,\n2021.\n[8]Rao Fu, Zehao Wen, Zichen Liu, and Srinath Sridhar. Anyhome: Open-vocabulary generation of structured\nand textured 3d homes. In ECCV , 2024.\n[9]Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Schölkopf. Graphdreamer: Composi-\ntional 3d scene synthesis from scene graphs. In CVPR , 2024.\n[10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 , 2025.\n[11] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann\nHeng. Can we generate images with cot? let’s verify and reinforce image generation step by step. arXiv\npreprint arXiv:2501.13926 , 2025.\n[12] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[13] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720 , 2024.\n10\n--- Page 11 ---\n[14] Kurt Leimer, Paul Guerrero, Tomer Weiss, and Przemyslaw Musialski. Layoutenhancer: Generating good\nindoor layouts from imperfect data. In SIGGRAPH Asia , 2022.\n[15] Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli, Dahua Lin,\nand Bo Dai. Proc-gs: Procedural building generation for city assembly with 3d gaussians. arXiv preprint\narXiv:2412.07660 , 2024.\n[16] Chenguo Lin and Yadong Mu. Instructscene: Instruction-driven 3d indoor scene synthesis with semantic\ngraph prior. In ICLR , 2024.\n[17] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue\nZhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through\ncoordinate alignment and chain-of-thought for embodied task planning. arXiv preprint arXiv:2501.10074 ,\n2025.\n[18] Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas J Guibas, and Peter Wonka. Generative layout modeling\nusing constraint graphs. In ICCV , 2021.\n[19] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss:\nAutoregressive transformers for indoor scene synthesis. In NeurIPS , 2021.\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML , 2021.\n[21] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model. In NeurIPS , 2023.\n[22] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen,\nMeenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen indoors: Photorealistic indoor\nscenes using procedural generation. In CVPR , 2024.\n[23] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber,\nand Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. arXiv\npreprint arXiv:2412.02193 , 2024.\n[24] Qwen Team. Qwen2.5: A party of foundation models, September 2024.\n[25] Qwen Team. Qwen2.5-vl, January 2025.\n[26] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025.\n[27] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. Sceneformer: Indoor scene generation with\ntransformers. In 3DV, 2021.\n[28] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin\nTong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint\narXiv:2412.01506 , 2024.\n[29] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language\nmodels reason step-by-step. arXiv preprint arXiv:2411.10440 , 2024.\n[30] Yixuan Yang, Junru Lu, Zixiang Zhao, Zhen Luo, James JQ Yu, Victor Sanchez, and Feng Zheng.\nLlplace: The 3d indoor scene layout generation and editing via large language model. arXiv preprint\narXiv:2406.03866 , 2024.\n[31] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber,\nRanjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments.\nInCVPR , 2024.\n[32] Tatiana Zemskova and Dmitry Yudin. 3dgraphllm: Combining semantic graphs and large language models\nfor 3d scene understanding. arXiv preprint arXiv:2412.18450 , 2024.\n[33] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu,\nand Jingyi Yu. Clay: A controllable large-scale generative model for creating high-quality 3d assets. In\nSIGGRAPH , 2024.\n[34] Qihang Zhang, Chaoyang Wang, Aliaksandr Siarohin, Peiye Zhuang, Yinghao Xu, Ceyuan Yang, Dahua\nLin, Bolei Zhou, Sergey Tulyakov, and Hsin-Ying Lee. Towards text-guided 3d scene composition. In\nCVPR , 2024.\n11\n--- Page 12 ---\n[35] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin\nYang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution\ntextured 3d assets generation. arXiv preprint arXiv:2501.12202 , 2025.\n[36] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and\nMing-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative\ngaussian splatting. In ICML , 2024.\n12\n--- Page 13 ---\nA Implementation Details\nA.1 Prompts\nIn this section, we provide prompts used for training and testing the model.\nCoT Data Generation. In order for the model to explicitly output logical thinking, we use the\nfollowing prompt to generate CoT data for BEV Layout Generator (In ablation experiments, we use\nthe same prompt to generate 3D layout CoT data, simply replacing the BEV layout format with the\n3D layout format):\nGiven a BEV layout for a scene , first output a short prompt\nsummarizing the scene , and then write a logical thought\nprocess when planning this layout , modeled after the\nfollowing chain of thought example . The layout follows the\nCSS style , where each line starts with the object description\nand is followed by its absolute position . Formally , each\nline should be like \" object { length : ?px; width : ?px;\ncenter_x : ?px; center_y : ?px; orientation : ? degrees ;}\". You\ncan simply interpret the length and width as the dimensions\nof the object itself , with center_x and center_y indicating\ntranslation (or the center point of the object ) and\norientation indicating rotation . Note that the 0 degrees\nrepresentation is aligned with the direction of the positive\nhalf - axis of the axes where \" width \" and \" center_y \" are\nlocated . The image is { max_length }px long and { max_width }px\nwide . Therefore , all bounding boxes should try not to exceed\n256 px after rotation .\nBelow are the steps of the chain of thought :\n1. Extract from the text description which objects should be\nplaced in the scene and how many of each of these are\nspecifically needed .\n2. List the order in which to place each type of object ,\ngenerally starting with the large and major objects , then\nmoving on to the decorative and minor objects associated with\nthem .\n3. Place each object in the order in 2. Each object is placed by\nfirst giving the dimension of the object , then the rotation ,\nand finally calculating the center point coordinates based\non where it should be in the scene . The process should take\ninto account the position of the object in the scene , its\nrelative position to the placed objects , and the constraints\nbetween the objects . In this step , you need to not only place\nthe object , but also give a detailed reason for placing it\nso.\n4. Organize the answers given in 3. to produce a final output\nthat meets the format requirements .\nHere is how the json format output should look :\n{\n\" prompt \": \"( The prompt you generate )\",\n\" response \": {\n\" Entity Extraction \": \"( Explanation of the objects extracted\nfrom the prompt )\",\n\" Order Decision \": \"( Explanation of the order in which the\nobjects should be placed , usually starting with major or\nlarge objects in the scene )\",\n\" Spatial Reasoning \": \"( Explanation of the dimensions ,\nrotation , and position (i.e. center point ) of each object\n, reasoning about each object should be as detailed as\n13\n--- Page 14 ---\npossible and take into account the scene and other\nobjects )\",\n\" Answer Organization \": \"( Final output in the required format\n, using lines like \" object { length : ?px; width : ?px;\ncenter_x : ?px; center_y : ?px; orientation : ? degrees ;}\") \"\n}\n}\nLayout Lifting. To complete each object’s 3D pose and textual description, we use the following\nprompt as input to 3D Layout Generator:\nGiven a sentence prompt that will be used to generate a scene\nand the BEV layout of this scene , lifting a 2D layout to a 3D\nlayout (i.e., predicting the range of heights an object\noccupies in space ) and designing a prompt for each object for\nobject asset generation . The generated layout should follow\nthe CSS style , where each line starts with the object\ndescription and is followed by its absolute position .\nFormally , each line should be like \" object { length : ?px;\nwidth : ?px; height : ?px; center_x : ?px; center_y : ?px;\ncenter_z : ?px; orientation : ? degrees ;}\". The given BEV\nlayout contains information other than height and center_z ,\nso the only information you need to add is about the objects\nin the z- axis . Be careful not to let objects that don ’t make\nsense appear overlapping . For example , chairs can go under\ntables , so they can overlap . And a lamp must go on top of a\ntable , so they can ’t overlap . The space is { max_length }px\nlong , { max_width }px wide , and 160 px high . Therefore , the\nheight of the bounding box should not exceed { max_height }px ,\ni.e. center_z +/- height /2 needs to be between 0 and {\nmax_height }. At the same time , for each object , prompt for\nobjects should be in one sentence of natural language ,\ndescribing its category , shape , and style . Finally give a\nlist in the same order as the objects in the layout , like [\nobj1_prompt , obj2_prompt , ...].\nHere is how the json format output should look :\n{\n\"3 D_layout \": [\n\"bed { length : 88 px; width : 40 px; height : 36 px; center_x : 120\npx; center_y : 60 px; center_z : 18 px; orientation : 0\ndegrees ;}\" ,\n...\n],\n\" object_prompts \": [\n\"A modern single bed with a rectangular frame and a wooden\nheadboard .\",\n...\n]\n}\nPrompt :\n{ text_description }\nBEV layout :\n{ bev_layout }\nQuantitative Evaluator Feedback. To obtain detailed, numerical feedback, we use the following\nprompt as input to Spatial Evaluator:\n14\n--- Page 15 ---\nYou are an expert assistant who is well versed in indoor scene\nlayout design . As an impartial judge , you are asked to make\nan in - depth evaluation of a 2D scene layout (BEV layout ),\nwhich is given by the bounding box of each object in the top\nview and labeled with the object category near the bounding\nbox . You can refer to the metadata of the layout to make a\njudgment . You need to judge whether each type of object in\nthe input BEV scene layout is in a reasonable position on\neach of the following four dimensions :\n1. Distance between objects : Evaluate whether objects are spaced\nappropriately to ensure functionality and accessibility . For\nexample , in organized environments such as classrooms or\noffices , furniture should be arranged with sufficient\nseparation to allow for easy movement . Desks in a classroom\nshould be spaced out so students can walk between them\ncomfortably , and sofas or desks in offices should not overlap\n. Note that in a top - down view , some overlap in bounding\nboxes may be physically reasonable when objects are\nvertically stacked , such as a ceiling lamp above a bed , or a\ncomputer placed on a desk . In such cases , evaluating bounding\nbox intersections alongside height context is essential .\n2. Quantitative alignment of objects : Assess whether the\nquantities of related objects are consistent with functional\nexpectations . For instance , in an office setting , the number\nof desks and chairs should correspond - each desk should\ntypically be paired with one chair .\n3. Size proportion of objects within the scene : Check whether\nthe relative sizes of objects make sense within the spatial\nlayout . For example , equipment such as computers in an office\nor lab instruments in a laboratory should be smaller than\nthe tables or workbenches they are placed on. Unnatural size\nratios , like a monitor larger than its desk , can suggest\nspatial implausibility .\n4. Orientation of objects : Verify that objects are oriented\nappropriately for their intended use within the layout . For\nexample , chairs in a classroom should face the blackboard ,\nand computer monitors in an office should be directed toward\nthe associated seating positions .\nPlease note that your choice must be based on a thorough\nunderstanding , analysis and evaluation of the image and the\nproblem . After the explanation of each dimension , answer the\nfinal evaluation . Finally , return your judgment in a legal\nJSON format , evaluating Yes if the location of a particular\nobject is considered reasonable in this dimension , and No if\nit is not. The json format and field definitions are as\nfollows :\n{\n\" object_class_name \": [\" Yes \" or \"No\", \"Yes \" or \"No\", \"Yes \" or \"No\n\", \"Yes \" or \"No \"] ( four judgments correspond to the previous\nfour dimensions )\n...\n}\nscene description : { scene_description }\nmax_length : { max_length } px ( horizontal axis )\nmax_width : { max_width } px ( vertical axis )\nBEV layout :\n{ bev_layout }\n{ metadata }\nSpatial Evaluator Feedback. To obtain high-level, semantic feedback, we use the following prompt\nas input to Spatial Evaluator:\n15\n--- Page 16 ---\nYou are an expert assistant who is well versed in indoor scene\nlayout design . As an impartial judge , you are asked to make\nan in - depth evaluation of the 2D scene layout ( BEV layout ),\nwhich is given by the bounding box of each object in the top\nview and labeled with the object category near the bounding\nbox . You can refer to both the picture and the layout\nmetadata for judging . You need to judge whether the\ninformation about each kind of object in the input BEV scene\nlayout is reasonable in each of the following three\ndimensions :\n1. Spatial alignment between objects : For example , the podium\nand projector in a classroom should be centrally aligned\neither horizontally or vertically , and lockers in locker\nrooms should be arranged in an orderly grid .\n2. Position of the objects within the layout : Consider whether\neach object ’s position is appropriate for the overall layout\ncontext . For example , in a single office the desk should be\nnear the center , and in a bank the counter should be near a\nwall .\n3. Consistency with Chain -of - Thought Descriptions : The physical\nplacements of objects should align with any provided textual\nchain -of - thought descriptions . For example , the chain of\nthought mentions placing the whiteboard next to the wall , but\nin the picture and digital layout the whiteboard is in the\nmiddle of the scene , which is unreasonable .\nPlease note that your choices must be based on a thorough\nunderstanding , analysis , and evaluation of the image and\nproblem . After the explanation of each dimension , answer the\nfinal evaluation . Return your judgment at the end of each\ndimension in a legal JSON format , evaluating Yes if the\nplacement of a particular object is considered reasonable in\nthat dimension , and No if not . The json format and field\ndefinitions are as follows :\n{\n\" object_class_name \": [\" Yes \" or \"No\", \"Yes \" or \"No\", \"Yes \" or \"No\n\"] ( three judgments correspond to the previous three\ndimensions )\n...\n}\nscene description : { scene_description }\nmax_length : { max_length } px ( horizontal axis in BEV)\nmax_width : { max_width } px ( vertical axis in BEV)\nBEV layout :\n{ bev_layout }\nchain of thought :\n{CoT}\nThe above two prompts are used to provide input to the evaluators during training, in order to compute\nCoT-Grounded Generative Layout Reward. During inference, we use similar prompts as input to\nthe evaluators to perform Iterative Asset-Layout Alignment, with the only change being that the\noutput format is adapted to provide object-level suggestions (while keeping the evaluation criteria\nunchanged).\nScene Description Generation Prompt. To construct diverse training and evaluation data, we use\nthe following prompt to instruct a language model to generate indoor scene descriptions at three\nlevels of granularity across multiple scene categories:\n16\n--- Page 17 ---\nYou are asked to generate indoor scene descriptions at three\nlevels of granularity : coarse , medium , and fine - grained .\nPlease follow the instructions carefully .\nThere are three types of granularity :\n1. Coarse : List the main objects in the room without mentioning\nwhere they are.\nExample : \"A home gym with a treadmill , yoga mat , dumbbell\nrack , water dispenser , and a large mirror .\"\n2. Medium : Describe the approximate spatial relationships\nbetween major object groups .\nExample : \"In a playroom , a toy shelf stands against the right\nwall , a bean bag lies in the corner near the window , and\na round play mat is placed in the center .\"\n3. Fine - grained : Provide precise , detailed spatial arrangements\namong individual objects .\nExample : \"A small square table is placed in the center of the\nroom . On the front right corner of the table sits a red\ntoolbox , with a measuring tape coiled beside it. A yellow\nstool is tucked in on the left side , and a desk lamp\nstands at the rear center of the table .\"\nYou will generate scene descriptions for { num_scene_types }\ndifferent indoor scene categories , excluding common\ncategories such as bedroom , living room , dining room , and\nstudy .\nFor each scene category , generate :\n{ num_coarse_per_type } descriptions at coarse granularity ,\n{ num_medium_per_type } at medium granularity ,\n{ num_fine_per_type } at fine - grained granularity .\nEach scene description should be accompanied by room dimensions :\nlength , width , and height ( each must be an integer <= 256) .\nOutput the results in a strict json format as follows :\n[\n{\n\" scene_type \": \" laundry room \",\n\" granularity \": \" coarse \",\n\" description \": \"A laundry room with a washing machine , dryer\n, laundry basket , shelves , and detergent bottles .\",\n\" room_size \": {\n\" length \": 256 ,\n\" width \": 171 ,\n\" height \": 240\n}\n},\n...\n]\nSimple Reward. To verify the effectiveness of CoT-Grounded Generative Layout Reward, we use\nGPT-4o to give a overall score for BEV layout.\nYou are an expert assistant who is well versed in indoor scene\nlayout design . As an impartial judge , you are asked to make\nan in - depth evaluation of the 2D scene layout ( BEV layout ),\nwhich is given by the bounding box of each object in the top\nview and labeled with the object category near the bounding\nbox . You can refer to both the picture and the layout\nmetadata for judging . Please score the scene based on\n17\n--- Page 18 ---\nphysical plausibility , semantic consistency , and degree of\ninstruction compliance .\nReturn your answer in legal JSON format . The format and field\ndefinitions are as follows :\n{\n\" score \": 1 -100\n}\nscene description : { scene_description }\nmax_length : { max_length } px ( horizontal axis in BEV)\nmax_width : { max_width } px ( vertical axis in BEV)\nBEV layout :\n{ bev_layout }\nchain of thought :\n{CoT}\nA.2 Training Details\nWe adopt a two-stage training pipeline to fine-tune the Qwen-2.5-32B. In the first stage, SFT is\nconducted on a generated CoT-augmented dataset containing approximately 6,500 CoT data of BEV\nlayout. Low-Rank Adaptation (LoRA) with a rank of 8 is applied to all target modules. The model is\ntrained for 3 epochs using a batch size of 1 and a gradient accumulation step of 8, with a learning\nrate of 1×10−4. A cosine learning rate scheduler with 10% warm-up is employed, and training is\nperformed using bfloat16 precision. The SFT stage is conducted on a single NVIDIA A800 GPU and\ntakes approximately 8 hours to complete.\nIn the second stage, DPO is employed to further enhance spatial reasoning via CoT-Grounded\nGenerative Layout Reward. The DPO dataset contains around 12,000 preference pairs, sampled such\nthat the reward difference between the preferred and rejected generations exceeds 20. The same\nLoRA configuration and optimization hyperparameters are used as in the SFT stage. The preference\nloss function is set to sigmoid withβ= 0.1. This stage is trained on 8 NVIDIA A800 GPUs for\napproximately 11 hours.\nA.3 Indoor Scene Description Generation Details\nTo support both training and evaluation, we generate a diverse set of textual indoor scene descriptions\nat three levels of granularity using GPT-4o following the prompt described in Appendix A.1. These\nlevels are designed to test and guide the model’s ability to interpret spatial semantics:\n1.Coarse : Lists the key objects in the scene without specifying spatial relationships.\nExample: “A kitchen with a refrigerator, stove, sink, countertop, dining table, and several cabinets,\nalong with a few bar stools.”\n2.Medium : Describes basic spatial layout and object grouping.\nExample: “In a home theater, a large screen is positioned against a wall, flanked by two speakers.\nIn front of it, a round coffee table holds popcorn and potato chips. A sofa sits behind the coffee\ntable, and several bean bags are arranged around it.”\n3.Fine-grained : Specifies detailed relative positions among objects.\nExample: “A wooden table holds a laptop on the front left corner, with a glass of colored pencils\nplaced to its right. At the center back of the table sits a blue vase filled with flowers, and in front\nof the vase lie a pair of black-framed glasses. On the left side of the table is a white coffee cup,\nwhile a wooden chair is positioned in front of the table.”\nTraining. For DPO training, we generate 200 scene descriptions across 40 indoor scene categories\nthat are not present in the 3D-Front dataset (e.g., excluding bedrooms, living rooms, dining rooms,\nand studies). Each category includes 5 descriptions and room sizes, with the three granularity levels\ndistributed in a 2:2:1 ratio (coarse:medium:fine-grained). These descriptions are directly used as\nprompts to generate layout samples for DPO training.\n18\n--- Page 19 ---\nEvaluation. For evaluation, we construct a separate set of 45 test cases across 15 indoor scene\ncategories, also distinct from those in 3D-Front. Each category is associated with three manually\ncrafted and verified descriptions (one per granularity level), along with a specific room size. All\ndescriptions are manually reviewed to ensure logical and spatial consistency. Minor corrections are\nmade where necessary—for example, adding missing chairs in office scenes—to ensure a realistic\nand challenging testbed for layout generation.\nB Impact Statement\nThis paper focuses on the technical advancements in realistic 3D indoor scene synthesis. The work\naims to enhance applications in virtual reality, gaming, and design, which could have positive societal\nimplications in these domains. However, this study does not directly address potential societal impacts,\nincluding possible negative consequences such as malicious or unintended uses (e.g., generating\nfake scenes), fairness considerations, privacy concerns, or security risks that might arise from the\napplication of this technology. The paper primarily presents technical research and does not discuss\nthe deployment of the technology or potential mitigation strategies for negative impacts.\nC More Generated Results\nIn Figure 5, we present additional qualitative results in a variety of scene types, including garages,\nclassrooms, bathrooms, and wine cellars. Our method consistently achieves strong physical plausibil-\nity and semantic alignment with language instructions, allowing users to exert fine-grained control\nover the generated content.\nA garage features a large car \ncentrally located, with storage \nshelves lining the back wall. A \nworkbench and a toolbox are \npositioned beneath a pegboard, \nwhile a bicycle and a lawnmower are \narranged nearby A classroom is furnished with a \nteacher's desk at the front, facing \nan arrangement of student desks \nand chairs. A whiteboard is mounted \non the front wall, and a projector \nhangs from the ceiling. A tall \nbookshelf also stands in the side A child’s bathroom with a low toilet, a \nstep stool, a bathtub with rubber \nducks, a colorful rug, and a small sink A wine cellar with storage and \ntasting areas is depicted. Along \ntwo opposing walls, extensive \nwooden racks are filled with \nwine bottles. On another wall, a \ncollection of oak barrels is \nstacked, with a wooden crate. A \ntasting table, accompanied by a \nsingle chair, is centrally located \nwithin the room. On the table, \nthere are two wine glasses and a \ncorkscrew. A temperature gauge \nis mounted on the wall above the \noak barrels, and two circular \nlighting fixtures on the ceiling \nFigure 5: More generated results based on language instructions with different granularities.\n19\n--- Page 20 ---\nD Illustration of Iterative Asset-Layout Alignment\nFigure 6 provides a visual illustration of Iterative Asset-Layout Alignment process described in\nSection 3.5. This figure highlights the feedback loop between scene generation and evaluation, where\neach rendered 3D scene undergoes spatial and semantic evaluation to identify inconsistencies. Based\non the evaluator’s feedback, targeted corrections are applied to the layout, including object size,\nposition, and orientation. The updated layout is then used to regenerate the scene. This iterative\nrefinement continues until the evaluators no longer detect violations or a preset iteration limit is\nreached.\nA child-sized low toilet with a white ceramic finish\nA long white bathtub \nwith a rectangular shape\nA rectangular colorful \nrug with playful patternsA step stool with \na square topA yellow rubber duck \nwith a round body\nA small rectangular \nporcelain sink\nQuantitative \nEvaluator\nSpatial \nEvaluator\nBEV Layout \nGenerator\n3D Layout \nGenerator1. make the \ntub taller\n2. lower the \nrubber duck\nObject Generation\nFigure 6: Illustration of Iterative Asset-Layout Alignment\n20",
  "text_length": 63066
}