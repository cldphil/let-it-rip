{
  "id": "http://arxiv.org/abs/2506.00924v1",
  "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using\n  LLM-Based Comment Analysis and Network MOS Comparison",
  "summary": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates.",
  "authors": [
    "Parsa Hassani Shariat Panahi",
    "Amir Hossein Jalilvand",
    "M. Hasan Najafi"
  ],
  "published": "2025-06-01T09:31:55Z",
  "updated": "2025-06-01T09:31:55Z",
  "categories": [
    "cs.NI",
    "cs.AI",
    "cs.HC"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00924v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00924v1  [cs.NI]  1 Jun 2025Bridging Subjective and Objective QoE: Operator-Level Aggregation\nUsing LLM-Based Comment Analysis and Network MOS Comparison\nParsa Hassani Shariat PanahiID, Amir Hossein JalilvandID, and Mohammad Hasan NajafiID\nJune 6, 2025\nAbstract\nThis paper introduces a dual-layer framework for network\noperator-side quality of experience (QoE) assessment that\nintegrates both objective network modeling and subjec-\ntive user perception extracted from live-streaming plat-\nforms. On the objective side, we develop a machine learn-\ning model trained on mean opinion scores (MOS) com-\nputed via the ITU-T P.1203 reference implementation,\nallowing accurate prediction of user-perceived video qual-\nity using only network parameters such as packet loss,\ndelay, jitter, and throughput–without reliance on video\ncontent or client-side instrumentation. On the subjective\nside, we present a semantic filtering and scoring pipeline\nthat processes user comments from live streams to extract\nperformance-related feedback. A large language model is\nused to assign scalar MOS scores to filtered comments in\na deterministic and reproducible manner.\nTo support scalable and interpretable analysis, we con-\nstruct a labeled dataset of 47,894 live-stream comments,\nof which approximately 34,000 are identified as QoE-\nrelevant through multi-layer semantic filtering. Each\ncomment is enriched with simulated Internet Service\nProvider attribution and temporally aligned using syn-\nthetic timestamps in 5-minute intervals. The result-\ning dataset enables operator-level aggregation and time-\nseries analysis of user-perceived quality. A delta MOS\n(∆MOS) metric is proposed to measure each Internet ser-\nvice provider’s deviation from platform-wide sentiment,\nallowing detection of localized degradations even in the\nabsence of direct network telemetry. A controlled outage\nsimulation confirms the framework’s effectiveness in iden-\ntifying service disruptions through comment-based trends\nalone.\nThe system provides each operator with its own sub-\njective MOS and the global platform average per interval,\nenabling real-time interpretation of performance devia-\ntions and comparison with objective network-based QoE\nestimates. The proposed architecture is scalable, privacy-\npreserving, and infrastructure-agnostic, offering a prac-\ntical solution for enhancing operator awareness through\nplatform-derived user feedback.1 Introduction\nThe exponential growth of internet-based services and\nmultimedia applications has dramatically increased the\ndemand for reliable, user-oriented quality assessments.\nAs network environments become more dynamic and het-\nerogeneous, ensuring optimal user satisfaction requires\ngoing beyond traditional quality of service metrics. This\nhas led to the emergence of quality of experience (QoE)\nas a more holistic and user-centric indicator that reflects\nhow users perceive service quality under diverse and often\nunpredictable conditions [1,2].\nTraditionally, QoE assessment has relied on either\nintrusive subjective testing, which is resource-intensive\nand difficult to scale, or objective estimation methods\nbased solely on static network-level parameters. These\napproaches often fail to account for the dynamic and\ncontext-sensitive nature of human perception, resulting\nin limited accuracy and generalizability–especially in real-\nworld deployment scenarios.\nTo overcome these challenges, researchers have increas-\ningly turned to hybrid QoE frameworks that fuse objec-\ntive network indicators with user-centric feedback and\nmachine learning techniques. For example, [3] proposed a\nmodel that integrates perceptual factors such as buffering\nduration alongside physical-layer metrics like RSRP and\nSINR, using an artificial neural network to improve pre-\ndiction accuracy in mobile environments. Expanding on\nthis paradigm, Gao et al. [4] emphasized the integration\nof contextual and psychological dimensions, including de-\nvice type, emotional state, and usage environment, into\ndeep learning and Bayesian models–enabling richer and\nmore individualized QoE inference.\nLightweight and scalable implementations of such\nframeworks have also gained traction. Ref. [5], for in-\nstance, provides cross-application QoE estimation using\ndecision tree models trained on passively collected net-\nwork and device-level metrics. By crowdsourcing end-user\nfeedback and minimizing overhead via UDP-based prob-\ning, this framework offers a real-time, practical solution\nfor mobile network monitoring and experience forecast-\ning. In a similar spirit, [6] proposed a refined approach\nfor fixed wireless access networks, where a calibrated cus-\ntomer experience index aligns conventional service KPIs\nwith subjective user feedback–yielding a low-complexity\nbut effective model for adaptive QoE evaluation. Crowd-\nsourcing has also proven critical for bridging the gap be-\n1\n--- Page 2 ---\ntween subjective perception and objective signals. The\nuser-centric web QoE evaluating framework [7] exempli-\nfies this by combining live subjective scores with auto-\nmated logging of network metrics during real web usage\nsessions. The resulting data-driven models demonstrated\nsuperior accuracy–about 5% higher than strong machine\nlearning baselines–highlighting the value of integrating\nuser subjectivity into predictive algorithms.\nFrom a methodological standpoint, Charonyktakis et\nal. [8] introduced a modular framework designed to dy-\nnamically select the most suitable machine learning al-\ngorithm ( e.g., support vector regression, artificial neural\nnetworks, gaussian naive bayes, decision trees) for a given\ndataset or user profile. Leveraging nested cross-validation\nand Bayesian feature selection, this framework adapts to\nboth data characteristics and user heterogeneity. The\nframework consistently outperforms conventional meth-\nods such as PESQ and the E-model across VoIP and video\nstreaming datasets, emphasizing the importance of adap-\ntive, per-user modeling for robust QoE prediction. To\nthis end, we propose a dual-layer QoE assessment frame-\nwork that bridges standardized network-side metrics with\nuser-perceived quality extracted from live content plat-\nforms. On the network operator side, objective MOS\nis estimated using an enhanced ITU P.1203-compliant\nmodel [9] applied to key performance indicators. In paral-\nlel, subjective MOS is inferred from user-generated feed-\nback through large language model (LLM) analysis. This\nintegrated approach enables comparative QoE evaluation\nacross operators and platforms.\n2 Contributions\nThis research’s primary contribution lies in developing\na dual-layer QoE assessment framework that facilitates\noperator-level comparison between subjective and ob-\njective quality metrics across mobile and multimedia\nnetworks. The framework synthesizes LLM-processed\nmean opinion scores (MOS) from user-generated con-\ntent and contrasts them with network parameter-driven\nMOS predictions generated by our enhanced ITU P.1203-\ncompliant machine learning model.\nThe principal innovations of this work include:\n•Aggregated Subjective QoE Estimation: We\nintroduce a processing pipeline that systematically\nfilters and evaluates real-world user feedback from\nlive streaming platforms through semantic analysis\nand LLM integration. The derived quality scores are\naggregated according to network operator identifica-\ntion (via IP ranges), establishing a scalable frame-\nwork for passive subjective QoE assessment.\n•Objective QoE Estimation Through ITU-\nCompliant Modeling: Building on our prior re-\nsearch, we implement an optimized Random Forest\nmodel inspired by ITU P.1203 standards to calculate\nobjective MOS from network key performance indica-\ntors (KPIs) including latency, jitter, packet loss, andbitrate. These metrics are aggregated per operator\nfor comparative analysis at the group level.\n•Subjective–Objective MOS Discrepancy\nAnalysis: Through systematic alignment of\ncomment-derived subjective MOS with network-\nbased objective MOS at the operator level, our\nframework identifies critical QoE mismatches –\nparticularly instances where technical measurements\noverestimate actual user satisfaction or fail to detect\nservice quality issues.\n•Three-Tier QoE Architecture Implementa-\ntion: Our solution organizes quality metrics into an\nAccess-Service-Experience hierarchy, building upon\nestablished QoE modeling literature [10–12] while\nmaintaining alignment with industry-standard mon-\nitoring architectures like ETSI TS 103 294. This\nstratified approach enables coherent interpretation of\nboth network performance metrics and user quality\nperceptions through operator-centric aggregation.\n•Open Architecture with Operational Scalabil-\nity: All core components – including content fil-\ntration algorithms, MOS calculation modules, and\ndata aggregation systems – are developed with repro-\nducibility and open-source principles. The privacy-\nconscious design supports real-time deployment in\noperational QoE monitoring dashboards while ensur-\ning scalability across network infrastructures.\n2.1 Paper Structure\nIn§3, we review related work on QoE estimation and\nthe use of LLMs in subjective quality assessment. §4 in-\ntroduces our proposed framework, detailing the semantic\nfiltering pipeline, MOS estimation model, and aggrega-\ntion methodology. In §5, we describe the dataset used\nin our experiments, including comment preprocessing, la-\nbeling, and timestamp simulation. §6 presents the re-\nsults and discusses how operators can interpret QoE from\nplatform-side comment analysis, including ∆MOS trends\nand outage detection. Finally, §7 concludes the paper\nwith a summary of contributions and outlines directions\nfor future research.\n3 Related Works\nAs summarized in Tab. 1, prior studies have addressed\nvarious facets of QoE assessment, from foundational mod-\nels [13] to ITU-aligned resource optimization strategies\n[14], and perceptual machine learning approaches [15].\nWhile works like Liotou et al. [16] and Barakabitze et\nal.[17] advanced human-centric modeling and ML-based\nQoE management, they lacked complete implementation\npipelines for data acquisition and MOS computation.\nMore recent studies [11, 18] offered subjective data tools\nand ML-based QoE prediction, yet none fully bridged\nthe gap between data collection, ITU-compliant model-\ning, and end-to-end QoE estimation.\n2\n--- Page 3 ---\nTable 1: Comparison of recent works on QoE and QoS estimation in multimedia and service platforms. The table\nsummarizes key contributions of each study across dimensions such as the use of subjective and objective inputs,\nintegration of LLMs, ability to produce QoE/QoS outputs, bridging of subjective and objective layers, support for\noperator-level insights, and real-time applicability. Subj.: subjective input ( e.g., comments, speech); Obj.: objective\ninput ( e.g., network or video metrics)\nRefrence Main Idea Subj.\nInputObj.\nInputLLM\nUs-\nageQoE/QoS\nOutputBridges\nSubj.-\nObj.Operator-\nLevel\nInsightReal-\nTime\nCapabil-\nity\nBarakovi´ c et\nal. [13]Early modeling and\nmonitoring of QoE✗ ✗ ✗ ✗ ✗ ✗ ✗\nSultan et\nal. [14]QoE evaluation\nin high-bandwidth\ncomms✓ ✓ ✗ ✓ ✗ ✓ ✗\nBarman et\nal. [15]Survey of adaptive\nstreaming QoE\nmodels✗ ✓ ✗ ✓ ✗ ✗ ✗\nLiotou et\nal. [16]Caching impact on\nQoE via ITU model✗ ✓ ✗ ✓ ✗ ✓ ✗\nBarakabitze\net al. [17]Future-oriented\nQoE management\nwith ML✓ ✗ ✗ ✓ ✗ ✓ ✗\nKougioumtzidis\net al. [11]ML-based predic-\ntion of QoE✗ ✓ ✗ ✓ ✗ ✓ ✗\nOmar et\nal. [18]ML-based QoE pre-\ndiction using net-\nwork data✓ ✓ ✗ ✓ ✗ ✓ ✗\nPanahi et\nal. [9]ITU P.1203-based\nMOS from network\nparams✗ ✓ ✗ ✓ ✗ ✓ ✓\nLLM4Band\n[19]Bandwidth estima-\ntion via LLM + of-\nfline RL✗ ✓ ✓ ✗ ✗ ✗ ✓\nRezaee &\nGundavelli\n[20]Detect verbal/vi-\nsual signs of poor\nQoE in calls✓ ✓ ✓ ✓ ✗ ✗ ✓\nJha &\nChuppala\n[21]LLM QoS serving\nwith RL, QUIC\nscheduling✗ ✓ ✓ ✓ ✗ ✗ ✓\nLiu et\nal. [22]LLM-enhanced\nQoS prediction\nfrom metadata✗ ✓ ✓ ✓ ✗ ✗ ✗\nThis Work Bridging subjective\n(comments) & ob-\njective QoE using\nLLMs✓ ✓ ✓ ✓ ✓ ✓ ✓\n3\n--- Page 4 ---\nComplementary research in content-level quality anal-\nysis has prioritized computational efficiency through se-\nlective processing techniques [23,24], a philosophy echoed\nin our network-oriented framework. By addressing prior\nlimitations and unifying subjective and objective QoE\nestimation, our approach delivers a complete, operator-\nscalable solution grounded in standard-based modeling\nand LLM-driven perception analysis. In our prior re-\nsearch [9], we developed a novel network-centric QoE\nassessment system that overcomes critical gaps in ex-\nisting methodologies. Distinct from earlier approaches,\nthis framework delivers a complete open-source pipeline\nfor calculating ITU P.1203-aligned MOS using exclusively\nnetwork-layer metrics—including latency, jitter, packet\nloss, throughput, and bitrate—eliminating dependency\non video content analysis. The system achieves 97%\nparity with the full ITU P.1203 reference model while\nsubstantially streamlining data acquisition workflows and\ncomputational resource requirements.\n•End-to-End Network Parameterization : Inte-\ngration of a Selenium-driven data collection engine\nthat automates HAR file generation, segment-level\nmetadata extraction, and MOS computation via the\nITU reference model, bypassing content inspection.\n•Machine Learning Optimization : Deployment of\na Random Forest regressor trained on 22,000 anno-\ntated network-performance samples, enabling direct\nMOS prediction from network telemetry with high fi-\ndelity (R2= 0.968) across diverse emulated network\nenvironments.\n•Operational Scalability : A lightweight architec-\nture supporting real-time monitoring scenarios, cou-\npled with full reproducibility through open-sourced\ncodebases, datasets, and implementation protocols.\nThis work advances beyond predecessors like Sul-\ntan et al. [14] and Omar et al. [18], which—despite in-\ncorporating machine learning—lack holistic, production-\ngrade implementations. By contrast, our framework pro-\nvides telecom operators with an immediately deployable\nsolution for QoE optimization using exclusively network-\naccessible data, eliminating reliance on proprietary con-\ntent or user-side instrumentation.\nRecent works have begun exploring the use of large lan-\nguage models to enhance network-layer QoE systems. For\nexample, LLM4Band [19] introduces a hybrid framework\nwhere an LLM augments offline reinforcement learning\nfor more accurate bandwidth estimation in RTC scenar-\nios. While this approach focuses solely on objective esti-\nmation and real-time control, it demonstrates the poten-\ntial of LLMs to generalize across diverse network traces.\nIn contrast, our current work applies LLMs directly to\nsubjective user feedback ( e.g., live-streamed comments),\nenabling end-to-end estimation of perceived QoE and al-\nlowing for a direct comparison between network-based\nand user-reported MOS at the operator level. Rezaee\nand Gundavelli [20] proposed a conceptual system forreal-time monitoring and enhancement of multi-user com-\nmunication using AI agents and LLMs. Their approach\ninvolves analyzing audio/video streams during telecon-\nferencing sessions to detect verbal and visual indicators\nof poor Quality of Experience ( e.g., ”I can’t hear you”),\nand correlating them with network telemetry to generate\na real-time QoE signal.\nJha and Chuppala [21] propose a QoS-aware LLM serv-\ning framework that dynamically routes requests between\nLLMs of varying latency and quality using deep reinforce-\nment learning. While their work focuses on backend serv-\ning efficiency–using QUIC stream scheduling and model\nselection to meet latency guarantees–our work addresses\na different facet of QoE: measuring user perception from\nlive-streamed comments and aligning it with network-\nbased MOS. Nonetheless, both works reflect an emerg-\ning trend of resource-aware LLM systems optimized for\napplication-level performance and user satisfaction. Liu\net al. [22] proposed the llmQoS model for QoS prediction\nin service recommendation, using LLMs to extract seman-\ntic features from textual descriptions of users and services.\nThese LLM-derived embeddings are combined with user-\nservice interaction history to enhance predictive perfor-\nmance under sparse data conditions. While their focus is\non objective metrics like throughput and response time,\nour work extends the role of LLMs into the subjective\ndomain–processing user-generated comments to infer per-\nceived MOS. This highlights a broader trend in QoS/QoE\nresearch toward leveraging LLMs to fill semantic or data\ngaps in traditional estimation pipelines.\n4 Proposed Framework\nIn our previous work [9], we introduced a framework\nthat accurately estimates the MOS using only network-\nlevel parameters derived from the ITU-T P.1203 stan-\ndard, without requiring video content or client-side met-\nrics. Building on that objective-side model, the current\nwork proposes a complementary framework for estimating\nsubjective MOS on the service platform side by analyz-\ning user comments. By semantically filtering and scoring\nquality-related feedback using a large language model, we\ncompute comment-based subjective MOS values that re-\nflect real user satisfaction Fig. 1. These subjective scores\nare then aggregated per operator and aligned with their\ncorresponding objective MOS values. This dual-source\napproach enables network operators to detect mismatches\nbetween perceived and measured quality, improve service\ndiagnosis, and achieve a more accurate and comprehen-\nsive understanding of user QoE by bridging the gap be-\ntween platform-side perception and operator-side delivery\nmetrics.\n4.1 Functional QoE Layer Mapping\nTo organize the complex range of factors influencing QoE,\nwe adopt a three-level functional model derived from ex-\nisting QoE-QoS monitoring architectures and modeling\nliterature [10–12] (see Tab.2), including ETSI TS 103 294\n4\n--- Page 5 ---\nHTTP Live \nStreaming \nPlatform \nLS1\nLS2\nLS3\nLSN\nLayer 1 \nSemantic Filtering \nLayer 2 \nCalculating MOS using LLM Not QoE related \nContent-related \nNot QoE related \nContent-related Not QoE related \nContent-related - Timestamp \n- Comment Text \n- ISP (based on IP) TS ISP Comment MOS \n5/11/2025 \n14:00 ISP 1 \n(85.155.x.x) listen in \nsoundcloud N\n5/11/2025 \n14:01 ISP 1 \n(85.155.x.x) I can't stay in \nlagging super \nhard1\n5/11/2025 \n14:02 ISP 2 \n(91.198.x.x) stream lagging \nfor anyone else 2\n5/11/2025 \n14:03 ISP 1 \n(85.155.x.x) did it just lag 3\n5/11/2025 \n14:04 ISP 3 \n(102.45.x.x) it really is even \nfps are better - \nturn4\n5/11/2025 \n14:05 ISP 2 \n(91.198.x.x) twitch lagging \nor sum 2\n5/11/2025 \n14:06 ISP 3 \n(102.45.x.x) no buffering 5Live Stream Comments Filtering and Scoring Result Dataset Figure 1: Semantic filtering and LLM-based MOS estimation applied to real-time live stream comments. Each entry\nis labeled with ISP information and timestamp, enabling subjective QoE analysis across providers.\nExperience Layer \nService Layer \nAccess Layer \nNetwork Performance \nDelay Jitter Throughput \netc.\nApplication responsiveness \nWeb page load time \nVideo playback smoothness etc.\nOverall user sentiment not necessarily tied to \nperformance (e.g., content or aesthetics) \nMood or emotional state while using the service \nComments expressing gratitude or praise to the \nstreamer/platform \netc.\nFigure 2: Three-layer QoE model illustrating how ob-\njective network metrics (Access Layer), application-level\nsymptoms (Service Layer), and subjective user percep-\ntions (Experience Layer) together form a complete view\nof user experience.and the layered decomposition described in [25]. These\nmodels typically distinguish among three key stages: (1)\ntechnical delivery, (2) application-level behavior, and (3)\nuser perception.\nAs illustrated in Fig.2, we formalize this structure into\na simplified yet operationally relevant architecture con-\nsisting of:\n•QoE Part 0 (Access Layer) : Network-level per-\nformance metrics,\n•QoE Part 1 (Service Layer) : Application observ-\nable behavior,\n•QoE Part 2 (Experience Layer) : User-perceived\nquality.\nEach layer corresponds to a distinct stage in the ser-\nvice delivery chain, with progressively decreasing levels of\ndirect observability and controllability by network opera-\ntors.\n1.QoE Part 0 (QoS) – Access Layer: This layer re-\nflects core network performance metrics that are both\nmeasurable and controllable by operators. These in-\nclude:\n•Packet loss,\n•Delay and jitter,\n•Throughput,\n•DNS resolution latency,\n•Handover quality (in mobile scenarios),\n•Radio coverage.\n5\n--- Page 6 ---\nTable 2: Hierarchical QoE layer model employed in this study. The Access Layer captures raw network performance\nmetrics, while the Service Layer characterizes observable application behavior, influenced by either network condi-\ntions or platform-specific implementations. The Experience Layer encompasses user-perceived quality, integrating\nboth lower layers; however, only performance-relevant components are retained within the framework to maintain\nalignment with operator objectives\nQoE Layer Content Scope Relation to Other Lay-\nersOperator Perspec-\ntivePlatform Perspec-\ntive\nExperience\nLayer\n(QoE Part\n2)User-perceived quality,\nsatisfaction, emotional\nor content-based feed-\nbackPerception of Service\n& Access Layers+ con-\ntent/emotion noise- Relevant only\nwhen tied to lower-\nlayer issues - Not\ndirectly measurable- Present in com-\nments - Non-\nperformance con-\ntent is filtered out\nService\nLayer\n(QoE Part\n1)Observable\napplication-level\nbehaviors ( e.g., buffer-\ning, lag, responsive-\nness, load delays)Can result from Access\nissues Or arise from\napp-side/platform\nlogic- Not directly mea-\nsurable - Opera-\ntionally important- Extracted from\nfiltered user com-\nments\nAccess\nLayer\n(QoE Part\n0)Core network deliv-\nery metrics ( e.g., de-\nlay, jitter, packet loss,\nthroughput)Base layer for delivery\nperformance- Fully measurable -\nControllable- Indirectly inferred\nfrom user feedback\nwhen problems oc-\ncur\nIn our framework, these metrics are input to a ma-\nchine learning model trained on ITU-T P.1203-based\nlabels. The output, an objective MOS, is aggregated\nby IP range to represent each operator’s delivery-\nlayer performance.\n2.QoE Part 1 – Service Layer: This layer repre-\nsents observable effects of network performance at\nthe application level, including:\n•Video playback smoothness,\n•Web page load time,\n•Call setup delay,\n•Application responsiveness.\nThese symptoms are not directly measurable from\nthe network side, but are crucial to the end-user\nexperience. In our framework, we infer these indi-\nrectly by semantically filtering user comments for\nexpressions of service degradation ( e.g., ”buffering,”\n”lag,” ”won’t load”). These filtered comments in-\ndicate service-layer performance problems, enabling\noperators to gain insight into disruptions they cannot\nobserve directly.\n3.QoE Part 2 – Experience Layer: This layer cap-\ntures users’ overall satisfaction, emotional response,\nor subjective quality judgment. Examples include:\n•Video MOS as rated by the user,\n•Voice call clarity ratings,\n•Comments like ”this is unwatchable” or ”perfect\nstream today”,\n•Overall user sentiment not necessarily tied to\nperformance ( e.g., content or aesthetics).Our framework generates subjective MOS scores for\nfiltered comments. However, we strictly include only\ncomments that reflect service-related dissatisfaction\nor satisfaction, not content preferences or emotional\nreactions. Thus, while the MOS scores are expressed\nin subjective form (QoE Part 2), they are semanti-\ncally bounded to QoE Part 1 phenomena.\n4.Clarification of Scope: To avoid misinterpreta-\ntion, we clarify that this work does not aim to capture\nfull-spectrum user perception as defined in broader\nQoE Part 2 or End-User Layer models. Specifically,\nwe exclude:\n•Comments reflecting personal content prefer-\nences, such as opinions on streamers, games, or\ntopics,\n•Emotionally expressive remarks that do not re-\nlate to technical or performance issues,\n•Aesthetic or visual quality judgments not tied\nto observable service behavior.\nAll subjective MOS scores in this study are lim-\nited to performance-related user feedback that can\nbe semantically mapped to technical degradation or\napplication-level disruptions ( e.g., buffering, lag, un-\nresponsiveness). This ensures the comment-derived\nQoE remains anchored in QoE Part 1 phenomena,\nsupporting meaningful comparison with objective\nnetwork-based MOS.\n4.2 Comment Collection from Live\nStreaming Platforms\nPrior studies have explored live stream comments from\nvarious perspectives. Some works focus on understand-\ning user intent and providing feedback suggestions us-\ning multimodal comment data including text, video, and\n6\n--- Page 7 ---\nProblem \nKeywords \nContext \nKeywords \nMeme \nKeywords Input \nComment \nMatch\n? \nNot \nMatch\n? Not \nrelevant QoE-Relevant \nNot \nrelevant Figure 3: Semantic filtering pipeline for identifying\nQoE-relevant comments. Each input comment under-\ngoes an initial check against a predefined list of meme\nkeywords; matches are immediately discarded. Non-\nmatching comments are then evaluated for the presence of\nboth problem-related and context-related keywords. Only\ncomments containing at least one problem keyword and\none context keyword are classified as QoE-relevant and\nadvanced for further analysis. The figure illustrates the\nconditional logic: blue arrows denote progression upon\nmeeting criteria, red arrows indicate rejection points, and\nthe green arrow signifies successful QoE-relevant com-\nment identification.\naudio [26]. Others have used time-synced comments to\nmodel user behavior patterns [27], estimate user senti-\nment [28], or detect offensive language in Twitch chats\nusing transfer learning techniques [29]. These efforts col-\nlectively demonstrate that live comments are a rich source\nof real-time viewer perception. To support our frame-\nwork, we leveraged public comment datasets, including\nthe TwitchChat dataset [30] to model interaction dynam-\nics, and partially used the Twitch.tv Chat Log Data [31]\nfor timestamped records and ISP-level aggregation.\n4.3 Semantic Filtering to Isolate QoE-\nRelevant Feedback\nFollowing the collection of raw live stream comments, we\napplied a two-layer semantic filtering pipeline designed\nto extract only those comments indicative of observ-\nable delivery and application-level performance issues–\ni.e., aligned with QoE Part 0 (Access Layer) and QoE Part\n1 (Service Layer). This ensures that downstream subjec-\ntive MOS estimation is grounded in technically meaning-\nful user feedback. Prior work has demonstrated the value\nof mining unstructured user-generated content for per-\nformance insights. For example, [32] show that semantic\ncontent similarity improves filtering accuracy in recom-\nmendation settings, aligning with our use of embedding-\nbased comment filtering. Similarly, [33] highlight the im-\nportance of integrating subjective QoE feedback into net-\nwork operations, a goal our approach supports by expos-ing comment-based MOS estimates to operators (Fig. 3).\n1.Preprocessing and Normalization: Each com-\nment undergoes a text cleaning process prior to anal-\nysis. We apply a series of transformations to reduce\nnoise and prepare the text for semantic evaluation:\n•Lowercasing for uniformity,\n•Character collapse: repeated characters ( e.g.,\n”laaag”) are collapsed to reduce expressive ex-\naggeration (”lag”),\n•Special character removal to eliminate emojis,\npunctuation, and Twitch emotes,\n•Whitespace normalization, Typo corrections:\ndomain-specific substitutions ( e.g., ”scuffedd”\n→”scuffed”).\nThis normalization reduces lexical variability and\nstrengthens both exact-match and embedding-based\nfeature extraction.\n2.Keyword and Contextual Match Filtering: We\nconstruct three keyword groups:\n•Problem keywords: Based on a curated set\nof quality-related expressions such as ”buffer”,\n”lagging”, ”audio delay”, and ”disconnect”, we\ndefine a list of problem keywords that reflect\ncommon service or delivery issues described by\nusers.\n•Context keywords: Domain phrases suggesting\nrelevance to video streaming or network ( e.g.,\n”stream”, ”video”, ”network”, ”ping”, ”dns”,\n”connection”)\n•Meme keywords: Non-informative platform-\nspecific expressions ( e.g., ”weirdchamp”,\n”kekw”, ”monkaw”) that indicate off-topic\nhumor or sarcasm.\nFor a comment to be considered QoE-relevant, it\nmust:\n•Match at least one problem keyword,\n•Match at least one context keyword,\n•Not match any meme keyword.\nThese rules filter out irrelevant or aesthetic commen-\ntary while preserving performance-related expres-\nsions.\n3.Semantic Similarity Scoring with Anchors:\nWe use the all-MiniLM-L6-v2 model from Sentence-\nTransformers, a lightweight transformer-based en-\ncoder optimized for semantic similarity [34]. This\nmodel is built on MiniLM architecture and fine-tuned\non a large corpus of sentence pairs, producing 384-\ndimensional embeddings that efficiently capture the\nsemantic meaning of short texts like user comments\n[35]. To evaluate relevance, cosine similarity is com-\nputed between each comment’s embedding and a set\n7\n--- Page 8 ---\nof predefined anchor embeddings. Cosine similarity\nmeasures the cosine of the angle between two vectors\nin high-dimensional space–values closer to 1 indicate\nstrong semantic alignment. This method allows us\nto select the most QoE-relevant comments for fur-\nther scoring and aggregation [35].\nEach candidate comment is encoded in the same em-\nbedding space and compared to anchor vectors us-\ning cosine similarity. The maximum similarity score\nacross all anchors is retained for each comment. Two\nthresholds are applied: To ensure robust filtering\nacross diverse comment lengths, we apply length-\nsensitive similarity thresholds. For short comments\n(fewer than MINWORDS = 5), a higher threshold of\nSHORT TEXT SIMTHRESHOLD = 0.40 is used. This\nis because short comments often lack sufficient con-\ntext, making them more susceptible to false positives\nin embedding similarity. For longer comments, we\napply a lower threshold of SIMTHRESHOLD = 0.28,\nas the added semantic richness provides more reliable\nembedding signals. This thresholding strategy helps\nbalance precision and recall, ensuring only truly\nQoE-relevant comments are retained. These thresh-\nolds balance recall and precision, filtering out am-\nbiguous or generic expressions while retaining those\nsemantically close to anchor complaints.\n4.Batched Evaluation and Final Filtering: The\nfull optimized filter batch function applies\nthe entire filtering pipeline across all comments in\na vectorized and batched manner. For each com-\nment, it first cleans and normalizes the text, checks\nfor the presence of meme, context, and problem key-\nwords, and computes semantic similarity with anchor\nphrases. Based on the word count and similarity\nthreshold, each comment is either retained or dis-\ncarded. The function returns a boolean mask identi-\nfying comments that are considered QoE-relevant:\n•Token length is evaluated,\n•Boolean flags are set for context, problem, and\nmeme match,\n•Cosine similarity is computed,\n•Thresholding is applied based on token length.\nOnly comments satisfying all filtering criteria\nare labeled as isloose qoecandidate =\nTrue . These are then exported to a CSV file\n(loose qoecandidates cleaned.csv ) for\ndownstream processing in the MOS scoring module.\nThis semantic filtering layer acts as a high-precision selec-\ntor for QoE-relevant feedback from a noisy, open-domain\ncomment stream. It enables the platform to isolate only\nthose comments which likely reflect service impairments\nor delivery issues relevant to operators, thus aligning sub-\njective MOS scoring with technically actionable insights.\nProcessed Comments \n1 4 -1Input Layer \nOutput Layer \nSubjective MOS Hidden Layers Layer 2 Layer 1 \nNot relevant Figure 4: Subjective MOS estimation using a LLM. After\nsemantic filtering in Layer 1, the processed comments are\npassed to Layer 2, where a transformer-based LLM inter-\nprets each input through multiple hidden layers. These\nlayers apply masked self-attention and feedforward mech-\nanisms to capture nuanced quality-related semantics. The\nmodel outputs a scalar value from the output layer, cor-\nresponding to a predicted MOS score ( e.g., 1–5) or –1 for\ncomments deemed not relevant. Non-relevant predictions\nare discarded, while valid subjective MOS scores are col-\nlected for downstream QoE analysis.\n4.4 Subjective MOS Estimation Using\nLarge Language Models\nRecent research has demonstrated the potential of LLMs\nfor subjective quality evaluation across various modali-\nties. For instance, [36] showed that auditory LLMs can\naccurately predict the MOS, perform A/B testing, and\ngenerate natural language quality explanations for speech\ncontent, with high correlation to human judgments. Simi-\nlarly, [37] evaluated the ability of instruction-tuned mod-\nels such as GPT, Gemini Pro, and Claude2 to capture\nand express subjective assessments. While results vary\nacross models, the study confirms that subjective inter-\npretation is feasible with appropriate prompting. Broader\nreviews of LLM alignment, such as [38], highlight the con-\ntextual nature of subjective feedback and the importance\nof feedback-driven tuning to align outputs with human\npreferences–an idea closely aligned with our approach.\nAdditionally, [39] provide a comprehensive taxonomy of\nuncertainty estimation techniques in LLMs, emphasizing\nthe challenges of overconfidence in scalar predictions.\nAfter filtering QoE-relevant comments, we apply a\ntransformer-based LLM to estimate subjective MOS\nbased on the semantic content of each comment. As\nshown in Fig. 4 this step enables quantification of user-\nperceived service quality using natural language inputs,\nbridging the gap between qualitative expressions and nu-\nmerical evaluation.\n1.Model Characteristics and Technical Speci-\nfications: The language model used for subjec-\ntive MOS estimation is a multilingual, multimodal,\ninstruction-tuned transformer released in 2024. It is\nbuilt on a decoder-only architecture and fine-tuned\nfor high-precision, task-specific generation, enabling\nrobust handling of structured instructions. The\nmodel supports text, image, and audio modalities,\nthough in this work, only its text-processing capabil-\n8\n--- Page 9 ---\nities are utilized for evaluating user comments.\nIt is designed for multilingual deployment, cover-\ning more than 50 languages and over 97% of global\nspeakers. With a context window of up to 128,000\ntokens, the model allows flexible prompt design, suit-\nable for contextualized feedback evaluation. Its in-\nference performance is optimized for low latency and\nhigh throughput, making it cost-effective and scal-\nable for large-volume subjective analysis tasks.\nThe model’s knowledge cutoff is October 2023, and\nno external retrieval or real-time access is enabled\nduring inference, ensuring responses are grounded\nsolely in prompt content. To ensure reproducibility\nand scoring stability, we configure the model in deter-\nministic mode (temperature = 0.0). Though the ex-\nact parameter count remains undisclosed, the model\nhas demonstrated state-of-the-art performance on\nsubjective interpretation tasks, as evidenced by its\nuse in recent studies on speech MOS prediction [36],\nsubjective preference extraction [37], and feedback-\naligned generation [38].\nIn line with observations from recent work on\nLLM uncertainty [39], our pipeline adopts a sim-\nplified verbalization-based scoring strategy, which\nelicits scalar responses directly through structured\nprompts. These properties make the model highly\nsuitable for semantic interpretation and subjective\nscalar scoring, particularly for assigning MOS values\nbased on natural user feedback in streaming contexts.\n2.Model Architecture: The LLM, which is used,\nis built on a decoder-only transformer architec-\nture, consistent with the design principles intro-\nduced in earlier models such as GPT-3 [40], but\nwith several architectural and efficiency enhance-\nments. In the decoder-only setup, input tokens are\nprocessed sequentially using causal (autoregressive)\nself-attention, where each token attends only to pre-\nvious tokens in the sequence.\nThis left-to-right structure is critical for coherent and\nfluent language generation. Text input is first tok-\nenized using a byte-level tokenizer, which improves\nupon previous tokenization schemes by reducing to-\nken counts for non-Latin scripts and compressing\nmultilingual input more effectively [41]. Each token\nis mapped to a dense embedding vector, and posi-\ntional encodings are added to preserve sequence or-\nder. These embeddings are then passed through a\ndeep stack of transformer decoder layers, each con-\nsisting of a LayerNorm, a masked multi-head self-\nattention mechanism, and a feed forward network\n(FFN), typically implemented as a two-layer MLP\nwith non-linear activation functions such as GELU.\nResidual connections follow both the attention and\nFFN sublayers to support gradient flow and model\ndepth. The LLM incorporates optimized attention\nmechanisms and parallelized computation strategies\nto support long sequences–up to 128,000 tokens–\nwhile maintaining memory and compute efficiency[41].\nAlthough OpenAI has not disclosed the model’s ex-\nact internal optimizations ( e.g., rotary embeddings\nor attention compression), its design allows for high-\nthroughput, low-latency inference. In our implemen-\ntation, the LLM is used with deterministic decod-\ning (temperature = 0.0) and without sampling con-\nstraints ( topk,topp), enabling consistent and re-\nproducible scalar outputs. We operate the model\nin single-turn inference mode, without requiring ses-\nsion memory across prompts, making it ideal for\none-shot subjective scoring tasks such as comment-\nlevel MOS estimation. The final hidden state is pro-\njected into vocabulary space via a linear output head,\nproducing a probability distribution from which the\nnext token is selected or, in our case, a numeric\nvalue is extracted directly from the model’s output.\nThis configuration leverages the LLM’s capabilities\nin structured reasoning and scalar output generation\nfor comment-driven QoE scoring in a stable, cost-\nefficient manner.\n3.Prompt Design and Instructional Framing:\nEach user comment is embedded into a pre-defined\nprompt designed to elicit a discrete satisfaction score.\nThe prompt introduces the comment in the context\nof using an online service (such as streaming, gaming,\nor browsing), followed by a description of a 5-point\nMOS rating scale:\n(a) Very Dissatisfied: Severe QoE problems; unus-\nable experience ( e.g., constant lags, freezes, or\ncrashes),\n(b) Dissatisfied: Major recurring issues; frequent\nbuffering or disconnects,\n(c) Neutral: Minor or occasional QoE disruptions,\n(d) Satisfied: Good service with negligible issues,\n(e) Very Satisfied: Perfect experience; no notice-\nable performance problems.\nTo maintain data integrity, the prompt includes an\nexplicit fallback condition: if the comment is com-\npletely unrelated to user experience or quality, the\nmodel is instructed to return –1. This prevents irrel-\nevant or misclassified inputs from distorting subjec-\ntive MOS distributions.\n4.Inference Configuration and Execution: Each\ncomment is scored using the LLM in isolation via\na single prompt-response interaction. We config-\nure the model for deterministic output by setting\ntemperature = 0.0 , removing sampling random-\nness, and ensuring stable and reproducible scores.\nThe interaction is strictly structured, and the model\nis instructed to return only a single numeric value,\neliminating extraneous text.\nA dedicated scoring function wraps this logic. It:\n•Embeds the comment into the pre-defined\nprompt,\n9\n--- Page 10 ---\n•Sends it to the LLM for evaluation,\n•Parses and validates the output,\n•Handles potential parsing errors gracefully,\n•Logs progress to the console for each step in the\nloop.\nA one-second delay is introduced between each scor-\ning operation to respect model rate limits and reduce\nthe risk of throttling.\n5.Postprocessing and Dataset Generation: Each\nscored comment is stored in a structured dictionary\ncontaining:\n•The original text,\n•The assigned MOS value (1–5 or –1 for unre-\nlated).\nThese records are aggregated into a dataset for\ndownstream analysis. The final output represents a\ncomment-level QoE scoring dataset.\n4.5 Simulation of Timestamps and ISP\nLabeling for Aggregation\nTo enable operator-level insights and temporal analysis\nfrom subjective QoE scores, we simulate realistic con-\nditions by assigning each comment to an Internet ser-\nvice provider (ISP) and generating synthetic timestamps.\nThis metadata allows us to aggregate MOS over time and\nacross simulated service providers, enabling the detection\nof performance trends, degradations, or anomalies.\n1.Filtering Valid MOS Data: After excluding all\nentries marked as ”–1” by the LLM (indicating\nnon-relevant comments), we retain approximately\n”40,000” valid comment-level MOS records. These\nrepresent user perceptions that are semantically\ngrounded in QoE-related issues and ready for aggre-\ngation and analysis.\n2.ISP Assignment and Timestamp Generation:\nTo simulate a diverse network environment, each\ncomment is randomly assigned to one of three hypo-\nthetical ISPs: ISP1, ISP2, or ISP3. This step reflects\nhow different user comments might be tied to dif-\nferent service providers in a real-world deployment.\nThe randomness simulates a reasonable distribution\nof users across ISPs and helps test the system’s abil-\nity to identify ISP-specific anomalies.\nIn parallel, we simulate timestamp metadata by as-\nsigning a synthetic timestamp to each comment us-\ning 3-second intervals starting from a fixed base time\n(e.g., 2024-01-01 12:00:00). This technique produces\na chronologically spaced stream of feedback, as if\ncomments were received continuously during a live\nsession or over a measurement period. For example:\n•Comment 0 →Timestamp = 2024-01-01\n12:00:00, ISP = ISP2Table 3: Example of aggregated subjective MOS data\ngrouped by 5-minute time windows and ISP. Each row\nrepresents the average user-perceived service quality and\nnumber of valid QoE-related comments recorded within a\nspecific time interval for a given provider. This structure\nenables temporal and cross-operator QoE analysis.\nTime Window ISP Comment Count Avg MOS\n12:00–12:05 ISP1 420 3.64\n12:00–12:05 ISP2 415 2.81\n12:00–12:05 ISP3 402 4.12\n12:05–12:10 ISP1 435 3.58\n... ... ... ...\n•Comment 1 →Timestamp = 2024-01-01\n12:00:03, ISP = ISP1\n•Comment 2 →Timestamp = 2024-01-01\n12:00:06, ISP = ISP3\n3.Time Windowing and Aggregation: To sup-\nport meaningful analysis over time, we group the\nsimulated timestamps into 5-minute windows using\ntime flooring ( timestamp.dt.floor(’5min’) ).\nWithin each time window and for each ISP, we com-\npute two key statistics:\n•Average MOS – representing the mean per-\nceived service quality\n•Comment Count – indicating volume or user ac-\ntivity\nTab. 3 results in a windowed table of per-ISP QoE over\ntime This format enables direct comparison of service\nquality across ISPs and across time, supporting both real-\ntime dashboards and offline analysis.\n5 Dataset\n5.1 Dataset Composition\nThe final dataset used in this study consists of 47,894\ncomment-level entries, each associated with a subjective\nMOS generated by an LLM. These entries were collected\nand processed following the semantic filtering and scor-\ning pipeline described in earlier sections. Each record\ncontains the original user comment and its corresponding\nMOS score, which ranges from 1 to 5 for quality-relevant\nfeedback, and –1 for comments deemed not QoE-relevant.\nAlthough the primary analysis in this paper focuses on\nthe 33,770 valid entries with scores in the [1–5] range, the\nfull dataset—including the 14,124 comments labeled –1—\nis preserved to support future research. These excluded\ncomments are valuable for tasks such as training classi-\nfication models to distinguish QoE-related and unrelated\nfeedback, or building LLMs capable of predicting the pres-\nence or absence of perceived quality feedback in unstruc-\ntured user language. The retained 33,770 comment-MOS\npairs form the basis of all temporal, operator-level, and\n10\n--- Page 11 ---\n1 2 3 4 5\nMOS025005000750010000125001500017500Comment CountDistribution of LLM-Derived MOSFigure 5: Distribution of MOS derived from LLM evalu-\nations of QoE-related user comments. The distribution is\nheavily skewed toward lower values, with scores of 1 and\n2 dominating. This reflects typical user behavior on live-\nstreaming platforms, where negative experiences prompt\nexplicit feedback, while positive or neutral ones are often\nunreported or ambiguously expressed.\ndelta-based analyses in subsequent sections. This design\nensures both data fidelity for scoring and flexibility for\nfuture machine learning experimentation.\n5.2 Subjective Score Distribution\nAfter semantic filtering and LLM-based scoring, the valid\nportion of the dataset contains 33,770 comments with\nsubjective MOS scores ranging from 1 (Very Dissatisfied)\nto 5 (Very Satisfied). These scores represent the language\nmodel’s interpretation of user satisfaction based solely on\ntextual feedback.\nA distribution analysis reveals that the majority of\nscores are heavily skewed toward the lower end of the\nscale, particularly MOS values of 2 and 1. This is an ex-\npected and rational pattern in open-user platforms such\nas live streaming services: users are more likely to vocal-\nize dissatisfaction ( e.g., lag, buffering, stuttering) than\npositive experiences. Positive feedback tends to be rare,\nshort, and ambiguous, while negative feedback is often ex-\nplicit and tied to technical issues. Distribution Insights:\n•MOS 2 is the most common label, suggesting\nwidespread expression of service impairments that\ndisrupt but do not fully block usability,\n•MOS 1 appears frequently, corresponding to severe\nuser complaints, often describing unwatchable or bro-\nken experiences,\n•MOS 3 appears moderately, often representing minor\nbuffering, momentary lags, or ambiguous tone,\n•MOS 4–5 are the least frequent, aligning with occa-\nsional praise or satisfaction that was strong enough\nto be semantically recognized as quality-related.The bar chart shown in Fig. 5 visualizes the comment\nfrequency per MOS level, clearly illustrating the class im-\nbalance. This skew is useful in training or evaluating\nmodels under realistic user feedback conditions, where\nnegative sentiment dominates explicitly stated commen-\ntary. This distributional bias also informs future data\naugmentation strategies or loss function adjustments if\nthe dataset is to be used for supervised learning tasks.\nOverall, the distribution affirms the practical assumption\nthat subjective QoE signal is louder when quality fails,\nand more diffuse when performance is acceptable.\n5.3 Simulated Metadata: ISP and Times-\ntamps\nTo enable time-series analysis and provider-level aggrega-\ntion of subjective QoE, each valid user comment in the\nfiltered dataset was enriched with two simulated meta-\ndata attributes: an ISP label and a timestamp. This\naugmentation transforms individual, isolated user feed-\nback entries into temporally and spatially contextualized\nobservations, facilitating longitudinal trend analysis and\ncomparative evaluation across providers. Prior to meta-\ndata generation, all comments labeled as –1 by the large\nlanguage model–indicating irrelevance to service quality–\nwere excluded. The resulting subset of 33,770 valid en-\ntries, each associated with a MOS score from 1 to 5, forms\nthe working dataset for simulation and analysis.\nTo reflect a multi-operator environment, each comment\nwas randomly assigned to one of three synthetic ISPs:\nISP1, ISP2, or ISP3. A fixed random seed was applied\nto ensure reproducibility of provider assignment across\nexperimental runs. Although these assignments are syn-\nthetic, they model the operational context in which users\nare distributed across multiple access networks and ser-\nvice providers. Simultaneously, we simulated the tem-\nporal aspect of feedback by assigning synthetic times-\ntamps to each comment. Starting from a base time of\nJanuary 1, 2024, at 12:00:00, timestamps were spaced at\nuniform 3-second intervals, creating a chronologically or-\ndered stream of feedback. This interval approximates\na plausible comment frequency during high-traffic live\nstreaming sessions. Each timestamp was then floored to\na 5-minute window to support aggregation at a coarser\ntemporal resolution. These time windows serve as the\nunit of analysis for computing provider-level and global\nstatistics in later sections.\nThe resulting dataset (as shown in Tab. 4) structure\nincludes five fields per record: the user’s original com-\nment ( original comment ), the associated subjective\nMOS score ( commen mos), the simulated service provider\n(ISP), the synthetic timestamp (timestamp), and the cor-\nresponding aggregation window ( time window ). This\nmetadata-enriched format allows for scalable, window-\nbased analysis of service quality as experienced by users\nacross providers and time.\n11\n--- Page 12 ---\nTable 4: Example entries from the enriched dataset\nused for subjective QoE analysis. Each record includes\na user comment filtered for QoE relevance, a language\nmodel–derived MOS score (ranging from 1 to 5), and sim-\nulated metadata: a service provider label (ISP1–ISP3), a\nsynthetic timestamp generated at 3-second intervals, and\na 5-minute aggregation window used for temporal binning\nand operator-level analysis.\nISP Timestamp Original Comment MOS\nISP1 2024-01-01\n12:00:00no buffering 5.0\nISP2 2024-01-01\n12:00:03twitch lagging or\nsum2.0\nISP1 2024-01-01\n12:00:06anyone else lose au-\ndio at the same\ntime as the dc2.0\n... ... ... ...\n5.4 Temporal Aggregation Format\nTo facilitate scalable analysis and time-aware trend detec-\ntion, all enriched records in the dataset were aggregated\ninto discrete temporal intervals. Each comment’s simu-\nlated timestamp was binned into a 5-minute window us-\ning floor rounding ( timestamp.dt.floor(’5min’) ).\nThis transformation allows for smoothing out individ-\nual fluctuations in user sentiment and enables operators\nto analyze trends over short, operationally relevant time\nframes. For each unique combination of time window and\nISP, we compute two core metrics:\n•Average MOS: The mean subjective score of all com-\nments received from a given ISP during a particular\ntime window.\n•Comment count: The total number of QoE-relevant\ncomments within that window.\nThese aggregated statistics are stored in a derived\ndataset, indexed by both time window and ISP. This\nstructure supports a wide range of time-series operations,\nincluding global MOS tracking, comparative provider\nanalysis, and detection of sudden shifts in user-reported\nservice quality.\nFormally, for each provider Pand time window T, the\naverage MOS is defined as:\nAvgMOS P,T=1\nNP,TNP,T/summationdisplay\ni=1MOS i (1)\nwhereNP,Tis the number of comments from provider P\nin time window T, and MOS iis the LLM-assigned score\nfor comment i.\nThis aggregation approach used in Equation 1, en-\nables flexible downstream analytics, such as identifying\ndegraded intervals, measuring user-perceived service re-\ncovery over time, and benchmarking cross-ISP disparities.\nIt also serves as the basis for visualizations and delta anal-\nysis presented in the §6.Fig. 6 illustrates an example of average MOS values\nover time, aggregated into 5-minute windows and grouped\nby ISP. The curves reveal temporal patterns of perceived\nservice quality and highlight variability across simulated\nproviders. Although network-side metrics are not directly\navailable, the figure reflects user-perceived performance\nat the platform level, capturing how streaming conditions\nmanifest in viewer sentiment. To complement this view,\nFig. 7 presents the ∆MOS the difference between each\nISP’s average MOS and the global average in the same\ntime window. This relative comparison makes it possible\nto isolate deviations from baseline satisfaction levels and\nidentify short-term service degradations or improvements.\nTogether, these plots form the foundation for detecting\nanomalies, evaluating platform impact, and benchmark-\ning simulated ISPs based on platform-side QoE commen-\ntary.\n6 Results and Operator-Side QoE\nInterpretation\nThis section analyzes the subjective QoE data generated\nby our platform-side comment processing pipeline. We\nexamine how aggregated MOS -both globally and per\nISP–can inform network operators about user-perceived\nquality. Through ∆MOS analysis and a simulated out-\nage scenario, we show how this framework enables detec-\ntion of localized degradations and supports real-time QoE\nmonitoring, even without direct access to network-layer\nmetrics.\n6.1 Modeling Assumptions\nTo isolate the impact of comment semantics on perceived\nquality, we assume equal user bases across ISPs and con-\nstant comment volume over time. These simplifications\nhelp highlight MOS trends from feedback alone, indepen-\ndent of real-world user or traffic variations.\n•Equal ISP user: We assume that all ISPs involved\nin the simulation serve an equal number of users on\nthe platform. This abstraction enables us to inter-\npret differences in average MOS scores solely as indi-\ncators of service quality, without being influenced by\ndisparities in user populations or the relative fraction\nof QoE-relevant comments per ISP. By neutralizing\nuser volume as a variable, the analysis focuses solely\non the semantic content of comments.\n•Uniform comment volume across time: It is\nalso assumed that the volume of QoE-related com-\nments remains consistent across all 5-minute time\nwindows throughout the day. In practice, comment\nactivity tends to be higher during peak hours ( e.g.,\n8 PM) and lower during off-peak periods ( e.g., 2\nAM). However, we intentionally disregard these nat-\nural fluctuations to simplify time-series analysis and\neliminate the need for temporal normalization.\n12\n--- Page 13 ---\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime Window1.01.52.02.53.03.54.04.5Mean MOS\nAvg LLM-Derived MOS Over Time (5-Min Windows)\nISP\nISP1\nISP2\nISP3Figure 6: Average MOS over time, aggregated in 5-minute windows and grouped by simulated ISPs (ISP1–ISP3).\nThe values reflect user-perceived quality inferred from comment-level analysis, not direct network metrics. This\nhighlights how platform-side experiences influence perceived quality, enabling comparison of user sentiment across\nproviders and revealing potential service degradations or inconsistencies over time.\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime Window0.4\n0.2\n0.00.20.40.6Delta MOS\n MOS (ISP - Global) Over Time\nISP\nISP1\nISP2\nISP3\nFigure 7: Temporal analysis of ∆ MOS, calculated as the difference between each ISP’s average MOS and the global\nplatform-wide MOS in each 5-minute window. Values above 0 indicate higher-than-average user satisfaction for that\nISP in a given window, while values below 0 reflect lower relative performance. This view captures subtle service\ndisparities between providers and highlights temporal fluctuations in user experience based on platform-derived\nsubjective feedback, rather than direct network measurements.\n13\n--- Page 14 ---\nWhile our assumptions simplify the analysis by treating\nISPs as having equal user bases and uniform comment\nvolumes, they abstract away real-world dynamics, such as\nvarying user distributions and temporal comment spikes.\nIn practice, platforms such as Twitch or YouTube have\naccess to detailed user and traffic data, allowing them\nto compute more accurate, weighted MOS and ∆MOS\nvalues. These platforms can enhance the framework by\nadjusting for user volume and engagement trends across\nISPs and timeframes.\n6.2 Platform-Wide MOS Calculation and\nValidation\nA core objective of our platform-side analysis is to pro-\nvide each network operator with a reliable and inter-\npretable reference for comparing their users’ perceived\nQoE against broader service-level sentiment. To this end,\nwe compute a platform-wide MOS for each time window,\nserving as a neutral benchmark that aggregates user feed-\nback across all ISPs. The platform-wide MOS reflects the\naverage subjective quality observed across the platform\nand is computed over 5-minute non-overlapping time in-\ntervals. For each such interval twe begin by calculating\nthe average MOS reported by users of each ISP i∈ I,\nwhereIdenotes the set of all simulated providers. De-\nnoting the average MOS for ISP iin time window tasµi,t,\nthe global platform-wide MOS is defined as:\nGlobal MOS t=1\n|I|/summationdisplay\ni∈Iµi,t (2)\nEquation 2 ensures that each ISP contributes equally to\nthe overall measure, avoiding biases due to volume dis-\nparities between providers. The resulting global curve–\nplotted in Fig. 6–captures the aggregate platform senti-\nment, accounting for both localized network impairments\nand shared platform-level phenomena such as CDN dis-\nruptions, streaming backend errors, or frontend interface\nlatency. However, while the global MOS trend is critical\nfor understanding the system-wide user experience, it is\ninsufficient for isolating ISP-specific circumstances. For\nthis reason, we also compute the ∆MOS which isolates\neach ISP’s relative deviation from the platform average.\nThis is formally defined for each ISPiand time window\ntas:\n∆MOS i,t =µi,t−Global MOS t (3)\nBy subtracting the platform-wide MOS in Equation 3,\n∆MOS allows each operator to interpret their perfor-\nmance relative to the shared service baseline, effectively\nfiltering out global trends and emphasizing provider-\nspecific quality shifts. As demonstrated in the ISP-\nspecific ∆MOS plots in later subsections, this dual-\nperspective–comparing local MOS and ∆MOS– enables\noperators to distinguish between platform-induced degra-\ndations and network-specific issues. The platform-wide\nMOS thus serves not only as a reference metric but also\nas a foundational element for performance accountability\nand anomaly attribution in multi-operator environments.6.3 ISP-Level ∆MOS Analysis and Out-\nage Thresholding\nWhile the platform-wide MOS offers a useful macro-level\nindicator of perceived service quality across the entire sys-\ntem, it does not account for localized performance dis-\nparities between individual ISPs. To enable provider-\nspecific diagnostics, we introduce the concept of delta\nMOS (∆MOS)–a per-ISP metric that quantifies the de-\nviation between each ISP’s perceived service quality and\nthe global platform average. Formally, for each provider i\nand time window t, letµi,trepresent the provider’s aver-\nage MOS, and let Global MOS tdenote the platform-wide\naverage across all ISPs in the same interval. The delta\nMOS is then defined in Equation 3 This formulation iso-\nlates relative quality performance: a ∆MOS close to zero\nindicates that the ISP is performing near the platform\naverage, while positive or negative values respectively in-\ndicate better- or worse-than-average performance.\nThe evolution of Global MOS tacross time is depicted in\nFig. 8. This global reference curve, computed by averag-\ning the per-ISP MOS scores within each 5-minute interval,\ncaptures the collective sentiment of all users on the plat-\nform. As such, it reflects the influence of both network\nand platform-side conditions and provides essential con-\ntext for interpreting ∆MOS trends. To visualize how each\nprovider deviates from this shared baseline, we generate\nindividualδMOS time series plots for each ISP, shown\nin Fig. 9a, Fig. 9b, and Fig. 9c. These figures trace each\nISP’s relative quality trajectory over time, helping op-\nerators assess stability, identify anomalies, and compare\nservice consistency.\nFor interpretability and threshold-based monitoring,\neach ISP-specific diagram includes:\n1. A dashed gray line at ∆MOS = 0, representing align-\nment with the platform average\n2. A dotted red line at ∆MOS = –0.4, marking a con-\nfigurable potential outage threshold\nThis threshold is not fixed by the system but is in-\nstead meant to be operator-defined, depending on busi-\nness goals or SLA tolerance. A drop below this level sig-\nnals a potentially significant service degradation unique\nto that ISP. Importantly, this threshold is independent\nof the global MOS trend. For instance, during periods\nof relatively stable global MOS ( e.g., Global MOS≈2.5),\na significant negative deviation for a single provider may\nstill indicate localized issues requiring operator interven-\ntion. The ∆MOS framework thus enables a dual-layer\nperspective: it empowers operators to assess how their\nservice compares to overall platform conditions, while fil-\ntering out platform-wide effects that could otherwise ob-\nscure local anomalies. This model supports precise root-\ncause attribution and provides a scalable foundation for\nreal-time QoE monitoring, anomaly detection, and SLA\nenforcement.\n14\n--- Page 15 ---\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime1.01.52.02.53.03.54.04.5Global MOS\nPlatform-Wide Mean MOS Over Time\nGlobal Avg MOS\nGlobal AvgFigure 8: Platform-wide MOS over time, averaged across all ISPs in 5-minute windows. The solid line represents\ntemporal fluctuations in user-perceived quality aggregated from subjective comment-based scores, while the dashed\nline indicates the overall average across the entire observation period. This global trend serves as a shared reference\nfor evaluating relative performance of individual ISPs, as used in the ∆MOS calculations in subsequent analysis.\n6.4 Simulated Outage Detection\nTo evaluate the responsiveness and interpretability of the\n∆MOS metric in identifying service degradation, we con-\nduct a controlled simulation of a provider-specific out-\nage. This experiment serves as a proof-of-concept for\nhow subjective platform-derived feedback can reveal lo-\ncalized anomalies in real time. We inject a synthetic out-\nage into the dataset by manually degrading the MOS for\nISP3 within a fixed temporal interval. Specifically, for all\nuser comments associated with ISP3 between 15:00 and\n16:30 on January 1, 2024, the MOS value is forcibly set to\n1.0, representing the lowest point on the QoE scale and\nreflecting severe user dissatisfaction. This manipulation\nemulates a period of significant network degradation from\nthe end-user’s perspective.\nFormally, the synthetic outage is applied using the fol-\nlowing mask:\n∀i∈ISP3,ift∈[15:00,16:30] =⇒MOS i= 1.0 (4)\nFollowing Equation 4 manipulation, the data is re-\naggregated at the 5-minute window level, and the per-ISP\nMOS scores are recalculated. The new delta MOS values\nare derived by subtracting the recomputed platform-wide\nMOS from the adjusted ISP-level averages, as described\nin subsection 6.3. The impact of this simulated outage is\nvisualized in Fig. 10, which shows a sharp and sustained\ndrop in ISP3’s ∆MOS during the affected interval. The\nplot clearly reflects a negative deviation well below the\n–0.4 outage threshold, reinforcing the utility of ∆MOS\nas a reliable signal for early detection of network-related\nquality problems. The red-shaded vertical band in the\nfigure marks the simulated outage window, aligning pre-\ncisely with the region where ISP3’s ∆MOS sharply de-\nclines.\nThis experiment demonstrates that even in the ab-\nsence of objective, network-side measurements, subjective\nplatform-derived feedback–when aggregated, filtered, and\nstructured–can serve as a high-resolution QoE monitor-\ning signal. Network operators receiving regular platform-\nside MOS summaries ( e.g., per 5-minute window) can\ndetect these anomalies, correlate them with ITU-P1203MOS (Ref. [9]), and localize issues with minimal latency.\nThis example illustrates a broader capability: combining\nsubjective QoE inference from user comments with real-\ntime ISP attribution enables scalable anomaly detection\nthat aligns with user experience, not just infrastructure\ntelemetry.\n6.5 Operator Reporting Model and In-\nteroperability\nThe ultimate goal of platform-side subjective QoE anal-\nysis is to deliver actionable insights to network operators\nin a manner that is interpretable, timely, and directly\naligned with user experience. To that end, the platform\nis designed to report two primary metrics to each operator\nfor every monitoring interval ( e.g., every 5 minutes):\n1. Local Operator MOS ( µi,t– the average Mean Opin-\nion Score derived from filtered user comments at-\ntributed to operator iin time window t\n2. Platform-Wide MOS (Global MOS t) – the overall\nplatform-wide average MOS for the same time win-\ndow, computed across all ISPs.\nThese two values form the basis for comparative analy-\nsis and performance accountability. From the operator’s\nperspective, this information enables the following capa-\nbilities:\n•Trend Monitoring: By tracking their own µi,tover\ntime, operators can observe fluctuations in user sen-\ntiment and correlate these with internal metrics such\nas packet loss, delay, or jitter.\n•Delta Interpretation: By comparing µi,tto\nGlobal MOS t, operators can assess whether drops in\nperceived quality are localized to their infrastruc-\nture or part of a platform-wide issue affecting all\nproviders.\n•QoE Validation: Operators can juxtapose subjec-\ntive MOS with objective QoE metrics computed from\nnetwork telemetry or ITU-T P.1203-based models\n15\n--- Page 16 ---\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime0.4\n0.3\n0.2\n0.1\n0.00.10.20.3Delta MOS\nISP1 MOS Over Time\nISP1 MOS\nPotential Outage Threshold(a) ∆MOS time series for ISP1 showing 5-minute deviations from the platform average. Values below –0.4 suggest potential\nservice degradation.\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime0.4\n0.3\n0.2\n0.1\n0.00.10.20.30.4Delta MOS\nISP2 MOS Over Time\nISP2 MOS\nPotential Outage Threshold\n(b) ∆MOS time series for ISP2 across 5-minute windows. Deviations from the 0 baseline show relative performance, with\nvalues below –0.4 indicating possible quality issues.\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime0.4\n0.3\n0.2\n0.1\n0.00.10.20.30.4Delta MOS\nISP2 MOS Over Time\nISP2 MOS\nPotential Outage Threshold\n(c) ∆MOS time series for ISP3 in 5-minute intervals, highlighting quality deviations from the platform average. Drops below\n–0.4 suggest potential service degradation.\nFigure 9: ∆MOS time series for ISP3 in 5-minute intervals, highlighting quality deviations from the platform average.\nDrops below –0.4 suggest potential service degradation.\n16\n--- Page 17 ---\n01-01 12 01-01 15 01-01 18 01-01 21 01-02 00 01-02 03 01-02 06 01-02 09 01-02 12 01-02 15\nTime1.25\n1.00\n0.75\n0.50\n0.25\n0.000.250.50Delta MOS\nISP3 MOS with Simulated Outage\nISP3 MOS\nOutage Threshold\nSimulated OutageFigure 10: ∆MOS time series for ISP3 with a simulated outage injected between 15:00 and 16:30 on January 1, 2024.\nDuring this window, all MOS scores were manually set to 1.0 to emulate severe user dissatisfaction. The resulting\n∆MOS values fall sharply below the platform average and cross the –0.4 outage threshold (red dotted line). The red\nshaded region highlights the duration of the simulated degradation, demonstrating the model’s ability to detect and\nisolate localized QoE disruptions using comment-based subjective scoring.\n(see [9]). This triangulation allows for the identi-\nfication of discrepancies, such as cases where net-\nwork KPIs appear healthy but users report poor\nexperiences–suggesting potential issues at the appli-\ncation layer or CDN routing.\n•Threshold-Based Alerts: Using pre-configured\nthresholds ( e.g.,δMOS ¡ –0.4), operators can be\nalerted to periods of underperformance without rely-\ning on user complaints or internal diagnostics alone.\nImportantly, the platform does not expose individual\nuser-level data or unfiltered comment content. Instead,\nit aggregates and anonymizes the feedback into time-\nwindowed summaries that preserve privacy while maxi-\nmizing diagnostic value. This reporting model provides a\nbridge between subjective user perception and operator-\nlevel service metrics, enabling a richer, dual-perspective\nunderstanding of QoE. It also supports real-time\n7 Conclusion and Future Work\n7.1 Conclusion\nThis work introduced a hybrid QoE monitoring frame-\nwork that combines operator-side objective estimation\nwith platform-side subjective feedback. On the network\nside, we used ITU-T P.1203-aligned models to estimate\nMOS based solely on network parameters–delay, jitter,\nand throughput–without requiring access to video content\nor user device data. On the platform side, we designed a\npipeline that filters live user comments for QoE relevance\nand employs the LLM to assign subjective MOS scores,\noffering a real-time view of user sentiment.\nBy aggregating subjective scores over 5-minute win-\ndows and associating them with simulated ISPs, we\nenabled comparative analysis using a relative metric,\n∆MOS, which highlights deviations from a platform-wide\nbaseline. Visualizations and a simulated outage demon-\nstrated the framework’s ability to detect localized QoE\ndegradations using only comment-derived insights.The proposed system is scalable, privacy-preserving,\nand independent of infrastructure constraints. By pro-\nviding operators with both subjective MOS scores and\nplatform-wide benchmarks, it bridges the gap between\ntechnical performance and user experience–empowering\nmore informed and user-centric operational decisions.\n7.2 Future Work\nSeveral directions may further extend the impact of this\nframework:\n•LLM fine-tuning on QoE-labeled datasets to improve\ndomain consistency and scoring robustness.\n•Integration with real operator telemetry for deeper\nalignment between subjective and objective metrics.\n•Multilingual and cross-platform comment processing\nto generalize beyond English Twitch streams.\n•Anomaly detection Assistant using ∆MOS trends\nand contextual metadata to identify and classify net-\nwork disruptions automatically.\n•Root-cause correlation by incorporating application-\nside, CDN-level, or regional infrastructure indicators.\nTogether, these extensions aim to evolve the framework\ninto a real-time QoE insight platform capable of assisting\nboth operators and service providers in delivering more\nreliable and satisfying user experiences.\nReferences\n[1] P. H. S. Panahi, A. H. Jalilvand, and A. Diyanat,\n“Enhancing quality of experience in telecommunica-\ntion networks: A review of frameworks and machine\nlearning algorithms,” 2024.\n[2] J. Arellano-Uson, E. Maga˜ na, D. Morato, and\nM. Izal, “Survey on quality of experience evalua-\ntion for cloud-based interactive applications,” Ap-\nplied Sciences , vol. 14, no. 5, p. 1987, 2024.\n17\n--- Page 18 ---\n[3] P. Anchuen and P. Uthansakul, “Investigation into\nuser-centric qoe and network-centric parameters\nfor youtube service on mobile networks,” in\nProceedings of the 7th International Conference on\nCommunications and Broadband Networking , ser.\nICCBN ’19. New York, NY, USA: Association\nfor Computing Machinery, 2019, p. 28–32. [Online].\nAvailable: https://doi.org/10.1145/3330180.3330186\n[4] Y. Gao, X. Wei, L. Zhou, and M. Guizani, “Qoe\nin multimedia domain: a user-centric quality assess-\nment,” Int. J. Multim. Intell. Secur. , vol. 3, pp. 162–\n186, 2018.\n[5] O. Belmoukadam, T. Spetebroot, and C. Barakat,\n“Acqua: A user friendly platform for lightweight net-\nwork monitoring and qoe forecasting,” 2019 22nd\nConference on Innovation in Clouds, Internet and\nNetworks and Workshops (ICIN) , pp. 88–93, 2019.\n[6] H. Gokcesu, O. Ercetin, and G. Kalem, “Enhanc-\ning qoe assessment in fwa: Leveraging network kpis\nand user feedback analysis,” 2023 IEEE 28th Inter-\nnational Workshop on Computer Aided Modeling and\nDesign of Communication Links and Networks (CA-\nMAD) , pp. 233–239, 2023.\n[7] X. Zheng, Y. Jin, X. Shi, Y. Song, and X. Zhao,\n“Ucwe: A user-centric approach for web quality of\nexperience measurement,” 2019 IEEE Intl Conf on\nParallel & Distributed Processing with Applications,\nBig Data & Cloud Computing, Sustainable Comput-\ning & Communications, Social Computing & Net-\nworking (ISPA/BDCloud/SocialCom/SustainCom) ,\npp. 928–935, 2019.\n[8] P. Charonyktakis, M. Plakia, I. Tsamardinos, and\nM. Papadopouli, “On user-centric modular qoe pre-\ndiction for voip based on machine-learning algo-\nrithms,” IEEE Transactions on Mobile Computing ,\nvol. 15, no. 6, pp. 1443–1456, 2016.\n[9] P. H. S. Panahi, A. Hossein Jalilvand, and\nA. Diyanat, “An efficient network-based qoe assess-\nment framework for multimedia networks using a\nmachine learning approach,” IEEE Open Journal of\nthe Communications Society , vol. 6, pp. 1653–1669,\n2025.\n[10] I. Orsolic, “Quality of experience estimation of en-\ncrypted video streaming by using machine learning\nmethods,” Ph.D. dissertation, 10 2020.\n[11] G. Kougioumtzidis, V. Poulkov, Z. D. Zaharis, and\nP. I. Lazaridis, “A survey on multimedia services qoe\nassessment and machine learning-based prediction,”\nIEEE Access , vol. 10, pp. 19 507–19 538, 2022.\n[12] European Telecommunications Standards Institute,\n“Speech and multimedia Transmission Quality\n(STQ); Quality of Experience; A Monitoring Archi-\ntecture,” ETSI, Tech. Rep. TS 103 294 V1.1.1, 2015.[13] S. Barakovi´ c, L. Skorin-Kapov et al. , “Survey and\nchallenges of qoe management issues in wireless net-\nworks,” Journal of Computer Networks and Commu-\nnications , vol. 2013, 2013.\n[14] M. T. Sultan and H. El Sayed, “Qoe-aware analysis\nand management of multimedia services in 5g and be-\nyond heterogeneous networks,” IEEE Access , 2023.\n[15] N. Barman and M. G. Martini, “Qoe modeling for\nhttp adaptive video streaming–a survey and open\nchallenges,” Ieee Access , vol. 7, pp. 30 831–30 859,\n2019.\n[16] E. Liotou, D. Xenakis, V. Georgara, G. Kourounio-\ntis, and L. Merakos, “Cache-enabled adaptive video\nstreaming: A qoe-based evaluation study,” Future\nInternet , vol. 15, no. 7, p. 221, 2023.\n[17] A. A. Barakabitze, N. Barman, A. Ahmad, S. Zad-\ntootaghaj, L. Sun, M. G. Martini, and L. Atzori,\n“Qoe management of multimedia streaming services\nin future networks: A tutorial and survey,” IEEE\nCommunications Surveys & Tutorials , vol. 22, no. 1,\npp. 526–565, 2019.\n[18] H. O. Hamidou, J. P. Kouraogo, O. Sie, and D. Tap-\nsoba, “Machine learning based quality of experience\n(qoe) prediction approach in enterprise multimedia\nnetworks,” in Proceedings of the 5th edition of the\nComputer Science Research Days, JRI 2022, 24-26\nNovember 2022, Ouagadougou, Burkina Faso , 2023.\n[19] Z. Wang, R. Lu, Z. Zhang, C. Westphal, D. He,\nand J. Jiang, “Llm4band: Enhancing reinforcement\nlearning with large language models for accurate\nbandwidth estimation,” in Proceedings of the 35th\nWorkshop on Network and Operating System Support\nfor Digital Audio and Video , ser. NOSSDAV ’25.\nNew York, NY, USA: Association for Computing\nMachinery, 2025, p. 43–49. [Online]. Available:\nhttps://doi.org/10.1145/3712678.3721880\n[20] A. Rezaee and S. Gundavelli, “REAL-TIME MON-\nITORING AND ENHANCEMENT OF MULTI-\nUSER COMMUNICATIONS WITH AI AGENTS,”\nhttps://www.tdcommons.org/dpubs series/7983,\nApril 2025, technical Disclosure Commons, Defen-\nsive Publications Series.\n[21] S. Jha and R. Chuppala, “Quality-of-service aware\nllm serving. ”\n[22] H. Liu, Z. Zhang, H. Li, Q. Wu, and Y. Zhang,\n“Large language model aided qos prediction\nfor service recommendation,” arXiv preprint\narXiv:2408.02223 , 2024.\n[23] M. Wang, Y. Huang, J. Lin, W. Xie, G. Yue,\nS. Wang, and L. Li, “Quality measurement of screen\nimages via foreground perception and background\nsuppression,” IEEE Transactions on Instrumenta-\ntion and Measurement , vol. 70, pp. 1–11, 2021.\n18\n--- Page 19 ---\n[24] M. Wang, J. Lin, J. Zhang, and W. Xie, “Fine-\ngrained region adaptive loop filter for super-block\nvideo coding,” IEEE Access , vol. 8, pp. 445–454,\n2019.\n[25] Z. Yetgin and Z. G¨ o¸ cer, “Quality of experience pre-\ndiction model for progressive downloading over mo-\nbile broadcast networks,” Telecommunication Sys-\ntems, vol. 58, pp. 55–66, 01 2014.\n[26] Z. Meng, Q. Gao, D. Guo, Y. Li, B. Li, H. Fei,\nS. Wu, F. Li, C. Teng, and D. Ji, “Mmlscu: A\ndataset for multi-modal multi-domain live streaming\ncomment understanding,” in Proceedings of the\nACM Web Conference 2024 , ser. WWW ’24.\nNew York, NY, USA: Association for Computing\nMachinery, 2024, p. 4395–4406. [Online]. Available:\nhttps://doi.org/10.1145/3589334.3645677\n[27] F.-L. Huang, G.-Q. Xie, and Z.-W. Chen, “Time-\nsync comments analyzation for understanding sub-\nscribers to live streaming services,” 2020.\n[28] Z. Deng, A. Ananthram, and K. McKeown, “Enhanc-\ning multimodal affective analysis with learned live\ncomment features,” in AAAI Conference on Artifi-\ncial Intelligence , 2024.\n[29] Z. Gao, S. Yada, S. Wakamiya, and E. Aramaki, “Of-\nfensive language detection on video live streaming\nchat,” in International Conference on Computational\nLinguistics , 2020.\n[30] C. Ringer, M. A. Nicolaou, and J. Walker,\n“Twitchchat: A dataset for exploring livestream\nchat,” in Proceedings of the AAAI Conference on Ar-\ntificial Intelligence and Interactive Digital Entertain-\nment , vol. 16, no. 1, 2020, pp. 259–265.\n[31] J. Kim, “Twitch.tv Chat Log Data,” 2019. [Online].\nAvailable: https://doi.org/10.7910/DVN/VE0IVQ\n[32] A. Y. Zaremarjal and D. Yiltas-Kaplan, “Seman-\ntic collaborative filtering recommender system using\ncnns,” in 2021 8th International Conference on Elec-\ntrical and Electronics Engineering (ICEEE) , 2021,\npp. 254–258.\n[33] M. Shirazipour, G. Charlot, G. Lefebvre, S. Kr-\nishnan, and S. Pierre, “Conex based qoe feedback\nto enhance qoe,” in Proceedings of the 2012 ACM\nWorkshop on Capacity Sharing , ser. CSWS ’12.\nNew York, NY, USA: Association for Computing\nMachinery, 2012, p. 27–32. [Online]. Available:\nhttps://doi.org/10.1145/2413219.2413228\n[34] D. K. Kontoe, “Document-level text simplification,”\n2023.\n[35] C. Galli, N. Donos, and E. Calciolari, “Performance\nof 4 pre-trained sentence transformer models in\nthe semantic query of a systematic review dataset\non peri-implantitis,” Information , vol. 15, no. 2,\n2024. [Online]. Available: https://www.mdpi.com/\n2078-2489/15/2/68[36] S. Wang, W. Yu, Y. Yang, C. Tang, Y. Li, J. Zhuang,\nX. Chen, X. Tian, J. Zhang, G. Sun et al. , “Enabling\nauditory large language models for automatic speech\nquality evaluation,” in ICASSP 2025-2025 IEEE In-\nternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP) . IEEE, 2025, pp. 1–5.\n[37] A. Kobayashi and S. Yamaguchi, “Extraction of\nsubjective information from large language models,”\nin2024 IEEE 48th Annual Computers, Software,\nand Applications Conference (COMPSAC) , 2024, pp.\n1612–1617.\n[38] H. Kirk, A. Bean, B. Vidgen, P. Rottger, and S. Hale,\n“The past, present and better future of feedback\nlearning in large language models for subjective hu-\nman preferences and values,” 01 2023, pp. 2409–2430.\n[39] Z. Xia, J. Xu, Y. Zhang, and H. Liu, “A survey of un-\ncertainty estimation methods on large language mod-\nels,” arXiv preprint arXiv:2503.00172 , 2025.\n[40] T. B. Brown, B. Mann, N. Ryder et al. , “Language\nmodels are few-shot learners,” Advances in neural\ninformation processing systems , vol. 33, pp. 1877–\n1901, 2020.\n[41] OpenAI, “Gpt-4o overview and capabilities,” https:\n//openai.com/index/hello-gpt-4o, 2024, accessed:\nMay 2025.\n19",
  "text_length": 79310
}