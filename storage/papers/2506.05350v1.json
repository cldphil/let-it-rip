{
  "id": "http://arxiv.org/abs/2506.05350v1",
  "title": "Contrastive Flow Matching",
  "summary": "Unconditional flow-matching trains diffusion models to transport samples from\na source distribution to a target distribution by enforcing that the flows\nbetween sample pairs are unique. However, in conditional settings (e.g.,\nclass-conditioned models), this uniqueness is no longer guaranteed--flows from\ndifferent conditions may overlap, leading to more ambiguous generations. We\nintroduce Contrastive Flow Matching, an extension to the flow matching\nobjective that explicitly enforces uniqueness across all conditional flows,\nenhancing condition separation. Our approach adds a contrastive objective that\nmaximizes dissimilarities between predicted flows from arbitrary sample pairs.\nWe validate Contrastive Flow Matching by conducting extensive experiments\nacross varying model architectures on both class-conditioned (ImageNet-1k) and\ntext-to-image (CC3M) benchmarks. Notably, we find that training models with\nContrastive Flow Matching (1) improves training speed by a factor of up to 9x,\n(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9\ncompared to training the same models with flow matching. We release our code\nat: https://github.com/gstoica27/DeltaFM.git.",
  "authors": [
    "George Stoica",
    "Vivek Ramanujan",
    "Xiang Fan",
    "Ali Farhadi",
    "Ranjay Krishna",
    "Judy Hoffman"
  ],
  "published": "2025-06-05T17:59:58Z",
  "updated": "2025-06-05T17:59:58Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05350v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05350v1  [cs.CV]  5 Jun 2025Contrastive Flow Matching\nGeorge Stoica12†Vivek Ramanujan2♢Xiang Fan2♢\nAli Farhadi2Ranjay Krishna2Judy Hoffman1\n1Georgia Tech2University of Washington\n†Correspondence to: gstoica3@gatech.edu♢Equal Contribution\nFigure 1. Training with Contrastive Flow-Matching ( ∆FM) improves natural image generation. (leftis baseline, right is with ∆FM)\nHere we show comparisons between images generated by diffusion models trained on ImageNet-1k ( 512×512). Each pair of images\nis generated with the same class and initial noise to ensure similar image structure for comparability. We see that our ∆FMobjective\nencourages significantly more coherent images and improves the consistency of global structure.\nAbstract\nUnconditional flow-matching trains diffusion models to\ntransport samples from a source distribution to a target dis-\ntribution by enforcing that the flows between sample pairs\nare unique. However, in conditional settings (e.g., class-\nconditioned models), this uniqueness is no longer guaran-\nteed—flows from different conditions may overlap, leading\nto more ambiguous generations. We introduce Contrastive\nFlow Matching, an extension to the flow matching objec-\ntive that explicitly enforces uniqueness across all condi-\ntional flows, enhancing condition separation. Our approach\nadds a contrastive objective that maximizes dissimilarities\nbetween predicted flows from arbitrary sample pairs. We\nvalidate Contrastive Flow Matching by conducting exten-\nsive experiments across varying model architectures on both\nclass-conditioned (ImageNet-1k) and text-to-image (CC3M)\nbenchmarks. Notably, we find that training models with Con-\ntrastive Flow Matching (1) improves training speed by a\nfactor of up to 9×, (2) requires up to 5×fewer de-noising\nsteps and (3) lowers FID by up to 8.9compared to training\nthe same models with flow matching. We release our code\nat:https://github.com/gstoica27/DeltaFM.git .1. Introduction\nFlow matching for generative modeling trains continuous\nnormalizing flows by regressing ideal probability flow fields\nbetween a base (noise) distribution and the data distribu-\ntion [ 25]. This approach enables straight-line generative\ntrajectories and has demonstrated competitive image synthe-\nsis quality. However, for conditional generation (e.g., class-\nconditional image generation), vanilla flow matching models\noften produce outputs that resemble an “average” of the\npossible images for a given condition, rather than a distinct\nmode of that condition. In essence, the model may collapse\nmultiple diverse outputs into a single trajectory, yielding\nsamples that lack the expected specificity and diversity for\neach condition [ 29,44]. By contrast, an unconditional flow\nmodel—tasked with covering the entire data distribution\nwithout any conditioning—implicitly learns more varied\nflows for different modes of the data. Existing conditional\nflow matching formulations do not enforce the flows to differ\nacross conditions , which can lead to this averaging effect\nand suboptimal generation fidelity.\nTo address these limitations and improve generation qual-\nity, recent work has explored enhancements to structure the\ngenerator’s representations and also proposed inference-time\nguidance strategies. For example, one approach is to in-\ncorporate a REPresentation Alignment (REPA) objective to\n1\n--- Page 2 ---\nFigure 2. ∆FM yields more discriminative and higher quality trajectories. (left) shows the result of standard flow-matching, where\nflows are straight but end up overlapping for similar class distributions. (right) shows how the addition of the ∆FMobjective results in\nmore distinct flows, resulting in images which are more representative of their respective classes.\nstructure the representations at an intermediate layer with\nthose from a high-quality pretrained vision encoder [ 44]. By\nusing feature embeddings from a DINO self-supervised vi-\nsion transformer [ 5,31], the generative model’s hidden states\nare guided toward semantically meaningful directions. This\nrepresentational alignment provides an additional learning\nsignal that has been shown to improve both training conver-\ngence and final image fidelity, albeit at the cost of requiring\nan external pretrained encoder and an auxiliary loss term.\nAnother popular technique is classifier-free guidance (CFG)\nfor conditional generation [ 18], which involves jointly train-\ning the model in unconditional and conditional modes (often\nby randomly dropping the condition during training). At in-\nference time, CFG performs two forward passes—one with\nthe conditioning input and one without—and then extrapo-\nlates between the two outputs to push the sample closer to\nthe conditional target [ 18,29]. While CFG can significantly\nenhance image detail and adherence to the prompt or class la-\nbel, it doubles the sampling cost and complicates training by\nnecessitating an implicit unconditional generator alongside\nthe conditional ones [11, 11, 20].\nWe propose Contrastive Flow Matching (∆FM), a new\napproach that augments the flow matching objective with an\nauxiliary contrastive learning objective. ∆FMencourages\nmore diverse and distinct conditional generations. It applies\na contrastive loss on the flow vectors (or representations) of\nsamples within each training batch, encouraging the model\nto produce dissimilar flows for different conditioning inputs.\nIntuitively, this loss penalizes the model if two samples with\ndifferent conditions yield similar flow dynamics, thereby\nexplicitly discouraging the collapse of multiple conditions\nonto a single “average” generative trajectory. As a result,\ngiven a particular condition, the model learns to generate\na unique flow through latent space that is characteristic of\nthat condition alone, leading to more varied and condition-\nspecific outputs. Importantly, this contrastive augmentation\niscomplementary to existing methods. It can be appliedalong with REPA, further ensuring that flows not only align\nwith pretrained features but also remain distinct across condi-\ntions. Likewise, it is compatible with classifier-free guidance\nat sampling time, allowing one to combine its benefits with\nCFG for even stronger conditional signal amplification.\nInspired by contrastive training objectives, ∆FMapplies\na pairwise loss term between samples in a training batch:\nfor each positive sample from the batch, we randomly sam-\nple a negative counterpart. We then encourage the model\nto not only learn the flow towards the positive sample but\nalso to learn the flow away from the negative sample. This\nis achieved by adding a contrastive loss to the flow match-\ning objective, which promotes class separability throughout\nthe flow. Our method is simple to implement and can be\neasily integrated into existing diffusion models without any\nadditional data and with minimal computational overhead.\nWe validate the advantages of ∆FMthrough (1) exten-\nsive experiments on conditional image generation using Im-\nageNet images across multiple SiT [ 29] model scales and\ntraining frameworks [ 29,44], and (2) text-to-image experi-\nments on the CC3M [ 37] with the MMDiT [ 14] architecture.\nThanks to contrastive flows, ∆FMconsistently outperforms\ntraditional diffusion flow matching in quality and diversity\nmetrics, achieving up to an 8.9-point reduction in FID-50K\non ImageNet, and 5-point reduction in FID on the whole\nCC3M validation set. It is also compatible with recent sig-\nnificant improvements in the diffusion objective, such as\nRepresentation Alignment (REPA) [ 44]. By encouraging\nclass separability, ∆FMis able to efficiently reach a given\nimage quality with 5×fewer sampling steps than a baseline\nFlow Matching model, translating directly to faster genera-\ntion. It also enhances training efficiency by up to 9×. Finally,\n∆FMstacks with classifier-free guidance, lowering FID by\n5.7% compared to flow matching models.\n2\n--- Page 3 ---\n2. Related works\nOur work lies in the domain of image generative models,\nprimarily diffusion and flow matching models. We augment\nflow-matching with a contrastive learning objective to pro-\nvide an alternative solution to classifier free guidance.\nGenerative modeling has rapidly advanced through two\nprimary paradigms: diffusion-based methods [ 19,39] and\nflow matching [ 25].Denoising diffusion models typically\nrely on stochastic differential equations (SDEs) and score-\nbased learning to iteratively add and remove noise [ 19]. De-\nnoising diffusion implicit models (DDIMs) [ 39] reduce this\nsampling complexity by removing non-determinism in the\nreverse process, while progressive distillation [ 34] further\naccelerates inference by shortening the denoising chain. Ad-\nvanced ODE solvers [ 6] and distillation methods [ 41] have\nalso enhanced sampling efficiency. Despite their success,\ndiffusion models can be slow at inference due to iterative\ndenoising [19].\nFlow matching [6] has been designed to reduce infer-\nence steps. It directly parameterizes continuous-time trans-\nport dynamics for more efficient sampling. Probability flow\nODEs [ 25,39] learn an explicit transport map between data\nand latent distributions. Unlike diffusion models, it bypasses\nseparate score estimation and stochastic noise, which re-\nduces function evaluations and tends to improve training\nconvergence [ 6]. A common type of flow matching algo-\nrithm popularized recently is the rectified flow [ 26], which\nrefines probability flow ODEs through direct optimal trans-\nport learning, improving numerical stability and sampling\nspeed. This approach mitigates the high computational bur-\nden of diffusion sampling while maintaining high-fidelity\nimage generation with fewer integration steps.\nSince both diffusion and flow matching models are trained\nto match the target distribution of real images, they often\nproduce ‘averaged’ samples that lack the sharp details and\nstrong conditional fidelity [ 17]. Regardless of how much\nthese models speed up, they often need to be invoked mul-\ntiple times with unique seed noise to find a high-fidelity\nsample. In response, guidance techniques have been in-\ntroduced to substantially promote high-fidelity synthesis.\nClassifier guidance [ 12], classifier-free guidance [ 17], en-\nergy guidance [ 8,27,40,45], and more advanced meth-\nods [ 9,20,21,23,38] improve fidelity and controllabil-\nity, without requiring multiple invocations. Although they\nachieve remarkable performance, they typically still require\nadditional computational overhead. CFG requires calling\nsampling from a second ‘unconditional’ generation and guid-\ning the ‘conditional’ generation away from the unconditional\nvariant [ 28,42,43,46]. We adapt the flow matching objec-\ntive with a contrastive loss between the transport vectors\nwithin a batch. By doing so, we achieve the same benefits of\nCFG, without the additional overhead of needing to train an\nunconditional generator or using one during inference.Contrastive learning was originally proposed for face\nrecognition [ 36], where it was designed to encourage a mar-\ngin between positive and negative face pairs. In generative\nadversarial networks (GANs), it has been applied to improve\nsample quality by structuring latent representations [ 4]. How-\never, to the best of our knowledge, it has not been explored\nin the context of visual diffusion or flow matching models.\nWe incorporate this contrastive objective to demonstrate its\nutility in speeding up training and inference of flow-based\ngenerative models.\n3. Background and motivation\nWe focus on flow matching models [ 25] due to its rising\npopularity as an effective training paradigm for generative\nmodels [ 1,2,24]. In this section, we provide a brief overview\nof flow matching through the perspective of stochastic inter-\npolants [2, 29], as it pertains to our work.\nPreliminaries. Letp(x)be an arbitrary distribution defined\non the reals, and let N(0,I)be a Gaussian noise distribution.\nThe objective of flow matching is to learn a transport between\nthe two distributions. That is, given an arbitrary ϵ∼ N(0,I),\na flow matching model gradually transforms ϵover time into\nanˆxthat is part of p(x). Stochastic interpolants [ 2,29] define\nthis transformation as a time-dependent stochastic process,\nwhere transformation steps are summarized as follows,\nˆxt=αtˆx+σtϵ (1)\nwhere αtandσtare decreasing and increasing time-\ndependent functions respectively defined on t∈[0, T], such\nthatαT=σ0= 1andα0=σT= 0. While theoretically,\nαt, σtneed not be linear, linear complexity is often sufficient\nto obtain strong diffusion models [25, 29, 44].\nFlow matching. Given such a process, flow matching mod-\nels learn to transport between noise to p(x)by estimating a\nvelocity field over an probability flow ordinary differential\nequation (PF ODE), dxt=v(xt, t)dt, whose distribution at\ntimetis the marginal pt(x). This velocity is given by the\nexpectations of ˆxandϵconditioned on xt,\nv(xt, t) = ˙αtE[ˆx|xt=x] + ˙σtE[ϵ|xt=x], (2)\nwhere ˙αt,˙σtare the time-based derivatives of αtandσt\nrespectively. Since, ˆxandϵare arbitrary samples from their\nrespective distributions, v(xt, t)is expected “direction” of\nall transport paths between noise and p(x)that pass through\nxtatt. While the optimal v(xt, t)is intractable, it can be\napproximated with a flow-model vθ(xt, t), by minimizing\nthe training objective:\nL(FM)(θ) =E\u0002\n||vθ(xt, t)−( ˙αtˆx+ ˙σtϵ)||2\u0003\n(3)\nKey to understanding the properties of flow matching is\nthe concept of flow uniqueness [ 25]. That is, flows fol-\nlowing the well-defined ODE cannot intersect at anytime\n3\n--- Page 4 ---\nt∈[0, T). As such, flow models can iteratively refine\nunique-discriminative features relevant to any x∼p(x)in\neach xt, leading to more efficient and accurate diffusion\npaths compared to other training paradigms [25].\nConditional flow matching. Commonly, p(x)may be a\nmarginal distribution over several class-conditional distribu-\ntions (e.g., the classes of ImageNet [ 33]). Training models\nin such cases is nearly identical to standard flow-matching,\nexcept that flows are further conditioned on the target distri-\nbution class:\nL(FM)\ncond(θ) =E\u0002\n||vθ(xt, t, y)−( ˙αtˆx+ ˙σtϵ)||2\u0003\n,(4)\nwhere ˆx∼p(x|y). Resultant models have the desirable\ntrait of being more controllable: their generated outputs can\nbe tailored to their respective input conditions. However,\nthis comes at the notable cost of flow-uniqueness. Specif-\nically these models only generate unique flows compared\nto others within the same class-condition, not necessarily\nacross classes. This inhibits xt’s from storing important\nclass-specific features and leads to poorer quality genera-\ntions. Second, the conditional flow matching objective trains\nmodels without knowledge of the distributional spread from\nother class-conditions, leading to flows that may generate\nambiguous outputs when conditional distributions overlap\n. This increases the likelihood of ambiguous generations\nthat form a mixture between different conditions, restricting\nmodel capabilities. We study these effects in Section 5.\n4. Contrastive Flow-Matching\nWe introduce Contrastive Flow Matching ( ∆FM), a novel\napproach designed to address the challenges of learning effi-\ncient class-distinct flow representations in conditional gen-\nerative models. Standard conditional flow matching (FM)\nmodels tend to produce flow trajectories that align across dif-\nferent samples, leading to reduced class separability. ∆FM\nextends the FM objective by incorporating a contrastive\nregularization term, which explicitly discourages alignment\nbetween the learned flow trajectories of distinct samples.\nIngredients. Let˜x∼p(x|˜y)denote a sample drawn from\nthe data distribution conditioned on an arbitrary class ˜y, and\nlet˜ϵ∼ N(0, I)represent an independent noise sample. To\nensure that the contrastive objective captures distinct flow\ntrajectories, we impose the conditions ˜x̸= ˆxand˜ϵ̸=ϵ,\nwhere ˜ymay or may not be equal to y. Importantly, we\ndo not assume the existence of a time step t∈[0, T]such\nthatxt=αt˜x+σt˜ϵ. Consequently, ˜xand˜ϵrepresent truly\nindependent flow trajectories in comparison to ˆxandϵ.\nThe contrastive regularization. Given vθ(xt, t, y)and an\narbitrary ˜x,˜ϵsample pair, the contrastive objective aims to\nmaximize the dissimilarity between the estimated flow of\nvθ(xt, t, y)from ϵtoˆx, and the independent flow producedby˜x,˜ϵ. We achieve this by maximizing the quantity,\nE\u0002\n||vθ(xt, t, y)−( ˙αt˜x+ ˙σt˜ϵ)||2\u0003\n. (5)\nSince ˜xis drawn from the marginal p(x)rather than p(x|y),\nEquation 5 trains flow matching models to produce flows\nthat are unconditionally unique.\nPutting it all together. We now define contrastive flow\nmatching as follows,\nL(∆FM)(θ) = E\"\n||vθ(xt, t, y)−( ˙αtˆx+ ˙σtϵ)||2\n−λ||vθ(xt, t, y)−( ˙αt˜x+ ˙σt˜ϵ)||2#\n(6)\nwhere λ∈[0,1)is a fixed hyperparameter that controls\nthe strength of the contrastive regularization. Thus, ∆FM\nsimultaneously encourages flow matching models to esti-\nmate effective transports from noise to corresponding class-\nconditional distributions (the flow matching objective), while\nenforcing each to be discriminative across classes (con-\ntrastive regularization). Note that ∆FM can be thought\nof as a generalization of flow matching, as ∆FMreduces\nto FM when λ= 0. We study the effects of varying λin\nSection 5.5.\nImplementation. Contrastive flow matching ( ∆FM) is eas-\nily integrated into any flow matching training loop, with\nminimal overhead. Algorithm 1 illustrates the implemen-\ntation of an arbitrary batch step, where navy text marks\nadditions to the standard flow matching objective. Thus,\n∆FMsolely depends on the information already available\nto the flow matching objective at each batch step, without\ncomputing any additional forward steps. Furthermore, ∆FM\nseamlessly folds into flow matching training regimes, mak-\ning it a “plug-and-play” objective for existing setups.\nAlgorithm 1 Contrastive Flow-Matching Batch Step\n1:Input: A model vθ, batch of Nflow examples\nF={(x1, y1, ϵ1), . . . , (xN, yN, ϵN)}where (xi, yi)∼\np(x, y)andϵi∼ N(0,I),βlearning rate, λ= 0.05.\n2:Output: Updated model parameters θ\n3:L(θ) = 0\n4:foriin range( N)do\n5: t∼U(0,1), xt=αtxi+σtϵi\n6: sample (˜x,˜y,˜ϵ)∼F,s.t.(˜x,˜y,˜ϵ)̸= (xi, yi, ϵi)\n7: ˆv=v(xt, t, yi), v= ˙αtxi+ ˙σtϵ,˜v= ˙αt˜x+ ˙σt˜ϵ\n8: L(θ)+ =||ˆv−v||2−λ||ˆv−˜v||2\n9:end for\n10:θ←θ−β\nN∇θL(θ)\nDiscussion. Figure 3 illustrates the effects of contrastive\nflow matching compared to flow matching. The figure shows\nthe resultant flows after training a small diffusion model in a\nsimple toy-setting. Specifically, we create a two-dimensional\n4\n--- Page 5 ---\nFigure 3. Contrastive Flow-Matching intrinsically separates\nflows between classes. We train a small three layer MLP flow-\nmatching model to transport between a two dimensional multivari-\nate noise distribution (violet) and two independent blue and orange\nclass distributions respectively. The class distributions are designed\nto have ∼50% overlap, and we plot the learned class-conditioned\nflows between noise samples and each respective class distribution\nusing class colors. Top: Flow-matching models learn overlapping\ntransports between distributions, generating outputs that lie in am-\nbiguous regions between the two classes. Bottom: Contrastive\nflow-matching models have significantly more discriminative flows,\ngenerating class-coherent samples while reducing ambiguity.\nviolet gaussian noise distribution and two independent two-\ndimensional class distributions (in blue and orange respec-\ntively) such that the latter distributions have ≈50% overlap.\nSamples from each distribution are represented as “dots”,\nwith those in the target distributions colored according to the\ngaussian kernel-density estimate between samples from each\nclass in their respective region. We observe that training\nthe model with flow matching (top) create flows with large\ndegrees of overlap between classes, generating samples with\nlower class-distinction. In contrast, training the same model\nwith contrastive flow matching (bottom) yields trajectories\nthat are significantly more diverse across classes, while also\ngenerating samples which capture distinct features of each\nrespective class.\n5. Experiments\nWe validate contrastive flow-matching ( ∆FM) through ex-\ntensive experiments across various model, training and\nbenchmark configurations. Overall, models trained with\n∆FMconsistently outperform flow-matching (FM) modelsacross allsettings.\nDatasets. We conduct both class-conditioned and text-to-\nimage experiments. We use ImageNet-1k [ 10] processed\nat both ( 256×256) and ( 512×512) resolutions for our\nclass-conditioned experiments, and follow the data prepro-\ncessing procedure of ADM [ 12] We then follow [ 44] and\nencode each image using the Stable Diffusion V AE [ 32] into\na tensor z∈R32×32×4. For text-to-image (t2i), we use the\nConceptual Captions 3M (CC3M) dataset [ 37] processed\nat (256×256) resolution and follow the data processing\nprocedure of [ 3]. We train all models by strictly following\nthe setup in [ 44], and use a batch size of 256 unless other-\nwise specified. We do not alter the training conditions to\nbe favorable to ∆FM, and we always set λ= 0.05when\napplicable.\nMeasurements. We report five quantitative metrics through-\nout our experiments. We report Fr ´echet inception distance\n(FID) [ 16], inception score (IS) [ 35], sFID [ 30], precision\n(Prec.) and recall (Rec.) [ 22] using 50,000 samples for our\nclass-conditioned experiments. Similarly, we report FID\nover the whole validation set in the text-to-image setting.\nWe use the SDE Euler-Maruyama sampler with wt=σtfor\nall experiments, and set the number of function evaluations\n(NFE) to 50 unless otherwise specified.\n5.1. Contrastive Flow-Matching Improves SiT\nImplementation details. We train on the state-of-the-art\nSiT [ 29] model architecture, using both SiT-B/2 and SiT-\nXL/2.\nResults. Table 1 summarizes our results. Overall, ∆FM\ndramatically improves over flow-matching in nearly all met-\nrics (only matching the flow-matching SiT-XL/2 model in\nrecall). Notably, employing ∆FMwith SiT-B/2 lowers FID\nby over 8 compared to flow-matching at both ImageNet res-\nolutions, highlighting the strength of ∆FMin smaller model\nscales. Similarly, ∆FMis robust to larger model scales and\noutperforms FM by over 3.2 FID when using SiT-XL/2.\n5.2. REPA is complementary\nREPresentation Alignment (REPA) [ 44] is a recently intro-\nduced training framework that rapidly improves diffusion\nmodel performance by strengthening its intermediate rep-\nresentations. Specifically, REPA distills the encodings of\nfoundation vision encoders (e.g., DiNOv2 [ 5]) into the hid-\nden states of diffusion models through the use of an aux-\nilliary objective. Notably, REPA can improve the training\nspeed of vanilla SiT models by over 17.5×, while further im-\nproving their performances [ 44].∆FMis easily integrated\ninto REPA and only requires replacing the flow-matching\nobjective.\nImplementation details. We apply REPA on the same SiT\nmodels as in Section 5.1, and use the distillation process de-\n5\n--- Page 6 ---\nMetrics\nModel FID ↓ IS↑ sFID↓Prec.↑Rec.↑\nSiT-B/2 42.28 38.04 11.35 0.5 0.62\n+ Using ∆FM 33.39 43.44 5.67 0.53 0.63\nSiT-XL/2 20.01 74.15 8.45 0.63 0.63\n+ Using ∆FM 16.32 78.07 5.08 0.66 0.63\n(a)ImageNet-1k (256x256) Results. ∆FMsignificantly outperforms flow-\nmatching models across nearly all metrics, and matches Recall on SiT-XL/2.\nMetrics\nModel FID ↓ IS↑ sFID↓ Prec.↑ Rec.↑\nSiT-B/2 50.26 33.58 14.88 0.57 0.61\n+ Using ∆FM 41.59 38.20 6.13 0.62 0.63\nSiT-XL/2 22.98 70.14 10.71 0.73 0.60\n+ Using ∆FM 19.67 72.58 4.98 0.76 0.60\n(b)ImageNet-1k (512x512) Results. Models trained with ∆FMeither\nsubstantially outperform or match their flow-matching counterparts in all\nmetrics.\nTable 1. SiT [ 29] results on ImageNet-1k ( 256×256; a) and\n(512×512; b). We train all models for 400K iterations following\n[44]. All metrics are measured with the SDE Euler-Maruyama\nsampler with NFE=50 and without classifier guidance. We use\nλ= 0.05for all models trained with ∆FMand do not change any\nother hyperparameters. ↑indicates that higher values are better,\nwith↓denoting the opposite.\nfined by [ 44] exactly. Specifically, we use distill DiNOv2 [ 5]\nViT-B [ 13] features into the 4th layer of the SiT-B/2, and the\n8th layer of the SiT-XL/2, and mirror their hyperparameter\nsetup.\nResults. We report results in Table 2. Similar to Section 5.1,\n∆FMsubstantially improves REPA models by as much as\n6.81 FID, and consistently improves flow-matching with\nmodel scale. This highlights the versatility of the contrastive\nflow-matching objective as a broadly applicable criterion for\ndiffusion model.\n5.3. Extending to text-to-image generation\nImplementation Details. We train models with the pop-\nular MMDiT [ 14] architecture from scratch on the CC3M\ndataset [ 37] for 400K iterations. For faster training, we\npair each model with REPA, and follow the recommended\ntraining protocol of [44].\nResults. Table 3 shows our results. ∆FMimproves over the\nflow matching baseline by 5FID, highlighting its seamless\ntransferability to the broader text-to-image setting. We show\nqualitative results in Appendix A.\n5.4. CFG stacks with contrastive flow matching\nContrastive flow matching offers advantages of Classifier-\nFree Guidance (CFG), without incurring additional computa-Metrics\nModel FID ↓ IS↑ sFID↓Prec.↑Rec.↑\nREPA SiT-B/2 27.33 61.60 11.70 0.57 0.64\n+ Using ∆FM 20.52 69.71 5.47 0.61 0.63\nREPA SiT-XL/2 11.14 115.83 8.25 0.67 0.65\n+ Using ∆FM 7.29 129.89 4.93 0.71 0.64\n(a)ImageNet-1k (256x256) Results with REPA. Adding ∆FMto REPA\nfurther improves SiT models across nearly all metrics, and by as much as\n6.81 FID.\nMetrics\nModel FID ↓ IS↑ sFID↓ Prec.↑ Rec.↑\nREPA SiT-B/2 31.90 56.96 13.78 0.67 0.62\n+ Using ∆FM 24.48 64.74 5.89 0.71 0.61\nREPA SiT-XL/2 11.32 119.72 10.21 0.76 0.63\n+ Using ∆FM 7.64 131.50 4.72 0.79 0.62\n(b)ImageNet-1k (512x512) Results with REPA. ∆FMis robust with\nREPA at large image resolutions, further improving performance across\nestablished metrics.\nTable 2. REPA SiT [ 29] results on ImageNet-1k ( 256×256; a) and\n(512×512; b). All models are trained for 400K iterations strictly\nfollowing the procedure in [ 44], and set λ= 0.05. We use the SDE\nEuler-Maruyama sampler with NFE=50 without classifier guidance\nfor all our metrics.\nMetricREPA-MMDiT\nFlow-Matching ∆FM\nFID↓ 24 19\nTable 3. ∆FM improves on CC3M 256 ×256. We use the SDE\nEuler-Maruyama sampler with NFE=50 without classifier-free guid-\nance.\ntional costs during inference. In this section, we demonstrate\nthat when computational resources permit, combining ∆FM\nwith CFG can yield further performance enhancements.\nAccounting for conflicts. CFG and ∆FMencourage flow\nmatching model generations to be unique and identifiable, in\ndifferent ways. Specifically, ∆FMtrains models whose con-\nditional flows are steered away from other arbitrary flows in\nthe training data, regardless of generation state ( xt). In con-\ntrast, CFG steers generations away from the unconditional\nflow estimates based on xt. Thus, the signals from each\nmay not always be aligned and naively coupling them may\nlead to conflicts and suboptimal generations. Fortunately,\nwe can quantify the amount of steerage ∆FMapplies on\nflow matching models by deriving the closed-form solution\nto Eq. 4: minθL(∆FM)(θ) =h\n(min θL(FM)(θ))−λˆTi/[1−λ],\nwhere ˆTis simply the mean of all sample trajectories from\nthe training set (please see App. B.1 for the full deriva-\ntion). Thus, ∆FMyields models which estimate flows away\n6\n--- Page 7 ---\nFigure 4. Contrastive flow-matching ( ∆FM) denoises significantly more efficiently than flow-matching. We visualize the expected final\nimage estimated by a flow-model when denoised every 5 steps for trajectories of length 30 steps using the SDE Euler-Maruyama sampler\nand do not use classifier guidance. We compare the trajectories of a REPA SiT-XL/2 [ 44] trained on ImageNet-256 [ 10] for 400K steps with\nflow-matching (FM), and the same model trained with the contrastive flow-matching ( ∆FM) objective. We show these trajectories in sets of\npairs generated from the same noise sample during inference, with the flow-matching model above our ∆FM version.\n7\n--- Page 8 ---\nModelCFG Terms Metric\nw σ low σhigh IS↑ FID↓ sFID↓\nREPA SiT-XL/2 1.75 0.0 0.75 280.33 2.09 5.55\n+ Using ∆FM 1.85 0.0 0.65 281.95 1.97 4.49\nTable 4. ImageNet 256 ×256 Results with CFG and NFE=50. “w”\ndenotes the classifier-free guidance (CFG) weight, and [σlow, σhigh]\nis the time interval under which CFG is applied. We report\nthe best results for each model after conducting a grid search\noverw∈ {1.25,1.75,1.8,1.85,2.25},σlow= 0 andσhigh∈\n{0.50,0.65,0.75,1.0}.∆FM outperforms FM on all metrics.\nfrom the data-driven unconditional trajectory, weighted by\nλ. While optimizer and training dynamics cannot guaran-\ntee that all models trained with ∆FMexactly decompose\ninto these terms, ˆTnevertheless approximates its effect on\nthese models. With ˆT, we can account for conflicts be-\ntween ∆FMand CFG by modifying the CFG equation to:\nˆCFG = (1−λ) [wv(xt|y) + (1 −w)v(xt|∅)]+λτ, where w\nis the guidance scale, ∅is the unconditional term and λis the\nsame parameter used during ∆FMtraining (Appendix B.2\ncontains the full derivation). Note that, we only apply ˆCFG\nwithin the specified guidance interval [σlow, σhigh], and use\nourunchanged ∆FM model outside this interval.\nResults. Table 4 summarizes the results. When paired\nwith CFG, ∆FMimproves flow matching models across all\nmetrics, demonstrating its efficacy in settings where compu-\ntational costs are not a constraint.\nAdditional Couplings. While we find that our proposed\ncoupling strategy for ∆FM and CFG works well for our\nsetting, other suitable variations may also exist. For instance,\none may instead reduce conflicts by following the equation:\n˜CFG = (w+λ)v(xt|y)−(1−w)v(xt|∅)−λˆT, where\nλ, and ware free hyperparameters. We leave such explo-\nration to future work.\n5.5. Analyzing Contrastive Flow-Matching\nUnderstanding the ∆FM weight ( λ).λdirectly controls\nhow unique flows are across classes. Increasing λencour-\nages every diffusion step to be fully discriminative, enabling\nmodels to encode distinct representations that integral to gen-\nerating strong visual outputs at each trajectory step. How-\never, setting it too high can lead to overly-separated flow\ntrajectories, making it difficult to capture the class structure\n(Table 5.5). However, λvalues that are too low mirror the\nflow matching objective. Notably, we find that λ= 0.05\nis stable across all model and dataset settings, consistently\nachieving strong performance.\nEarlier class differentiation during denoising. In Figure 4,\nwe study flow trajectories of standard flow matching (FM)\nand flow matching with ∆FM. To do this, we take partially\ndenoised latents at various intermediate time steps along aMetric∆FMλValues\n0.0 0 .001 0 .01 0 .05 0 .1 0 .15\nIS↑115.83 115.70 119.41 129.89 116.27 82.20\nFID↓ 11.14 10.93 9.93 7.29 9.86 19.21\nTable 5. λ= 0.05is ideal. We show an ablation of the ∆FM\nweight parameter λ. A too large λproduces degenerate distribu-\ntions that do not model class structure well. Too low λis essen-\ntially identical to flow-matching, with very little effect on training.\nλ= 0.05is best and we use this for all our experiments.\nMetrics\nModel Batch Size FID ↓ IS↑ sFID↓\nREPA SiT-B/2 256 42.28 38.04 11.35\n+ Using ∆FM 256 33.39 43.44 5.67\nREPA SiT-B/2 512 24.45 69.15 11.42\n+ Using ∆FM 512 17.06 81.41 5.29\nREPA SiT-B/2 1024 22.00 76.15 11.76\n+ Using ∆FM 1024 15.23 88.53 5.20\nREPA SiT-XL/2 256 11.14 115.83 8.25\n+ Using ∆FM 256 7.29 129.89 4.93\nREPA SiT-XL/2 512 10.15 129.43 9.00\n+ Using ∆FM 512 6.36 146.17 5.42\nTable 6. ∆FM Scales with Batch Size. We train all models for\n400K iterations and strictly follow the protocol of [ 44]. All metrics\nare measured with the SDE Euler-Maruyama sampler with NFE=50\nand without classifier guidance. We use λ= 0.05for all models\ntrained with ∆FMand do not change any other hyperparameters. ↑\nindicates that higher values are better, with ↓denoting the opposite.\nImprovement using ∆FMevenly scales with batch-size, and even\noutperforms flow-matching models with half the batch-size.\ntrajectory with total length 30. While initially both follow\nsimilar trajectories, they quickly diverge within the first sev-\neral steps of the denoising process. For instance, the model\ntrained with ∆FMproduces more structurally coherent im-\nages earlier (around 15 to 20 steps in) than with FM. The\niconic features of each class, such as slanted bridge surfaces\n(Figure 4 (top-left)), animal eyes (Figure 4 (upper-left and\ntop-right), and train windows (Figure 4 (upper-right)), are\nmore clearly visible early on during the diffusion process of\nthe∆FMmodel. This enables ∆FMto ultimately generate\nhigher quality images at the final timestep.\nEffects of batch size on ∆FM.In Table 5.5, we study the ef-\nfects of batch size on our loss. It is well known that batch size\nhas an important effect on contrastive style losses [5, 7, 15]\nthat draw negatives within the batch. This can be under-\nstood as a sample diversity issue. If the batch size is larger\nthan negative samples within the batch are more representa-\ntive of the true distribution. In this table, we see a similar\n8\n--- Page 9 ---\nFID-50K10010050200250Total Denoising StepsFlow-MatchingContrastive Flow-Matching\n;100150\nTraining Iteration50K100K1M2M9x Faster200KFlow-MatchingContrastive Flow-MatchingFID-50K1050Figure 5. ∆FM requires significantly fewer training iterations\nand inference-time denoising steps. We plot FID-50k on Ima-\ngeNet 256x256 with different numbers of training iterations and\ndenoising steps. We see that ∆FMoutperforms the baseline with\n9×fewer training iterations and 5×reduction in the number of\ninference-time denoising steps, indicating that ∆FMis more effi-\ncient in both training and inference.\ntrend: larger batch sizes are important for maximizing the\nperformance of ∆FMacross several model scales. We also\nmaintain our improvements over the REPA baseline through\nall batch sizes and model scales.\nImproved training and inference speed. In Figure 5 (left),\nwe see the significant improvements in training speed from\nthe∆FMobjective. We reach the same performance (mea-\nsured by FID-50k) as baseline with 9×fewer training iter-\nations. In Figure 5 (right), we also demonstrate significant\nimprovements at inference time. With our objective, we\nreach superior performance with only 50 denoising steps\ncompared to the baseline with 250 denoising steps. This is a\nlinear 5 ×improvement in training efficiency. Taken together,\nthese results emphasize the important gains in computational\nefficiency achieved by our method.\n6. Conclusion\nWe introduced Contrastive Flow Matching ( ∆FM), a simple\naddition to the diffusion objective that enforces distinct,\ndiverse flows during image generation. Quantitatively, ∆FM\nresults in improved image quality with far fewer denoising\nsteps ( 5×faster) and significantly improved training speed\n(9×faster). Qualitatively, ∆FM improves the structural\ncoherence and global semantics for image generation.\nAll of this is achieved with negligible extra compute per\ntraining iteration. Finally, we show that our improvements\nstack with the recently proposed Representation Alignment\n(REPA) loss, allowing for strong gains in image generation\nperformance. Looking forward, ∆FMshows the possibility\nthat deviating from perfect distribution modeling in the\ndiffusion objective might result in better image generation.\nReferences\n[1]Michael S Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants. arXiv preprint\narXiv:2209.15571 , 2022. 3[2]Michael S Albergo, Nicholas M Boffi, and Eric Vanden-\nEijnden. Stochastic interpolants: A unifying framework for\nflows and diffusions. arXiv preprint arXiv:2303.08797 , 2023.\n3\n[3]Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In CVPR , 2023. 5\n[4]Gongze Cao, Yezhou Yang, Jie Lei, Cheng Jin, Yang Liu, and\nMingli Song. Tripletgan: Training generative model with\ntriplet loss, 2017. 3\n[5]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In ICCV ,\n2021. 2, 5, 6, 8\n[6]Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid Duvenaud. Neural ordinary differential equations. In\nAdvances in Neural Information Processing Systems , 2018. 3\n[7]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. ICLR , 2020. 8\n[8]Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L\nKlasky, and Jong Chul Ye. Diffusion posterior sam-\npling for general noisy inverse problems. arXiv preprint\narXiv:2209.14687 , 2022. 3\n[9]Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin\nNam, and Jong Chul Ye. Cfg++: Manifold-constrained clas-\nsifier free guidance for diffusion models. arXiv preprint\narXiv:2406.08070 , 2024. 3\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn2009 IEEE conference on computer vision and pattern\nrecognition , pages 248–255. Ieee, 2009. 5, 7\n[11] Alakh Desai and Nuno Vasconcelos. Improving image syn-\nthesis with diffusion-negative sampling, 2024. 2\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems , 34:8780–8794, 2021. 3, 5\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR , 2021. 6\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn,\nZion English, and Robin Rombach. Scaling rectified flow\ntransformers for high-resolution image synthesis. ICML , 2024.\n2, 6\n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual repre-\nsentation learning. CVPR , 2020. 8\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems , 30, 2017.\n5\n9\n--- Page 10 ---\n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598 , 2022. 3\n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance, 2022. 2\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Advances in Neural Information\nProcessing Systems , 2020. 3\n[20] Tero Karras, Miika Aittala, Tuomas Kynk ¨a¨anniemi, Jaakko\nLehtinen, Timo Aila, and Samuli Laine. Guiding a dif-\nfusion model with a bad version of itself. arXiv preprint\narXiv:2406.02507 , 2024. 2, 3\n[21] Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas De-\nmeester, and Luca Ambrogioni. Dynamic negative guidance\nof diffusion models: Towards immediate content removal. In\nNeurips Safe Generative AI Workshop 2024 . 3\n[22] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall\nmetric for assessing generative models. NeurIPS , 2019. 5\n[23] Tuomas Kynk ¨a¨anniemi, Miika Aittala, Tero Karras, Samuli\nLaine, Timo Aila, and Jaakko Lehtinen. Applying guidance\nin a limited interval improves sample and distribution quality\nin diffusion models. arXiv preprint arXiv:2404.07724 , 2024.\n3\n[24] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matthew Le. Flow matching for generative\nmodeling. In ICLR , 2023. 3\n[25] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximil-\nian Nickel, and Matthew Le. Flow matching for generative\nmodeling. In ICLR , 2023. 1, 3, 4\n[26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow, 2022. 3\n[27] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan\nLi, and Jun Zhu. Contrastive energy prediction for exact\nenergy-guided diffusion sampling in offline reinforcement\nlearning. In International Conference on Machine Learning ,\npages 22825–22855. PMLR, 2023. 3\n[28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao. Latent consistency models: Synthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378 , 2023. 3\n[29] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M.\nBoffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring\nflow and diffusion-based generative models with scalable\ninterpolant transformers. 2024. 1, 2, 3, 5, 6\n[30] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W\nBattaglia. Generating images with sparse representations.\narXiv preprint arXiv:2103.03841 , 2021. 5\n[31] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V .\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby,\nMido Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr\nBojanowski. DINOv2: Learning robust visual features with-\nout supervision. TMLR , 2024. 2[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684–10695, 2022. 5\n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge,\n2015. 4\n[34] Tim Salimans and Jonathan Ho. Progressive distillation\nfor fast sampling of diffusion models. arXiv preprint\narXiv:2202.00512 , 2022. 3\n[35] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems , 29, 2016. 5\n[36] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A unified embedding for face recognition and clus-\ntering. In 2015 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , page 815–823. IEEE, 2015. 3\n[37] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, image\nalt-text dataset for automatic image captioning. In Proceed-\nings of ACL , 2018. 2, 5, 6\n[38] Rahul Shenoy, Zhihong Pan, Kaushik Balakrishnan, Qisen\nCheng, Yongmoon Jeon, Heejune Yang, and Jaewon Kim.\nGradient-free classifier guidance for diffusion model sam-\npling. arXiv preprint arXiv:2411.15393 , 2024. 3\n[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In International Conference on\nLearning Representations , 2021. 3\n[40] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mar-\ndani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash\nVahdat. Loss-guided diffusion models for plug-and-play con-\ntrollable generation. In International Conference on Machine\nLearning , pages 32483–32498. PMLR, 2023. 3\n[41] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space, 2021. 3\n[42] Tianwei Yin, Micha ¨el Gharbi, Taesung Park, Richard Zhang,\nEli Shechtman, Fredo Durand, and William T Freeman. Im-\nproved distribution matching distillation for fast image syn-\nthesis. arXiv preprint arXiv:2405.14867 , 2024. 3\n[43] Tianwei Yin, Micha ¨el Gharbi, Richard Zhang, Eli Shecht-\nman, Fredo Durand, William T Freeman, and Taesung Park.\nOne-step diffusion with distribution matching distillation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 6613–6623, 2024. 3\n[44] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong,\nJonathan Huang, Jinwoo Shin, and Saining Xie. Representa-\ntion alignment for generation: Training diffusion transformers\nis easier than you think, 2024. 1, 2, 3, 5, 6, 7, 8\n[45] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Un-\npaired image-to-image translation via energy-guided stochas-\ntic differential equations. Advances in Neural Information\nProcessing Systems , 35:3609–3623, 2022. 3\n10\n--- Page 11 ---\n[46] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai\nHuang. Long and short guidance in score identity distilla-\ntion for one-step text-to-image generation. arXiv preprint\narXiv:2406.01561 , 2024. 3\n11\n--- Page 12 ---\nA. Text-to-Image Qualitative Results\nWe visualize generations between our REPA-MMDiT mod-\nels described in Section 5.3 trained with flow-matching (FM)\nloss and with ∆FMon CC3M with a batch size of 256 for\n400K iterations in Figure 6. We plot images in pairs, with\nFM images on the left and ∆FMimages on the right, and\nshow the respective caption for each pair above. All im-\nages are generated without classifier-free guidance and using\nNFE=50, and are the same images used in Table 3.\nB. Deriving Contrastive-Flow Matching Inter-\nference\nB.1. Closed-form solution to Eq. 4\nWe first re-introduce Eq. 4 for convenience,\nL(∆FM)(θ) = E\"\n||vθ(xt, t, y)−( ˙αtˆx+ ˙σtϵ)||2\n−λ||vθ(xt, t, y)−( ˙αt˜x+ ˙σt˜ϵ)||2#\nMinimizing the expectation, expanding all norms and letting\nv(θ) =v(xt, t, y), we can simplify the expectation to:\n= min\nθE\n(1−λ)v(θ)Tv(θ)\n−2v(θ)T[( ˙αtˆx+ ˙σtϵ)−λ( ˙αt˜x+ ˙σt˜ϵ)]\n+( ˙αtˆx+ ˙σtϵ)T( ˙αtˆx+ ˙σtϵ)\n−λ( ˙αt˜x+ ˙σt˜ϵ)T( ˙αt˜x+ ˙σt˜ϵ)\n(7)\n= min\nθE\"\n(1−λ)v(θ)Tv(θ)\n−2v(θ)T[( ˙αtˆx+ ˙σtϵ)−λ( ˙αt˜x+ ˙σt˜ϵ)]#\n(8)\n∝∼min\nθE\n\f\f\f\f\f\f\f\f\f\f\f\f\f\f√\n1−λv(θ)\n−( ˙αtˆx+ ˙σtϵ)−λ( ˙αt˜x+ ˙σt˜ϵ)√\n1−λ\f\f\f\f\f\f\f\f\f\f\f\f\f\f2\n2\n (9)\nSetting the gradient with respect to v(θ)to0,\n√\n1−λv(θ)∗= E\u0014( ˙αtˆx+ ˙σtϵ)−λ( ˙αt˜x+ ˙σt˜ϵ)√\n1−λ\u0015\n(10)\nv(θ)∗=E [ ˙αtˆx+ ˙σtϵ]−λE [ ˙αt˜x+ ˙σt˜ϵ]\n1−λ(11)\nFinally, observe that E [ ˙αtˆx+ ˙σtϵ]is the solution to the\nflow-matching objective. Setting E [ ˙αt˜x+ ˙σt˜ϵ] =ˆTand\nobserving that xtdoes not depend on ˆxorˆϵwe obtain:\nmin\nθL(∆FM)(θ) =minθL(FM)(θ)−λˆT\n1−λ(12)\nB.2. Coupling with CFG\nClassifier-free guidance (CFG) is originally defined over the\nflow-matching solution of minθL(FM). Re-writing Eq. 12and substituting it into the CFG equation, we obtain:\nCFG =wv(FM)(xt, t, y) + (1 −w)v(FM)(xt, t,∅)(13)\n=\nwh\n(1−λ)v(∆FM)(xt, t, y) +λˆTi\n−(1−w)h\n(1−λ)v(∆FM)(xt, t,∅) +λˆTi\n (14)\n=\"\n(1−λ)\"\nwv(∆FM)(xt, t, y)\n+(1−w)v(∆FM)(xt, t,∅)#\n+λˆT#\n(15)\nLetting v(xt|y) = v(∆FM)(xt, t, y)andv(xt|∅) =\nv(∆FM)(xt, t,∅), we obtain the Eq. from Section 5.4:\nˆCFG = (1−λ) [wv(xt|y) + (1 −w)v(xt|∅)] +λˆT.\n12\n--- Page 13 ---\nFigure 6. CC3M side-by-side generations between a REPA-MMDiT model trained with flow-matching (left) and ∆FM (right).\nModels are trained for 400K iterations using a batch-size of 256 and images are generated without classifier-free guidance and using\nNFE=50.\n13",
  "text_length": 47150
}