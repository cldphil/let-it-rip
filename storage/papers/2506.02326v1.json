{
  "id": "http://arxiv.org/abs/2506.02326v1",
  "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target",
  "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.",
  "authors": [
    "Berk Atil",
    "Namrata Sureddy",
    "Rebecca J. Passonneau"
  ],
  "published": "2025-06-02T23:48:16Z",
  "updated": "2025-06-02T23:48:16Z",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.02326v1",
  "full_text": "--- Page 1 ---\narXiv:2506.02326v1  [cs.CL]  2 Jun 2025♪Something Just Like TRuST ♪*: Toxicity Recognition of Span and Target\nDisclaimer: Due to the topic studied here, the paper contains offensive words.\nBerk Atil Namrata Sureddy Rebecca J. Passonneau\nPenn State University\n{bka5352,nqs5685,rjp49}@psu.edu\nAbstract\nToxicity in online content, including content\ngenerated by language models, has become a\ncritical concern due to its potential for negative\npsychological and social impact. This paper\nintroduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that\nmerges existing datasets, and has labels for tox-\nicity, target social group, and toxic spans. It\nincludes a diverse range of target groups such\nas ethnicity, gender, religion, disability, and\npolitics, with both human/machine-annotated\nand human/machine-generated data. We bench-\nmark state-of-the-art large language models\n(LLMs) on toxicity detection, target group iden-\ntification, and toxic span extraction. We find\nthat fine-tuned models consistently outperform\nzero-shot and few-shot prompting, though per-\nformance remains low for certain social groups.\nFurther, reasoning capabilities do not signif-\nicantly improve performance, indicating that\nLLMs have weak social reasoning skills.\n1 Introduction\nOffensive and toxic content is pervasive in social\nmedia and online forums. Because large language\nmodels (LLMs) are trained on online data, they\nlearn to generate toxic output (Gehman et al., 2020;\nHartvigsen et al., 2022). Exposure to toxic con-\ntent might lead to lack of empathy, prejudice (Pluta\net al., 2023), stress, and depression (Saha et al.,\n2019). Social groups that might need special at-\ntention such as people with developmental disor-\nders can be more vulnerable to toxic content (Kim\net al., 2023). Furthermore, LLMs are being used\nin high-stakes conversational applications such as\neducation (Yu et al., 2025; de Araujo et al., 2025)\nand healthcare (Yeo et al., 2024, 2025). Exposure\nto offensive language can lead to issues with self-\nesteem, anger, and anxiety (Kansok-Dusche et al.,\n2023). Within the NLP community there has been\n*An allusion to Coldplay’s “Something Just Like This.”\nFigure 1: Histogram of distinct tokens within toxic spans\n(x-axis) ordered by their total count (y-axis). Note that the\nfigure has a vertical \"tear\" at around 150 on the x-axis to show\ncontinuation out beyond 500 distinct tokens, but with counts\ntoo low to be visible. The most common 25 words are shown\ndescending on a diagonal.\nincreasing interest in toxicity detection (Davidson\net al., 2017; Rawat et al., 2024; Duan et al., 2025;\nKhurana et al., 2025) and toxicity mitigation (Suau\net al., 2024; Ermis et al., 2024; Pozzobon et al.,\n2023a; Li et al., 2024; Liu et al., 2021). However,\nexisting toxicity datasets differ in format, leading\nto lack of uniformity in benchmarking. We build\non existing datasets to create TRuST to improve\nbenchmarking, and to provide a more comprehen-\nsive picture of online use of toxic language.\nExisting toxicity datasets for English differ in\nsize, source, and annotation scheme. They can\nrange from about 5K examples (Zampieri et al.,\n2023) to four times that size (Davidson et al., 2017;\nMathew et al., 2021) for human-generated posts\nfrom social media (typically twitter), or an or-\nder of magnitude larger for AI-generated datasets\n(Hartvigsen et al., 2022). Davidson et al. (2017)\nintroduced a toxicity scheme for hate speech, of-\nfensive language, or neither that has been widely\n1\n--- Page 2 ---\nadopted (Almohaimeed et al., 2023; Mathew et al.,\n2021), although binary labels for offensiveness\nhave also been used and combined with hierarchi-\ncal schemes for the target of offense (Zampieri\net al., 2019). Categorizations of target include\nhigher level classes for individual, group or other\n(Zampieri et al., 2019), as well as very fine-grained\nlabels, thus the THOS dataset (Almohaimeed et al.,\n2023) has 31 targets (called topics). Some datasets\nalso include identification of the spans of words\nthat support identification of toxicity or offensive-\nness within a post or comment (Zampieri et al.,\n2023; Mathew et al., 2021; Pavlopoulos et al.,\n2022). We review existing datasets and their anno-\ntation schemes to arrive at a synthesis that includes\ntoxicity, target and span. In particular, we define\na closed set of social group targets that includes\nhalf a dozen higher level classes and over 20 fine-\ngrained subgroups. Our efforts produce TRuST, a\nlarge scale dataset of over 300K examples, created\nby merging many existing datasets that represent di-\nverse social media sources as well as AI-generated\nexamples. We also train human annotators to label\na subset of 11,500 examples, reaching interannota-\ntor agreement scores in line with previous work.\nTo investigate the benefits of TRuST, we carry\nout three activities: an analysis of the characteris-\ntics of toxic language in social media (cf. Fig. 1);\nbenchmark tests of LLMs on toxicity detection; au-\ntomatic annotation of all the TRuST examples apart\nfrom the manually annotated subset, using the best\nmethod from our benchmarking tests. We bench-\nmark LLMs using zero-shot, few-shot, and Chain-\nof-Thought prompts and compare LLMs to pre-\ntrained language models (PLM) that we fine-tune.\nWe find that fine-tuned PLMs perform somewhat\nbetter than LLMs. We also find that performance\nremains low on certain social groups, suggesting\nthat social reasoning in LLMs needs improvement.\nOur contributions are: (1) TRuST, a comprehen-\nsive dataset with toxicity, target social group, and\ntoxic span labels; (2) a comprehensive definition of\ntoxicity based on a synthesis of previous work; (3)\nbenchmarking of SOTA LLMs, and their compari-\nson with previous SOTA models.\n2 Related Work\n2.1 Definitions of Toxicity\nSince the perception of toxicity depends on factors\nsuch as context, background, demographics, it is\nsubjective. This has led to work that provides par-allel annotations of toxicity from different demo-\ngraphic groups (Mostafazadeh Davani et al., 2024).\nFurther, types of toxicity annotation have ranged\nfrom narrower categories such as hate speech to\nbroader types of offense.\nDavidson et al. (2017) define hate speech HS as\nhatred towards a social group or humiliation/insult\nto the members of a group. Additionally, HS might\ninclude threats or reference to violence. Nockleby\n(2000) defines HS as any communication that dis-\nparages a person or a group on the basis of a social\ncharacteristic such as race, color, ethnicity, gender,\nsexual orientation, nationality, religion, or other\ncharacteristic. These works generally agree that\nHS is directed at a target.\nOther types of toxic content go beyond hate\nspeech. Dorn et al. (2024) state that toxic con-\ntent includes using slurs in a pejorative way, at-\ntacking or criticizing a minority, promoting hate\nspeech, negative stereotypes, etc. The definition\nused in PerspectiveAPI (Lees et al., 2022) for toxic\ncontent is “rude, disrespectful or unreasonable lan-\nguage that is likely to make someone leave a dis-\ncussion”. Kumar et al. (2021) propose that toxicity\ninvolves identity-based attacks such as racism, bul-\nlying, threats of violence, sexual harassment, and\nincludes emotional harm and hate speech.\nOne issue in the literature about toxicity is that\nsome terms are used interchangeably, or one of\nthem is seen as a subcategory of the other. There-\nfore, there is a lack of consensus which leads to\nconfusion and inconsistency (Khurana et al., 2022).\nMoreover, some works consider every content with\nslurs or swear words as toxic, which may not be\nthe case (e.g. you are f***ing beautiful).\nWe take toxicity to comprise three categories:\nhate speech, abusive language, and sexual harass-\nment. Hate speech is defined as offensive and\ndiscriminatory discourse towards a group or an indi-\nvidual based on characteristics such as race or reli-\ngion, thus always has a target. It includes negative\nstereotyping (negative traits and characteristics at-\ntributed to a group), racism (discrimination against\nor negative attitudes towards individuals/groups be-\nlonging to a certain race), sexist language (discrim-\nination or behaviors that foster stereotypes based\non a gender), and discrimination based on sexual\norientation. Abusive language is content with in-\nappropriate words such as profanity or disrespect-\nful terms for people based on sociodemographic\ncharacteristics. It includes psychological threats\n(expressions of an intent of harms such as humilia-\n2\n--- Page 3 ---\ntion, causing distress, constant criticism). Our last\ncategory is sexual harrasment which includes un-\nwelcome sexual moves, requests of sexual favors,\nor other unwanted physical/verbal behaviors of a\nsexual nature towards someone. In our work, toxic\nlanguage often has a target, but can also involve use\nof offensive words in an aggressive fashion without\ntargeting a specific social group, e.g., “honestly?\nI can handle kpop stans dragging armys but just\nstay the f**k away from bts they’ve done lit rally\nnothing to y’all.” Our annotation instructions (see\nAppendix C) include a binary label for toxicity that\ncovers these three categories.\n2.2 Datasets\nDavidson et al. (2017) produced one of the earliest\ndatasets focusing on hate speech and offensive-\nness in social media. Their dataset has around 25k\ntweets with three-way labels for hate speech, offen-\nsive language or neither, assigned by crowdworkers.\nOLID (Zampieri et al., 2019) has 14k tweets and\nhierarchical offensiveness labels. The first level\nis offensive or not. Offensive examples are then\nlabeled as targeting a group or not. The last level\nis for the targeted offensive tweets for whether the\ntarget is an individual, social group, or any other\ncategory such as an organization.\nInclusion of annotations for target social group\nsupports deeper understanding of which are the\ngroups that are targeted most often, and other bias-\nrelated issues. TBO (Zampieri et al., 2023) is a\nrecent dataset that has target and toxic span an-\nnotations, both of which support explainability.\nHowever, their target annotation involves identi-\nfying specific words in toxic language samples,\nrather than more general categories, which limits\ntheir utility. The THOS hate speech dataset (Almo-\nhaimeed et al., 2023) has two-level categories for\ntarget: the first is more high level such as country or\nreligion (6 classes), and the second is more detailed\nbelonging to the first level categories (31 classes)\nsuch as a term for a country (China), practitioner of\na religion (Muslim), or racial group (Black). Simi-\nlar to THOS, we employ two levels with 8 higher\nlevels (including No Target), 5 of which are broken\ndown into two or more subclasses.\nAnother annotation type that supports explain-\nability is the span of words that constituted the of-\nfense, as in ToxicSpans (Pavlopoulos et al., 2022).\nHateXplain (Mathew et al., 2021) has target group\nannotations based on religion, gender etc. and ra-\ntionales that are the spans on which annotators’labeling decision is based. The rational annota-\ntion can contain words for target group. Our set\nof targets is more diverse in the number of target\ngroups.\nThe cited works mostly focus on explicit hate\nspeech; implicit hate speech is harder to detect.\nElSherief et al. (2021) propose a new dataset with\n19k tweets with an implicit versions of hate and\ntoxicity (e.g., irony). Social Bias Frames (Sap et al.,\n2020) creates a formalism that models pragmatic\nframes. They have annotations for targeted group,\nimplied statement, offensiveness, use of in-group\nlanguage etc. Alongside datasets with social me-\ndia texts, some works create data using genera-\ntive language models, such as ToxiGen (Hartvigsen\net al., 2022). The latter focuses on implicit hate\nspeech; they generate both toxic and benign exam-\nples for 13 minority groups using GPT-3 (Brown\net al., 2020). Their dataset has 274k examples and\nlabels are based on the prompts used for GPT-3\nwith the assumption that if the prompt is toxic, the\ngenerated text is also toxic. FairPrism (Fleisig et al.,\n2023b) is another machine-generated dataset that\nbuilds on ToxiGen and Social Bias Frames. They\ncollect human annotations on stereotyping and de-\nmeaning harms, target social group, and providing\nnon-aggregated annotations to support fairness.\nSome argue that the context, author, and reader\nare important factors for perception of toxic lan-\nguage (Cowan and Hodge, 1996; Nieto and Boyer,\n2006). This motivates Zhou et al. (2023) to use\nGPT3.5 to build CobraFrames, with contextual an-\nnotations for speaker and listener, target group, and\nemotional reactions or power dynamics between\nthe speaker and listener.\n2.3 Prediction of Toxicity, Targets and Spans\nThe SOTA for toxicity detection is PerspectiveAPI\n(Lees et al., 2022), in which multilingual BERT\n(Devlin et al., 2019) is first trained, then multiple\nsingle-language convolutional neural networks are\ndistilled. However, it has been criticized for hav-\ning only moderate correlation with humans (Welbl\net al., 2021; Schick et al., 2021), and over-reliance\non surface patterns such as swear words, resulting\nin a high rate of false positives (Rosenblatt et al.,\n2022). Another issue is that they sometimes release\nupdates without notification or explanation of im-\nprovements, making previous findings questionable\n(Pozzobon et al., 2023b).\nA common approach for target social group is\na multi-layer perceptron classifier whose input is\n3\n--- Page 4 ---\nTarget Count (%) Toxic % T. Count (%) T. Toxic %\nNo target 4121 ( 35.96) 38.26 358 (36.46) 37.99\nEthnicity 2050 ( 17.78) 55.10 170 (17.31) 51.76\nblack 723 ( 6.24) 74 .90 64 (6.52) 70.31\nwhite 278 ( 2.45) 46.43 21 (2.14) 38.10\nasian 272 ( 2.34) 47.80 23 (2.34) 43.48\nnative 169 ( 1.51) 32 .63 16 (1.63) 18 .75\nchinese 157 ( 1.32) 37.95 8 (0.81) 37.50\no. ethnicity 129 ( 1.13) 43.66 11 (1.12) 72 .73\nmexican 114 ( 0.97) 38.52 9 (0.92) 44.44\narab 105 ( 0.90) 65.49 7 (0.71) 57.14\nlatino 103 ( 0.93) 45.30 11 (1.12) 27.27\nPolitics 1281 ( 11.05) 63.12 103 (10.49) 72.82\nGender 1152 ( 9.92) 49.56 87 (8.86) 55.17\nlgbtq+ 521 ( 4.50) 50.00 38 (3.87) 55.26\nwoman 492 ( 4.24) 50 .75 38 (3.87) 57 .89\nman 121 ( 1.02) 48.44 9 (0.92) 44 .44\no. gender 18 ( 0.17) 14 .29 2 (0.20) 50.00\nReligion 1112 ( 9.77) 58.62 99 (10.08) 53.53\nmuslim 528 ( 4.58) 55.11 41 (4.18) 41 .46\njewish 474 ( 4.21) 66 .60 49 (4.99) 65 .31\no. religion 110 ( 0.98) 40 .65 9 (0.92) 44.44\nOther 825 ( 7.22) 51.49 78 (7.94) 52.56\nother 466 ( 4.07) 54 .97 44 (4.48) 56 .82\nrefugee 188 ( 1.66) 41 .15 17 (1.73) 41 .18\nmiddle east 171 ( 1.49) 53.48 17 (1.73) 52.94\nCountry 545 ( 4.73) 29.60 50 (5.09) 26.00\no. country 357 ( 3.11) 30 .10 34 (3.46) 32 .35\nUS 188 ( 1.61) 28 .57 16 (1.63) 12 .50\nDisability 412 ( 3.57) 30.22 37 (3.77) 29.73\nTotal 11498 47.89 982 47.35\nTable 1: Statistics for our human annotated data showing\nthe total count (and percentage of the total) for each higher\nlevel or lower-level social group, and the percentage of each\nthat are labeled toxic. Lower-level groups with the highest\nand lowest proportion of toxic texts are in red and green font,\nrespectively. The last two columns are for the test set (T.). In\ntargets, o. means other, native means native american.\nembeddings of examples from a pretrained encoder,\nsuch as BERT (Mathew et al., 2021). LLMs have\nalso been used (Zhou et al., 2023).\nToxic span prediction is usually treated as a\nmulti-token classification problem where a label is\npredicted for each token (He et al., 2024). Span-\nBERT (Joshi et al., 2020), a pretrained model for\nhigh quality embeddings for predicting spans of\ntext, is the SOTA encoder for this task. While there\nis little work on using LLMs for toxic span ex-\ntraction, it has been done for Romanian toxic span\nidentification using GPT4 (Paraschiv et al., 2023).\n3 Dataset\nTRuST re-annotates data from ToxicSpan (CC-BY\n4.0) (Pavlopoulos et al., 2022), HateXplain (CC-\nBY 4.0)(Mathew et al., 2021), TBO (CC-BY 4.0)\n(Zampieri et al., 2023), Thos (CC-BY 4.0) (Almo-\nhaimeed et al., 2023), and ToxiGen (CC-BY 4.0)\n(Hartvigsen et al., 2022). We observed some noise\nin the annotations, which is inevitable due to the\nsubjective nature of toxicity. The ToxiGen toxicity\nlabels, however, were particularly noisy, due to the\nstrong assumption of assigning the prompt labels to\nthe generated text. Where necessary, we added an-notations for target group and span. TRuST there-\nfore has three types of labels, toxicity, target group\nand span, throughout. The full dataset has nearly\n300K examples (see Table 6).\nTo provide a high-quality benchmark for testing\ntoxicity models, we collected human annotations\nfor binary toxicity, target social group, and toxic\nspans for a subset of over 10k examples. We have\n24 target social groups, including “no target”. Note\nthat we separate “Chinese” from Asian because it\nincludes a significantly large number of examples.\nAfter testing different models on prediction of the\nthree types of labels, we annotate the rest of the\ndata with the best-performing models.\n3.1 Human Annotation Procedure\nTo collect the manual annotations, we hired six un-\ndergraduate students studying computer science or\ndata science who have some experience with data\nanalysis; they were paid $10/hour. We recruited stu-\ndents from different backgrounds (Indian, Chinese,\nWhite) and demographics to have more diversity,\ngiven the cultural and demographic differences in\ntoxicity (Mostafazadeh Davani et al., 2024; Fleisig\net al., 2023a). They were provided with detailed in-\nstructions (see Appendix C). To ensure good qual-\nity, we did three iterations of annotator training.\nWe should note that by the time we collected toxic\nspan annotation, which was done last, only three\nof the original six annotators were still available.\nOur inter-annotator agreement scores using Krip-\npendorf’s α(Krippendorff, 2013) are 0.56, 0.66,\nand 0.55 for toxicity, target social group and spans,\nrespectively. Agreement scores in previous work\n(Krippendorff’s αunless otherwise noted) are, for\ntoxicity: 0.46 (Mathew et al., 2021), 0.51 (Sap\net al., 2020) and 0.64 (Hartvigsen et al., 2022);\nfor target social group: 0.50 (Sap et al., 2020);\nfor spans: 0.55 Cohen’s kappa (Pavlopoulos et al.,\n2022)). To calculate agreement for spans, we incor-\nporate MASI (Passonneau, 2006), a distance metric\nfor inter-annotator agreement on sets, based on a\nweighted Jaccard, where the weight is higher for\nset subsumption than for set intersection, which in\nturn is higher than for disjunction.\nAfter annotators were trained, they first anno-\ntated target social group for every example (includ-\ning no target), followed by toxicity. During the\ntoxicity annotation, they also double-checked the\npreviously labeled target social group. The exam-\nples where a previous target social group annota-\ntion was considered incorrect were labeled by all\n4\n--- Page 5 ---\nstudents, where we applied majority voting to get\nthe final label. Span annotation came last, during\nwhich students did a sanity check on the toxicity\nlabel, where those judged incorrect were again re-\nlabeled by the three remaining annotators, with\nmajority voting for the final label. Notably, span\nannotation identified either individual words that\nled to the judgment that the example was toxic, or\napplied to the entire sentence.\n3.2 Toxicity and Target Group Labels\nTable 1 shows the total count for each higher- or\nlower-level target group in the human-annotated\nsubset, along with the proportion of these that are\nlabeled toxic. Most target groups are ethnicity-\nbased; the least frequent target group is for \"disabil-\nity.\" In general, we have almost a balanced dataset\nfor toxic versus non-toxic content (47.37% of the\nexamples are toxic), but the rate varies greatly\nwithin each group; for example, 75% of the ex-\namples for the social group \"black\" are toxic, while\nonly 33% are for \"native american.\"\n3.3 Span Labels\nThe total number of examples with annotation of\nthe toxic span is 5,506 (47.89% of 11,498, per Ta-\nble 1), where some examples have multiple spans.\nTo calculate descriptive statistics on span tokens,\nwe first apply stemming. We merge the strings\n“ni**a” and “ni**er”. The mean length of spans is\n1.91 words, the median is 1, and the maximum is\n11. In 33% of cases with span annotation, the span\nconstituted the entire sentence. There are 7065\nspans in total, among which 1334 are unique. The\nhistogram in Figure 1 shows the span tokens occur-\nring more than once is highly skewed. We show\nthe 25 most common span tokens, with “ni**er”,\n“f**k”, “stupid”, “idiot”, and “b**ch” at the top.\nSome words are specific to particular groups such\nas “black” or “kike” but others such as “kill” or\n“stupid” are used more generally.\n4 Experiments\nOur experiments utilize the human-annotated data\nof 11,498 examples divided into validation, training\nand test as follows. The validation set is 495 exam-\nples, with randum selection plus enforcing a mini-\nmum of 5 examples per target group. The randomly\nselected test set has 982 examples. We compare per-\nformance of multiple baselines on the three tasks\nof detection of toxicity, target social group, andModel Accuracy Precision Recall F1\nPerspectiveAPI 0.50 0.48 0.77 0.59\nRoBERTa 0.79 0.76 0.82 0.79\nBERT 0.79 0.77 0.79 0.78\nGPT4o 0.75 0.70 0.85 0.77\nSonnet 0.77 0.72 0.83 0.77\nLlama70b 0.77 0.71 0.86 0.78\nLlama8b 0.73 0.66 0.88 0.76\nReasoning and CoT\nD. Llama70b 0.75 0.69 0.84 0.76\nD. Llama8b 0.70 0.63 0.90 0.74\no4-mini 0.78 0.71 0.90 0.79\nGPT4o-cot 0.74 0.73 0.71 0.72\nLlama8b-cot 0.73 0.67 0.84 0.75\nLlama70b-cot 0.75 0.72 0.75 0.74\nSonnet-cot 0.73 0.73 0.67 0.70\nTable 2: Toxicity detection results. D. models are R1 distilled\nversions.\nspan detection for toxic examples. We first com-\npare PLMs versus zero-shot LLMs on each task in\nturn, including a custom model in the case of toxic-\nity detection, followed by a subsection on whether\nprior knowledge of target social group improves\ntoxicity detection. Then we compare these results\nto the use of reasoning models and in-context learn-\ning. We report accuracy, precision, recall and F1\nfor the first two tasks, but omit accuracy for toxic\nspan detection, as the exact matching needed for\naccuracy is less informative than the breakdown of\nF1 into recall and precision on toxic span words.\nThe final subsection presents an error-analysis.\nPLM baselines for toxicity and target so-\ncial group use BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) with linear classifier\nlayers, and for span prediction we use SpanBERT\nJoshi et al. (2020) (see appendix B). We include\nfour LLMs: GPT4o (OpenAI et al., 2024), Claude\n3.7 Sonnet (Anthropic, 2025), and Llama3.1 (70b\nand 8b) (Grattafiori et al., 2024) (see Appendix E\nfor the prompt). We use temperature=0, set the\nseed, and use the default values for others for de-\nterminism, although it is not guaranteed (Atil et al.,\n2025). For toxicity, we also include PerspectiveAPI\n(Lees et al., 2022), a neural network that provides\na probability that an example is toxic. We used the\nvalidation set to identify the best probability for the\nbinary class cutoff, which was 0.20.\n4.1 Toxicity\nThe toxicity results in the top of Table 2 show that\nfine-tuned PLMs perform slightly better than the\nLLMs. Except for Llama8b, LLMs perform sim-\nilarly. Surprisingly, PerspectiveAPI’s accuracy is\nrandom. All models have higher recall which is\n5\n--- Page 6 ---\nModel Accuracy Precision Recall F1\nBERT 0.76 0.68 0.82 0.72\nRoBERTa 0.73 0.63 0.81 0.70\nGPT4o 0.75 0.67 0.78 0.70\nSonnet 0.74 0.62 0.75 0.67\nLlama70b 0.65 0.55 0.68 0.58\nLlama8b 0.48 0.15 0.15 0.15\nReasoning and CoT\no4-mini 0.72 0.58 0.63 0.59\nD. Llama70b 0.69 0.24 0.27 0.25\nD. Llama8b 0.61 0.17 0.17 0.17\nGPT4o-cot 0.75 0.68 0.75 0.70\nLlama8b-cot 0.40 0.07 0.06 0.06\nLlama70b-cot 0.68 0.50 0.58 0.53\nSonnet-cot 0.75 0.62 0.70 0.65\nTable 3: Target group prediction results.\nModel Accuracy Precision Recall F1\nBERT 0.80 0.77 0.82 0.79\nRoBERTa 0.78 0.74 0.81 0.77\nGPT4o 0.66 0.64 0.65 0.64\nSonnet 0.71 0.65 0.70 0.66\nLlama70b 0.57 0.46 0.48 0.44\nLlama8b 0.49 0.07 0.07 0.07\nReasoning and CoT\no4-mini 0.67 0.45 0.47 0.45\nD. Llama70b 0.64 0.17 0.18 0.17\nD. Llama8b 0.56 0.07 0.07 0.07\nGPT4o-cot 0.68 0.56 0.61 0.58\nLlama8b-cot 0.44 0.05 0.05 0.05\nLlama70b-cot 0.57 0.31 0.31 0.31\nSonnet-cot 0.70 0.55 0.60 0.57\nTable 4: Higher level target results.\npreferable here, where false negatives are worse\nthan false positives.\n4.2 Target Social Group\nSimilar to Zampieri et al. (2023), for two baselines\nwe train a neural network with a linear classifier\nlayer on top of BERT or RoBERTa encoder. As\nabove, we also test the same SOTA LLMs (cf. ap-\npendices F and G for the prompts).\nSimilar to toxicity detection results, fine-tuning\na model slightly outperforms LLMs, as seen for\nthe fine-grained target results in Table 3. However,\nLlama models do not perform as well as GPT4o\nand Sonnet. The F1 for Llama8b is especially\nlow, showing an inability to understand social tar-\ngets of toxicity. Table 4 shows the results for the\nhigher-level target groups. Interestingly, LLMs per-\nform worse at this task than the fine-grained target\ngroups (9% decrease for GPT4o, 3% decrease for\nSonnet etc.). Confusion matrices for GPT4o and\nSonnet (the two top-performing LLMs), show that\nGPT4o mixes “other” and “ethnicity” predictions\nwith “no target”, and Sonnet mixes “no target” with\n“ethnicity”. Additionally, both mix “ethnicity” withModel Precision Recall F1\nSpanBERT 0.72 0.71 0.70\nGPT4o 0.55 0.79 0.65\nSonnet 0.66 0.45 0.53\nLlama70b 0.66 0.22 0.33\nLama8b 0.48 0.48 0.48\nReasoning and Cot\no4-mini 0.63 0.40 0.49\nD. Llama70b 0.45 0.87 0.59\nD. Llama8b 0.43 0.58 0.4\nSonnet-cot 0.63 0.5 0.56\nGPT4o-cot 0.55 0.63 0.59\nLlama70b-cot 0.68 0.46 0.54\nLlama8b-cot 0.68 0.16 0.26\nTable 5: Toxic span prediction results.\n“other”. This indicates “other” as a high level target\nis more confusing than the fine-grained targets.\n4.3 Toxic Span\nTable 5 shows that SpanBERT outperforms the\nLLMs. GPT4o’s F1 approaches SpanBERT’s, but\nthe other LLMs do much worse.\n4.4 Target Group and Toxicity Detection\nWe conduct experiments to assess whether prior\nknowledge about target social group could improve\ntoxicity detection. For the RoBERTa/BERT-based\nmodels, we either add this information at the text\nlevel as “The target social group is <social group>”\nor at the embedding level. For LLMs, we use two\ntypes of prompting. In the first, we assign the social\ngroup persona to the model; combining persona\nwith self-correction has been effective (Xu et al.,\n2024). In the second, we include the social group\nin the text of the prompt. We find no differences\nbetween the methods (cf. Table 7 of the Appendix).\n4.5 Reasoning Models\nWe also experiment with chain-of-thought (CoT)\n(Wei et al., 2023) and the reasoning models o4-mini\nand R1 Distilled Llama70b/8b . Although reason-\ning helps in science or logic (Jaech et al., 2024;\nDeepSeek-AI, 2025; Zhang et al., 2024; Wei et al.,\n2023), it does not improve detection of toxicity or\ntarget group, and gives mixed results on span pre-\ndiction. CoT helps Llama70b and Sonnet, but not\nLlama8b and Gpt4o. Reasoning usually increases\nrecall, which indicates that models predict more\ntokens less precisely.\n4.6 In-Context Learning\nFew-shot learning improves LLM performance in\nmany tasks, such as question answering (Brown\n6\n--- Page 7 ---\n(a) Toxicity few-shot results.\n (b) Target group few-shot results.\n(c) Higher Target Group Few-Shot Results.\n (d) Toxic Span Few-Shot Results.\nFigure 2: Few-Shot Comparison Figures\net al., 2020). There are many strategies for select-\ning few-shot examples. Previous work suggested\nthat similarity to the example in question is effec-\ntive (Paraschiv et al., 2023; Liu et al., 2022; Zebaze\net al., 2024) was borne out in our early experi-\nments. We use Linq-Embed-Mistral (Kim et al.,\n2024) to create embeddings, and retrieve similar\nexamples from our training data. Figure 2 shows\nresults for few-shot up to six, for 4 non-reasoning\nmodels. For toxicity detection, examples help only\nfor GPT4o, and requires at least three examples to\nsurpass zero-shot. For toxic span prediction, one\nexample is enough to increase the performance for\nLlama models, whereas Sonnet requires at least\ntwo examples. For target group prediction, few-\nshot learning helps all models, and the improve-\nments are more dramatic, e.g., from 0.48 to 0.64\nfor Llama8b with one example. However, apart\nfrom prediction of target group, LLMs still do not\nsurpass the performance of fine-tuned PLMs.\n4.7 Error Analysis\nThe best results reported above (accuracy; F1) are\nfrom the RoBERTa classifier for toxicity (0.79;0.79), the BERT classifier for fine-grained target\n(0.76; 0.72) and course-grained target (0.80; 0.79);\nSpanBERT for toxic spans (F1 of 0.70). In general,\nLLMs do better at toxicity detection than they do\non the other two tasks, and reasoning/CoT helps\nvery little. Here we look into differences across so-\ncial groups for toxicity and target group prediction,\nwhether span detection is easier when the whole\nsentence is the toxic span, and performance differ-\nences on machine-generated text.\nDifferences across social groups. For the er-\nror analysis, we broke down the accuracy results\ninto accuracy per target social group for toxicity\nand target group detection. The Figure 3 accuracy\nheatmap for toxicity detection shows wide variation\nacross target groups, ranging from as low as 41%\nup to 100%. Apart from the categories for \"other\ngender\" and \"other ethnicity,\" where most models\nhave very high performance, all models show non-\nuniform performance across groups. In some cases,\npairs of models correlate in their performance (e.g.\nSpearman correlations are 0.89 for GPT4o and Son-\nnet; 0.87 for o4-mini and Llama70b; 0.83 for Son-\nnet and Llama70b), but in no case model perfor-\n7\n--- Page 8 ---\nFigure 3: Toxicity detection accuracy by target group\nFigure 4: F1 scores of the models on the span data labeled\nas “all sentence” (x-axis) vs others (specific spans are found,\ny-axis). Reasoning models, few-shot prompted models, and\nzero-shot models are labeled with a different color.\nmance correlates well with data support.\nWe also found large differences in target group\naccuracy across target social groups (see Appendix\nFigure 6). For example, Llama8b has a 42% differ-\nence in accuracy for \"black\" versus \"white.\" The\nmodels struggle in categories such as \"other coun-\ntry\" or \"other gender,\" possibly because these cate-\ngories are necessarily more heterogenous.\nSpan detection for sentences versus subsen-\ntence spans. As mentioned in Section 3.1, toxic\nspans sometimes consist of the entire sentence\n(35% of the test set). Figure 4 plots F1 for full\nsentence toxic spans on the x-axis by subsentence\nspans on the y-axis. Most models, including Span-\nBERT, have much higher performance on full-\nsentence toxic spans. Distilled Llama models are\nfar worse at detecting toxic subsentence spans.LLM-generated text. Our test data composition\nis 40% GPT-3 generated (from ToxiGen). Breaking\ndown accuracy by human versus LLM origin shows\n20% greater accuracy on the GPT-3 generated text.\n5 Conclusion\nTo further research into identifying and mitigating\ntoxicity in LLMs, we have presented TRuST, a\ndataset of over 298K examples, where about 38%\nare toxic, and have been labeled for 24 fine-grained\ntarget social groups (including \"no target\"), and\ntoxic spans. A subset of over 11K examples were\nmanually annotated by a diverse group of annota-\ntors whose inter-annotator agreements on the three\nlabeling tasks were higher than reported in previous\nwork. We benchmarked 13 methods for detection\naccuracy and F1 on the three aspects of toxicity.\nUnsurprisingly, the PLM classifiers outperformed\nthe LLMs. Neither CoT nor reasoning models led\nto higher LLM performance, thus current methods\nto improve LLM reasoning do not address social\nreasoning well, if at all.\nThe 287K examples that were not human-\nannotated have been automatically annotated using\nthe best performing models: a RoBERTa PLM for\ntoxicity, a BERT PLM for target group, and Span-\nBERT for toxic span. This dataset will be released\nif the paper is accepted for publication. We offer\nTRuST as a contribution toward more robust meth-\nods for toxicity detection, and perhaps to support\ndevelopment of corpora and methods to mitigate\ntoxicity, such as unlearning (Chen and Yang, 2023;\nLiu et al., 2024), which to our knowledge has not\nyet been applied to toxicity.\n8\n--- Page 9 ---\n6 Limitations\nThe work presented here carries out only a prelim-\ninary investigation of baseline methods for auto-\nmatic identification of toxicity, target social group\nand toxic span detection. The LLMs methods did\nnot explore sophisticated prompt engineering. Al-\nthough the size of the dataset is competitive, it is\nnot sufficiently large to have separate annotations\nfor some important subgroups. Although we at-\ntempted to recruit a pool of annotators that was\nsocially diverse, this was limited due to lack of\nfunds to recruit more than six annotators.\nReferences\nSaad Almohaimeed, Saleh Almohaimeed, Ashfaq Ali\nShafin, Bogdan Carbunar, and Ladislau Bölöni. 2023.\nTHOS: A benchmark dataset for targeted hate and\noffensive speech. arXiv preprint arXiv:2311.06446 .\nAnthropic. 2025. Claude 3.7 sonnet. Large Language\nModel.\nBerk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu,\nRebecca J. Passonneau, Evan Radcliffe, Guru Rajan\nRajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture,\nZhe Wu, Lixinyu Xu, and Breck Baldwin. 2025. Non-\ndeterminism of \"deterministic\" llm settings. Preprint ,\narXiv:2408.04667.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, and 1 others. 2020. Language models are\nfew-shot learners. Advances in neural information\nprocessing systems , 33:1877–1901.\nJiaao Chen and Diyi Yang. 2023. Unlearn what\nyou want to forget: Efficient unlearning for llms.\nPreprint , arXiv:2310.20150.\nGloria Cowan and Cyndi Hodge. 1996. Judgments of\nhate speech: The effects of target group, publicness,\nand behavioral responses of the target. Journal of\nApplied Social Psychology , 26(4):355–374.\nThomas Davidson, Dana Warmsley, Michael Macy, and\nIngmar Weber. 2017. Automated hate speech de-\ntection and the problem of offensive language. In\nProceedings of the international AAAI conference on\nweb and social media , volume 11.1, pages 512–515.\nAdelson de Araujo, Pantelis M. Papadopoulos, Susan\nMcKenney, and Ton de Jong. 2025. Investigating\nthe impact of a collaborative conversational agent\non dialogue productivity and knowledge acquisition.\nInternational Journal of Artificial Intelligence in Ed-\nucation .\nDeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Rea-\nsoning Capability in LLMs via Reinforcement Learn-\ning. Preprint , arXiv:2501.12948.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 conference\nof the North American chapter of the association\nfor computational linguistics: human language tech-\nnologies, volume 1 (long and short papers) , pages\n4171–4186.\nRebecca Dorn, Lee Kezar, Fred Morstatter, and Kristina\nLerman. 2024. Harmful speech detection by lan-\nguage models exhibits gender-queer dialect bias. In\nProceedings of the 4th ACM Conference on Equity\nand Access in Algorithms, Mechanisms, and Opti-\nmization , pages 1–12.\nXiaoni Duan, Zhuoyan Li, Chien-Ju Ho, and Ming Yin.\n2025. Exploring the cost-effectiveness of perspective\ntaking in crowdsourcing subjective assessment: A\ncase study of toxicity detection. In Proceedings of\nthe 2025 Conference of the Nations of the Americas\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers) , pages 2359–2372, Albuquerque, New\nMexico. Association for Computational Linguistics.\nMai ElSherief, Caleb Ziems, David Muchlinski, Vaish-\nnavi Anupindi, Jordyn Seybolt, Munmun De Choud-\nhury, and Diyi Yang. 2021. Latent hatred: A bench-\nmark for understanding implicit hate speech. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 345–363,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nBeyza Ermis, Luiza Pozzobon, Sara Hooker, and Patrick\nLewis. 2024. From one to many: Expanding the\nscope of toxicity mitigation in language models. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2024 , pages 15041–15058, Bangkok,\nThailand. Association for Computational Linguistics.\nEve Fleisig, Rediet Abebe, and Dan Klein. 2023a.\nWhen the majority is wrong: Modeling annotator dis-\nagreement for subjective tasks. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing , pages 6715–6726, Singapore.\nAssociation for Computational Linguistics.\nEve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin\nBlodgett, Hal Daumé III, Alexandra Olteanu, Emily\nSheng, Dan Vann, and Hanna Wallach. 2023b. Fair-\nPrism: Evaluating fairness-related harms in text gen-\neration. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 6231–6251, Toronto,\nCanada. Association for Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n3356–3369, Online. Association for Computational\nLinguistics.\n9\n--- Page 10 ---\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, and 542 others. 2024. The llama 3 herd of\nmodels. Preprint , arXiv:2407.21783.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nInProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 3309–3326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nXinlei He, Savvas Zannettou, Yun Shen, and Yang\nZhang. 2024. You only prompt once: On the capa-\nbilities of prompt learning on large language models\nto tackle toxic content. In 2024 IEEE Symposium on\nSecurity and Privacy (SP) , pages 770–787. IEEE.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, and 1\nothers. 2024. Openai o1 system card. arXiv preprint\narXiv:2412.16720 .\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the association for\ncomputational linguistics , 8:64–77.\nJulia Kansok-Dusche, Cindy Ballaschk, Norman\nKrause, Anke Zeißig, Lisanne Seemann-Herz, Sebas-\ntian Wachs, and Ludwig Bilz. 2023. A systematic re-\nview on hate speech among children and adolescents:\nDefinitions, prevalence, and overlap with related phe-\nnomena. Trauma, violence, & abuse , 24(4):2598–\n2615.\nUrja Khurana, Eric Nalisnick, and Antske Fokkens.\n2025. DefVerify: Do hate speech models reflect their\ndataset‘s definition? In Proceedings of the 31st Inter-\nnational Conference on Computational Linguistics ,\npages 4341–4358, Abu Dhabi, UAE. Association for\nComputational Linguistics.\nUrja Khurana, Ivar Vermeulen, Eric Nalisnick, Mar-\nloes Van Noorloos, and Antske Fokkens. 2022. Hate\nspeech criteria: A modular approach to task-specific\nhate speech definitions. In Proceedings of the Sixth\nWorkshop on Online Abuse and Harms (WOAH) ,\npages 176–191, Seattle, Washington (Hybrid). Asso-\nciation for Computational Linguistics.\nJunseong Kim, Seolhwa Lee, Sangmo Gu Jihoon Kwon,\nYejin Kim, Minkyung Cho, Jy yong Sohn, and\nChanyeol Choi. 2024. Linq-embed-mistral:elevating\ntext retrieval with improved gpt data through task-\nspecific control and quality refinement. Linq AI Re-\nsearch Blog.Sohyun An Kim, Lauren Baczewski, Maria Pizzano,\nConnie Kasari, and Alexandra Sturm. 2023. Discrim-\nination and harassment experiences of autistic college\nstudents and their neurotypical peers: Risk and pro-\ntective factors. Journal of Autism and Developmental\nDisorders , 53(12):4521–4534.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam:\nA method for stochastic optimization. Preprint ,\narXiv:1412.6980.\nKlaus Krippendorff. 2013. Content analysis: An in-\ntroduction to its methodology , third edition. Sage,\nThousand Oaks, CA.\nDeepak Kumar, Patrick Gage Kelley, Sunny Consolvo,\nJoshua Mason, Elie Bursztein, Zakir Durumeric, Kurt\nThomas, and Michael Bailey. 2021. Designing toxic\ncontent classification for a diversity of perspectives.\nInSeventeenth Symposium on Usable Privacy and\nSecurity (SOUPS 2021) , pages 299–318.\nAlyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai\nGupta, Donald Metzler, and Lucy Vasserman. 2022.\nA New Generation of Perspective API: Efficient Mul-\ntilingual Character-level Transformers. Preprint ,\narXiv:2202.11176.\nXiaochen Li, Zheng Xin Yong, and Stephen Bach. 2024.\nPreference tuning for toxicity mitigation generalizes\nacross languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2024 , pages\n13422–13440, Miami, Florida, USA. Association for\nComputational Linguistics.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nInProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n6691–6706, Online. Association for Computational\nLinguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures ,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nSijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper,\nNathalie Baracaldo, Peter Hase, Yuguang Yao,\nChris Yuhao Liu, Xiaojun Xu, Hang Li, Kush R.\nVarshney, Mohit Bansal, Sanmi Koyejo, and Yang\nLiu. 2024. Rethinking machine unlearning for large\nlanguage models. Preprint , arXiv:2402.08787.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. Preprint , arXiv:1907.11692.\n10\n--- Page 11 ---\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2021. HateXplain: A benchmark dataset for\nexplainable hate speech detection. In Proceedings\nof the AAAI conference on artificial intelligence , vol-\nume 35.17, pages 14867–14875.\nAida Mostafazadeh Davani, Mark Diaz, Dylan K Baker,\nand Vinodkumar Prabhakaran. 2024. D3CODE: Dis-\nentangling disagreements in data across cultures on\noffensiveness detection and evaluation. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing , pages 18511–18526,\nMiami, Florida, USA. Association for Computational\nLinguistics.\nLeticia Nieto and MF Boyer. 2006. Understanding op-\npression: Strategies in addressing power and privi-\nlege. Colors NW , pages 30–33.\nJohn T Nockleby. 2000. Hate speech. Encyclopedia\nof the American Constitution (2nd ed., edited by\nLeonard W. Levy, Kenneth L. Karst et al., New York:\nMacmillan, 2000) , pages 1277–1279.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec\nRadford, Aleksander M ˛ adry, Alex Baker-Whitcomb,\nAlex Beutel, Alex Borzunov, Alex Carney, Alex\nChow, Alex Kirillov, and 401 others. 2024. Gpt-4o\nsystem card. Preprint , arXiv:2410.21276.\nAndrei Paraschiv, Teodora Andreea Ion, and Mihai Das-\ncalu. 2023. Offensive text span detection in Roma-\nnian comments using large language models. Infor-\nmation , 15(1):8.\nRebecca J. Passonneau. 2006. Measuring Agreement on\nSet-valued Items (MASI) for Semantic and Pragmatic\nAnnotation. In LREC , pages 831–836.\nJohn Pavlopoulos, Leo Laugier, Alexandros Xenos, Jef-\nfrey Sorensen, and Ion Androutsopoulos. 2022. From\nthe detection of toxic spans in online discussions to\nthe analysis of toxic-to-civil transfer. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 3721–3734, Dublin, Ireland. Association for\nComputational Linguistics.\nAgnieszka Pluta, Joanna Mazurek, Jakub Woj-\nciechowski, Tomasz Wolak, Wiktor Soral, and\nMichał Bilewicz. 2023. Exposure to hate speech\ndeteriorates neurocognitive mechanisms of the abil-\nity to understand others’ pain. Scientific Reports ,\n13(1):4127.\nLuiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara\nHooker. 2023a. Goodtriever: Adaptive toxicity mit-\nigation with retrieval-augmented models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023 , pages 5108–5125, Singapore.\nAssociation for Computational Linguistics.Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara\nHooker. 2023b. On the challenges of using black-box\nAPIs for toxicity evaluation in research. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing , pages 7595–7609,\nSingapore. Association for Computational Linguis-\ntics.\nAnchal Rawat, Santosh Kumar, and Surender Singh\nSamant. 2024. Hate speech detection in social media:\nTechniques, recent trends, and future challenges. Wi-\nley Interdisciplinary Reviews: Computational Statis-\ntics, 16(2):e1648.\nLucas Rosenblatt, Lorena Piedras, and Julia Wilkins.\n2022. Critical Perspectives: A Benchmark Revealing\nPitfalls in PerspectiveAPI. In Proceedings of the Sec-\nond Workshop on NLP for Positive Impact (NLP4PI) ,\npages 15–24.\nKoustuv Saha, Eshwar Chandrasekharan, and Munmun\nDe Choudhury. 2019. Prevalence and psychological\neffects of hateful speech in online college communi-\nties. In Proceedings of the 10th ACM conference on\nweb science , pages 255–264.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\nBias Frames: Reasoning about Social and Power Im-\nplications of Language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics , pages 5477–5490, Online. Association\nfor Computational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-Diagnosis and Self-Debiasing: A proposal for re-\nducing corpus-based bias in nlp. Transactions of the\nAssociation for Computational Linguistics , 9:1408–\n1424.\nXavier Suau, Pieter Delobelle, Katherine Metcalf, Ar-\nmand Joulin, Nicholas Apostoloff, Luca Zappella,\nand Pau Rodríguez. 2024. Whispering experts: Neu-\nral interventions for toxicity mitigation in language\nmodels. arXiv preprint arXiv:2407.12824 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models. Preprint ,\narXiv:2201.11903.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato,\nSumanth Dathathri, John Mellor, Lisa Anne Hen-\ndricks, Kirsty Anderson, Pushmeet Kohli, Ben Cop-\npin, and Po-Sen Huang. 2021. Challenges in detox-\nifying language models. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021 ,\npages 2447–2469, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nRongwu Xu, Zian Zhou, Tianwei Zhang, Zehan Qi,\nSu Yao, Ke Xu, Wei Xu, and Han Qiu. 2024. Walk-\ning in others’ shoes: How perspective-taking guides\nlarge language models in reducing toxicity and bias.\n11\n--- Page 12 ---\nInProceedings of the 2024 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n8341–8368, Miami, Florida, USA. Association for\nComputational Linguistics.\nYee Hui Yeo, Allistair Clark, Muskaan Mehra, Itai\nDanovitch, Karen Osilla, Ju Dong Yang, Alexan-\nder Kuo, Hyun-Seok Kim, Aarshi Vipani, Yun Wang,\nWalid Ayoub, Hirsh Trivedi, Jamil S. Samaan, Tiffany\nWu, Vijay H. Shah, Omer Liran, and Brennan Spiegel.\n2024. The feasibility and usability of an artificial\nintelligence-enabled conversational agent in virtual\nreality for patients with alcohol-associated cirrhosis:\nA multi-methods study. Journal of Medical Extended\nReality , 1(1):257–270.\nYee Hui Yeo, Yuxin Peng, Muskaan Mehra, Jamil\nSamaan, Joshua Hakimian, Allistair Clark, Karisma\nSuchak, Zoe Krut, Taiga Andersson, Susan Persky,\nOmer Liran, and Brennan Spiegel. 2025. Evaluating\nfor evidence of sociodemographic bias in conversa-\ntional ai for mental health support. Cyberpsychology,\nBehavior, and Social Networking , 28(1):44–51.\nShuzhen Yu, Alexey Androsov, and Hanbing Yan. 2025.\nExploring the prospects of multimodal large language\nmodels for automated emotion recognition in educa-\ntion: Insights from gemini. Computers & Education ,\n232:105307.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. Predicting the type and target of offensive\nposts in social media. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers) , pages 1415–1420, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nMarcos Zampieri, Skye Morgan, Kai North, Tharindu\nRanasinghe, Austin Simmmons, Paridhi Khandelwal,\nSara Rosenthal, and Preslav Nakov. 2023. Target-\nbased offensive language identification. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers) , pages 762–770, Toronto, Canada. Association\nfor Computational Linguistics.\nArmel Zebaze, Benoît Sagot, and Rachel Bawden. 2024.\nIn-context example selection via similarity search\nimproves low-resource machine translation. Preprint ,\narXiv:2408.00397.\nXuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei\nGao, and Min Lin. 2024. Chain of preference opti-\nmization: Improving chain-of-thought reasoning in\nllms. Advances in Neural Information Processing\nSystems , 37:333–356.\nXuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas David-\nson, Jena D. Hwang, Swabha Swayamdipta, and\nMaarten Sap. 2023. COBRA frames: Contextual\nreasoning about effects and harms of offensive state-\nments. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023 , pages 6294–6315,Toronto, Canada. Association for Computational Lin-\nguistics.\n12\n--- Page 13 ---\nA Statistics on the Whole Dataset\nTarget H. Count (%) H. Toxic % M. Count (%) M. Toxic % C. Count (%) C. Toxic %\nNo Target 3618 ( 36.10) 38.92 73211 ( 25.40) 24.03 76829 ( 25.76) 24.73\nEthnicity 1763 ( 17.59) 56.21 68962 ( 23.93) 48.02 70725 ( 23.71) 48.22\nblack 626 ( 6.25) 75.56 15654 ( 5.43) 59.93 16280 ( 5.46) 60.53\nwhite 244 ( 2.43) 49.18 8691 ( 3.02) 51.29 8935 ( 3.00) 51.24\nasian 238 ( 2.37) 49.16 12961 ( 4.50) 45.36 13199 ( 4.43) 45.43\nnative american 143 ( 1.43) 36.36 8817 ( 3.06) 34.04 8960 ( 3.00) 34.07\nchinese 139 ( 1.39) 37.41 10242 ( 3.55) 41.53 10381 ( 3.48) 41.47\nother ethnicity 108 ( 1.08) 38.89 5156 ( 1.79) 35.47 5264 ( 1.76) 35.54\nmexican 95 ( 0.95) 35.79 6221 ( 2.16) 42.65 6316 ( 2.12) 42.54\narab 88 ( 0.88) 65.91 2267 ( 0.79) 70.27 2355 ( 0.79) 70.11\nlatino 82 ( 0.82) 52.44 5044 ( 1.75) 49.15 5126 ( 1.72) 49.20\nPolitics 1132 ( 11.30) 62.54 9624 ( 3.34) 57.41 10756 ( 3.61) 57.95\nGender 1000 ( 9.98) 50.20 34280 ( 11.89) 42.67 35280 ( 11.83) 42.88\nlgbtq+ 456 ( 4.55) 51.10 15598 ( 5.41) 41.50 16054 ( 5.38) 41.77\nwoman 431 ( 4.30) 50.58 14839 ( 5.15) 47.94 15270 ( 5.12) 48.02\nman 102 ( 1.02) 49.02 5248 ( 1.82) 43.45 5350 ( 1.79) 43.55\nother gender 11 ( 0.11) 9.09 1531 ( 0.53) 14.30 1542 ( 0.52) 14.27\nReligion 966 ( 9.64) 60.97 30381 ( 10.54) 50.65 31347 ( 10.51) 50.97\nmuslim 468 ( 4.67) 58.33 12707 ( 4.41) 44.31 13175 ( 4.42) 44.81\njewish 407 ( 4.06) 68.06 14765 ( 5.12) 58.23 15172 ( 5.09) 58.49\nother religion 91 ( 0.91) 42.86 5127 ( 1.78) 31.44 5218 ( 1.75) 31.64\nOther 716 ( 7.14) 51.96 14771 ( 5.12) 43.73 15487 ( 5.19) 44.11\nother 412 ( 4.11) 55.34 2489 ( 0.86) 52.95 2901 ( 0.97) 53.29\nrefugee 160 ( 1.60) 42.50 4710 ( 1.63) 39.53 4870 ( 1.63) 39.63\nmiddle east 144 ( 1.44) 52.78 7408 ( 2.57) 42.54 7552 ( 2.53) 42.73\nCountry 470 ( 4.69) 31.49 23189 ( 8.05) 25.02 23659 ( 7.93) 25.15\nother country 308 ( 3.07) 30.52 12511 ( 4.34) 25.01 12819 ( 4.30) 25.14\nunited states 162 ( 1.62) 33.33 9080 ( 3.15) 25.55 9242 ( 3.10) 25.69\nDisability 357 ( 3.56) 30.53 24332 ( 8.44) 27.05 24689 ( 8.28) 27.11\nTotal 10222 48.16 288233 37.79 298255 38.14\nTable 6: The statistics on the training data including machine annotated data. The second and third columns are human annotated\ntraining data (H), the fourth and fifth are machine annotated training data (M), and the last two are the combination of both (C).\nThe results for the higher level categories for human annotation are computed by combining the fine-grained category results.\nFor the machine generated, however, we carry out independent tests of the higher and lower categories. As a result, we find a\ndiscrepancy of about 10% of cases.\n13\n--- Page 14 ---\nB Experimental Details\nWe use AdamW optimizer (Kingma and Ba, 2017)\nto fine-tune PLMs. We use a batch size of 8. We\ntried 0.01, 0.0001, 0.00001, 0.05,0.005,0.0005,\n0.00005 for learning rate and we chose the best\none for each task, we chose the best one based on\nthe validation performance. The best learning rate\nis 1e-05 for all tasks and embedders. We ran the\nexperiments on a single NVIDIA RTX A6000 GPU\nand it took 2 hours for each experiment.\nC Annotation Instructions\nGoal Creating annotated data for toxic language,\nwhere, very generally, a sentence is toxic if it has\nnegative stereotyping, hate speech, racism, psycho-\nlogical threat, sexual harassment, abusive language,\nsexism, discrimination based on sexual orientation,\nor any other type of explicit language that might\nhurt or affect a member of some sociodemographic\ngroup badly.\nTask Annotate these 3 categories:\n• Toxicity (binary)\n•Social target group (24 categories belonging\nto 6 higher level groups including other, and\nno target)\n•Toxic Span (words that make the sentence\ntoxic, only for toxic sentences.)\nAssumptions\n•There is no speaker information (who pro-\nduced the language)\n•There is no audience information (who the\nspeaker was addressing, not necessarily the\ntarget).\n•The toxicity label should take into account the\ntarget social group (note that the same words\nmight be toxic for one social group target and\nnot for another)\n•There is no context , so texts should be inter-\npreted as they are, based on easily available\nreadings.\n•If you see a politician’s name/party name,\nchoose politics.\n•If the target is individual, we are not interested\nin so they should be labeled as no target.• If there are multiple targets, try to choose the\ndominating one. In case they are equally tar-\ngeted, choose other.\nTarget Groups Higher Level Target Groups\nWe have 5 distinct higher level social targets, plus\nlabels for “Other” category (there is a target but it\ndoesn’t fall into these categories) and absence of a\ntarget (“No Target”).\n• Ethnicity/Nationality\n• Gender\n• Religion\n• Disability\n• Country\n• Other\n• No Target\nFull Set of Target Groups We have more spe-\ncific targets within higher level categories. For\neach category, except individual and disability, we\nalso have other. You should use the label “other\n< higher _level _target > ” in the case that the\ngroup belongs to < higher _level _target > and\nnot among the ones listed below (e.g. for the state-\nment “Turkish people are useless”, the target is\nethnicity, but there is no Turkish category so it\nshould be other ethnicity)\n•Black, white, Asian, Mexican, Native Amer-\nican, Chinese, Latino, and Arab belong to\nethnicity .\n•LGBTQ+, Woman, and Man belong to gen-\nder.\n• Muslim and Jewish belong to religion .\n• Disability does not have any sub-targets.\n•United States is the only country belonging to\ncountry . You should label as “other country”\nif it is another country.\n• Middle East and refugee belong to other .\n• Immigrants belongs to refugee\nNote: Do not forget about the “other” categories\nwithin each category, except individual and disabil-\nity!\n14\n--- Page 15 ---\nToxicity Label Criteria For assigning posi-\ntive/negative toxicity; toxic if any of the following\ncategories, where the first five (blue font) are va-\nrieties of hate speech, the next two are examples\nof abusive language other than sexual harrassment,\nthe final category.\n•Negative stereotyping : Traits, characteris-\ntics and generalizations that are negatively at-\ntributed towards a group. e.g. Turkish people\nare selfish\n•Hate speech : Offensive and discriminatory\ndiscourse towards a group or an individual\nbased on characteristics such as race or reli-\ngion. E.g. All Jews should be killed.\n•Racism : Usually a bias that includes accep-\ntance of negative stereotypes, and racial dis-\ncrimination (discrimination; different treat-\nment of different age, gender, racial etc., race\nin this case) against individuals/groups be-\nlonging to a certain race. E.g Arabs should\nnot be offered any jobs.\n•Sexist language : Discrimination or behaviors\nthat foster stereotypes based on a gender. E.g.\nMen cannot be successful in teaching\n•Discriminating based on sexual orientation :\nE.g. Being lesbian should be banned.\n•Psychological threat : It is an expression of\nan intent of harm, such as humiliation, causing\nstress, or constant criticism. E.g. You always\nsuck\n•Abusive language : Piece of text that contains\ninappropriate words such as swear profanity,\nand disrespectful nicknames. E.g It is no won-\nder everyone says you’re a jerk\n•Sexual harassment : Unwelcome sexual\nmoves, sexual favor requests or other phys-\nical/verbal behaviors of a sexual nature to-\nwards someone. E.g. Your breasts are amaz-\ning.\nNote: there are other situations of language use\nthat can be perceived as toxic that do not fall under\nour guidelines given our assumptions, e.g., some\nmicroaggressions might be considered toxic but\nto identify them would require other information\nsuch as the audience, more of the overall context\nin which the language occurs, etc.Toxic Spans You should choose the word(s) that\ncauses the toxicity of the statement (e.g. We need\ntoignore the needs of disabled people).\nGeneral Instructions\n• We should think as being in the target group.\n•You should consider multiple possible\ninterpretations since almost every state-\nment/utterance has multiple “readings”; If\nthere is an easily available understanding that\nleads to being non-toxic, it should be labeled\nas non-toxic.\n•For the higher level target, you need to check\nif the opinion is towards some group, indi-\nvidual or no one. There might be more than\none group, in that case try to choose the domi-\nnant group. Also, if it is for an individual and\nseems like they belong to a social group, do\nnot try to infer a social group for them, instead\nlabel them as “individual”\nD Potential Risks\nDue to the nature of our work, TRuST has some\npotential risks of increasing toxic content. How-\never, all text data were already public before our\nwork. We add human and machine annotations to\nthese texts, which should not increase the current\npotential risk. We hope this work will be used for\ngood, not to make the system more harmful.\nE LLM Prompt for Toxicity\nSystem Prompt: You are an expert in linguistic and soci-\nological analysis. Your task is to analyze a text and determine\nif it is toxic or not. Toxicity here refers to negative stereo-\ntyping, hate speech, racism, sexist language, discrimination\nbased on sexual orientation, physical or psychological threat,\nand abusive language. Note that there are other situations of\nlanguage use that can be perceived as toxic that do not fall\nunder our guidelines, given our assumptions For each text,\nat the end, respond in the following format: Toxicity Label:\n[toxic/non-toxic]\nUser Prompt: Text: <text>\nF LLM Prompt for Target Group\nSystem Prompt: You are an expert in linguistic and\nsociological analysis. Your task is to determine which social\ngroup is targeted by the sentiment or content of each text. For\neach text, use the provided list of social groups and always\nselect the group that is most directly addressed or implied. If\nan individual not involved in politics is mentioned, or if no\ngroup is targeted, respond with ’Targeted Group: no target’.\nIf multiple groups are targeted with no clear dominance, or if\na group is certainly targeted but not clearly identified, respond\nwith ’Targeted Group: other.’ Here are the social groups:\n15\n--- Page 16 ---\n- no target\n- other country\n- united states (notes: must be related to the United States as a\nwhole)\n- black\n- asian (note: ethnicity, excludes Chinese)\n- white\n- arab\n- latino (note: ethnicity, includes Hispanic and excludes Mexi-\ncan)\n- native american\n- chinese\n- mexican\n- other ethnicity\n- jewish\n- muslim\n- other religion\n- politics (note: must reference a political figure or party)\n- woman\n- man\n- lgbtq+\n- other gender\n- disability\n- middle east\n- refugee (note: includes immigrants)\n- other For each text, at the end, respond in the following for-\nmat: Targeted Group: [Group Name] User Prompt: Text:\n<text>\nG LLM Prompt for Higher Level Target\nGroup\nSystem Prompt: You are an expert in linguistic and\nsociological analysis. Your task is to determine which social\ngroup is targeted by the sentiment or content of each text. For\neach text, use the provided high level social groups and always\nselect the group that is most directly addressed or implied. If\nan individual not involved in politics is mentioned, or if no\ngroup is targeted, respond with ’Targeted Group: no target’.\nIf multiple groups are targeted with no clear dominance, or if\na group is certainly targeted but not clearly identified, respond\nwith ’Targeted Group: other.’ Here are the social groups: - no\ntarget\n- country\n- ethnicity\n- religion\n- politics\n- gender (note: includes lgbtq+)\n- disability\n- other (note: includes politics that references a political figure\nor party)\nFor each text, at the end, respond in the following format:\nTargeted Group: [Group Name]\nUser Prompt: Text: <text>\nH LLM Prompt for Toxic Span\nSystem Prompt: You are an expert in linguistic and soci-\nological analysis. You are an expert in linguistic analysis. Thetask is to label toxic sequences in user-generated comments.\nYou place the toxic sequences between [[ and ]] and do not\nchange the text otherwise. If the overall text is toxic but not\nsome specific words based on the meaning, you should output\n\"all sentence\". A word can be toxic because of its usage in\nthe sentence and it may not be toxic on its own. You need re-\nview the replies before output. Be concise and exact. Answer\nonly with JSON statements such as: \"output\": \"lol crybaby\n[[faggot]]\"\nUser Prompt: Text: <text>\n16\n--- Page 17 ---\nI Effect of Target Group on Toxicity\nDetection\nModel Acc. Pre. Recall F1\nRoBERTa 0.79 0.76 0.82 0.79\nRoBERTa w target 0.78 0.74 0.83 0.78\nGPT4o 0.75 0.70 0.85 0.77\nGPT4o persona 0.75 0.73 0.74 0.74\nGPT4o w target 0.73 0.74 0.68 0.71\nSonnet 0.77 0.72 0.83 0.77\nSonnet persona 0.72 0.72 0.68 0.70\nSonnet w target 0.73 0.74 0.68 0.70\nLlama70b 0.77 0.71 0.86 0.78\nLlama70b persona 0.77 0.72 0.83 0.77\nLlama70b w target 0.77 0.72 0.86 0.78\nD. Llama70b 0.75 0.69 0.84 0.76\nD. Llama70b persona 0.75 0.69 0.87 0.77\nD. Llama70b w target 0.76 0.71 0.83 0.76\nLlama8b 0.73 0.66 0.88 0.76\nLlama8b persona 0.74 0.68 0.88 0.76\nLlama8b w target 0.74 0.67 0.89 0.76\nD. Llama8b 0.70 0.63 0.90 0.74\nD. Llama8b persona 0.68 0.60 0.92 0.73\nD. Llama8b w target 0.71 0.64 0.90 0.75\no4-mini 0.78 0.71 0.90 0.79\no4-mini persona 0.78 0.70 0.91 0.79\no4-mini w target 0.78 0.71 0.92 0.80\nTable 7: The effect of target group on toxicity detection\n17\n--- Page 18 ---\nJ Accuracy Per Higher Target Group\nFigure 5: Accuracy For each Higher Target Group\nK Target Group Accuracy Per Target Group\nFigure 6: Target group prediction accuracy for each target group\n18",
  "text_length": 64608
}