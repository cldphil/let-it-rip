{
  "id": "http://arxiv.org/abs/2506.05242v1",
  "title": "SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid\n  Neuron Encryption",
  "summary": "Large language models (LLMs) with diverse capabilities are increasingly being\ndeployed in local environments, presenting significant security and\ncontrollability challenges. These locally deployed LLMs operate outside the\ndirect control of developers, rendering them more susceptible to abuse.\nExisting mitigation techniques mainly designed for cloud-based LLM services are\nfrequently circumvented or ineffective in deployer-controlled environments. We\npropose SECNEURON, the first framework that seamlessly embeds classic access\ncontrol within the intrinsic capabilities of LLMs, achieving reliable,\ncost-effective, flexible, and certified abuse control for local deployed LLMs.\nSECNEURON employs neuron-level encryption and selective decryption to\ndynamically control the task-specific capabilities of LLMs, limiting\nunauthorized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically related neurons\nand construct a layered policy tree for handling coupled neurons. We then\nintroduce a flexible and efficient hybrid encryption framework for millions of\nneurons in LLMs. Finally, we developed a distribution-based decrypted neuron\ndetection mechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that SECNEURON satisfies IND-CPA Security and\nCollusion Resistance Security under the Task Controllability Principle.\nExperiments on various task settings show that SECNEURON limits unauthorized\ntask accuracy to below 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-related\nmalicious code generation was reduced from 59% to 15%. SECNEURON also mitigates\nunauthorized data leakage, reducing PII extraction rates to below 5% and\nmembership inference to random guesses.",
  "authors": [
    "Zhiqiang Wang",
    "Haohua Du",
    "Junyang Wang",
    "Haifeng Sun",
    "Kaiwen Guo",
    "Haikuo Yu",
    "Chao Liu",
    "Xiang-Yang Li"
  ],
  "published": "2025-06-05T17:01:28Z",
  "updated": "2025-06-05T17:01:28Z",
  "categories": [
    "cs.CR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05242v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05242v1  [cs.CR]  5 Jun 2025SECNEURON : Reliable and Flexible Abuse Control\nin Local LLMs via Hybrid Neuron Encryption\nZhiqiang Wang∗, Haohua Du†, Junyang Wang∗, Haifeng Sun∗, Kaiwen Guo‡, Haikuo Yu∗, Chao Liu‡, Xiang-Yang Li∗\n∗University of Science and Technology of China\nEmail: {sa21221041, iswangjy, sun1998, yhk7786 }@mail.ustc.edu.cn, xiangyangli@ustc.edu.cn\n†Beihang University\nEmail: duhaohua@buaa.edu.cn\n‡Ocean University of China\nEmail: {kevinguo, liuchao }@ouc.edu.cn\nAbstract — Large language models (LLMs) with diverse capa-\nbilities are increasingly being deployed in local environments,\npresenting significant security and controllability challenges.\nThese locally deployed LLMs operate outside the direct control\nof developers, rendering them more susceptible to abuse. Existing\nmitigation techniques mainly designed for cloud-based LLM\nservices are frequently circumvented or ineffective in deployer-\ncontrolled environments.\nWe propose S ECNEURON , the first framework that seamlessly\nembeds classic access control within the intrinsic capabilities of\nLLMs, achieving reliable, cost-effective, flexible, and certified\nabuse control for local deployed LLMs. S ECNEURON employs\nneuron-level encryption and selective decryption to dynamically\ncontrol the task-specific capabilities of LLMs, limiting unautho-\nrized task abuse without compromising others. We first design a\ntask-specific neuron extraction mechanism to decouple logically\nrelated neurons and construct a layered policy tree for handling\ncoupled neurons. We then introduce a flexible and efficient hybrid\nencryption framework for millions of neurons in LLMs. Finally,\nwe developed a distribution-based decrypted neuron detection\nmechanism on ciphertext to ensure the effectiveness of partially\ndecrypted LLMs. We proved that S ECNEURON satisfies IND-\nCPA Security and Collusion Resistance Security under the Task\nControllability Principle. Experiments on various task settings\nshow that S ECNEURON limits unauthorized task accuracy to\nbelow 25% while keeping authorized accuracy loss with 2%.\nUsing an unauthorized Code task example, the accuracy of abuse-\nrelated malicious code generation was reduced from 59% to 15%.\nSECNEURON also mitigates unauthorized data leakage, reducing\nPII extraction rates to below 5% and membership inference\nto random guesses. Additionally, S ECNEURON enables one-time\nencryption & transmission and multi-party selective decryption,\nrequiring millisecond-level key generation and byte-level key\nexchange for local LLM capability adjustment.\nI. I NTRODUCTION\nLocal deployment of LLMs is an increasingly popular\nparadigm, chosen by many organizations and individuals\nfor private or domain-specific application [1]–[4]. However,\nLLMs incorporate increasingly diverse task capabilities that\nexceed the usage requirements of specific deployers, thereby\nincreasing abuse-related risks in local deployment scenarios.\nFor instance, students can use LLMs to generate assignments\nor even academic misconduct articles [5], while malicious\nFig. 1: Workflow of S ECNEURON . Developer encrypts their\nLLM once ( One-time Encryption ). Different deployers down-\nload the same encrypted LLM and dynamically decrypt au-\nthorized tasks while restricting unauthorized capabilities to\nmitigate abuse. ( Multi-party Selective Decryption ). The devel-\noper maintained and released one single encrypted LLM, while\ndeployers also download it once even if permissions change\n(One-time Transmission ).\nusers can exploit models like GPT to write harmful code and\nphishing emails [6].\nPreventing the abuse of LLM has gradually become a focal\npoint of both societal and academic concern. Existing solutions\ncan primarily be categorized into two aspects: 1) temporary\ndefences that constrain user interactions, such as malicious\ninput/output (I/O) filter [19]–[21] or safety system prompt\nand 2) adjust the LLMs to refuse inappropriate queries or\ntailor for specific tasks, such as safety alignment [9]–[12]or\ntask-specific fine-tuning/distillation [7], [8], [22]–[24]. These\nworks predominantly address scenarios where the LLM is\ndeployed in the cloud, meaning that the runtime environment\nis controlled by the model developer and the abuse\nbehaviours are pre-known (e.g., according to human values).\nConsequently, their effectiveness relies on two assumptions: 1)\nconstraints on user interactions should be honestly enforced,\nand 2) sufficient resources and capabilities to adjust the\nLLM according to explicitly defined abuse behaviours. These\nassumptions make it challenging to apply these methods to\nlocal deployments.\nLocal LLMs are deployed on private clouds or local PCs,\n--- Page 2 ---\nTABLE I: Comparison of Potential Security Mechanisms for Mitigating Abuse of Local LLMs\nMethod Limit UnAuth. TaskReliable FlexibileData Protect\nLimit UnAuth. Capability Robust∗Intrinsic★Customized Dynamic+\nDistillation/Fine-tunning [7], [8] # # #  G #†# #\nSafety Alignment [9]–[12]  # #  # # #\nUnlearning [12]–[14]     G #†#  \nWatermarking [15]–[18] # # # # # # #\nMalicious I/O Detection [19]–[21]  # # #   #\nSECNEURON        \n1.Limit UnAuth. Task refers to restricting LLMs from completing unauthorized tasks; Limit UnAuth. Capability directly limits the\nmodel’s underlying capabilities, making it inherently unable to perform unauthorized tasks (low performance), and is more robust and\nreliable.\n2.∗: robustness against malicious prompt for abuse, like jailbreak or prompt injection.★: is extremely important for local deployment,\nas temporary security mechanisms can be easily bypassed or removed locally.†: requires significant overhead to fine-tune or maintain\nmultiple versions of LLMs, which is impractical.+: tasks of local LLMs can be dynamically adjusted with minimal overhead.\nFig. 2: Pipeline for local deployment of LLMs. S ECNEURON\nenhances security during distribution and deployment.\nwhere the runtime environment is controlled by the model\ndeployer (users who might be malicious). Moreover, de-\nployment requirements may be customized and dynamic (e.g.,\nthe function of a local LLM in a family may vary depending\non the intended user - certain capabilities should be restricted\nwhen children access the model). Therefore, abuse in local\ndeployments extends beyond the pre-known generation of\nmalicious content to include using any unauthorized model\nfunctionalities by malicious deployers. For instance, a hospital\nmight require only the medical diagnosis capabilities of LLMs,\nwhile needing to strictly limit its code generation ability. It is\nevident that the two aforementioned assumptions are difficult\nto meet in this context: 1) deployer-controlled environments\ncannot impose constraints on deployer behaviour, and 2)\nadjusting the model for various deployment requirements is\nprohibitively expensive. In summary, the expansion of abuse\ndefinition and changes in deployment scenarios make ex-\nisting methods challenging to adapt, as shown in Table I.\nNotably, while Trusted Execution Environments (TEEs) [25],\n[26] provide strong security protection for local LLMs, they\nfocus primarily on the confidentiality and integrity of critical\nparameters during the running time and cannot prevent abuses.\nThere is an urgent need for effective methods to dynamically\nimplement task-level abuse control for locally deployed LLMs.\nAs previously discussed, imposing constraints on deployer\nbehaviour in deployer-controlled environments is impossible.\nHence, we propose that the abuse control mechanisms\nfor local LLMs must directly operate on the instinct\ncapabilities of models, i.e., limiting capabilities on unau-\nthorized tasks. For example, even if users employ adversarial\nprompts, an LLM without code generation capabilities wouldstill be unable to generate malicious code (e.g., Figure 12 in\n§6.3). Besides, the mechanisms should be efficient and flexible\nfor LLMs with millions of neurons to support customized\ndeployment. It brings two challenges that must be addressed:\n•C1: How to ensure that capabilities on unauthorized\ntasks are limited without disrupting the authorized tasks?\nWe aim to limit the LLMs’ capabilities (performance on\ngenerating next tokens) on unauthorized tasks rather than\nmerely restricting the completion of these behaviors (e.g.,\nsafety alignment or I/O detection). Deactivating or pruning im-\nportant neurons for specific tasks, like unlearning [13], could\nbe an effective method to limit their capabilities. However,\nLLMs are optimized for multitasks during training, making\nit challenging to fully isolate these logically related neurons\nfor different tasks [27], [28], restricting unauthorized tasks\nmay inadvertently affect others. Therefore, more effective\nmechanisms are required for algorithmically decoupling task-\nspecific neurons and strategies to address unintended coupling.\n•C2: How to enable local LLMs adaptable to customized\ndeployer permission with minimal overhead?\nExisting solutions for customized LLMs (Unlearning or dis-\ntillation/pruning for specific tasks) are inherently irreversible.\nDevelopers require significant resources for fine-tuning or\nmaintaining and transmitting multiple versions of LLMs to\nmeet different user permissions, with the overhead increasing\nlinearly with the number of permissions. Neurons for specific\ntasks should be dynamically forcibly disabled or activated\n(reversible) to meet the customized and dynamic permission\nrequirements of different deployment scenarios.\nFortunately, we found that cryptographic tools can provide\na reversible and training-free method tailored for customized\ncapability limitation: encrypting specific neurons limits ca-\npability on certain tasks while decrypting them restores the\ncapability. Based on this intuition, we designed and imple-\nmented S ECNEURON , a secure mechanism that enables one-\ntime encryption of the LLM, with different users dynamically\ndecrypting and gaining different capabilities (Figure 1). Com-\npared to the methods in Table I, S ECNEURON is characterized\nby its cost-effectiveness, flexibility, and reliability.\nSECNEURON DESIGN. The core of S ECNEURON are\nneuron encryption and selective decryption: deployers can\n--- Page 3 ---\ndynamically decrypt the neurons they are authorized to access,\nexecuting only the authorized tasks. Firstly, we designed a\npenalty-based task-specific neuron extraction mechanism to\nenhance existing neuron importance analysis methods com-\nplemented by an efficient mechanism for handling coupled\nneurons ( Addressing C1 ). Then, we propose a hybrid en-\ncryption framework, particularly designed for LLMs with\nmillions of neurons, that balances the flexibility of attribute-\nbased encryption with the efficiency of symmetric encryption\n(Addressing C2 ).Policy Layer : Neurons are assigned differ-\nent keys and access policies based on task relevance. The\nCiphertext-Policy Attribute-Based (CP-ABE) with a carefully\ndesigned policy tree is used to manage keys and coupled\nneurons across different tasks. Execution Layer : Advanced\nEncryption Standard (AES) is employed for neuron parameters\nencryption and decryption; each neuron only needs to be\nencrypted once. Deployers dynamically obtain decryption keys\nbased on their attributes, allowing them to decrypt only the\nauthorized portions. Since undecrypted neurons can degrade\nthe overall performance of partially decrypted LLM, we de-\nsigned an undecrypted neuron detection mechanism based on\nthe randomness distribution of ciphertext for adaptive pruning.\nSECNEURON offers the following advantages:\nFlexible and Efficient. SECNEURON implements an effi-\ncient mechanism with one-time encryption & transmission,\nand multi-party decryption (Figure 1), enables dynamic capa-\nbility updates and flexible permission configuration based on\nuser attributes. For example, permissions such as (Institution =\nHospital) AND (Licence = True) can restrict access to LLMs’\ndiagnosis functionality.\nReliable and Certified. SECNEURON enforces certified\ncapability constraints through neuron-level encryption. Once\na neuron’s association with an unauthorized task is explicitly\nidentified, S ECNEURON can theoretically ensure that it cannot\nbe activated or utilized, providing a provable safeguard against\ntask abuse.\nWe summarize three contributions of this work:\n•We propose a novel abuse mitigation mechanism -\nSECNEURON - which creatively integrates classic access con-\ntrol policies with the intrinsic capabilities of LLMs, en-\nabling flexible, reliable, and certified abuse control even under\ndeployer-controlled environments. To the best of our knowl-\nedge, S ECNEURON is the first dynamic task-level capability\nmanagement method for LLMs and can serve as a plugin to\nsecure existing deployment pipelines (Figure 2).\n•We introduce a task-specific reusable neuron decoupling\nand managing algorithm that enables task-grained capability\ncontrol at the neuron level. We further propose a hybrid\nhierarchical encryption framework to support efficient and\nflexible encryption and decryption of millions of neurons. Ad-\nditionally, we develop a ciphertext distribution-based neuron\nidentification algorithm to ensure the effectiveness of partially\ndecrypted LLMs.\n•SECNEURON effectively limits the accuracy of unautho-\nrized tasks to below 25% while ensuring authorized tasks\nare impacted by less than 2%. It also prevents unauthorized\nFig. 3: Multi-task performance after fine-tuning for Code\ntask (CodeLlama [7]) or distillation for Story task (Distill-\nGPT2 [8]). The capabilities of other tasks have not been\nsignificantly limited and can still be abused.\ntraining data extraction, with success rates of PII lower than\n5% and MIA nearly 0%. Furthermore, S ECNEURON reduces\nthe encryption and transmission overhead associated with\npermissions from linear to constant levels, requiring only\nmillisecond-level key generation and byte-level key exchange\nfor local LLM capability updates.\nII. B ACKGROUND AND RELATED WORKS\nA. Motivating Use Cases\nSECNEURON offers developers a framework for controlled\nLLM distribution in local deployment scenarios. This section\noutlines the motivating use cases for S ECNEURON , including\nexisting and future scenarios.\nSecure and Controllable Model Publishing. Plenty of\nLLMs are published on platforms like Hugging Face, allowing\nusers to download and deploy them locally. S ECNEURON\ncan be seamlessly integrated into existing pipelines, providing\na low-cost solution to enhance both the security (encrypted\nmodel transmission) and controllability during the distribution\nprocess (Figure 2).\nFine-grained Commercial Licensing. Model develop-\ners create large-scale systems capable of executing diverse\nhigh-value tasks concurrently. SecNeuron facilitates domain-\nspecific licensing to various clients, enabling customizable task\nactivation according to individual client needs.\nDynamic On-device Deployment. An increasing number\nof smart devices come pre-installed with LLMs. When device\npermissions change (e.g., switching to child mode), S ECNEU-\nRON can activate or disable specific tasks (e.g., social content)\nthrough a lightweight key exchange, avoiding re-downloading\nthe LLM.\nB. Security Issues: Multi-level Abuse\nTask Abuse (Model Level). Once the model is distributed,\ndevelopers lose control over how the model is used, creating\nrisks of performing unauthorized tasks. On the one hand, it\nposes a serious threat to developers’ intellectual property, as\nthe functionality of the LLM represents algorithmic innova-\ntions and the significant costs associated with training. On the\nother hand, this may violate legal boundaries. For example,\nmalicious users may exploit unauthorized coding capabilities\nto generate harmful scripts [6].\n--- Page 4 ---\nTraining Data Extraction (Data Level). LLMs may mem-\norize details from their training data, making it possible for\nmalicious users to reverse-engineer training data from the\nmodel’s outputs. The training dataset, containing domain-\nspecific knowledge and trade secrets, is also essential for\ndeveloper’s intellectual property. Moreover, some datasets\ninclude large amounts of sensitive data (such as PII in Enron\nEmail Dataset [29]), posing significant security risks.\nC. Related Works for Mitigating Abuse\nDistillation & Pruning & Fine-tuning for Specific Task.\nThese approaches are designed to adapt LLMs to specific\ntasks [7], [8], [22]–[24], which effectively preserve LLMs’\ncapabilities for authorized (target) tasks but do not impose\nstrict constraints on unauthorized ones. As illustrated in Fig-\nure 3, even after fine-tuning or distillation, the model retains its\ncapability to perform other tasks, which may lead to potential\nabuse.\nSafety Alignment. Safety alignment [9]–[12] aligns LLMs\nwith specific safety objectives and constraints through fine-\ntuning or reinforcement learning, ensuring their behaviour\nadheres to authorized tasks. Such methods, which primarily\nrely on refusal to respond, can limit the behaviour of unau-\nthorized tasks but fail to fundamentally restrict the underlying\ncapabilities of LLMs, leaving them vulnerable to adversarial\nprompts [30]–[32], [32].\nUnlearning . Unlearning [12]–[14] aims to remove or mit-\nigate knowledge or patterns that a model has previously\nlearned, aligning closely with our goal of limiting the model’s\ncapabilities. However, unlearning is irreversible, meaning that\nonce capabilities for a specific task of an LLM are restricted,\nthey cannot be restored, making it unsuitable for dynamic,\ncustomized local deployment.\nWatermarking. Watermarking techniques embed invisi-\nble markers for copyright verification by modifying model\nparameters [33]–[35] or output distribution [36]. Recently,\nnumerous watermarking methods tailored for LLMs have been\nproposed [15]–[18], especially, [17] proposes watermark-based\nprotections to address misuse of local LLMs. However, these\napproaches fall under post hoc detection methods and cannot\nproactively prevent misuse.\nMalicious I/O Detection. They work by monitoring and\nrestricting behavior on unauthorized tasks through external\ninput and output detection [19]–[21]. It is widely used in\ncloud-based LLM applications and is potentially an efficient\nmethod for customizing model tasks across different de-\nployment scenarios. However, such temporary solutions can\nbe easily bypassed or removed when LLM is deployed in\ndeployer-controlled environments [37].\nD. CP-ABE & AES-CTR\nThis section introduces cryptography algorithms used in the\npaper.\nCiphertext-Policy Attribute-Based Encryption (CP-\nABE) . CP-ABE [38], [39] is an advanced cryptographic tech-\nnique that enables fine-grained access control over encrypteddata. Data owners define access policy, which specifies the\nconditions under who can decrypt. Decryption is only possible\nif the user’s attributes satisfy the access policy embedded in\nthe ciphertext. A CP-ABE Cryptor ( E1in Algorithm 1,4)\nincludes four main phases:\n(1)Setup (PK, MSK ←E1.setup ()): Initialize bilinear\ngroup G1and target group GT; generate PK andMSK .\n(2)Encrypt (Cp←E1.encrypt (PK, M, p )): use the\nPK to encrypt Mand embed the access policy pinto the\nciphertext Cp.pis often represented as a tree, where the\nnodes correspond to logical operators such as AND, OR, and\nthreshold gates.\n(3)KeyGen (SK ←E1.keyGen (PK,A)): generate\nattribute-based secret key SK byPK and attribute A.\n(4)Decrypt (M′←E1.decrypt (PK, SK, C p)): attempt\nto decrypt the ciphertext Cpusing SK. If attributes Asatisfy\nthe access policy in the ciphertext, the decryption succeeds;\notherwise, it fails.\nAdvanced Encryption Standard - Counter Mode (AES-\nCTR). AES-CTR [40], [41] is a widely used symmetric\nencryption mode that transforms AES into a stream cipher.\nBy combining a unique nonce and an incrementing counter,\nAES-CTR generates a keystream derived from the master\nkey, which is then XORed with the plaintext for encryption\nor with the ciphertext for decryption. It is highly efficient,\nparallelizable, and supports random access to encrypted data,\nmaking it suitable for applications.\nIII. O VERVIEW\nA. Threat Model\nAs shown in Figure 1, our system consists of two primary\nentities: the Model Developer andModel Deployer .\nModel Developer . Model developers train LLMs with the\ncapability to perform a wide range of tasks, and then distribute\nthese well-trained models to model deployers for local deploy-\nment and use. The training process may involve high-value\nor diverse datasets, as well as proprietary architectures and\ntraining methodologies. Consequently, developers may seek to\ndistribute these models under controlled conditions for local\ndeployment, specifically by defining task-level access policies\ntailored to different deployers to ensure the proper use of their\nLLMs.\nModel Deployer . The model deployer, in this context,\nacts as the adversary. Different deployers have dynamic and\ncustomized deployment requirements, often a specific task\nsubset of the LLM. They have access to a locally deployed\n(fully white-box) LLM with authorization for some tasks\n(Partially Authorized Users) or none at all (Passive At-\ntacker). Any capabilities of LLMs that exceed the authorized\nscope of deployers pose the risk of abuse through the following\nmalicious behaviours:\n•Unauthorized Task Abuse aims to illegally invoke the\nLLM to perform unauthorized tasks.\n•Training Data Extraction represents a more advanced\nattacker aiming to infer unauthorized training data from the\nmodel outputs. We specifically consider two common training\n--- Page 5 ---\nFig. 4: Overview of S ECNEURON framework. S ECNEURON (blue part) serves as a plug-and-play secure tool in model\ndistribution and local deployment pipeline. When permissions change, developers can adjust the capability of the local LLM\nby only simple key exchanges (red dotted line) with O(1)complexity ( ms−level computation and B−level transmission\noverhead), eliminating re-encrypt and re-transmit LLM with O(M)complexity ( mins−level computation and GB−level\ntransmission overhead).\ndata extraction attacks for LLMs: membership inference [42]\nand PII extraction [43].\nUsage Scope. SECNEURON offers a secure framework\nfor controlled model distribution and local deployment that\nmitigates potential abuse of locally deployed LLMs. Ensur-\ning security during runtime from parameter theft has been\nextensively studied [25], [26]. S ECNEURON is orthogonal to\nthem and can combine for more comprehensive protection\n(§8). S ECNEURON aims to prevent users from abusing LLM\ncapabilities for which they lack authorization and can resist\ncollusion, i.e., performing unauthorized tasks by combining\npartially authorized keys or permissions.. The reconstruction\nof full model capabilities by collusive users who possess\nauthorization for all tasks falls outside the scope of this\nstudy, as such attacks stem from underlying access policy\nvulnerabilities (Details in §5.3).\nB. Design Goals and Formulation\nLLMs are fine-tuned using specific training datasets to\nenable specific downstream tasks. Based on this, we divide\ntasksTaccording to the training datasets D, meaning that\ndifferent datasets Dtcorrespond to different tasks t. The\nprotection scope of S ECNEURON can be summarized as a\ntuple <D,T>, i.e., the high-value datasets and the model\ncapabilities trained on them, restricting them from being\nunauthorized abuse and extraction.\nGiven a well-trained LLM Mand a set of tasks T,\nSECNEURON achieves secure and controllable distribution and\ndeployment through encryption and selective decryption of\nneurons in LLM:\nDevelopers define access policy P and encrypt\nLLM using S ECNEURON (Algorithm 1: CM,CP//K←\nSECNEURON .Enc(M,T,P));\nDeployers decrypt the encrypted model based\non their attribute A (Algorithm 4: MA←\nSECNEURON .Dec(CM,CP//K,A)).\nMAneeds to meet the following objectives:1) Preserve the performance of authorized tasks. For any\ntasktin the authorized task set TA, the performance of MA\nis essentially consistent with that of M.\n∀t∈ TA,Pt(M)− Pt(MA)< γt, (1)\nγtis a small constant, where smaller indicate greater similarity\nin performance. Pt(M)represents the performance (generat-\ning correct next tokens) on task tforM, which, in this paper,\nis measured using Accuracy (Eqution 7).\n2) Limit the capabilities of unauthorized tasks. For task t\nin the unauthorized task set TU, performance of MAshould\nbe limited.\n∀t∈ TU,Pt(MA)< δt, (2)\nδtrepresents a small positive number that defines the upper\nbound of the model’s performance on unauthorized tasks.\n3) Prevent extraction of unauthorized training data. For task\ntin unauthorized task set TU, prevent the extraction of Dtfrom\nMA:\n∀t∈ TU, MIA (MA, Dt)< ϵ, PII (MA, Dt)< ϵ, (3)\nMIA (·)(Membership Inference Attack) [42] and PII(·)\n(Personally Identifiable Information Attack) [43] refer to the\nattack success rate of two commonly used training data\nextraction methods. ϵis a small positive number that limits\nthe success rate.\nDevelopers can adjust γt, δt, ϵaccording to the value or\nsecurity requirements of the task t(data). For example, for\nhighly sensitive and valuable Health tasks, a smaller δtcan\nbe set, whereas, for Story task, the restrictions on δtcan be\nrelaxed.\nSECNEURON must also fulfill the following practicality\nobjectives:\n1) Low Overhead. Encryption and decryption must have\nefficient computational overhead for plenty of neurons, and the\nencrypted model should not introduce excessive transmission\noverhead.\n2) Flexibility. SECNEURON should support complex per-\nmission configurations, allowing locally downloaded LLMs to\n--- Page 6 ---\nachieve efficient capability adjustments based on the permis-\nsion of deployers.\nC. Challenges and Solutions\n•C1: How to ensure that capabilities on unauthorized\ntasks are limited without disrupting the authorized tasks?\nDeactivating or pruning neurons corresponding to unautho-\nrized tasks seems like an effective way to limit capability on\nthese tasks. However, modern LLMs are inherently multitask\nsystems designed to handle a wide range of tasks by leveraging\nshared neuron representations, meaning multiple tasks may\nactivate the same neurons. Existing pruning methods [22]–\n[24] overlook the coupling of neurons across different tasks.\nSpecifically, important neurons for unauthorized tasks may\nalso partially contribute to authorized tasks. Naively pruning\nthem could unintentionally degrade the performance of autho-\nrized tasks. Similarly, focusing solely on preserving important\nneurons for the target task does not guarantee restriction of\nunauthorized tasks (Figure 3).\nSolution. Recognizing that perfect, mutually exclusive neu-\nron isolation for each task is often impractical, our approach\nfocuses on managing and mitigating the effects of neuron\ncoupling. To achieve this, we first introduce the penalty factor\nλto enhance the decoupling of task-specific neurons [13]\n(critical for target tasks but insignificant for others). However,\noverlapping neurons across different tasks are sometimes un-\navoidable. Thus, we embed the control of overlapping neurons\ninto the access tree of CP-ABE (Figure 6), eliminating the\nneed for additional management.\n•C2: How to enable local LLMs adaptable to customized\ndeployer permission with minimal overhead?\nDeployer permissions are dynamic and require flexible capa-\nbility adjustment for local LLMs. For example, when children\nuse LLMs, it is necessary to limit the capability of social\ncontent generation. Existing solutions for customized LLMs\nare irreversible, imposing prohibitive overhead on develop-\ners and deployers: Developers must maintain multiple task-\nspecific model versions (including encryption, storage, and\ntransmission), and deployers must repeatedly download and\ndeploy the corresponding LLM versions. As the complexity\nof permission combinations increases, the management cost\nand overhead grow linearly. While task-specific distillation or\npruning [44]–[46] can reduce the cost of a single transmis-\nsion, the cumulative cost of multiple transmissions remains\nsignificant.\nSolution. From the cryptographic perspective, we propose\na reversible capability limitation mechanism for customized\nLLM needs: encrypting neurons limits capability on certain\ntasks while decrypting them restores the capability. Specifi-\ncally, we introduce a hybrid encryption mechanism to handle\nthe vast number of parameters in LLMs, balancing the flexi-\nbility of CP-ABE and the efficiency of AES:\n1)Policy Layer (CP-ABE Encryptor ): Assigns AES keys to\nneurons based on their task relevance and binds access policies\nto these keys. Then, CP-ABE is used to encrypt and manage\nthem.Algorithm 1: Encryptor: S ECNEURON .Enc()\nData: Original Model: M, Task List: T, Policy P\nResult: Encrypted Model: CM,Policy &Key\nEncrypted by CP-ABE: CP//K\n1CP//K← ∅,CM← M ;\n2Create CP-ABE Cryptor E1and AES Cryptor E2;\n3▶Neuron Selector\n4Select important neurons for each task:\nS ← SECNEURON .select (M,T);\n5▶CP-ABE Encryptor:\nEncABE (PK,P//K)→ CP//K\n6CP-ABE Init.: PK, MSK ←E1.setup ();\n7Decompose all possible combinations of Sinto\ndisjoint subsets: U ← {T\nt∈T′St\\S\nt∈T \\T′St:T′⊆\nT,T′̸=∅} ∪ { Ω\\S\nt∈TSt};\n8foreach subset N ∈ U do\n9 Select a random element kgtfrom the target group\nGT;\n10 Create access policy pforkgtbased on P;\n11 Encrypt kgtandpusing PK byE1:\nc←E1.encrypt (PK, k gt, p);\n12 CP//K← CP//K∪c;\n13▶AES Encryptor: EncAES (K,M)→ CM\n14 Generate 64-bit AES key kfrom kgt;\n15 foreach neuron n∈ N do\n16 Encrypt Musing kbyE2:\nCM\nn←SECNEURON .encrypt (k,M, n, E 2)\n17return CM,CP//K;\n2)Execution Layer (AES Encryptor ): Uses AES-CTR to\nencrypt neuron parameters in LLMs, with AES keys for each\nneuron generated and managed by Policy Layer .\nDeployers download the complete encrypted model and\nthen obtain the authorized keys based on their attributes\n(permissions) to decrypt and utilize permitted tasks adaptively.\nIV. S ECNEURON DESIGN\nAs shown in Figure 4, S ECNEURON has two compo-\nnents: the Encryptor (Algorithm 1: S ECNEURON .Enc())\nfor the model developer and the Decryptor (Algorithm 4:\nSECNEURON .Dec()) for the model deployer. The Encryptor\nis executed only once to generate an encrypted version of the\nLLM. Different deployers can adaptively use the Decryptor to\naccess the authorized portions of the encrypted model based\non their permissions.\nA. Encryptor for Model Developer\nNeuron Selector. The Neuron Selector is used to identify\ntask-specific neurons (important only for the target task but not\nfor other tasks) for each task in T(Lines 3-4 in Algorithm 1).\nReferenced from [13], we use the mean of absolute activation\nto calculate the importance of each neuron, and then we\nintroduce λas a penalty factor to calculate the task-specific\nscore. Let nbe a neuron and denote its activations by zn.\n--- Page 7 ---\nAlgorithm 2: SECNEURON .select ()\nData: Original Model: M, Task List: T, Importance\nScore Function S, Importance Threshold: τ\nResult: Selected Neurons: S\n1S ← ∅ ;\n2foreach taskt∈ T do\n3St← ∅;\n4 Calculate the task-specific score for each neuron n:\nsn←S(T, t, n);\n5 Normalize and Sort sin descending order;\n6 Select Neurons:\n7St← S t.append (n)if\nSt.sum () +sn< τ·s.sum ();\n8S ← S ∪ S t;\n9return S\nGiven a task t∈ T and its sampled training dataset Dt, we\ndefine task-specific scoring function Sas:\nI(t, n) :=P\nd∈Dtzn(d)\n|Dt|, (4)\nS(T, t, n) :=I(t, n)−λ·max\nt′∈T,t′̸=tI(t′, n) (5)\nThe larger the value of S, the more important neuron nis\nspecific for task t. Therefore, we select neurons from the\nlargest Svalue for each task, continuing until the cumulative\nsum exceeds the threshold τ.The calculation of I(t, n)\ncan adopt other effective neuron importance estimation\nmethods. In particular, the neuron selection algorithm is\nshown in Algorithm 2.\nCP-ABE Encryptor (Policy Layer). CP-ABE Encryptor\nis responsible for key management and does not directly\nparticipate in model encryption (Lines 5-12 in Algorithm 1).\nThe process involves three key steps: CP-ABE Init. ,AES Key\n& Policy Gen. andCP-ABE Enc. :\n1) CP-ABE Init. Directly invoke the setup mechanism of\nCP-ABE to generate a public key PK and a master secret\nkeyMSK , which are used to derive attribute-based secret\nkeysSK for users.\n2) AES Key & Policy Gen. For selected important neurons S,\nwe decompose them into multiple disjoint subsets to address\nthe overlap between task-specific neurons of different tasks\n(the overlapping neurons are treated as a separate subset.\nFor example, neurons from t1, neurons from t2, and neurons\nshared by both t1andt2are decomposed into distinct subsets\nU={St1,St2,St1∩ St2}) (Line 7 in Algorithm 1). For each\nsubset N ∈ U , we randomly select an kgtfrom GTgroup\nof CP-ABE to generate the AES key k, which serves as the\nencryption key for all neurons in that subset (Figure 5). At\nthe same time, the access policy tree is constructed, assigning\npolicy pto each key (Line 8-10 in Algorithm 1). Furthermore,\nneurons that are not selected by any task are designated as\na separate common subset. These common neurons are also\nassigned an encryption keys with the access policy that permits\nFig. 5: Illustration of AES key assignment.\nCode Health Math\nDeveloper Deployer\nInst. Hospital Lic. True…\n…  …Neur on-Level\nPolicy\nUser -Level Policy\nFig. 6: Access policy tree for the CP-ABE encryptor, ∧repre-\nsents logical AND, while ∨represents logical OR. An example\nof authorized access is highlighted in red: Deployers meet\nInstitution =Hospital and Licence =True are allowed\nto use the Health task of the LLM. They can access keys such\nas{Key H,Key (H∩C),...}to decrypt the corresponding\nneurons.\nany authorized task to access them, thereby enhancing security\nagainst passive attackers.\nAccess to overlapping neurons is also integrated as part\nof the policy tree, eliminating the need for additional man-\nagement steps. Specifically, we divide the policy tree into\ntwo layers based on tasks (Figure 6): Neuron-level Policy\nand User-level Policy. To minimize the impact on authorized\ntasks, Neuron-level Policy employs ORnodes to manage the\noverlap of neurons and is a built-in, immutable mechanism.\nUser-level Policy specifies the access rights of deployers with\ndifferent attributes for each task, allowing developers to adjust\nthe policy flexibly according to specific requirements.\n3) CP-ABE Enc. Encrypt all generate kgtand embed the\npolicy pinto the ciphertext, with directly using the CP-ABE\nencryption function E1.encrypt (·).\nAES Encryptor (Execution Layer). The AES Encryptor is\nused for encrypting each neuron in the LLM model by AES-\nCTR (Lines 13-16 in Algorithm 1).\nGiven a neuron nand its input xn, the neuron feedforward\nprocess (MLP layer) can be summarized as:\nx′\nn=Mn.WOUT·σ(Mn.WIN·xn+Mn.BIN), (6)\nwhere WINandBINare the input weight and bias, respec-\ntively, σ(·)is the activation function (e.g., ReLU or Sigmoid),\nWOUT represents the output weight. As shown in Algorithm\n3, AES Encryptor simultaneously encrypts the parameters\n--- Page 8 ---\nAlgorithm 3: SECNEURON .encrypt ()\nData: Encrypt Key: k, Original Model: M, Neuron\nIndex: n, AES Cryptor E2\nResult: Encrypted Neuron parameters: CM\nn\n1CM\nn← M n;\n2Init CTR Mode: E2.MODE ←CTR,\nE2.COUNTER ←n;\n3Encrypt WIN:\nCM\nn.WIN\nn=E2.encrypt (k,Mn.WIN);\n4Encrypt WOUT :\nCM\nn.WOUT\nn =E2.encrypt (k,Mn.WOUT);\n5Encrypt BIN:CM\nn.BIN\nn=E2.encrypt (k,Mn.BIN);\n6return CM\nn\nMn.WOUT ,Mn.WINandMn.BINto encrypt a single\nneuron n(with the same encrypt key). Moreover, to ensure\nthat the decryption side can randomly decrypt any neuron, we\nutilize the AES encryption in CTR mode ( E2.encrypt (·)) and\npassnas the counter value.\nB. Decryptor for Model Deployer\nCP-ABE Decryptor (Policy Layer) . The CP-ABE De-\ncryptor derives the authorized AES decryption key based on\nattributes of deployers (lines 2-9 in Algorithm 4). Firstly, the\ndeployer requests an attribute-based secret key SK based on\ntheir attributes A. Then, SK is used to decrypt CP//Kto\nobtain the authorized kgt(the access policy is already em-\nbedded in the ciphertext, so the CP-ABE decryption function\nE1.decrypt (·)can be directly invoked without explicit per-\nmission checks). Finally, S ECNEURON convert each correctly\ndecrypted kgtto AES key kfor subsequent AES decryption.\nAES Decryptor (Policy Layer) . AES Decryptor is used\nto decrypt model parameters. We provide two decryption\nmechanisms: Transmission-efficient decryption (T-E dec.) and\nComputation-efficient decryption (C-E dec.) . Algorithm 4 uses\nT-E dec. as an example.\n•Transmission-efficient decryption (T-E dec.): Transmit\nonly the encrypted LLM (identical size to original LLM) and\nCP-ABE ciphertext once, eliminating additional transmission\noverhead but requiring decryption of the entire LLM.\nSince there is no metadata assistance, AES Decryptor at-\ntempts to decrypt each neuron using all authorized keys (Lines\n10-17 in Algorithm 4), requiring verification of whether neu-\nrons have been correctly decrypted (Line 15 in Algorithm 4).\nAES encryption operates at the byte stream level, causing neu-\nrons of different data types to exhibit distinctive characteristics\nafter encryption due to their varied byte-level serialization\npatterns in memory (Figure 14). Consequently, we propose\nan efficient undecrypted neuron detection mechanism for two\npredominant parameter types in LLM: INT andFLOAT .\nFLOAT16. Encryption of FLOAT16 parameters may affect\nthe ’exponent’ bits, leading to extremely large outlier val-\nues(216). Such anomalous values are absent in well-trained\nLLMs. Therefore, by detecting these outliers in the neuronAlgorithm 4: Decryptor: S ECNEURON .Dec()\nData: Encrypted Model: CM, Policy &Key Encrypted\nby CP-ABE: CP//K, Attribute List: A, Public\nkey:PK\nResult: Decrypted Model: MA\n1Create CP-ABE Cryptor E1and AES Cryptor E2;\n2▶CP-ABE Decryptor:\nDecABE (PK,CP//K,A)→ K′\n3Request attribute-based secret key SK using PK and\nA;\n4Init authorized keys: K′← ∅;\n5foreach ciphertext c∈ CP//Kdo\n6 Decrypt cusing PK andSK byE1:\nkgt←E1.decrypt (PK, SK, c );\n7 ifkgtis correct decrypted then\n8 Generate 64-bit AES key kfrom kgt;\n9 K′← K′∪k;\n10▶AES Decryptor: DecAES (K′,CM)→ MA\n11MA←0;\n12foreach neuron nofCMdo\n13 foreach keyk∈ K′do\n14 Decrypt CM\nnusing kbyE2:\nm←SECNEURON .decrypt (k,CM, n, E 2);\n15 ifmis correct decrypted then\n16 MA\nn←m;\n17 break ;\n18▶Adaptive Pruner\n19 ifMA\nn== 0 then\n20 Prune neuron nfromMA;\n21return MA;\nFig. 7: The distribution ( WIN) of encrypted neurons exhibits\nnotable differences compared to original neurons.\nparameters, we can determine whether it has been decrypted\n(the same with FLOAT32 ).\nINT8. The range of INT8 model parameters lies between\n[-128, 127] and remains invariant even after encryption, pre-\ncluding the use of outliers for detection. After encryption, the\ndata follows a uniform distribution, whereas the parameters of\na trained model exhibit a specific, non-uniform distribution.\nTherefore, we can determine whether a neuron has been\ndecrypted by its distribution pattern (Figure 7).\n--- Page 9 ---\nTABLE II: Computational & Transmission Complexity.\nFirst Deployment Capability Update\nComputational Transmission Computational Transmission\nEncryptor O(M) O(M) O(1) O(1)\nT-E Dec. O(|K′| · M )∗O(M) O(|K′| · M )∗O(1)\nC-E Dec.†O(Mt) O(M+Nneuron ) O(Mt) O(1)\n1: CP-ABE is used for encrypting and managing keys, with overhead sig-\nnificantly smaller than the processing of LLMs. We disregard its overhead\nto simplify the complexity analysis.\n2:∗represents the worst-case complexity, |K′|refers to number of autho-\nrized keys.\n3†:Nneuron refers to the number of neurons, Nneuron <<M(for a\n6.7B LLM, the size of Nneuron is only around 500KB.); Mtrefers to\nauthorized portion of LLM for task t.\nSpecifically, for FLOAT-type neurons, we select the max-\nimum value of their input matrix Mn.WINas the metric\n(m); for INT-type neurons, we use the variance of the sta-\ntistical histogram of Mn.WINas the metric ( vH). The final\ndetermination of whether a neuron is correctly decrypted is\nmade through threshold comparison. We tested the detection\neffectiveness across different models and achieved a success\nrate of 100% in all cases (§6.5).\n•Computation-efficient decryption (C-E dec.): Transmit\nadditional metadata indicating which key each neuron uses,\nonly decrypting the authorized neurons.\nWhen deploying, users download the LLM along with\nmetadata Fthat indicates the AES key used for each neuron.\nThe size of Fequals the number of neurons with transmission\noverhead smaller than the LLM itself. During decryption, each\nneuron first retrieves its key based on Fand then determines\nwhether it is authorized (whether CP-ABE can decrypt it). If\nauthorized, the neuron is decrypted; otherwise, it undergoes\nadaptive pruning. When permissions change, only neurons\nwith changed permissions need to be adjusted. For single-task\ndecryption, this approach can reduce decryption overhead by\n40%. Table II analyzes the computational and transmission\ncomplexities of different decrypt methods.\nAdaptive Pruner. The Adaptive Pruner prunes all unau-\nthorized neurons (lines 18-20 in Algorithm 4), accelerating\nthe inference of locally deployed LLMs without affecting the\nperformance on authorized tasks.\nV. S ECURITY ANALYSIS\nIn this section, we analyzed the security of S ECNEURON .\nFirst, we defined the Task Controllability Principle to ensure\nthat all tasks can be effectively protected and proved the Task\nCapacity Upper Bound for a given LLM, a necessary but\ninsufficient condition for judging whether a task configuration\nis reasonable (§5.1). Then, we proved that S ECNEURON\nsatisfies IND-CPA Security (§5.2) and Collusion Resistance\nSecurity (§5.3).\nA. Task Controllability Principle\nWhen serving as a separate unauthorised task, any task in\nTneeds to satisfy Equation 2, meaning each task requires a\nsufficient number of task-specific neurons with no intersectionwith other tasks. To achieve this goal, we define the Task\nControllability Principle.\nDEFINITION 5.1.(Task Controllability Principle) Given\na LLM Mand tasks T, for each task t∈ T, there exists a\nneuron set S′\nt⊆ Stsatisfying the following conditions:\n1) For any two different tasks t1, t2∈ T satisfying: S′\nt1∩\nS′\nt2=∅;\n2) Removing S′\ntcauses the performance of task tto fall\nbelow the target threshold: Pt(M \\ S′\nt)< δt,Pt(·)refers to\nperformance on t.\nIf satisfying the Task Controllability Principle, the task\nsetTis controllable, meaning any task t∈ T can be\nconstrained to perform within its target threshold δt. Notably,\nthe Task Controllability Principle should not be interpreted as\nan assumption of absolute physical separation for all neurons\ninvolved in a task, but rather as a controllability principle\nthat defines how specific task capabilities can be effectively\nconstrained. S ECNEURON does not assume a priori that all\nneurons across different tasks are isolatable, but rather seeks to\nconstruct such effective isolation neuron sets. Despite general\nneuron entanglement in large LLMs, their large-scale neural\narchitectures allow us to readily pinpoint neuron subsets for\neach task(comprising approximately 15%) that satisfy the Task\nControllability Principle. However, for a given model M, the\nnumber of tasks it can handle is not unlimited. We further\npropose and prove the Task Capacity Upper Bound theorem,\nwhich establishes a necessary but not sufficient condition to\ndetermine whether Mcan effectively manage all tasks in set\nT.\nTheorem 1. (Task Capacity Upper Bound) For the LLM M\nand a task set Tto satisfy the Task Controllability Principle,\nit is necessary to ensure:P\nt∈T|Ct| ≤ |M| .\nSee Appendix X for the proof. | · | refers to the number\nof neurons, Ctis the minimal critical neuron set for task\nt, defined as the smallest set of neurons whose removal\ncauses the model’s performance to fall below δt. In practical\nimplementations, this can be approximated using a greedy\nstrategy, where neurons are pruned in descending order of\nimportance until the target threshold is met.\nB. IND-CPA Security\nTheorem 2. If CP-ABE and AES-CTR schemes utilized in\nSECNEURON are IND-CPA secure, then SECNEURON is IND-\nCPA secure.\nProof. Assume that SecNeuron is not IND-CPA secure. This\nmeans there exists an adversary Awho can distinguish be-\ntween ciphertexts of two plaintexts with a non-negligible\nadvantage.\nSECNEURON consists of two cryptographic components:\n1)CP-ABE encryptor for task-specific keys under access poli-\ncies; 2)AES-CTR encryptor for neurons using keys derived\nfrom the CP-ABE.\nIfAsuccessfully distinguishes ciphertexts, we can construct\na reduction that breaks either: 1)The IND-CPA security of\n--- Page 10 ---\nCP-ABE (by using Ato distinguish CP-ABE encrypted task\nkeys), or 2)The IND-CPA security of AES-CTR (by using A\nto distinguish AES-CTR encrypted neurons).\nEither case contradicts our assumption that both schemes\nare IND-CPA secure. Therefore, SecNeuron must be IND-CPA\nsecure.\nC. Collusion Resistance Security\nTheorem 3. If CP-ABE (including its GTgroup) and AES-\nCTR encryption schemes are secure and the Task Control-\nlability Principle is satisfied, then SecNeuron is resistant to\ncollusion attacks.\nProof. Assume that S ECNEURON is not resistant to collusion\nattacks, which implies that a group of attackers (multiple users\nwith different authorized tasks) can collude to use LLMs for\ntasks that none of them individually has permission to access.\nCollusion attacks may take the following forms: 1)combine\ntheir respective keys to derive an AES key for an unauthorized\ntask; 2)combine their privileges to break the encryption of an-\nother unauthorized task; 3)leverage accessibly coupled neurons\nto perform unauthorized tasks. For example, consider an attack\ngroup authorized for tasks t1andt3. They have access to the\ncoupled neurons {St1∩St2,St2∩St3,St1∩St2∩St3}, and they\nwish to leverage these neurons to perform the unauthorized\ntaskt2.\nFor 1), S ECNEURON randomly selects keys from the GT\ngroup of CP-ABE, ensuring independence and unpredictability\nbetween task keys. This means that even if attackers obtain\nkeys for multiple authorized tasks, they cannot derive any other\nkeys because there is no mathematical correlation between\ndifferent keys.\nFor 2), attackers would need to break either the CP-ABE or\nAES-CTR encryptor. Even if attackers have access privileges\nto multiple authorized tasks, according to the security of CP-\nABE, they cannot decrypt ciphertexts that do not satisfy their\naccess structures.\nFor 3), due to the Task Controllability Principle, for any\ntasktthere must exist a non-overlapping neuron set S′\ntthat\ncannot be accessed through coupled keys, and S′\ntis sufficient\nto render tunusable.\nAny successful collusion would contradict our security as-\nsumptions. Hence, S ECNEURON must be resistant to collusion\nattacks.\nAnother potential collusion scenario involves users with\nauthorized t1only and users with authorized t2only collab-\norating to utilize both t1andt2tasks jointly. Even worse, a\ngroup of attackers who collectively possess authorization for\nall tasks can collude and recover the original model. However,\nit extends beyond S ECNEURON ’s primary threat model, which\nfocuses on preventing users from accessing functionalities for\nwhich they lack authorization. If such operations need to\nbe prohibited, the restriction should be explicitly defined in\nthe access control policy or combined with methods such asTABLE III: Summary of Tasks and Datasets.\nTask Dataset on Hugging Face Train Data Extraction\nCode codeparrot/github-code-clean ✘\nHealth enelpol/rag-mini-bioasq-qas-clean ✘\nEmail LLM-PBE/enron-email PII Extraction\nStory roneneldan/TinyStories ✘\nMath camel-ai/physics ✘\nArxiv haritzpuerto/the pile 00arxiv Membership Inference\nImageNet∗ILSVRC/imagenet-1k ✘\n1: ImageNet is divided into 4 subcategories, serving as 4 distinct\ntasks ( Animals, Plants & landscapes, Food, Transportation ).\nTEE, rather than relying solely on S ECNEURON to implement\nlogical isolation (§8).\nVI. E VALUATION\nA. Implementation\nWe implemented S ECNEURON based on the Charm [47]\n(CP-ABE Cryptor) and Crypto [48] (AES Cryptor) libraries.\nSECNEURON uses Cython [49] to accelerate loop operations\nfor Python, all stream encryption is performed on the CPU\nand supports parallel operations. Additionally, we use mean\nabsolute activation to evaluate neuron importance, but this\nis not necessarily the optimal choice. Any other efficient\nmechanism for quantifying neuron importance can be used\nto enhance the effectiveness of S ECNEURON .\nB. Experimental Setup\nDatasets & Tasks. We evaluated S ECNEURON across\nmultiple datasets from different domains, with each dataset\ncorresponding to a specific domain task (as summarized in\nTable III), including Code, Story, Email, Health, and Arxiv.\nNotably, the Email dataset was also used to test PII extraction\nfollowing [43], while the Arxiv dataset was used to assess\nmembership inference attacks following [42].\nLLMs. We tested various LLMs with different architec-\ntures and parameter scales, including OPT [50] (OPT-6.7b,\nOPT-30b), Galactica [51] (Galactica-6.7b, Galactica-30b), and\nGemma-2 [52] (Gemma-2-9b, Gemma-2-27b). Furthermore,\nwe selected the image-based model Vit-Base-Patch16 [53] to\ndemonstrate the wide-ranging applications of S ECNEURON . It\nis important to note that S ECNEURON functions primarily as\nan encryption mechanism, independent of specific models or\nimportance selection methods.\nC. Overall Performance\nWe validate S ECNEURON at task level and data level:\nTask Level. SECNEURON aims to dismantle the capability\nitself. If the model cannot even predict the correct tokens for\na task, it demonstrates a more fundamental incapacitation than\nsimply outputting a refusal message. Thus, unless otherwise\nspecified, all tasks are considered prediction tasks, and the\nperformance is evaluated using Accuracy . For a given task t,\nits accuracy is calculated by Equation (7):\nAccuracy t=P\nx∈D′\ntCorrectTokens (x)\nP\nx∈D′\ntTotalTokens (x), (7)\n--- Page 11 ---\n(a) LLMs Configured with Story and Health Tasks.\n(b) LLMs Configured with Code and Health Tasks\nFig. 8: Effectiveness of Task-Level Capabilities Control: limiting unauthorized tasks while preserving authorized ones. Admin is\nequivalent to the baseline model performance without S ECNEURON . Notably, although some LLMs demonstrate high accuracy\non unauthorized code tasks (primarily due to elevated baseline performance), they can no longer effectively complete coding\nwork (Figure 12).\nD′\ntrepresents the test dataset for task t. Disabling Accuracy\nof a specific task to 0is almost impossible because LLMs\nare trained on vast amounts of textual data and possess a\ngeneral ability to predict the next token. Therefore, the task\nis considered unusable when the Accuracy of a task tfalls\nbelow a threshold δt. As shown in Figure 12, even though\nAccuracy Code remains above 35%, it is no longer capable of\ngenerating meaningful code.\nSECNEURON effectively limits LLM capabilities on unau-\nthorized tasks without significantly compromising authorized\ntasks. Figure 8 evaluates the effectiveness of S ECNEURON\nacross two task setting LLM (Code VS Health and Story VS\nHealth) with four permission levels: Admin (full access to all\ntasks), [Task ]-only User (Partially Authorized Deployers with\naccess limited to a specific task [Task ]), and Passive Attackers\n(without any permissions).\n•For Admin users, the decrypted LLM maintains full\naccuracy across all tasks without any performance degradation,\neffectively preserving the model’s utility.\n•For Passive Attackers, S ECNEURON provides robust pro-\ntection, resulting in 0% accuracy across all tasks. Passive\nAttackers would need to perform an exhaustive search of 2128\n(length of AES key) combinations to gain access to any task\nof LLM.\n•For[Task ]-only Deployer, the partially decrypted LLM\nmaintains accuracy within 2%of the original performance on\nauthorized tasks. Conversely, accuracy decreases by more than\n40% or falls below 25% (10% for OPT) for unauthorized tasks,\nlimiting the model’s capabilities on unauthorized tasks and\nmitigating potential abuse. Furthermore, attempting to recover\ncapabilities for other tasks would also require an exhaustive\nsearch of 2128possibilities due to Collusion Resistance Secu-\nrity.\nData Level. SECNEURON successfully defends against PII\nExtraction attacks and MIA for unauthorized datasets.\nFig. 9: Effectiveness in Preventing Data-Level Abuse.\n•PII Extraction attacks. We utilized OPT-6.7b as the target\nLLM, setting Story as the authorized task while treating\nEmail and its associated dataset as unauthorized content. For\nevaluation, we employed PII inference described in [43]\nand adopted Succ RECON as assessment metric, where higher\naccuracy values indicate more severe leakage of training data.\nAs illustrated in Figure 9, S ECNEURON effectively reduces\ntheSucc RECON from about 30% to below 5%.\n•Membership Inference Attacks. We also employed OPT-\n6.7b as the target LLM, setting Story as the authorized task\nwhile treating arXiv and its associated dataset as unautho-\nrized content. For evaluation, we implemented collection-level\nMIA described in [42] as Membership Inference Attacks and\nadopted AUROC as assessment metric, where higher values\nindicate more successful attacks. As illustrated in Figure 9,\nSECNEURON effectively reduces the MIA AUROC to ap-\nproximately 50%, essentially rendering the attack equivalent\nto random guessing.\nMulti-task Flexibility. To verify the flexibility of S EC-\nNEURON , we further configured multiple tasks (Health, Email,\nCode, Math, Story) for one LLM and selected different Per-\nmission Lists (dynamic authorized combinations of different\ntask capabilities for one encrypted LLM) for testing. As shown\nin Table IV, even with multiple tasks, S ECNEURON effectively\n--- Page 12 ---\nTABLE IV: Multi-task Effectiveness with Dynamic Permissions: Selective Decryption Based on one Single Encrypted Model\nPermissions ListOPT-6.7b (Accuracy)Permissions ListGemma-2-27b (Accuracy)\nHealth Email Code Math Story Health Email Code Math Story\n[✔—✔—✔—✔—✔]†47.21% 60.74% 71.71% 65.76% 55.89% [✔—✔—✔—✔—✔]†53.99% 63.03% 81.99% 86.35% 60.08%\n[✘—✔—✔—✔—✔] 25.66% 60.57% 71.41% 65.54% 55.42% [✘—✔—✔—✔—✔] 28.45% 63.23% 80.89% 86.51% 60.51%\n[✔—✘—✘—✔—✔] 46.28% 28.88% 25.00% 62.06% 55.31% [✘—✘—✘—✔—✔] 21.64% 29.44% 36.73% 82.69% 55.95%\n[✔—✘—✘—✔—✘] 45.58% 28.01% 24.76% 62.23% 23.50% [✘—✔—✔—✘—✔] 27.0% 60.56% 78.20% 47.03% 59.25%\n[✔—✔—✔—✔—✘] 46.70% 59.58% 71.12% 65.82% 21.93% [✘—✔—✘—✔—✔] 27.76% 59.95% 50.75% 84.95% 59.41%\n[✘—✘—✘—✘—✘] 0.00% 0.00% 0.00% 0.00% 0.00% [✘—✘—✘—✘—✘] 0.00% 0.00% 0.00% 0.00% 0.00%\n1. For testing convenience, we use a fixed threshold τfor all tasks. In practice, τcan be adjusted based on the importance of different tasks to achieve\nbetter results. For example, a larger τcan be set for high-value or privacy-sensitive tasks such as Code or Health.\n2.†: Model performance under all task authorization is equivalent to the baseline performance without S ECNEURON ;✔: authorized task; ✘: unauthorized\ntask with accuracy represented by gray cells.\nFig. 10: Effectiveness of\nMitigating Malicious Code\nFig. 11: Comparison with\nNaive Pruning\nTABLE V: Detailed Overhead Measurements for OPT-6.7B.\nFirst Deployment Capability Update\nComputationa Transmission Computational Transmission\nEncryptor 136.65s 6.4GB+ 8.9KB 0.006s 694B\nT-E Dec. 167.48s 6.4GB+ 8.9KB 167.48s 694B\nC-E Dec. 44.87s 6.4GB+ 8.9KB+ 513 KB 44.87s 694B\nNaive Enc. 136.41s 6.4GB 136.41s 6.4GB\nrestricts unauthorized tasks while minimally impacting autho-\nrized ones. Notably, while our experiments use tasks as the\nbasic permission unit, S ECNEURON can be flexibly extended\nto different users (Authorize different tasks based on user\nattributes) in practical applications, as illustrated by the policy\ntree design (User Level Policy) in Figure 6.\nMitigating Abuse of Malicious Code Generation as An\nExample. Figure 12 illustrates a runtime example. We used a\npotential prompt that might be applied for ransomware gen-\neration to query the local LLM (Gemma-2-27b). The original\nLLM (Gemma-2 with full permissions) could generate code\nthat met the requirements accurately, implying that anyone\nwithout coding knowledge could easily leverage LLM to gen-\nerate potentially malicious code. After applying S ECNEURON\nto limit the code generation task, the LLM essentially lost\nits ability to generate code. Moreover, this operation does\nnot affect the authorized task (Math). Figure 10 compares\nthe accuracy of generating malicious code by malicious code\ndataset1, showing a significant reduction after applying S EC-\nNEURON .\nD. Overhead\nTable V presents detailed overhead measurements for the\nOPT-6.7B LLM with five tasks, S ECNEURON requires only an\n1Er1111c/Malicious code classification dataset in Hugging Face\nFig. 12: Examples of Gemma-2 with unauthorized Code\ntask and authorized Math Task. S ECNEURON limits the code\nCapability of LLM, preventing it from producing meaningful\ncode and thereby mitigating potential abuse.\nadditional 8.9KB CP-ABE ciphertext (additional 513.79KB\nforC-E dec. ) along with a 0.12sCP-ABE encryption overhead\nduring initial encryption. This process is executed only once.\nSubsequently, each capabilities change operation requires only\n0.006sfor key generation and 694BforSK transmission.\nThis overhead is nearly negligible compared to naive methods\nthat encrypt and transmit the entire model with each permis-\nsion change. Similarly, the decryption party only needs to\ndownload the complete encrypted LLM and CP-ABE cipher-\ntext once. Updating the capabilities of local LLM requires\ntransmitting only the SK(694B), while traditional approaches\nneed to re-distribute the entire LLM ( 6.4GB). The encryption\nand transmission overhead for updating LLM capability are\nindependent of the model itself, and the larger the model, the\ngreater the overhead savings S ECNEURON achieves. For C-E\nDec., only the corresponding neurons need decryption, while\nT-E Dec. requires attempting to decrypt using all authorized\nkeys.\nFurthermore, the Adaptive Pruner can dynamically reduce\n--- Page 13 ---\nFig. 13: Effect of Model Architecture and Size.\nTABLE VI: Effectiveness of Image-based Large models.\nPermissions ListViT-Base-Patch16\nAnimals Plants & Land. Food Transportation\n[✔—✔—✔—✔] 81.79% 84.35% 82.03% 84.51%\n[✔—✔—✘—✔] 80.69% 84.20% 16.75% 83.69%\n[✔—✔—✔—✘] 79.16% 83.50% 81.15% 23.27%\n[✔—✔—✘—✘] 78.04% 82.65% 17.49% 25.46%\n[✔—✘—✘—✔] 78.06% 33.69% 16.66% 82.84%\n[✘—✘—✘—✘] 0.00% 0.00% 0.00% 0.00%\nthe GPU memory during local execution. When disabled for\nindividual tasks, it can effectively prune approximately 12%\nof MLP neurons.\nE. Micro-Benchmarks\nEffectiveness of Task-specific Scoring. Figure 11 compares\nthe effectiveness of S ECNEURON (pruning by S) and the naive\npruning (pruning by I). We use ∆Accuracy for evaluation,\nwhere a smaller ∆Accuracy for authorized tasks (x-axis) and\na larger ∆Accuracy for unauthorized tasks (y-axis) indicate\nbetter performance. S ECNEURON outperforms naive pruning\nthanks to our task-specific neuron scoring.\nCross-modal Extension. We use ViT-Base-Patch16 to vali-\ndate the effectiveness of S ECNEURON on large-scale image\nmodels. Table VI presents the performance of the model\nunder different permission settings. Results demonstrate that\nSECNEURON is also effective for image-based LLMs.\nEffect of Model Size. Figure 13 evaluates the effectiveness\non different model sizes and architectures using ∆Accuracy .\nThe results demonstrate that S ECNEURON achieves better\nresults on models with more neurons N.\nEffectiveness of undecrypted neuron detection. Our de-\ntection mechanism can achieve 100% identification of unde-\ncrypted (incorrectly decrypted) neurons. Table VII summarizes\nthe statistical distribution ranges of vHandmfor all decrypted\nand undecrypted neurons across different model architectures.\nThere is a clear distinction between decrypted and undecrypted\nneurons, allowing us to set thresholds to fully distinguish them\neasily.\nVII. E THIC\nThis work uses only public datasets and focuses on de-\nsigning security mechanisms for the local LLMs. No human\nsubjects are involved, and no personal data is collected or\nprocessed during this research.TABLE VII: Range of vHandmfor different neurons.\nModelINT8 ( vH) FLOAT32 ( m)\nUndecrypted Decrypted Undecrypted Decrypted\nOPT [ 1.6−7,4.1−7] [ 1.4−5,3.1−5] [-inf,inf] [0.01,0.17]\nGalactic [ 1.5−7,3.3−7] [ 1.2−5,2.6−5] [-inf,inf] [0.01,0.64]\nGemma-2 [ 2.4−7,6.8−7] [ 2.4−5,7.1−5] [inf,inf] [0.01,0.35]\nGPT2 [ 3.4−7,7.4−7] [ 1.1−5,3.4−5] [-inf,inf] [0.06,1.10]\nVIT [ 7.5−7,1.4−6] [ 1.1−5,2.2−5] [4.936,3.438] [0.11,2.42]\nLLama [ 1.2−7,2.8−7] [1.3−5,2.7−5] [-inf,inf] [0.03,0.82]\nVIII. D ISCUSSION AND LIMITATION\nTEE Integration. SECNEURON is orthogonal to TEEs that\nsafeguard model parameters during runtime. It can integrate\nwith TEE, where the partially decrypted LLM reduced param-\neter size MAis more suitable to deploy within TEEs. This\nsetup not only protects the model’s parameters from being\nstolen but also prevents users from obtaining the complete\nmodel through multiple authorization attempts. Furthermore,\nall deployment-related keys, including attribute-based secret\nkeySK and authorized AES keys K′, can be stored within\nthe TEEs to enhance overall security.\nConfiguration of Tasks. SECNEURON seeks to manage\ntasks selected from different domains. Finer-grained task de-\ncomposition (such as distinguishing between Python Code\ntask and Java Code task) demonstrates limited practical util-\nity in real-world scenarios. These highly analogous tasks\nshould instead be treated as one task within the S ECNEURON\nframework. Besides, the number of unauthorized tasks that\nSECNEURON can simultaneously restrict is constrained, and\nTheorem 1 provides a theoretical foundation for understanding\nthis limitation. To formulate better access policies, developers\nare suggested to use neuron importance analysis tools for\ninitial task assessment (§5.1)\nHyperparameter Setting. A fixed τfor all tasks may\nnot yield optimal results for every task, as the importance\nof different tasks and the original accuracy on each task\ncan vary significantly. Although S ECNEURON supports setting\nindividual τvalues for each task, these configurations are\ncurrently based on empirical methods (larger τcan be set for\nhigh-value or sensitive tasks). In the future, more theoretical\nanalysis will be needed to guide the selection and optimization\nofτ.\nIX. C ONCLUSION\nIn this work, we proposed a new perspective to prevent\nabuse of locally deployed LLMs by integrating classic access\ncontrol policies with the intrinsic capabilities of LLMs. We\nimplemented S ECNEURON , a neuron encryption and selective\ndecryption mechanism for flexible and reliable abuse control\nlocal deployment of LLMs. With S ECNEURON , developers\ncan dynamically enforce restrictions on the capabilities of\nLLMs for unauthorized tasks without compromising autho-\nrized ones, even within deployer-controlled environments.\nExtensive evaluation showed that S ECNEURON effectively\nlimits LLM performance on unauthorized tasks (also prevents\nextraction of their training data) while supporting flexible and\nefficient capability updates.\n--- Page 14 ---\nFig. 14: Binary storage formats of FLOAT16 andINT8 .\nX. PROOF\nProof of T HEOREM 5.1.\nProof. For a given LLM Mand a set of tasks T, satisfying\nthe Neuron Isolation Principle requires:\nX\nt∈T|S′\nt|=|[\nt∈TS′\nt| ≤ |M| (8)\nWe select the smallest neuron set Smin\nt= arg min S⊆S′\nt|S|,\nthat satisfies the Neuron Isolation Principle. Thus:\nX\nt∈T|Smin\nt| ≤X\nt∈T|S′\nt| ≤ |M| (9)\nCtis defined as the smallest set of neurons without consid-\neration of the Neuron Isolation Principle, such that: Ct=\narg min S⊆S t|S|. Since S′\nt⊆ S t,Smin\nt is also a candidate\nsolution for Ct:\n|Ct| ≤ |Smin\nt|with:(\n|Ct|=|Smin\nt|,ifCt∩S\nt′̸=tC′\nt=∅,\n|Ct|<|Smin\nt|,otherwise.\n(10)\nThus:X\nt∈T|Ct| ≤X\nt∈T|Smin\nt| ≤X\nt∈T|S′\nt| ≤ |M| (11)\nFinally, we can prove the Task Capacity Upper BoundP\nt∈T|Ct| ≤ |M|\nREFERENCES\n[1] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, L. Hou,\nK. Clark, S. R. Pfohl, H. Cole-Lewis et al. , “Toward expert-level medical\nquestion answering with large language models,” Nature Medicine , pp.\n1–8, 2025.\n[2] K. Hau, S. Hassan, and S. Zhou, “Llms in mobile apps: Practices,\nchallenges, and opportunities,” arXiv preprint arXiv:2502.15908 , 2025.\n[3] Edge Evolve, “Building AI Security: The On-Premise Advan-\ntage.” https://www.edgeevolve.com/building-ai-security-the-on-premise-\nadvantage/, 2025-03-04.\n[4] B. P. Kumar and M. S. Ahmed, “Beyond clouds: Locally runnable llms\nas a secure solution for ai applications,” Digital Society , vol. 3, no. 3,\np. 49, 2024.\n[5] F. R. Elali and L. N. Rachid, “Ai-generated research\npaper fabrication and plagiarism in the scientific community,”\nPatterns , vol. 4, no. 3, p. 100706, 2023. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S2666389923000430\n[6] K. Wiggers, “How cybercriminals are using chatgpt to build hacking\ntools and ransomware,” VentureBeat , March 2023, accessed on March\n21, 2025. [Online]. Available: https://venturebeat.com/security/chatgpt-\nransomware-malware/[7] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan,\nY . Adi, J. Liu, R. Sauvestre, T. Remez, J. Rapin, A. Kozhevnikov,\nI. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,\nA. D ´efossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier,\nT. Scialom, and G. Synnaeve, “Code llama: Open foundation models\nfor code,” 2024. [Online]. Available: https://arxiv.org/abs/2308.12950\n[8] isarth, “Distill gpt-2 story generator,”\nhttps://huggingface.co/isarth/distill gpt2 story generator/discussions,\n2023, accessed: 2025-04-05.\n[9] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,\nJ. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe, “Training language models to\nfollow instructions with human feedback,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2203.02155\n[10] ——, “Training language models to follow instructions with human\nfeedback,” in Proceedings of the 36th International Conference on\nNeural Information Processing Systems , ser. NIPS ’22. Red Hook,\nNY , USA: Curran Associates Inc., 2022.\n[11] P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and\nD. Amodei, “Deep reinforcement learning from human preferences,” in\nProceedings of the 31st International Conference on Neural Information\nProcessing Systems , ser. NIPS’17. Red Hook, NY , USA: Curran\nAssociates Inc., 2017, p. 4302–4310.\n[12] J. Foster, S. Schoepf, and A. Brintrup, “Fast machine unlearning\nwithout retraining through selective synaptic dampening,” 2023.\n[Online]. Available: https://arxiv.org/abs/2308.07707\n[13] N. Pochinkov and N. Schoots, “Dissecting language models:\nMachine unlearning via selective pruning,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2403.01267\n[14] L. Bourtoule, V . Chandrasekaran, C. A. Choquette-Choo, H. Jia,\nA. Travers, B. Zhang, D. Lie, and N. Papernot, “Machine unlearning,”\nin2021 IEEE Symposium on Security and Privacy (SP) , 2021, pp. 141–\n159.\n[15] R. Zhang and F. Koushanfar, “Watermarking large language models\nand the generated content: Opportunities and challenges,” arXiv preprint\narXiv:2410.19096 , 2024.\n[16] Y . Liang, J. Xiao, W. Gan, and P. S. Yu, “Watermarking techniques\nfor large language models: A survey,” arXiv preprint arXiv:2409.00089 ,\n2024.\n[17] Y . Xu, A. Liu, X. Hu, L. Wen, and H. Xiong, “Mark your llm: Detecting\nthe misuse of open-source large language models via watermarking,”\narXiv preprint arXiv:2503.04636 , 2025.\n[18] P.-G. Ye, Z. Li, Z. Yang, P. Chen, Z. Zhang, N. Li, and J. Zheng,\n“Periodic watermarking for copyright protection of large language\nmodels in cloud computing security,” Computer Standards & Interfaces ,\np. 103983, 2025.\n[19] B. Cao, Y . Cao, L. Lin, and J. Chen, “Defending against alignment-\nbreaking attacks via robustly aligned LLM,” in Proceedings of\nthe 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , L.-W. Ku, A. Martins, and\nV . Srikumar, Eds. Bangkok, Thailand: Association for Computational\nLinguistics, Aug. 2024, pp. 10 542–10 560. [Online]. Available:\nhttps://aclanthology.org/2024.acl-long.568/\n[20] Y . Xie, M. Fang, R. Pi, and N. Gong, “GradSafe: Detecting\njailbreak prompts for LLMs via safety-critical gradient analysis,”\ninProceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , L.-W. Ku,\nA. Martins, and V . Srikumar, Eds. Bangkok, Thailand: Association\nfor Computational Linguistics, Aug. 2024, pp. 507–518. [Online].\nAvailable: https://aclanthology.org/2024.acl-long.30/\n[21] Z. Zhang, J. Yang, P. Ke, F. Mi, H. Wang, and M. Huang,\n“Defending large language models against jailbreaking attacks through\ngoal prioritization,” in Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\nL.-W. Ku, A. Martins, and V . Srikumar, Eds. Bangkok, Thailand:\nAssociation for Computational Linguistics, Aug. 2024, pp. 8865–8887.\n[Online]. Available: https://aclanthology.org/2024.acl-long.481/\n[22] R. R. Selvaraju, P. Chattopadhyay, M. Elhoseiny, T. Sharma, D. Batra,\nD. Parikh, and S. Lee, “Choose your neuron: Incorporating domain\nknowledge through neuron-importance,” in Proceedings of the European\nConference on Computer Vision (ECCV) , September 2018.\n[23] K. Liu, R. A. Amjad, and B. C. Geiger, “Understanding indi-\n--- Page 15 ---\nvidual neuron importance using information theory,” arXiv preprint\narXiv:1804.06679 , vol. 19, pp. 5171–5180, 2018.\n[24] R. Song, S. He, S. Jiang, Y . Xian, S. Gao, K. Liu, and Z. Yu, “Does\nlarge language model contain task-specific neurons?” in Proceedings\nof the 2024 Conference on Empirical Methods in Natural Language\nProcessing , 2024, pp. 7101–7113.\n[25] F. Mo, A. S. Shamsabadi, K. Katevas, S. Demetriou, I. Leontiadis,\nA. Cavallaro, and H. Haddadi, “Darknetz: towards model privacy at\nthe edge using trusted execution environments,” in Proceedings of\nthe 18th International Conference on Mobile Systems, Applications,\nand Services , ser. MobiSys ’20. New York, NY , USA: Association\nfor Computing Machinery, 2020, p. 161–174. [Online]. Available:\nhttps://doi.org/10.1145/3386901.3388946\n[26] Phala Network. (2024) Host llm in gpu tee. Phala Network Docs.\n[Online]. Available: https://docs.phala.network/llm-in-gpu-tee/llm-in-tee\n[27] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/1910.10683\n[28] R. Caruana, “Multitask learning,” Machine learning , vol. 28, pp. 41–75,\n1997.\n[29] W. W. Cohen, “Enron email dataset,” 2015. [Online]. Available:\nhttps://www.cs.cmu.edu/ enron/\n[30] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: how does llm\nsafety training fail?” in Proceedings of the 37th International Conference\non Neural Information Processing Systems , ser. NIPS ’23. Red Hook,\nNY , USA: Curran Associates Inc., 2023.\n[31] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson,\n“Universal and transferable adversarial attacks on aligned language\nmodels,” 2023. [Online]. Available: https://arxiv.org/abs/2307.15043\n[32] Z. Ba, J. Zhong, J. Lei, P. Cheng, Q. Wang, Z. Qin, Z. Wang,\nand K. Ren, “Surrogateprompt: Bypassing the safety filter of\ntext-to-image models via substitution,” in Proceedings of the 2024\non ACM SIGSAC Conference on Computer and Communications\nSecurity , ser. CCS ’24. New York, NY , USA: Association for\nComputing Machinery, 2024, p. 1166–1180. [Online]. Available:\nhttps://doi.org/10.1145/3658644.3690346\n[33] S. Szyller, B. G. Atli, S. Marchal, and N. Asokan, “Dawn:\nDynamic adversarial watermarking of neural networks,” 2021. [Online].\nAvailable: https://arxiv.org/abs/1906.00830\n[34] B. D. Rouhani, H. Chen, and F. Koushanfar, “Deepsigns: A generic\nwatermarking framework for ip protection of deep learning models,”\narXiv preprint arXiv:1804.00750 , 2018.\n[35] H. Chen, B. D. Rouhani, and F. Koushanfar, “Blackmarks: Black-\nbox multibit watermarking for deep neural networks,” arXiv preprint\narXiv:1904.00344 , 2019.\n[36] S. Abdelnabi and M. Fritz, “Adversarial watermarking transformer:\nTowards tracing text provenance with data hiding,” in 2021 IEEE\nSymposium on Security and Privacy (SP) . IEEE, 2021, pp. 121–140.\n[37] J. Rando, “Do not write that jailbreak paper,” in The\nFourth Blogpost Track at ICLR 2025 , 2025. [Online]. Available:\nhttps://openreview.net/forum?id=TbN25IjHyC\n[38] J. Bethencourt, A. Sahai, and B. Waters, “Ciphertext-policy attribute-\nbased encryption,” in 2007 IEEE Symposium on Security and Privacy\n(SP ’07) , 2007, pp. 321–334.\n[39] J. Tomida, Y . Kawahara, and R. Nishimaki, “Fast, compact, and\nexpressive attribute-based encryption,” Cryptology ePrint Archive, Paper\n2019/966, 2019. [Online]. Available: https://eprint.iacr.org/2019/966\n[40] D. Selent, “Advanced encryption standard,” Rivier Academic Journal ,\nvol. 6, no. 2, pp. 1–14, 2010.\n[41] J. Nechvatal, E. Barker, L. Bassham, W. Burr, M. Dworkin, J. Foti,\nand E. Roback, “Report on the development of the advanced encryption\nstandard (aes),” Journal of research of the National Institute of Standards\nand Technology , vol. 106, no. 3, p. 511, 2001.\n[42] H. Puerto, M. Gubri, S. Yun, and S. J. Oh, “Scaling up membership\ninference: When and how attacks succeed on large language models,”\n2024. [Online]. Available: https://arxiv.org/abs/2411.00154\n[43] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz,\nand S. Zanella-B ´eguelin, “Analyzing leakage of personally\nidentifiable information in language models,” in 2023\nIEEE Symposium on Security and Privacy , IEEE. IEEE\nComputer Society, May 2023, pp. 346–363. [Online]. Avail-\nable: https://www.microsoft.com/en-us/research/publication/analyzing-\nleakage-of-personally-identifiable-information-in-language-models/[44] V . Sanh, T. Wolf, and A. M. Rush, “Movement pruning: adaptive sparsity\nby fine-tuning,” in Proceedings of the 34th International Conference on\nNeural Information Processing Systems , ser. NIPS ’20. Red Hook, NY ,\nUSA: Curran Associates Inc., 2020.\n[45] P. Michel, O. Levy, and G. Neubig, Are sixteen heads really better than\none? Red Hook, NY , USA: Curran Associates Inc., 2019.\n[46] Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. E.\nGonzalez, “Train large, then compress: rethinking model size for ef-\nficient training and inference of transformers,” in Proceedings of the\n37th International Conference on Machine Learning , ser. ICML’20.\nJMLR.org, 2020.\n[47] J. A. Akinyele, C. Garman, I. Miers, M. W. Pagano,\nM. Rushanan, M. Green, and A. D. Rubin, “Charm:\nA framework for rapidly prototyping cryptosystems,” 2025,\navailable at https://github.com/JHUISI/charm. [Online]. Available:\nhttps://github.com/JHUISI/charm\n[48] C. Simpkins and C. Russ, “Crypto: Simple symmet-\nric gpg file encryption and decryption,” 2025, available\nat https://github.com/chrissimpkins/crypto. [Online]. Available:\nhttps://github.com/chrissimpkins/crypto\n[49] S. Behnel, R. Bradshaw, D. Woods, M. Valo, and L. Dalc ´ın, “Cython:\nC-extensions for python,” https://cython.org/, 2024, an optimising static\ncompiler for both the Python programming language and the extended\nCython programming language.\n[50] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\nC. Dewan, M. Diab, X. Li, X. V . Lin, T. Mihaylov, M. Ott,\nS. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and\nL. Zettlemoyer, “Opt: Open pre-trained transformer language models,”\n2022. [Online]. Available: https://arxiv.org/abs/2205.01068\n[51] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn,\nE. Saravia, A. Poulton, V . Kerkez, and R. Stojnic, “Galactica:\nA large language model for science,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2211.09085\n[52] G. Team, M. Riviere, S. Pathak, and et al., “Gemma 2: Improving\nopen language models at a practical size,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2408.00118\n[53] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2010.11929",
  "text_length": 76307
}