{
  "id": "http://arxiv.org/abs/2505.24875v1",
  "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL",
  "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.",
  "authors": [
    "Yu Zhang",
    "Yunqi Li",
    "Yifan Yang",
    "Rui Wang",
    "Yuqing Yang",
    "Dai Qi",
    "Jianmin Bao",
    "Dongdong Chen",
    "Chong Luo",
    "Lili Qiu"
  ],
  "published": "2025-05-30T17:59:48Z",
  "updated": "2025-05-30T17:59:48Z",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24875v1"
}