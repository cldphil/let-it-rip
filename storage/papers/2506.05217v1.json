{
  "id": "http://arxiv.org/abs/2506.05217v1",
  "title": "DSG-World: Learning a 3D Gaussian World Model from Dual State Videos",
  "summary": "Building an efficient and physically consistent world model from limited\nobservations is a long standing challenge in vision and robotics. Many existing\nworld modeling pipelines are based on implicit generative models, which are\nhard to train and often lack 3D or physical consistency. On the other hand,\nexplicit 3D methods built from a single state often require multi-stage\nprocessing-such as segmentation, background completion, and inpainting-due to\nocclusions. To address this, we leverage two perturbed observations of the same\nscene under different object configurations. These dual states offer\ncomplementary visibility, alleviating occlusion issues during state transitions\nand enabling more stable and complete reconstruction. In this paper, we present\nDSG-World, a novel end-to-end framework that explicitly constructs a 3D\nGaussian World model from Dual State observations. Our approach builds dual\nsegmentation-aware Gaussian fields and enforces bidirectional photometric and\nsemantic consistency. We further introduce a pseudo intermediate state for\nsymmetric alignment and design collaborative co-pruning trategies to refine\ngeometric completeness. DSG-World enables efficient real-to-simulation transfer\npurely in the explicit Gaussian representation space, supporting high-fidelity\nrendering and object-level scene manipulation without relying on dense\nobservations or multi-stage pipelines. Extensive experiments demonstrate strong\ngeneralization to novel views and scene states, highlighting the effectiveness\nof our approach for real-world 3D reconstruction and simulation.",
  "authors": [
    "Wenhao Hu",
    "Xuexiang Wen",
    "Xi Li",
    "Gaoang Wang"
  ],
  "published": "2025-06-05T16:33:32Z",
  "updated": "2025-06-05T16:33:32Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05217v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05217v1  [cs.CV]  5 Jun 2025DSG-World: Learning a 3D Gaussian World Model\nfrom Dual State Videos\nWenhao Hu1,2, Xuexiang Wen2, Xi Li1, Gaoang Wang1,2\n1College of Computer Science and Technology, Zhejiang University\n2ZJU-UIUC Institute, Zhejiang University\nAbstract\nBuilding an efficient and physically consistent world model from limited obser-\nvations is a long-standing challenge in vision and robotics. Many existing world\nmodeling pipelines are based on implicit generative models, which are hard to train\nand often lack 3D or physical consistency. On the other hand, explicit 3D methods\nbuilt from a single state often require multi-stage processing—such as segmen-\ntation, background completion, and inpainting—due to occlusions. To address\nthis, we leverage two perturbed observations of the same scene under different\nobject configurations. These dual states offer complementary visibility, alleviating\nocclusion issues during state transitions and enabling more stable and complete re-\nconstruction. In this paper, we present DSG-World , a novel end-to-end framework\nthat explicitly constructs a 3D Gaussian World model from DualState observa-\ntions. Our approach builds dual segmentation-aware Gaussian fields and enforces\nbidirectional photometric and semantic consistency. We further introduce a pseudo-\nintermediate state for symmetric alignment, and design collaborative co-pruning\nand co-pasting strategies to refine geometric completeness. DSG-World enables\nefficient real-to-simulation transfer purely in the explicit Gaussian representation\nspace, supporting high-fidelity rendering and object-level scene manipulation with-\nout relying on dense observations or multi-stage pipelines. Extensive experiments\ndemonstrate strong generalization to novel views and scene states, highlighting the\neffectiveness of our approach for real-world 3D reconstruction and simulation.\n1 Introduction\nConstructing reliable world models from sparse observations remains a fundamental challenge in\nvision and robotics [ 46,30,23,22,1,41,3]. While recent methods often leverage implicit generative\nframeworks [ 36,33,43,32], these approaches typically suffer from training instability and lack\nexplicit 3D structure or physical grounding, limiting their applicability in tasks requiring interaction,\ncontrol, or simulation. In contrast, recent advances in 3D Gaussian Splatting [ 16] enable explicit\nscene reconstruction by representing geometry and appearance with compact Gaussian primitives.\nSome methods, such as Gaussian Grouping [ 38], attempt real-to-simulation ( real2sim ) transformation\nby combining instance-level segmentation with inpainting-based refinement. While partially effective,\nthese multi-stage pipelines face several challenges. Inaccurate 2D segmentation—especially near\nobject boundaries or under occlusion—leads to imprecise masks that propagate into 3D, causing\nmisclassified Gaussians and visible artifacts. These issues often require additional post-processing,\nincreasing system complexity. Moreover, inpainting struggles to recover fine background details\nin cluttered or high-frequency regions, resulting in unrealistic or blurry outputs. Such limitations\nundermine the fidelity and robustness of the real2sim process and hinder downstream tasks requiring\naccurate scene understanding.\nPreprint.\n--- Page 2 ---\nSegmented\nGaussian Field\nPost-processing\nInpainting\nTarget GaussianObject \nGaussian\nComposition\nTarget GaussianObject \nGaussianSegmented \nGaussian Field\nTarget GaussianSegmented\nGaussian Field\nOptimized  \nGaussian\n（a）single state with  \nmulti -stage processing（b）multiple object with  \ndense observations（c）Our end to end dual state \ngaussian world model\nOptimization\nBackground \nGaussian\nT\nT\n TFigure 1: Comparison of different paradigms for constructing 3D Gaussian world models. (a) Tradi-\ntional single-state pipelines rely on multi-stage post-processing and inpainting, which may introduce\naccumulated artifacts. (b) Object-centric approaches require dense multi-view observations of all\ncomponents, followed by explicit composition. (c) Our proposed end-to-end dual-state Gaussian\nworld model jointly optimizes two complementary segmented Gaussian fields via cross-state supervi-\nsion, enabling accurate and compact target Gaussian reconstruction without requiring inpainting or\ndense observations.\nIn parallel, recent research in real2sim transfer and world modeling [ 2,39,37,20,11,45] has begun\nexploring the integration of Gaussian Splatting into interactive and physically grounded simulation\nframeworks. Methods such as RoboGSim [ 18] and SplatSim [ 26] leverage Gaussian representations\nto construct photorealistic virtual environments from real-world observations. However, these ap-\nproaches heavily rely on dense multi-view object captures to build high-fidelity scene representations,\nwhich limits their scalability in practical deployment scenarios. Moreover, their primary focus re-\nmains on enhancing appearance realism or facilitating policy transfer, rather than explicitly modeling\nobject-level state transitions.\nWe argue that remains unresolved: how can we efficiently build an accurate and consistent world\nmodel of a scene from a minimal number of real observations, while avoiding multi-stage\nprocessing and multi-object scanning?\nTo tackle this, we leverage two perturbed observations of the same scene under different object\nconfigurations. These dual states provide complementary supervision—revealing background regions\nthat may be occluded in one state but visible in the other. Moreover, the relative motion between\nstates helps delineate clearer object boundaries, enhancing the spatial precision of 3D Gaussian\nsegmentation.\nMotivated by these insights, we propose DSG-World , a unified framework that constructs a Dual\nStateGaussian World model from only two static observations. Our method builds dual segmentation-\naware Gaussian fields and applies bidirectional supervision to jointly optimize geometric and semantic\nconsistency. To further enhance alignment, we introduce a pseudo-intermediate state as a shared\nreference space for fusion and simulation. Finally, we design collaborative co-pruning and co-pasting\nstrategies to improve geometric completeness. Unlike traditional pipelines that rely on multi-stage\nheuristics or inpainting, DSG-World directly operates in the explicit Gaussian space, enabling efficient\nand high-fidelity simulation under arbitrary scene configurations.\nIn summary, our work makes the following contributions:\n•We present a novel dual-state modeling strategy that constructs a complete and consistent 3D\nGaussian field from two perturbed scene states, simplifying state simulation and significantly\nimproving the accuracy and robustness of world model transitions.\n•We enforce mutual consistency by transforming each Gaussian field into the configuration\nof the other and supervising both photometric and semantic outputs in both directions.\n2\n--- Page 3 ---\n•We construct a virtual intermediate Gaussian field via geometric constraints, serving as a\nsymmetric reference to facilitate alignment and comparison across the two observed states.\n•We support object-level novel scene simulation by applying rigid transformations directly in\nthe Gaussian representation space, without relying on latent representations.\n2 Related Works\n2.1 3D Gaussian Segmentation\nRecent methods have extended Gaussian Splatting to perform scene segmentation [ 44,12,13].\nGaussianEditor [ 9] projects 2D segmentation masks back onto 3D Gaussians via inverse rendering.\nLanguage-driven approaches like LangSplat [ 24], LEGaussians [ 28], and others [ 8,25] leverage\nCLIP features for open-world scene understanding. Gaussian Grouping [ 38] attaches segmentation\nfeatures to each Gaussian and aligns multi-view segment IDs using video segmentation methods [ 10],\nwhile Gaga [ 21] further addresses ID inconsistency across views through a 3D-aware memory\nbank. FlashSplat [ 27] introduces a fast, globally optimal linear programming-based approach for\n3D Gaussian segmentation. OpenGaussian [ 34] and InstanceGaussian [ 17] use contrastive learning\nto obtain point-level 3D segmentation. GaussianCut [ 15] models the scene as a graph and employs\ngraph-cut optimization to separate foreground and background Gaussians. COB-GS [ 40] improves\nsegmentation accuracy via boundary-adaptive Gaussian splitting and visual refinement, enabling\nsharper boundaries while maintaining rendering fidelity.\nObtaining a 3D segmentation alone is insufficient for real2sim simulation, as 2D segmenter biases\noften lead to inaccurate 3D masks, requiring post-processing and Gaussian inpainting [ 19,6,14] to\nfill background holes caused by object movement. This multi-stage pipeline is complex and prone to\nerror accumulation. In contrast, our method leverages two complementary scene states to provide\nmutual visibility, enabling end-to-end reconstruction without explicit inpainting. Object transfers\nacross states help calibrate imperfect segmentations, resulting in a clean and consistent Gaussian\nworld model.\n2.2 Interactive World Modeling and Real2Sim Simulation\nSome approaches construct implicit generative world models from video data. UniSim[ 36] simulates\nreal-world interactions by predicting visual outcomes conditioned on diverse actions using a unified\nautoregressive framework over heterogeneous datasets. iVideoGPT[ 33] models interactive dynamics\nby encoding visual observations, actions, and rewards into token sequences for scalable next-token\nprediction with compressive tokenization. However, these methods lack 3D and physical consistency\nand are generally difficult to train.\nRecent methods aim to build interactive simulators by integrating reconstructed real scenes into\nphysics engines. For instance, RoboGSim [ 18] embeds 3D Gaussians into Isaac Sim; SplatSim [ 26]\nreplaces meshes with Gaussian splats for photorealistic rendering; PhysGaussian [ 35] and Spring-\nGaus [ 42] enable mesh-free physical simulation with Newtonian or elastic models; NeuMA [ 7]\nrefines simulation using image-space gradients. However, these approaches typically rely on dense,\nper-object 3D capture. In contrast, our method is much simpler and more lightweight—requiring\nonly two real videos and known rigid transformations. By explicitly constructing dual-state Gaussian\nfields, we achieve consistent and efficient Real2Sim conversion without multi-object scanning or\ncomplex simulation components.\n3 Preliminary and Problem Definition\n3.1 Segmented Gaussian Splatting\nSegmented Gaussian Splatting [38] models a scene as a set of 3D Gaussians, each parameterized as\nG={x,Σ,α,c,s}, where xdenotes the 3D center position, Σrepresents the spatial covariance\nmatrix, αis the opacity coefficient, cis the RGB color vector, and sis a learnable feature vector used\nfor segmentation.\n3\n--- Page 4 ---\nDuring rendering, each Gaussian is projected onto the 2D image plane using a differentiable α-\nblending mechanism. Both the final pixel color Cand segmentation feature Sare computed by\naccumulating Gaussian contributions weighted by their projected opacities α′\ni:\nC=X\ni∈Nciα′\nii−1Y\nj=1(1−α′\nj), S =X\ni∈Nsiα′\nii−1Y\nj=1(1−α′\nj) (1)\n3.2 Motivation and Core Definition\nWe define the world model as a process of explicit object-level state transition , where changes\nin the scene are modeled as discrete transitions between physically grounded states. Specifically,\nwhen an object undergoes a rigid transformation and relocates within the scene, it is interpreted as a\ntransition from one explicit state to another. Unlike generative models that implicitly predict future\nstates in a latent space—often resulting in physically inconsistent or ambiguous interpretations—our\nformulation maintains spatial and physical coherence by directly modeling the geometry and motion\nof objects in 3D space.\n3.3 Modeling with Dual-State Observations\nConstructing a world model from a single static observation is inherently difficult, as it typically relies\non segmentation, post-processing, and inpainting—each prone to introducing errors or inconsistencies.\nTo mitigate these challenges, we propose building the world model from two static observations\ncaptured from distinct scene states. By leveraging the object-level motion between these states, we\njointly refine 3D segmentation and recover occluded background regions. This enables extrapolation\nto arbitrary intermediate states and facilitates the construction of a consistent 3D Gaussian world\nmodel. Formally, the modeling process is defined as:\n(G∗\n1,G∗\n2) = arg min\nG1,G2Ljoint (2)\nGiven the optimized pair (G∗\n1,G∗\n2)and object-level transformation T, we synthesize a new target\nGaussian field Gtthrough explicit transformation:\nWGS:{G∗\n1,G∗\n2}T− → G t. (3)\n3.4 Gaussian State Transfer\nLetI1andI2denote the image sets captured from two observed scene states, with corresponding\nsegmentation masks S1andS2. Given a transformation Tthat describes the motion of foreground\nobjects between state 1 and state 2, our goal is to reconstruct the 3D geometry of both states using\nGaussian primitives and to enable simulation and interpolation across different scene configurations.\nWe first construct two Gaussian fields, G1andG2, from (I1,S1)and(I2,S2), respectively. Each\nfield encodes geometry and appearance information of the scene in its corresponding state. The\ntransformation between the fields is then formulated as:\nG2=T1→2(G1),G1=T2→1(G2) (4)\n3.5 Object-Aware Transformation Modeling\nTo model scene-level transformations, the operator Tis defined as an object-aware function that\napplies per-Gaussian rigid transformations based on semantic identity. Let the Gaussian field be\ndecomposed into foreground and background subsets:\nG=Gfg∪ G bg,Gfg=O[\no=1G(o)\nfg (5)\nwhere each foreground object ois associated with a rigid transformation T(o). For any Gaussian\ngi∈ G, letoidenote the object it belongs to. Then, Tis applied as:\nT(gi) =\u001aT(oi)·gi,ifgi∈ G fg\ngi, ifgi∈ G bg(6)\nThis formulation ensures spatially consistent transformation and geometric fidelity of object-level\nmotion while preserving the static background.\n4\n--- Page 5 ---\n4 Method\n𝐺𝐺1={𝑥𝑥1,Σ1,𝛼𝛼1,𝑐𝑐1,𝑠𝑠1} 𝐺𝐺1→𝑝𝑝={𝑥𝑥1′,Σ1′,𝛼𝛼1,𝑐𝑐1,𝑠𝑠1} 𝐺𝐺1→2={𝑥𝑥1′′,Σ1′′,𝛼𝛼1,𝑐𝑐1,𝑠𝑠1}\n𝐺𝐺2={𝑥𝑥2,Σ2,𝛼𝛼2,𝑐𝑐2,𝑠𝑠2} 𝐺𝐺2→𝑝𝑝={𝑥𝑥2′,Σ2′,𝛼𝛼2,𝑐𝑐2,𝑠𝑠2} 𝐺𝐺2→1={𝑥𝑥2′′,Σ2′′,𝛼𝛼2,𝑐𝑐2,𝑠𝑠2}State1 State2\nGaussian State \nTransferPseudo -state \nAlignmentCollaborative \nCo-PruningBidirectional \nAlignment𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑆𝑆𝐼𝐼𝑆𝑆𝐼𝐼1\n𝑆𝑆𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑆𝑆𝑆𝑆 𝐼𝐼𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝐼𝐼𝑆𝑆𝑆𝑆1𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑆𝑆𝐼𝐼𝑆𝑆𝐼𝐼2\n𝑆𝑆𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑆𝑆𝑆𝑆 𝐼𝐼𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝐼𝐼𝑆𝑆𝑆𝑆2\nState1 \nGaussian State2 \nGaussian Aera after \nremovalObject \nBoundary\nFigure 2: Overview of our dual-state Gaussian optimization pipeline. Given two complementary scene\nobservations from different states (State1 and State2), we construct two segmented Gaussian fields, G1\nandG2. During state transfer (e.g., G1→G1→2), object motion is applied to the segmented region.\nHowever, Gaussians near object boundaries (dashed circles) often remain static due to incomplete or\ninaccurate boundary segmentation during training. This leads to residual inconsistencies across states,\nparticularly at the edges of moving objects. To enable realistic and artifact-free image under arbitrary\nconfigurations in simulation, we introduce a bidirectional alignment mechanism and pseudo-state\nsupervision ( G1→pandG2→p) to regularize the transferred Gaussians and promote coherent boundary\nalignment. A co-pruning strategy is further employed to refine consistency and remove redundant or\nmisaligned components.\n4.1 Bidirectional Alignment\nTo ensure geometric and semantic consistency across different scene states, we enforce that the\nrendered outputs from transformed Gaussian fields align with the ground-truth observations in\nthe corresponding target states. As mentioned before, we apply transformation T1→2toG1and\ntransformation T2→1toG2. For any viewpoint v, the transformed Gaussian fields are rendered into\nRGB images and segmentation masks, which are then compared with the corresponding ground-truth\nobservations (Iv\n1,Sv\n1)and(Iv\n2,Sv\n2)from the original states. The total alignment loss combines\nphotometric and segmentation consistency, defined as:\nLalign(G1,G2, θ) =∥R(T1→2(G1), v)− Iv\n2∥1+∥R(T2→1(G2), v)− Iv\n1∥1\n+ CE ( fθ(M(T1→2(G1), v)),Sv\n2) + CE ( fθ(M(T2→1(G2), v)),Sv\n1)(7)\nwhere R(·, v)andM(·, v)denote the rendered RGB image and segmentation feature, respectively,\nfrom viewpoint v, as defined in Equation 1. The segmentation output is obtained via a shared classifier\nfθ, which is jointly applied to both G1andG2. Specifically, fθconsists of a linear layer that projects\neach identity embedding to a (K+ 1) -dimensional space, where Kis the number of instance masks\nin the 3D scene [ 38]. The cross-entropy loss CE(·,·)measures the semantic alignment between\npredicted and ground-truth masks.\nThis bidirectional consistency encourages each transformed Gaussian field to accurately reconstruct\nthe scene content of the opposite state, thereby reinforcing object-level correspondence and enhancing\nalignment across different scene configurations.\n4.2 Pseudo-state Guided Alignment\nTo enhance the generalizability of the world model across diverse scene configurations, we introduce\na pseudo-state Gpthat serves as an intermediate reference for supervision. This pseudo-state is\nconstructed by applying geometric constraints, such as collision and boundary regularization, to\n5\n--- Page 6 ---\nsynthesize a virtual configuration between the two observed states. Unlike the original states, the\npseudo-state is not tied to any specific observation but provides a common frame that facilitates\nconsistent alignment between G1andG2.\nWe compute transformation matrices T1→pandT2→pto transfer the original fields G1andG2into\nGp. By transforming both fields into this shared pseudo-state, we enable direct comparison and\nalignment of their rendered outputs. Specifically, we render the transformed fields from the same\nviewpoint vand enforce photometric and semantic consistency between them. The corresponding\nloss is defined as:\nLpseudo(G1,G2, θ) =∥R(T1→p(G1), v)−R(T2→p(G2), v)∥1\n+ CE ( fθ(M(T1→p(G1), v)), fθ(M(T2→p(G2), v))) (8)\nBy leveraging a dynamically constructed pseudo-state as an adaptive supervision signal, the model\ncan better reconcile differences between the two input states and generalize more effectively to unseen\nor intermediate scene configurations.\n4.3 Collaborative Co-Pruning\nTo suppress residual artifacts introduced by imperfect segmentation during cross-state Gaussian\ntransfer, we propose a co-pruning mechanism that filters out spatially inconsistent Gaussians by\nleveraging geometric agreement between the two states. When a Gaussian field is transferred from\none state to another, unmatched or misaligned points may remain due to occlusion, noise, or over-\nsegmentation. Our strategy prunes these outliers by checking whether transferred Gaussians can be\nreliably explained by the geometry of the target field.\nFor each transformed Gaussian gi∈T1→2(G1), we identify its nearest neighbor gj∈ G 2using\nEuclidean distance. A Gaussian is marked for pruning if the spatial deviation between giandgj\nexceeds a predefined threshold τ. The binary pruning indicator miis computed as:\nmi= 1\u0000\n∥xi−xj∥2> τ\u0001\n, (9)\nwhere xiandxjare the 3D centers of giandgj, and 1(·)denotes the indicator function. Gaussians\nwithmi= 1are discarded as unreliable or redundant. A symmetric process is applied in the opposite\ndirection, using G2transformed to the frame of G1to prune outliers in G2, resulting in a collaborative\nco-pruning scheme.\n4.4 Training Objective\nThe overall training objective combines three loss terms:\nLjoint(G1,G2, θ) =Lr(G1, θ) +Lr(G2, θ) +λaLalign(G1,G2, θ) +λpLpseudo(G1,G2, θ),(10)\nwhereLrdenotes the same reconstruction loss adopted from Gaussian Grouping [ 38] (detailed in the\nappendix), Lalignenforces bidirectional rendering consistency, and Lpseudo introduces regularization\nthrough pseudo-state supervision. The weights λaandλpare used to balance the contributions of\neach term.\n5 Dataset\nTo support dual-state scene modeling, we construct both synthetic and real-world datasets. The\nsynthetic dataset is generated in Blender [ 4], where Ntextured objects from BlenderKit [ 5] are placed\nin a static background. A second state is created by altering object poses, ensuring no ground region\nis occluded in both states to preserve complementary visibility. Real-world data is captured similarly\nusing handheld RGB cameras. The dataset includes 7 synthetic and 5 real scenes.\nFor evaluation, we create a test state for each scene by randomly repositioning objects. We render\nimages from test-view cameras and compute PSNR and SSIM against the ground truth, measuring\nsimulation fidelity under novel configurations. More implementation and dataset details are provided\nin the appendix.\n6\n--- Page 7 ---\n6 Experiment\n6.1 Experimental Setup\nImplementation details During training, we first optimize the segmented Gaussians using only\nLreconfor 10,000 epochs, then jointly train with LalignandLpseudo to refine the dual Gaussian field for\nanother 10,000 epochs. The output classification linear layer has 16 input channels and 256 output\nchannels. The pruning threshold parameter τis set to 0.5. In training, we set λa= 1.0andλ=1.0.\nWe use the Adam optimizer for both gaussians and linear layer, with a learning rate of 0.0025 for\nsegmentation feature and 0.0005 for linear layer. All datasts are trained for 20K iterations on a single\nNVIDIA 4090 GPU.\nBaselines We compare our method with representative Gaussian Splatting-based simulation frame-\nworks, applying necessary adaptations for fair evaluation. Existing pipelines often involve multi-stage\nprocessing, including segmentation, background completion, inpainting, and fine-tuning. We include\nsegmentation methods based on inverse rendering (GaussianEditor [ 9]), feature-based segmentation\n(Gaussian Grouping [ 38]), and graph optimization (GaussianCut [ 15]); Gaussian Grouping* denotes\nthe variant with convex hull filtering. To enhance simulation quality, we apply kNN-based feature\npropagation from Gaussian Grouping and LaMa [ 29] inpainting for hole filling, and extend the Graph-\nCut setup with depth-aware Gaussian completion. We also include Decoupled Gaussian [ 31], which\nsegments objects using Gaussian Grouping, then performs remeshing and LaMa-based refinement to\ncomplete the scene.\n6.2 Novel State Simulation\nSimulation Ground Truth Simulation Ground TruthGaussianEditor Gaussian Grouping* Gaussian Grouping Gaussiancut DSG- world (ours)\nFigure 3: Qualitative comparison of simulated scene reconstruction under different segmentation-\nbased Gaussian pipelines. We evaluate on both synthetic (top) and real-world (bottom) scenes. While\nexisting methods (e.g., GaussianEditor, Gaussian Grouping, and Gaussiancut) struggle with object\nmixing, boundary artifacts, or background corruption, our method (DSG-world) achieves significantly\nmore accurate and complete simulation results, closely matching the ground-truth scene configuration.\nThe qualitative results on both virtual and real datasets are shown in Figure 3. GaussianEditor\n(inverse rendering based) fails to segment object boundaries precisely, causing edge artifacts that\nrequire heavy post-processing for simulation. Gaussian Grouping (segmentation-feature based) often\n7\n--- Page 8 ---\nSimulation Ground Truth Simulation Ground TruthGaussian Grouping* DecoupledGaussian+I Gaussian Grouping*+I Gaussiancut+I DSG- world (ours)\nFigure 4: Visual comparison of synthetic (top) and real-world (bottom) scenes simulation across\ndifferent segmentation and inpainting pipelines. We evaluate multiple baseline methods including\nGaussian Grouping*, Gaussian Grouping*+I (with inpainting), DecoupledGaussian+I, and Gaussian-\ncut+I, against our proposed DSG-world on both synthetic (top) and real (bottom) scenes.\nmiss inner-object features, leading to floating Gaussians after motion. Graussiancut (graph-based)\nperforms best among baselines, though slight boundary artifacts persist.\nFor fair comparison, we adopt a unified post-processing pipeline for methods that exhibit significant\nscene gaps after object movement, as shown in Fig. 4. This pipeline includes background Gaussian\ncompletion, 2D inpainting, and Gaussian finetuning. The KNN-based completion in Gaussian\nGrouping often leads to occlusion of existing objects. Gaussiancut with depth-based completion\nand LaMa inpainting achieves the most visually coherent result. DecoupledGaussian, which builds\nupon Gaussian Grouping, provides remeshing performance comparable to depth completion but\nstill introduces floating artifacts. In contrast, our method achieves the highest PSNR and SSIM\nfor novel-state simulation across both datasets, while remaining end-to-end and avoiding complex\nmulti-stage post-processing.\nTable 1: Quantitative comparison of simulation quality across different 3D Gaussian segmentation\npipelines on the synthetic and real-world datasets. Methods are categorized into segmentation-only,\nsegmentation with inpainting (Seg.+Inpainting), and our proposed dual-state framework (DSG-world)\nType Model Completion Sim Real\nPSNR SSIM PSNR SSIM\nSegmentationGaussianEditor [9] - 25.82 0.929 23.25 0.801\nGaussian Grouping [38] - 26.22 0.913 22.74 0.777\nGaussian Grouping* [38] - 22.29 0.901 22.37 0.790\nGaussiancut [15] - 26.79 0.941 23.43 0.809\nSeg.+InpaintingGaussian Grouping* [38] knn 29.31 0.892 23.28 0.805\nDecoupledGaussian [31] remesh 29.50 0.891 24.28 0.804\nGraphcut [15] depth 30.88 0.937 24.40 0.810\nDual-State DSG-world (ours) Co-pasting 38.37 0.974 27.52 0.859\n8\n--- Page 9 ---\nAs shown in Table 1, segmentation-only methods yield lower PSNR and SSIM, while adding\ninpainting improves performance—particularly on synthetic scenes with large, textured floors that\nexpose noticeable holes after object movement. In contrast, real-world scenes typically involve\nsmaller movable objects and mostly textureless floors, resulting in less visible gaps. Instead of relying\non inpainting, we leverage the complementary nature of dual-state Gaussians to directly retrieve\nground Gaussians from the alternate state for completion, a process we define as Co-pasting. As the\nresults show, completion-based methods perform similarly overall, while our dual-state approach\nachieves the highest accuracy on both datasets.\n6.3 Ablation Study\nTable 2 presents the ablation study evaluating the contribution of each component in our framework:\nBidirectional Alignment (B), Collaborative Co-Pruning (C), and Pseudo-state Guided Alignment\n(P). Using only Bidirectional Alignment already provides a strong baseline, achieving a PSNR of\n36.57. Introducing Co-Pruning yields a slight improvement in structural quality. This is because\nBidirectional Alignment tends to reassign residual Gaussians to have background-like colors or\nreduced opacity. While Co-Pruning helps eliminate these floaters, its overall impact on PSNR is\nlimited. In contrast, incorporating Pseudo-state Guided Alignment results in a substantial increase in\nPSNR. This improvement arises from the fact that occlusion ambiguities cannot be fully resolved\nwith only two configurations, additional pseudo-states provide richer supervision across multiple\nviewpoints, enhancing alignment between the two Gaussian fields and leading to more consistent and\nphotorealistic reconstructions.\nTable 2: Ablation study of B (Bidirectional\nAlignment), C (Collaborative Co-Pruning), and\nP (Pseudo-state Guided Alignment).\nB C P PSNR ↑SSIM↑\n\" - - 36.57 0.974\n\" \" - 36.96 0.977\n\" \" \" 38.37 0.974Table 3: PSNR and SSIM of G1andG2in Sim1\nand Sim2 scene, with and without pseudo-state\nsupervision.\nSim1 Sim2\nPSNR SSIM PSNR SSIM\nG137.04 0.979 37.51 0.9774\nG237.16 0.977 36.14 0.9747\nG∗\n1 39.26 0.9844 37.59 0.9776\nG∗\n2 39.26 0.9843 37.50 0.9774\n6.4 Dual Guassian Convergence\nWe investigate the convergence behavior of two Gaussian fields trained from different scene states.\nWithout Pseudo-state Guided Alignment, we denote the fields as G1andG2, and with Pseudo-state\nGuided Alignment, as G∗\n1andG∗\n2. In the absence of Pseudo-state Guided Alignment, PSNR and SSIM\ndiffer significantly when evaluated in the target state. These discrepancies stem from occlusions and\nviewpoint differences that lead to misalignments between the two fields. Even with Bidirectional\nAlignment, such inconsistencies persist, indicating incomplete convergence.\nBy incorporating Pseudo-state Guided Alignment, we enforce consistency across object compositions\nin both fields, allowing them to observe complementary content and provide mutual supervision. This\npromotes convergence toward a shared and coherent optimized representation. Empirically, Gaussian\nfields trained from either state yield nearly identical PSNR and SSIM when evaluated under the same\nconfiguration, demonstrating effective alignment and mutual consistency.\n7 Limitations and Broader impacts\nDespite its effectiveness, DSG-world has several limitations. To ensure end-to-end reconstruction,\ndual-state observations are designed to be complementary, avoiding occlusion overlap. However, if\nobject perturbations are too minor, some regions may remain unobserved in both states, hindering\naccurate reconstruction. Additionally, our model does not handle lighting variations, causing static\nshadows even when objects move, which affects realism. Incorporating relighting into the framework\ncould further enhance simulation fidelity in future work.\n9\n--- Page 10 ---\n8 Conclusion\nWe present DSG-World, an end-to-end framework for 3D world modeling from dual-state observa-\ntions. By leveraging complementary visibility, our method mitigates occlusion issues and removes\nthe need for complex post-processing. Through bidirectional consistency and pseudo-state alignment,\nit generates high-quality Gaussian fields for accurate rendering and simulation. Extensive results\ndemonstrate its effectiveness and scalability for real-to-simulation tasks in vision and robotics.\nReferences\n[1]Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world\nmodels. arXiv preprint arXiv:2412.03572 , 2024.\n[2]Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni,\nand Efstratios Gavves. Dream to manipulate: Compositional world models empowering robot\nimitation learning with imagination. arXiv preprint arXiv:2412.14957 , 2024.\n[3]Daniel M Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar,\nAlex Durango, and Daniel LK Yamins. Unifying (machine) vision via counterfactual world\nmodeling. arXiv preprint arXiv:2306.01828 , 2023.\n[4]Blender Online Community. Blender - a 3d modelling and rendering package. https://www.\nblender.org , 2023.\n[5]BlenderKit. Blenderkit: 3d asset library for blender. https://www.blenderkit.com , 2023.\nAccessed in 2023.\n[6]Chenjie Cao, Chaohui Yu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Mvinpainter: Learning\nmulti-view consistent inpainting to bridge 2d and 3d editing. arXiv preprint arXiv:2408.08000 ,\n2024.\n[7]Junyi Cao, Shanyan Guan, Yanhao Ge, Wei Li, Xiaokang Yang, and Chao Ma. Neuma: Neural\nmaterial adaptor for visual grounding of intrinsic dynamics. Advances in Neural Information\nProcessing Systems , 37:65643–65669, 2024.\n[8]Runnan Chen, Xiangyu Sun, Zhaoqing Wang, Youquan Liu, Jiepeng Wang, Lingdong Kong,\nJiankang Deng, Mingming Gong, Liang Pan, Wenping Wang, et al. Ovgaussian: Generalizable\n3d gaussian segmentation with open vocabularies. arXiv preprint arXiv:2501.00326 , 2024.\n[9]Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai,\nLei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing\nwith gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 21476–21485, 2024.\n[10] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee.\nTracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 1316–1326, 2023.\n[11] Xiaoshen Han, Minghuan Liu, Yilun Chen, Junqiu Yu, Xiaoyang Lyu, Yang Tian, Bolun Wang,\nWeinan Zhang, and Jiangmiao Pang. Re ˆ 3 sim: Generating high-fidelity simulation data via\n3d-photorealistic real-to-sim for robotic manipulation. arXiv preprint arXiv:2502.08645 , 2025.\n[12] Wenhao Hu, Wenhao Chai, Shengyu Hao, Xiaotong Cui, Xuexiang Wen, Jenq-Neng Hwang,\nand Gaoang Wang. Pointmap association and piecewise-plane constraint for consistent and\ncompact 3d gaussian segmentation field. arXiv preprint arXiv:2502.16303 , 2025.\n[13] Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, and Zhaoxiang\nZhang. Semantic anything in 3d gaussians. arXiv preprint arXiv:2401.17857 , 2024.\n[14] Sheng-Yu Huang, Zi-Ting Chou, and Yu-Chiang Frank Wang. 3d gaussian inpainting with\ndepth-guided cross-view consistency. arXiv preprint arXiv:2502.11801 , 2025.\n10\n--- Page 11 ---\n[15] Umangi Jain, Ashkan Mirzaei, and Igor Gilitschenski. Gaussiancut: Interactive segmentation\nvia graph cut for 3d gaussian splatting. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems , 2024.\n[16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian\nsplatting for real-time radiance field rendering. ACM Transactions on Graphics , 42(4):1–14,\n2023.\n[17] Haijie Li, Yanmin Wu, Jiarui Meng, Qiankun Gao, Zhiyao Zhang, Ronggang Wang, and Jian\nZhang. Instancegaussian: Appearance-semantic joint gaussian representation for 3d instance-\nlevel perception. arXiv preprint arXiv:2411.19235 , 2024.\n[18] Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun\nTseng, and Ruiping Wang. Robogsim: A real2sim2real robotic gaussian splatting simulator.\narXiv preprint arXiv:2411.11839 , 2024.\n[19] Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu,\nYujun Shen, and Yang Cao. Infusion: Inpainting 3d gaussians via learning depth completion\nfrom diffusion prior. arXiv preprint arXiv:2404.11613 , 2024.\n[20] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng Chen, Wenlong Ma, Chenglong Li,\nLin Wang, Hengzhen Feng, Lu Shi, et al. Robo-gs: A physics consistent spatial-temporal model\nfor robotic arm with hybrid representation. arXiv preprint arXiv:2408.14873 , 2024.\n[21] Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, and Ming-Hsuan Yang. Gaga: Group\nany gaussians via 3d-aware memory bank. arXiv preprint arXiv:2404.07977 , 2024.\n[22] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human\nvideos. arXiv preprint arXiv:2308.10901 , 2023.\n[23] Jing-Cheng Pang, Nan Tang, Kaiyuan Li, Yuting Tang, Xin-Qiang Cai, Zhen-Yu Zhang, Gang\nNiu, Masashi Sugiyama, and Yang Yu. Learning view-invariant world models for visual robotic\nmanipulation. In The Thirteenth International Conference on Learning Representations .\n[24] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d\nlanguage gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 20051–20060, 2024.\n[25] Jiaxiong Qiu, Liu Liu, Zhizhong Su, and Tianwei Lin. Gls: Geometry-aware 3d language\ngaussian splatting. arXiv preprint arXiv:2411.18066 , 2024.\n[26] Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor,\nand Abhisesh Silwal. Splatsim: Zero-shot sim2real transfer of rgb manipulation policies using\ngaussian splatting. arXiv preprint arXiv:2409.10161 , 2024.\n[27] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Flashsplat: 2d to 3d gaussian splatting\nsegmentation solved optimally. In European Conference on Computer Vision , pages 456–472.\nSpringer, 2024.\n[28] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and Shao-Hua Guan. Language embedded 3d gaus-\nsians for open-vocabulary scene understanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 5333–5343, 2024.\n[29] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha,\nAleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky.\nResolution-robust large mask inpainting with fourier convolutions. In Proceedings of the\nIEEE/CVF winter conference on applications of computer vision , pages 2149–2159, 2022.\n[30] Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan,\nMasayoshi Tomizuka, Marc Pollefeys, and Hesheng Wang. Nerf in robotics: A survey. arXiv\npreprint arXiv:2405.01333 , 2024.\n[31] Miaowei Wang, Yibo Zhang, Rui Ma, Weiwei Xu, Changqing Zou, and Daniel Morris. De-\ncoupledgaussian: Object-scene decoupling for physics-based interaction. arXiv preprint\narXiv:2503.05484 , 2025.\n11\n--- Page 12 ---\n[32] Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vid-\nman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation.\nAdvances in Neural Information Processing Systems , 37:41051–41075, 2024.\n[33] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long.\nivideogpt: Interactive videogpts are scalable world models. Advances in Neural Information\nProcessing Systems , 37:68082–68119, 2024.\n[34] Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao,\nHaocheng Feng, Errui Ding, Jingdong Wang, et al. Opengaussian: Towards point-level 3d\ngaussian-based open vocabulary understanding. arXiv preprint arXiv:2406.02058 , 2024.\n[35] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang.\nPhysgaussian: Physics-integrated 3d gaussians for generative dynamics. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4389–4398, 2024.\n[36] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and\nPieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114 ,\n1(2):6, 2023.\n[37] Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, and Jiangmiao Pang.\nNovel demonstration generation with gaussian splatting enables robust one-shot manipulation.\narXiv preprint arXiv:2504.13175 , 2025.\n[38] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit\nanything in 3d scenes. arXiv preprint arXiv:2312.00732 , 2023.\n[39] Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng,\nMuhammad Zubair Irshad, and Ken Goldberg. Real2render2real: Scaling robot data without\ndynamics simulation or robot hardware. arXiv preprint arXiv:2505.09601 , 2025.\n[40] Jiaxin Zhang, Junjun Jiang, Youyu Chen, Kui Jiang, and Xianming Liu. Cob-gs: Clear object\nboundaries in 3dgs segmentation based on boundary-adaptive gaussian splitting. arXiv preprint\narXiv:2503.19443 , 2025.\n[41] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong,\nand Chuang Gan. 3d-vla: A 3d vision-language-action generative world model. arXiv preprint\narXiv:2403.09631 , 2024.\n[42] Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yunzhu Li. Reconstruction and simulation of\nelastic objects with spring-mass 3d gaussians. In European Conference on Computer Vision ,\npages 407–423. Springer, 2024.\n[43] Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek\nGupta. Unified world models: Coupling video and action diffusion for pretraining on large\nrobotic datasets. arXiv preprint arXiv:2504.02792 , 2025.\n[44] Runsong Zhu, Shi Qiu, Zhengzhe Liu, Ka-Hei Hui, Qianyi Wu, Pheng-Ann Heng, and Chi-Wing\nFu. Rethinking end-to-end 2d to 3d scene segmentation in gaussian splatting. arXiv preprint\narXiv:2503.14029 , 2025.\n[45] Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, and Hang Zhao. Vr-robo:\nA real-to-sim-to-real framework for visual robot navigation and locomotion. arXiv preprint\narXiv:2502.01536 , 2025.\n[46] Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, and Hesheng Wang. 3d gaussian\nsplatting in robotics: A survey. arXiv preprint arXiv:2410.12262 , 2024.\n12",
  "text_length": 40200
}