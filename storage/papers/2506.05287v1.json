{
  "id": "http://arxiv.org/abs/2506.05287v1",
  "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?",
  "summary": "The emergence of multimodal large language models (MLLMs) has driven\nbreakthroughs in egocentric vision applications. These applications necessitate\npersistent, context-aware understanding of objects, as users interact with\ntools in dynamic and cluttered environments. However, existing embodied\nbenchmarks primarily focus on static scene exploration, emphasizing object's\nappearance and spatial attributes while neglecting the assessment of dynamic\nchanges arising from users' interactions. To address this gap, we introduce\nEOC-Bench, an innovative benchmark designed to systematically evaluate\nobject-centric embodied cognition in dynamic egocentric scenarios. Specially,\nEOC-Bench features 3,277 meticulously annotated QA pairs categorized into three\ntemporal categories: Past, Present, and Future, covering 11 fine-grained\nevaluation dimensions and 3 visual object referencing types. To ensure thorough\nassessment, we develop a mixed-format human-in-the-loop annotation framework\nwith four types of questions and design a novel multi-scale temporal accuracy\nmetric for open-ended temporal evaluation. Based on EOC-Bench, we conduct\ncomprehensive evaluations of various proprietary, open-source, and object-level\nMLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object\ncognitive capabilities of MLLMs, establishing a robust foundation for\ndeveloping reliable core models for embodied systems.",
  "authors": [
    "Yuqian Yuan",
    "Ronghao Dang",
    "Long Li",
    "Wentong Li",
    "Dian Jiao",
    "Xin Li",
    "Deli Zhao",
    "Fan Wang",
    "Wenqiao Zhang",
    "Jun Xiao",
    "Yueting Zhuang"
  ],
  "published": "2025-06-05T17:44:12Z",
  "updated": "2025-06-05T17:44:12Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05287v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05287v1  [cs.CV]  5 Jun 2025\nEOC-Bench: Can MLLMs Identify, Recall, and\nForecast Objects in an Egocentric World?\nYuqian Yuan1,2,3∗Ronghao Dang2,3∗Long Li2,3∗Wentong Li1∗Dian Jiao1\nXin Li2,3Deli Zhao2,3Fan Wang2,3Wenqiao Zhang1Jun Xiao1Yueting Zhuang1\n1Zhejiang University2DAMO Academy, Alibaba Group3Hupan Lab\nProject Page\n Code\n Benchmark\nAbstract\nThe emergence of multimodal large language models (MLLMs) has driven break-\nthroughs in egocentric vision applications. These applications necessitate persistent,\ncontext-aware understanding of objects, as users interact with tools in dynamic and\ncluttered environments. However, existing embodied benchmarks primarily focus\non static scene exploration, emphasizing object’s appearance and spatial attributes\nwhile neglecting the assessment of dynamic changes arising from users’ interac-\ntions. To address this gap, we introduce EOC-Bench , an innovative benchmark\ndesigned to systematically evaluate object-centric embodied cognition in dynamic\negocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated\nQA pairs categorized into three temporal categories: Past, Present, and Future,\ncovering 11 fine-grained evaluation dimensions and 3 visual object referencing\ntypes. To ensure thorough assessment, we develop a mixed-format human-in-\nthe-loop annotation framework with four types of questions and design a novel\nmulti-scale temporal accuracy metric for open-ended temporal evaluation. Based\nonEOC-Bench , we conduct comprehensive evaluations of various proprietary,\nopen-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for\nadvancing the embodied object cognitive capabilities of MLLMs, establishing a\nrobust foundation for developing reliable core models for embodied systems.\n1 Introduction\nThe rapid advancement of multimodal large language models (MLLMs) [ 1,2,3,4] has paved the\nway for the development of intelligent systems that can comprehend and interact with the visual\nworld. Among these innovations, egocentric vision, where systems perceive environments from a\nhuman-like first-person perspective, has gained significant attention due to its critical applications in\nfields such as augmented reality [5], embodied AI [6, 7] and robotic manipulation [8, 9, 10].\nUnderstanding objects precisely within egocentric contexts presents unique challenges that extend\nbeyond conventional vision tasks. It demands a continuously evolving, context-aware comprehension\nof objects, encompassing their types, usages, states, and interactions, as users dynamically interact\nwith tools and undertake various operational tasks. In egocentric environments, particularly in densely\ncluttered settings like kitchens and laboratories, objects exhibit three critical properties: (1) Fleeting\nvisibility , indicating dynamic changes in state and position due to frequent occlusions and shifts in\nviewpoint; (2) Visual ambiguity , arising from similar-looking items in close spatial proximity; and\n(3)Temporal dependency , where current states rely on historical interactions and inform future\n*Equal contribution.\nPreprint.\n--- Page 2 ---\nBenchmark #Videos #Samples Question Type AnnotatorReal\nScenesEgocentricObject\nDynamicsTime\nSensitiveVisual\nPrompts\nMVBench [11] 3,673 4,000 Close Template ✓ ✗ ✓ ✗ ✗\nVideoRefer-Bench [12] 598 1,400 Open/Close Human ✓ ✗ ✓ ✗ M\nCharades-STA [13] 1,334 4,233 Open Automatic/Human ✓ ✗ ✓ ✓ ✗\nScanQA [14] - 4,976 Open Automatic/Human ✓ ✓ ✗ ✗ ✗\nSQA3D [15] - 2,143 Open Human ✓ ✓ ✗ ✗ A\nEnv-QA [16] 3,489 12,760 Open Template ✗ ✓ ✓ ✗ ✗\nOpenEQA [17] 180 1,600 Open Human ✓ ✓ ✗ ✗ ✗\nVSI-Bench [18] 288 5,000 Open/Close Template/Human ✓ ✓ ✗ ✗ ✗\nECBench [19] 386 4,324 Open/Close Human ✓ ✓ ✓(6%) ✗ ✗\nEOC-Bench (Ours) 656 3,277 Open/Close Human ✓ ✓ ✓ ✓ P,B,M\nTable 1: Comparison of widely adopted Embodied/General VideoQA benchmarks with our\nEOC-Bench .P,B,MandArepresent visual prompts for object referencing, specifically as point,\nbox, mask and arrow, respectively.\noutcomes. Successful object perception in these scenarios requires models capable of sustaining\npersistent visual grounding while simultaneously processing spatiotemporal details. Unfortunately,\nexisting benchmarks fail to systematically evaluate this capability.\nAs shown in Table 1, existing embodied benchmarks, such as the closed-vocabulary ScanQA [ 14]\nand SQA3D [ 15], focus on understanding static scene through closed-vocabulary queries based on\ntask-specific datasets. Consequently, these benchmarks lack the scope to evaluate task-generalized\nMLLMs for broader cognitive capabilities. More recently, OpenEQA [ 17], VSI-Bench [ 18], and\nECBench [ 19] have developed open-vocabulary benchmarks to evaluate MLLMs’ question-answering\n(QA) capabilities in indoor embodied video contexts. Despite these promising advancements, current\nbenchmarks primarily concentrate on static scene exploration, such as home tours, and predominantly\nevaluate appearance and spatial attributes. They often overlook dynamic interactions within egocentric\noperational environments, where users engage actively with tools and perform various tasks involving\nobjects. Building these capabilities is crucial for advancing embodied system.\nTo bridge this gap, we introduce EOC-Bench , a novel object-centric video benchmark designed to\nrigorously evaluate the object cognition capabilities of MLLMs in egocentric operational scenarios.\nDrawing on the premise that effective AI assistants must comprehensively comprehend objects across\ntemporal dimensions, EOC-Bench structures questions into three temporally grounded categories:\nPast, Present, and Future.\n•Past: The Past category evaluates MLLMs’ ability to perceive and understand the historical\ndynamics of objects, a skill crucial for enhancing long-term task execution. As illustrated in\nFig. 1, this capability is further subdivided into four types: Object State Retrospection, Object\nLocation Retrospection, Object Relationship Evolution, and Absolute Time Perception. The last is\nparticularly vital, as accurate timestamp awareness of model can contextualize interactions and\ntemporal changes, which has received little attention in previous benchmarks.\n•Present: The Present category test MLLMs’ perception of scene information at the current moment.\nImportantly, resolving these questions often require more than just observing the current frame; a\ncomprehensive understanding of the entire video is necessary for accuracy. As shown in Fig. 1,\nin addition to common abilities such as Immediate State Recognition, Purpose and Function\nInference, and Object Relationship, we introduce Anomaly Perception to handle embodied tasks in\nspecific scenarios. This capability tests whether MLLMs can avoid being misled by counterintuitive\narrangements within the scene and answer questions based on factual information about the objects.\n•Future: By observing the world, human can not only understand past events but also predict future\noccurrences. The capability to foresee future events in objects is crucial for avoiding dangers and\nadapting to new situations. For instance, as shown in the State Change Prediction part in Fig. 1, if a\nmodel identifies a heat-sensitive object near a heat source, it can anticipate temperature changes\nand alert people to move the object to prevent hazards. Future prediction types are divided into\nState Change Prediction, Dynamic Relationship Prediction, and Trajectory and Motion Prediction,\nassessing MLLMs’ proficiency in forecasting dynamic interactions and movements.\nTo ensure a comprehensive evaluation, we develop a mixed-format annotation framework featuring\ndiverse question types ( e.g., true/false, single-choice, multiple-choice and open-ended questions),\nas visualized in Fig. 1. Specially, for open-ended questions, we focus on continuous temporal\nanalysis and introduce a multi-scale temporal accuracy metric to quantitatively assess temporal\nperception performance. Additionally, traditional text-based prompts for object referencing often fail\n2\n--- Page 3 ---\nObject State RetrospectionObject Location RetrospectionObject Relationship EvolutionAbsolute Time PerceptionImmediate State RecognitionObject RelationshipPurpose and Function InferenceAnomaly PerceptionTrajectory and Motion PredictionState Change PredictionDynamic Relationship Prediction1234\nWhat is the relative position of <object 0> and <object 1> originally?What state change occurred to the <object 1>?Where was <object 0> before?How long has <object 0> been washed?What is the relationship between the <object 0> and the <object 1>?What is <object 0> used for?What is in the <object 1>?1234\nCan <object 0> be drunk?123How will the temperature of <object 1> change?If I take <object 0> away, will it affect <object 1>?What direction will<object 0> drive towards?\nPast\nPresent\nFutureMixed-formatAnnotationsTrue/FalseSingle ChoiceMultiple ChoiceOpen-ended Questions\nYesNo<A><B><C><D><A><B><C><D>\n<object 1><object 0><object 0><object 1>\n<object 1><object 0><object 0>\n<object 0>8s  GT:10s50\nAnnotate\nCheck\n+//\nVisual promptFigure 1: Overview of EOC-Bench .EOC-Bench assesses Embodied Object Cognition capabilities of\nMLLMs in egocentric videos across three dimensions - Past, Present, and Future - encompassing\n11categories. EOC-Bench includes 3,277 object-level QA pairs utilizing a mixed-format human-in-\nthe-loop annotation framework across diverse tasks and conditions. EOC-Bench aims to reveal the\nlimitations of MLLMs and promote the development of robust egocentric cognition systems.\nto clearly specify objects in dynamic egocentric scenes. Descriptions like \"the leftmost bowl\" become\nmeaningless when containers are rearranged during washing, and \"the spoon\" lacks clarity among\nmultiple candidates in the kitchen. To address this issue, we introduce visual referencing prompts,\nincluding point, box and mask, as shown in Fig. 1, which provide persistent, unequivocal object\nreferences while preserving the spatiotemporal context essential for precise object comprehension.\nThe final benchmark includes 3,277 question-answer pairs, covering 11 fine-grained evaluation\ndimensions and 3 object referencing types. We have conducted a meticulous human-in-the-loop\nlabeling process, followed by comprehensive cross-checking and verification to ensure quality.\nBuilding upon our EOC-Bench benchmark, we systematically evaluate the egocentric object cognition\ncapabilities of a range of MLLMs, including both open-source and proprietary general-purpose mod-\nels [3,4,1,20,21], as well as specialized object-level MLLMs [ 12,22,23]. Notably, all mainstream\nMLLMs exhibit clear deficiencies in object-level temporal perception, particularly concerning abso-\nlute temporal awareness, where they significantly lags behind human-level performance, emphasizing\nits difficulty and relevance for our community.\n2 Related Work\n2.1 General Video Understanding Benchmarks\nWith the advancement of MLLMs [ 3,24,1,21,25,26,27,4,2,28,29,30,31,32,33,34], which have\ndemonstrated strong visual understanding and reasoning capabilities, there is an increasing emphasis\non comprehensively and systematically evaluating their video understanding abilities [ 11,35,36,37].\nExisting video understanding benchmarks primarily focus on general-purpose video comprehension\ntasks, such as action recognition [ 38,39,40], video caption [ 41,42,43], temporal grounding [ 38,\n44,13], temporal reasoning [ 45,46,47,44], long video understanding [ 48,49,11,50,36,51],\nvideo referring [ 12] and expert-level reasoning [ 52,53]. For instance, Video-MME [ 35] conducts\nan extensive evaluation of MLLMs across a variety of video-related tasks, such as recognition and\nperception. Similarly, MVBench [ 11] introduces an innovative framework for constructing spatial-\ntemporal tasks. However, general VideoQA benchmarks predominantly focus on YouTube videos\nthat capture everyday life, human actions, and movies, often neglecting to include egocentric videos\nand embodied-specific QA formats.\n3\n--- Page 4 ---\nAnomalyPerceptionPurpose and Function InferenceDynamic Relation-ship PredictionState ChangePredictionTrajectory andMotion Prediction\nKitchenLiving roomDining roomBedroomBathroomOfficeOutdoorGarageStoreSports roomOthers\n321\n148\n118\n69\n55\n36\n35\n23\nEPIC-KITCHENS (239)Ego4D(201)Charades-ego(294)MECCANO(12)Self-recording(110)\n(a) Overview of EOC-Bench dimensions(b) Video source distribution(c) Number of various scenario categories\n34%\n28%\n24%\n13%\n1%Figure 2: Overall data distribution of EOC-Bench .(a)EOC-Bench encompasses three temporal\ndimensions: Past, Present, and Future, comprehensively evaluating 11 embodied cognitive abilities.\n(b) The dataset comprises videos from four distinct open video sources as well as self-recorded\nvideos. (c) It spans a wide range of scenarios, offering a rich diversity of contexts for analysis.\n2.2 Embodied Video Understanding Benchmarks\nIn embodied scenarios, VideoQA-based evaluations serve as effective tools for assessing a model’s\ncomprehension of its environment and tasks. Datasets such as ScanQA [ 14], SQA3D [ 15], and\nEnv-QA [ 54] are typically used for traditional scene question answering, characterized by a closed\nvocabulary. These datasets often exhibit a strong text bias and offer a relatively limited variety of\nquestion forms. On the other hand, RoboVQA [ 16], EgoPlan-Bench [ 55] & EgoPlan-Bench2 [ 56], and\nPCA-Bench [ 57] are introduced test the task-planning abilities of MLLMs. EgoSchema [ 58] utilizes\nfirst-person footage from Ego4D [ 59] to enable video reasoning tasks. More recently, VSI-Bench [ 18]\nhas been developed to specifically evaluate visual-spatial intelligence in MLLMs, and STI-Bench [ 60]\nhas been further developed to evaluate the spatial-temporal world understanding. OpenEQA [ 17]\nand ECBench [ 19] systematically investigate the embodied indoor cognition of MLLMs, providing\na wider scope of evaluation diversity. However, these benchmarks mainly focus on static scene\nexploration, neglecting dynamic first-person operational interactions involving hand and object\nmovements. Furthermore, while these benchmarks try to assess models’ embodied cognitive abilities\nthrough text-based object referencing, they fall short of adequately evaluating models’ capabilities in\nobject-level spatiotemporal reasoning, which is crucial for real-world interactions. In contrast, we\nhave meticulously crafted EOC-Bench to systematically analyze the object-level embodied cognition\nof MLLMs in complex dynamic operational scenes.\n3 EOC-Bench\n3.1 Overview\nAs illustrated in Fig. 2, we introduce EOC-Bench , a meticulously crafted benchmark designed\nto quantitatively assess the object cognition abilities of MLLMs using dynamic egocentric videos.\nEOC-Bench comprises 3,277 question-answer pairs derived from 656 real-world videos. These videos\nare sourced from four publicly available first-person datasets: EPIC-KITCHENS [ 61], Ego4D [ 59],\nCharades-ego [ 62], and MECCANO [ 63], as well as our self-recorded videos captured in various\nenvironments. EOC-Bench includes three dimensions: Past, Present and Future, with a total of 11\ntasks aimed at evaluating a model’s object comprehension capabilities including memory, perception\nand knowledge in ego-centric world. Notably, to achieve accurate object referencing in dynamic\nscenarios, we introduce three types of visual object prompts: bounding boxes, points and masks.\n3.2 Benchmark Construction\n3.2.1 Video Collection\nOur benchmark integrates four established egocentric video datasets: EPIC-KITCHENS [ 61], which\nfeatures kitchen-related scenarios; Ego4D [ 59], encompassing a broad array of daily activities;\n4\n--- Page 5 ---\nCharades-ego [ 62], capturing activity instances across various rooms; and MECCANO [ 63], depicting\nindustrial-like environments where participants construct toy models. These datasets collectively\ncover both indoor and outdoor environments, covering a wide spectrum of activities. To enhance\nscenario diversity, we develop a stratified sampling strategy. Initially, we sample 1,000 videos each\nfrom Charades-ego [ 62] and Ego4D [ 59] and annotate them for scene categories using Qwen2-VL-\n72B [ 24]. We further enhance scene diversity by randomly sampling from videos featuring the same\nsetting, followed by thorough manual quality control to eliminate clips with low information. This\nprocess results in 294 high-quality videos from Charades-ego and 201 from Ego4D. For datasets\nlike EPIC-KITCHENS and MECCANO with uniform scenes, we randomly choose 239 and 12\nrepresentative videos, respectively. All selected videos are uniformly trimmed to durations of 3-10\nminutes for efficient annotation. To address gaps in existing datasets, we self-curate 110 videos\ncapturing three under-represented domains: anomaly perception, physical world dynamics, and\nelectrical appliance operation. To ensure diversity, 5 volunteers contribute to the collection process .\n3.2.2 Capability Taxonomy\nDrawing inspiration from established general VideoQA benchmarks [ 11,36], we propose a hierar-\nchical taxonomy to systematically characterize embodied object cognition capabilities, as shown in\nFigure 2-(a). EOC-Bench comprehensively encompasses three temporal dimensions of first-person\nvideo understanding: Past, Present, and Future.\nPast. This dimension assesses a model’s ability to perceive and interpret the temporal dynamics\nof objects, a critical skill for long-term and complex operations. This capability enables models to\nenhance their current understanding by integrating insights from past interactions. The Past dimension\nis specifically divided into four categories:\n•Object State Retrospection (OSR): Evaluates the capability to monitor changes in object attributes\nincluding color, shape, size, posture, temperature, and motion.\n•Object Location Retrospection (OLR): Measures historical positioning accuracy across multiple\ngranularity: macro-level (room-scale), meso-level (platform/container positioning), and micro-level\n(precise location).\n•Object Relationship Evolution (ORE): Examines changes in object relationships, encompassing\nspatial relationships, motion state dynamics, and temporal sequence relationships.\n•Absolute Time Perception (ATP): Assesses absolute time cognition precision through two key\naspects, including pinpointing specific time points and understanding time durations.\nPresent. This category focuses on evaluating MLLMs’ ability to understand current scenes, with a\nfocus on the perceptual abilities. Crucially, while emphasizing immediate perception of object states\nand environmental conditions, some questions necessitate integration of information from preceding\nframes, demanding a comprehensive understanding of the video for accurate responses. This aspect\nis categorized into four types:\n•Immediate State Recognition (ISR): Evaluates the model’s ability to identify the current state\nof objects, including attributes such as material, shape, functional state, surface condition, pose,\nmotion state, and temperature.\n•Object Relationship (OR): Analyzes inter-object dynamics, including spatial, functional, or\ncomparative relationships between existing objects.\n•Purpose and Function Inference (PFI): Requires deducing the potential uses or functions of ob-\njects based on their external characteristics, materials, configurations, and the contextual scenarios\nin which they are observed.\n•Anomaly Perception (AP): Measures the model’s proficiency in detecting unusual or incongruous\nvisual inputs, with an emphasis on counter-sense co-occurrence. For instance, Fig. 1 illustrates\na scenario where a cosmetic product is placed in an atypical setting, such as a kitchen, to assess\ncommon sense interference in visual interpretation.\nFuture. In embodied intelligence systems, predictive capabilities extend beyond mere observation,\nempowering proactive adaptation to environmental changes. The capability to foresee future events\nis crucial for avoiding hazards and flexibly adapting to changing circumstances. This dimension\n5\n--- Page 6 ---\nKitchenware and TablewareCleaning and Bathroom SuppliesFood and BeveragesHousehold Appliances and FurnitureElectronics and AccessoriesClothing and TextilesOthersTools and EquipmentOffice SuppliesArts and HobbiesContainers and StorageVehicles and Outdoor Products27%2%2%2%4%5%7%7%9%10%12%14%(a) Frequency distribution of top-20object categories in EOC-Bench(b) Video duration(c) Option distributionFigure 3: Statistic analysis ofEOC-Bench : (a) substantial diversity in object categories and usage\ntaxonomies, (b) a wide range of video durations correlated with question count, and (c) a balanced\ndistribution of response options across each question type.\nrelies on the model’s ability to utilize physical laws and common sense knowledge for prediction and\ninference. This dimension is divided into three categories:\n•Trajectory and Motion Prediction (TMP): Anticipates the future path or dynamic motion changes\nof an object based on its current motion and location, enabling models to understand and interact\nwith moving objects more effectively.\n•State Change Prediction (SCP): Predicts future changes in an object’s state due to ongoing actions\nor environmental fluctuations, enabling preemptive response to imminent changes.\n•Dynamic Relationship Prediction (DRP): Foresees potential alterations in inter-object relation-\nships, aiding in the prevention of upcoming collisions or other interactions.\n3.2.3 Construction of Question-Answer Pairs\nTo ensure the high quality of our benchmark, we have developed a sophisticated human-in-the-loop\ndata curation pipeline specifically for the creation of EOC-Bench , and we recruit 10 highly trained\nuniversity students as annotators to participate in the annotation process. Our methodology adopts\na category-independent approach, assigning volunteers a predetermined number of tasks related to\nvarious cognitive abilities. This strategy guarantees a balanced representation of question-answer\n(QA) pairs, covering both rare and common cognitive abilities. EOC-Bench features a mixed-format\nannotation framework with four types of labeling: True/False, Single-Choice, Multiple-Choice\nquestions, which require explicit options, while Open-Ended questions are crafted to primarily focus\non absolute timestamps information for temporal perception abilities.\nDespite leveraging human-annotated data sources and implementing a meticulously designed QA\ngeneration protocol, certain ambiguities and errors may still occur, such as visual prompt offsets,\nomissions, and ambiguous options. To address these issues, a thorough filtering process is carried\nout post-labeling . This involves rigorous cross-checking and verification among annotators to ensure\nboth format accuracy and content validity.\n3.2.4 Evaluation Metrics\nOur EOC-Bench includes diverse question types: True/False ( T F), Single-Choice Answer ( SCA ),\nMultiple-Choice Answer ( MCA ) and Open-Ended Questions ( OQ). Following established prac-\ntices [ 11,35], we adopt conventional Accuracy based on exact matches for the first three tasks. For\nOpen-Ended Questions, which require assessing open-ended continuous temporal predictions, we\nintroduce a novel metric, Multi-Scale Temporal Accuracy (MST A ), to accurately evaluate OQtasks.\nSpecially, we develop a relative error percentage tolerance mechanism to accommodate varying\nerror tolerance across different time durations, whether long or short periods. Given a ground\ntruth timestamp Tgtand a predicted time Tpred, we first calculate the absolute deviation ∆T=\n|Tpred−Tgt|. We then establish dynamic error margins using relative percentage thresholds C=\n{1%,10%,20%,30%}, setting scale-adaptive boundaries {α·Tgt|α∈C}. These thresholds are\nderived from human error analysis, which is detailed in the Section A.1. A prediction satisfies\nthreshold αwhen ∆T≤α·Tgt. The final MST A score is computed by averaging performance\nacross temporal scales using:\nMST A =1\n4X\nα∈C1(∆T≤α·Tgt). (1)\n6\n--- Page 7 ---\nBy utilizing various thresholds, MST A strikes a balance between strictness and flexibility: lower\nthresholds demand precise alignment, while higher thresholds allow for variability in responses.\n3.3 Benchmark Statistics\nEOC-Bench comprises 3,277 QA pairs, systematically evaluating MLLMs across 11cognitive per-\nspectives. These include 1,422 questions focused on the Past dimension, 1,348 on the Present, and\n507on the Future. Each question is associated with one or more objects, and the corresponding\nvisual prompts are annotated on the final frame of the video. The benchmark incorporates a wide\narray of object types, encompassing 728categories that cover various usage scenarios. The category\ndistribution, along with the top 20 categories, is displayed in Fig. 3-(a). Additionally, Fig. 3-(b)\nillustrates the distribution of average video durations, which vary widely from several seconds to over\nsix minutes. To maintain an even probability distribution for each response option, we rearranged the\norder of different answer types, as depicted in Fig. 3-(c).\n4 Experiment\n4.1 Experimental Setup\nBased on EOC-Bench , we comprehensively evaluate a diverse range of general-purpose MLLMs,\nincluding both proprietary MLLMs and open-source models. For proprietary MLLMs , we evaluate\nGPT-4o [ 3], GPT-4o-mini [ 3] and Gemini-2.0-flash [ 4]. Among open-source MLLMs , we test\nQwen2.5-VL [ 1], InternVL2.5 [ 20], VideoLLaMA2&3 [ 21,25], LLaV A-OneVision [ 27], LLaV A-\nVideo [ 64], NVILA [ 65], LongV A [ 31] and VideoLLaV A [ 28]. Additionally, we assess the object-\nfocused MLLMs including VideoRefer [ 12], ViP-LLaV A [ 23], Osprey [ 22] and SPHINX-V [ 66]. For\nall models, we perform zero-shot inference to assess their object cognition capabilities using their\ndefault settings. More detailed configurations are provided in the Section C.1.\n4.2 Main Results\nIn this section, we provide a detailed performance comparison and analysis. Table 2 reports the main\nexperimental results.\nBaselines. The “Random” entry in the first row denotes random guessing. For multiple-choice\nanswers, we randomly select the number of options and the corresponding choices. For open-ended\nquestions in Absolute Time Perception (ATP) task within the Past dimension, values are randomly\nselected between 0 and video length. Additionally, we also assess human performance on EOC-Bench\nusing video input with three volunteers.\nProprietary MLLMs. Despite a significant performance gap compared to human capabilities, the\nleading proprietary model, GPT-4o [ 3], delivers commendable results with 61.83%. GPT-4o [ 3]\nsuccessfully meets the passing criteria across various subtasks, showcasing its potential in multiple\ndomains. However, the model faces challenges in the Past dimension, particularly with Absolute Time\nPerception (ATP) and Object Relationship Evolution (ORE), even when timestamps are provided for\neach frame. This indicates the model’s limited capacity to perceive and remember temporal changes.\nThe difficulties encountered by GPT-4o [ 3] in these areas underscore a significant opportunity for\nimprovement, highlighting the need for advancements in temporal awareness and memory retention.\nOpen-source MLLMs. Top-tier open-source models, like InternVL2.5-78B [ 20], still show a\nnoticeable gap compared to closed-source models, trailing GPT-4o [ 3] by 9.5%. Other state-of-\nthe-art Video-LLMs on existing benchmarks, such as Qwen2.5-VL [ 1], VideoLLaMA3 [ 21], and\nNVILA [ 65], underperform on our tasks, particularly in Object Relationship Evolution (ORE) and\nAbsolute Time Perception (ATP). A substantial number of these models are tagged with grey marks,\nindicating significant limitations in their memory recall capabilities.\nObject-level MLLMs. Object-level MLLMs, such as the recent VideoRefer [ 12], outperform\nmany competitive models, highlighting the effectiveness of the object-level representation learning.\nHowever, they still face challenges in the Object Relationship Evolution (ORE) task when dealing\nwith dense, similar objects in complex operational scenes, and in the Absolute Time Perception (ATP)\ntask with dynamic temporal changes. Given the scarcity of open-source object-level video MLLMs,\nwe also evaluated some image-level MLLMs, like ViP-LLaV A [ 23], Osprey [ 22] and SPHINX-V [ 66].\n7\n--- Page 8 ---\nMethod Input MeanPast Present Future\nOSR OLR ORE ATP Mean ISR OR PFI AP Mean TMP SCP DRP Mean\nRandom - 24.87 29.36 26.56 26.46 10.92 24.75 26.30 23.30 21.29 26.47 24.41 27.80 21.96 34.09 26.43\nHuman - 94.63 96.95 93.49 94.71 74.30 90.67 99.33 98.23 96.77 93.14 97.99 95.12 95.79 90.91 94.67\nProprietary Multimodal Foundation Models\nGemini-2.0-flash [4] 32f 45.45 50.42 34.42 22.84 10.32 29.78 61.98 56.34 69.35 51.96 61.50 52.20 54.70 48.86 52.53\nGPT-4o-mini∗[3] 32f 49.47 53.26 52.35 29.68 21.10 39.47 58.46 49.26 67.74 58.82 58.31 56.59 50.00 54.55 53.45\nGemini-2.0-flash∗[4] 32f 57.38 63.46 65.10 32.56 28.60 47.87 68.84 57.52 69.68 65.69 65.95 58.54 64.02 57.95 60.75\nGPT-4o∗[3] 32f 61.83 66.04 71.93 46.56 34.46 54.91 71.46 52.85 78.18 62.75 67.32 69.61 68.69 68.97 69.11\nOpen-Source Multimodal Foundation Models\nVideoLLaV A-7B [28] 8f 34.11 31.86 37.94 27.58 13.14 27.97 41.04 35.10 40.97 37.25 39.24 40.98 31.78 44.32 37.67\nLongV A-7B [31] 32f 35.34 36.84 43.36 17.83 15.32 28.69 38.19 36.58 48.06 42.16 40.36 39.02 42.06 40.91 40.63\nNVILA-8B [65] 32f 37.69 37.40 46.61 20.89 12.09 29.69 44.39 41.59 49.03 46.08 44.88 42.44 38.32 44.32 41.03\nVideoLLaMA2.1-7B [25] 16f 37.74 44.88 42.82 19.22 11.64 30.08 47.24 37.17 51.94 39.22 45.18 40.00 36.92 44.32 39.45\nQwen2.5-VL-3B [1] 1fps 38.17 38.78 48.78 23.96 7.66 30.34 49.92 38.94 45.16 38.24 45.18 42.93 36.57 50.00 41.45\nVideoLLaMA3-2B [21] 1fps 38.41 37.12 46.88 21.17 11.26 29.57 49.92 43.36 48.39 38.24 47.03 43.41 36.11 43.18 40.28\nLLaV A-OV-7B [27] 32f 40.46 40.72 45.53 22.84 9.53 30.15 54.10 43.07 52.58 46.08 50.37 47.32 37.38 46.59 43.00\nVideoLLaMA2-72B [25] 16f 41.55 43.77 51.22 24.23 6.46 32.03 50.08 37.46 58.06 45.10 48.37 49.27 50.47 51.14 50.10\nLLaV A-Video-7B [64] 32f 41.82 44.32 48.51 22.56 9.76 31.82 54.27 43.66 55.81 49.02 51.56 45.85 40.65 47.73 43.98\nQwen2.5-VL-7B [1] 1fps 43.13 47.37 46.34 21.45 8.18 31.38 57.29 44.54 59.35 49.02 53.93 48.78 46.30 46.59 47.35\nInternVL2.5-8B [20] 32f 45.15 45.71 54.47 39.00 9.76 37.87 55.44 48.97 54.84 41.18 52.60 49.76 38.79 53.41 45.76\nVideoLLaMA3-7B [21] 1fps 46.04 45.15 52.85 24.51 15.54 35.00 57.96 48.67 62.58 49.02 56.01 52.20 49.54 48.86 50.49\nLLaV A-OV-72B [27] 32f 47.88 46.81 50.95 26.46 12.91 34.81 64.15 51.33 64.52 49.02 59.87 58.05 46.73 54.55 52.66\nLLaV A-Video-72B [64] 32f 49.59 49.03 56.91 26.74 24.02 39.59 63.32 47.20 63.87 50.00 58.38 56.10 55.14 47.73 54.24\nQwen2.5-VL-72B [1] 1fps 49.87 51.25 51.22 40.11 8.48 38.41 61.31 47.79 67.10 57.84 58.98 56.10 60.65 54.55 57.76\nInternVL2.5-38B [20] 32f 52.31 55.40 59.62 30.92 10.89 39.89 64.15 54.28 71.29 64.71 63.35 60.98 54.67 57.95 57.79\nInternVL2.5-78B [20] 32f 52.33 53.46 63.96 33.15 12.01 41.35 66.67 50.74 67.10 52.94 61.72 67.80 50.47 54.55 58.19\nObject-level Multimodal Models\nOsprey-7B [22] 1f 27.36 22.71 20.33 15.88 7.41 16.78 42.88 29.50 32.58 29.41 36.13 39.51 30.37 28.41 33.73\nSPHINX-V-13B [66] 1f 29.21 25.48 23.31 13.37 3.83 16.79 41.71 31.27 44.19 39.22 39.47 41.46 31.02 39.77 36.74\nViP-LLaV A-7B [23] 1f 32.82 35.73 36.86 17.55 8.26 25.00 42.88 35.99 46.45 26.47 40.73 34.63 29.91 40.91 33.73\nVideoRefer-7B [12] 16f 40.44 47.37 55.01 23.40 10.59 34.69 48.91 39.82 53.55 38.24 46.88 41.95 35.51 43.18 39.45\nTable 2: Performance of representative MLLMs on EOC-Bench .The best results are marked\nwith orange . The results below random guess are marked with grey . Entries in grey indicate\nimage-level methods that use only the last frame as input. ∗: We manually added a timestamp before\neach frame. [Nf] denotes that the model takes N frames uniformly sampled from a video as input.\nWhile these models underperform in the Past dimension, which requires memory of previous frames,\nthey still deliver reasonably performance in the Present and Future dimensions.\n4.3 Analysis Across Different Question Types\nWe conduct an analysis of the models’ results across different question types to facilitate a more\ncomprehensive horizontal and vertical examination, as illustrated in Table 3.\nSmaller MLLMs Often Struggle with Multiple-Choice Questions. Many MLLMs face challenges\nin answering multiple-choice questions ( MCA ), often scoring lower than random guess (indicated\nby a grey mark). This issue is particularly evident in smaller models, those with 7B parameters or\nfewer. We surmise that these smaller models have overfitted to simple single-choice questions during\ntraining, hindering their ability to follow instructions for handling questions with multiple options.\nFew MLLMs are Time-sensitive. TheOQ metric, which measures the model’s ability to perceive\npast time, indicates that the some models perform below random guessing levels, with 9/21. Even the\nstrongest open-source model scores only 24.02%, just 13.1% above random chance. This underscores\na crucial capability that is lacking in most models, yet is essential in the field of embodied AI.\nLarger MLLMs Excel in Handling Future-Oriented Problems. Future-oriented tasks demand a\ncombination of commonsense reasoning and extensive knowledge. Our observations indicate that as\nthe size of the model increases, so does its reasoning capability. For instance, Qwen2.5-VL [ 1] with\n3B, 7B, and 72B parameters, as well as VideoLLaMA3 [ 21] with 2B and 7B parameters, demonstrate\nsignificantly improved performance in these tasks. This trend suggests that larger MLLMs are better\nequipped to tackle problems that require forward-thinking and predictive reasoning, due to their\nenhanced capacity to integrate and process complex patterns of information.\nPast-Oriented Questions Pose Greater Challenges to MLLMs. Through a comparative analysis\nof similar problem types, we discover that models generally perform worse on questions related to\npast events compared to other categories. While smaller models may grapple with future-oriented\nproblems, larger models often fall short when addressing past-oriented questions. This difficulty in\naccurately recalling and processing past information is a prevalent issue among current MLLMs,\nindicating a significant area for improvement in their design and training.\n8\n--- Page 9 ---\nMethod InputMean Past Present Future\nSCA MCA T F OQ SCA MCA T F OQ SCA MCA T F SCA MCA T F\nRandom - 26.27 18.34 50.00 10.92 29.51 20.56 50.00 10.92 24.72 14.95 50.00 23.53 18.60 50.00\nProprietary Multimodal Foundation Models\nGPT-4o∗[3] 32f 69.03 54.44 63.86 34.46 67.00 49.06 55.56 34.46 69.29 58.08 68.85 72.76 63.95 61.46\nGPT-4o-mini∗[3] 32f 57.31 32.80 58.76 21.10 52.07 26.26 44.44 21.10 60.79 41.24 67.14 58.20 34.88 54.08\nGemini-2.0-flash∗[4] 32f 68.68 28.49 63.28 28.60 66.15 20.14 44.44 28.60 70.66 37.63 71.43 68.11 34.88 59.18\nOpen-Source Multimodal Foundation Models\nVideoLLaV A-7B [28] 8f 41.55 11.11 54.80 13.14 41.24 8.36 33.33 13.14 42.34 14.95 58.57 39.63 11.63 54.08\nVideoLLaMA2.1-7B [25] 16f 47.67 9.01 52.78 11.64 45.40 8.36 55.56 11.64 51.29 9.28 50.00 41.27 10.81 54.46\nVideoLLaMA2-72B [25] 16f 53.15 13.15 51.67 6.46 52.21 5.23 55.56 6.46 52.77 22.68 51.43 56.63 18.92 51.49\nLongV A-7B [31] 32f 41.73 16.40 54.24 15.32 41.24 9.06 44.44 15.32 41.97 23.71 61.43 42.11 24.42 50.00\nNVILA-8B [65] 32f 49.73 0.53 55.37 12.09 47.41 0 66.67 12.09 51.85 1.55 57.14 48.30 0 53.06\nInternVL2.5-8B [20] 32f 54.95 23.99 57.63 9.76 55.11 22.30 55.56 9.76 56.55 26.80 62.86 49.23 23.26 54.08\nInternVL2.5-38B [20] 32f 64.45 26.28 62.71 10.89 62.67 10.10 55.56 10.89 66.70 43.30 67.14 61.30 41.86 60.20\nInternVL2.5-78B [20] 32f 64.32 26.63 61.58 12.01 62.55 16.38 55.56 12.01 65.13 40.21 68.57 65.94 30.23 57.14\nLLaV A-OV-7B [27] 32f 54.18 0 57.63 9.53 49.43 0 55.56 9.53 58.67 0 61.43 50.77 0 55.10\nLLaV A-OV-72B [27] 32f 61.32 12.17 61.02 17.22 55.49 1.74 77.78 17.22 66.05 22.68 67.14 59.75 23.26 55.10\nLLaV A-Video-7B [64] 32f 56.23 0 57.06 9.76 52.33 0 55.56 9.76 59.78 0 67.14 53.87 0 50.00\nLLaV A-Video-72B [64] 32f 62.73 10.23 60.45 24.02 59.27 2.44 66.67 24.02 65.22 18.56 62.86 62.85 17.44 58.16\nQwen2.5-VL-3B [1] 1fps 50.50 1.44 56.67 7.66 50.44 0 66.67 7.66 51.85 3.09 58.57 46.25 2.67 54.46\nQwen2.5-VL-7B [1] 1fps 54.25 13.67 62.22 8.18 49.81 6.27 66.67 8.18 58.39 23.71 68.57 51.35 16.00 57.43\nQwen2.5-VL-72B [1] 1fps 61.45 27.16 54.44 8.48 57.12 21.60 33.33 8.48 63.19 34.54 61.43 66.07 29.33 51.49\nVideoLLaMA3-2B [21] 1fps 49.98 3.70 57.06 11.26 47.29 1.05 55.56 11.26 53.23 6.19 64.29 45.68 6.90 52.04\nVideoLLaMA3-7B [21] 1fps 57.15 17.27 55.00 15.54 52.33 9.41 44.44 15.54 60.89 26.29 62.86 56.46 24.00 50.50\nVideoRefer-7B [12] 16f 54.27 0 54.24 10.59 57.12 0 55.56 10.59 54.43 0 60.00 46.75 0 50.00\nTable 3: Performance of representative MLLMs across different question types: SCA (Single-\nChoice Answer), MCA (Multi-Choice Anwer), T F (True/False), OQ (Open-Ended Question). The\nbest results are marked with orange . The results below random guess are marked with grey .∗:\nWe manually added a timestamp before each frame.\nObject State RetrospectionObject Location RetrospectionObject Relationship EvolutionAbsolute Time PerceptionImmediate State RecognitionObject RelationshipPurpose and Function InferenceAnomaly PerceptionTrajectory and Motion PredictionState Change PredictionDynamic Relationship PredictionOpen-Ended QuestionTrue/FalseMulti-ChoiceAnswerSingle-ChoiceAnswerTrue/FalseMulti-ChoiceAnswerMulti-ChoiceAnswer\nSingle-ChoiceAnswerTrue/FalseSingle-ChoiceAnswerCategory   PastCategory   PresentCategory   Future\nFigure 4: Comparison of mainstream MLLMs on EOC-Bench . Left: Performance on 11 evaluation\ntasks within EOC-Bench .Right: Performance across different question types spanning Past, Present\nand Future categories.\n4.4 Qualitative Results\nTo intuitively showcase the performance of mainstream MLLMs, including both proprietary MLLMs\nand open-source models, across various evaluation dimensions of EOC-Bench , we provide a detailed\ncomparison illustrated in Fig. 4. We assess the models across the 11 evaluation tasks, as well as\nmultiple question types spanning three temporal categories.\n4.5 Multi-Frame Gain\nWe assess the multi-frame gain for frames 1, 8, and 32 within EOC-Bench . The strong proprietary\nMLLMs, GPT-4o [ 3] and Gemini-2.0-flash [ 4], exhibits a substantial performance boost, gaining\n24.6% and 20.1% when moving from single-frame input to 32-frame input setting. This improvement\nis particularly pronounced in past-oriented tasks, with an improvement of 49.2% and 60.2%. These\nfindings underscore the critical role of multi-frame reasoning in the EOC-Bench , especially for\nmemory recall tasks. The ability to access information from previous frames can significantly enhance\n9\n--- Page 10 ---\nMean Past Present Future\n# Frames 1f 8f 32f γ↑ 1f 8f 32f γ↑ 1f 8f 32f γ↑ 1f 8f 32f γ↑\nGPT-4o∗[3] 49.6 58.6 61.8 24.6 36.8 50.6 54.9 49.2 60.2 64.7 67.3 11.8 58.0 65.2 69.1 19.1\nGemini-2.0-flash∗[4] 47.8 51.2 57.4 20.1 29.9 37.7 47.9 60.2 64.7 63.4 66.0 2.0 53.1 57.0 60.8 14.5\nInternVL2.5-78B [20] 47.6 51.3 52.3 9.9 33.1 38.9 41.4 24.0 59.8 64.3 61.7 3.2 55.8 56.4 58.2 4.3\nVideoLLaMA3-7B [21] 42.1 45.5 46.2 10.4 28.5 34.3 36.1 26.7 54.6 55.0 55.1 0.9 46.5 49.7 50.5 8.6\nTable 4: Performance of representative MLLMs with varying input frames. ‘1f’ denotes using\nonly the last frame, while ‘8f/32f’ refers to frames that are uniformly sampled, including the last\nframe. γrepresents the rate of increase in performance from ‘1f’ to ‘32f’.\nboth current and future understanding. Other open-source models , such as InternVL2.5-78B [ 20],\nand VideoLLaMA3-7B [ 21], demonstrate similar trends. However, their ability to effectively process\nmultiple frames is comparatively weaker, resulting in less pronounced performance improvements.\nThis highlights the potential benefits of enhancing multi-frame processing capabilities in MLLMs to\nachieve more substantial performance gains across a variety of tasks.\n5 Conclusion\nIn this paper, we presented EOC-Bench , an innovative benchmark aimed at evaluating the embodied,\nobject-level cognition capabilities of MLLMs. EOC-Bench thoroughly assesses MLLMs within the\nscenes involving dynamic egocentric interactions across three temporal dimensions: Past, Present\nand Future. To ensure high quality, we developed a mixed-format human-in-the-loop annotation\nframework and introduced a multi-scale temporal accuracy metric to enhance the precision of open-\nended questions. Extensive evaluations conducted on EOC-Bench across a range of proprietary and\nopen-source models, have revealed that many MLLMs face challenges in effectively performing\nembodied object cognition tasks, particularly in recalling and processing past information as well as\nin absolute time perception. We hope EOC-Bench will drive progress in developing MLLMs capable\nof understanding a more complex and diverse physical world.\nReferences\n[1]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025.\n[2]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS , 2023.\n[3]Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[4]Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nDamien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n[5]Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi,\nRichard Newcombe, and Yuheng Carl Ren. Aria digital twin: A new benchmark dataset for egocentric\n3d machine perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npages 20133–20143, 2023.\n[6]Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott Vainio,\nZheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household\nactivities in virtual, interactive, and ecological environments. In Conference on robot learning , pages\n477–490. PMLR, 2022.\n[7]Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Lijin Yang, Xinyuan Chen, Yaohui Wang, Zheng\nNie, Jinyao Liu, et al. Vinci: A real-time embodied smart assistant based on egocentric vision-language\nmodel. arXiv preprint arXiv:2412.21080 , 2024.\n[8]Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, and Lap-Pui Chau. A\nsurvey of embodied learning for object-centric robotic manipulation. arXiv preprint arXiv:2408.11537 ,\n2024.\n[9]Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming\nLiu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic\nmanipulation. In CVPR , pages 18061–18070, 2024.\n10\n--- Page 11 ---\n[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. In ICML , pages 8469–8488. PMLR, 2023.\n[11] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\nLuo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In CVPR , 2024.\n[12] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao,\nWenqiao Zhang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm.\nCVPR , 2025.\n[13] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language\nquery. In ICCV , pages 5267–5275, 2017.\n[14] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering\nfor spatial scene understanding. In CVPR , pages 19129–19139, 2022.\n[15] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.\nSqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474 , 2022.\n[16] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan,\nChristine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi, et al. Robovqa: Multimodal\nlong-horizon reasoning for robotics. In ICRA , pages 645–652, 2024.\n[17] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha\nSilwal, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR , 2024.\n[18] Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space:\nHow multimodal large language models see, remember, and recall spaces. In CVPR , 2025.\n[19] Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, et al. Ecbench: Can\nmulti-modal foundation models understand the egocentric world? a holistic embodied cognition benchmark.\nInCVPR , 2025.\n[20] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\nHao Tian, et al. Expanding performance boundaries of open-source multimodal models with model, data,\nand test-time scaling. arXiv preprint arXiv:2412.05271 , 2024.\n[21] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng,\nYuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image\nand video understanding. arXiv preprint arXiv:2501.13106 , 2025.\n[22] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey:\nPixel understanding with visual instruction tuning. In CVPR , pages 28202–28211, 2024.\n[23] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P Meyer, Yuning Chai, Dennis Park, and Yong Jae\nLee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In CVPR , pages\n12914–12923, 2024.\n[24] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\nresolution. arXiv preprint arXiv:2409.12191 , 2024.\n[25] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi\nZhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio\nunderstanding in video-llms. arXiv preprint arXiv:2406.07476 , 2024.\n[26] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding. arXiv preprint arXiv:2306.02858 , 2023.\n[27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\nYanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 ,\n2024.\n[28] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united\nvisual representation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023.\n[29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023.\n11\n--- Page 12 ---\n[30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\ndetailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 ,\n2023.\n[31] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang,\nHaoran Tan, et al. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852 , 2024.\n[32] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang.\nTokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392 , 2024.\n[33] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang,\nMengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, and Beng Chin Ooi. Healthgpt:\nA medical large vision-language model for unifying comprehension and generation via heterogeneous\nknowledge adaptation. In ICML , 2025.\n[34] Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou,\nZheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, and Yueting Zhuang. Hyperllava: Dynamic visual and\nlanguage expert tuning for multimodal large language models, 2024.\n[35] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\nYunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of\nmulti-modal llms in video analysis. arXiv preprint arXiv:2405.21075 , 2024.\n[36] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-\nvideo: A long-form multi-shot benchmark for holistic video understanding. In NeurIPS , 2025.\n[37] Perception test: A diagnostic benchmark for multimodal video models , 2023.\n[38] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity understanding. In CVPR , pages 961–970, 2015.\n[39] Jiaying Liu, Sijie Song, Chunhui Liu, Yanghao Li, and Yueyu Hu. A benchmark dataset and comparison\nstudy for multi-modal human action analytics. TOMM , 2020.\n[40] Andong Deng, Taojiannan Yang, and Chen Chen. A large-scale study of spatiotemporal representation\nlearning with a new benchmark on action recognition. In ICCV , pages 20519–20531, 2023.\n[41] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events\nin videos. In ICCV , pages 706–715, 2017.\n[42] Rikito Takahashi, Hirokazu Kiyomaru, Chenhui Chu, and Sadao Kurohashi. Abstractive multi-video\ncaptioning: Benchmark dataset construction and extensive evaluation. In LREC-COLING , pages 57–69,\n2024.\n[43] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating\nlarge video description models. arXiv preprint arXiv:2407.00634 , 2024.\n[44] Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, et al. Vilma: A zero-shot benchmark for\nlinguistic and temporal grounding in video-language models. arXiv preprint arXiv:2311.07022 , 2023.\n[45] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou.\nTempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476 , 2024.\n[46] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to\nexplaining temporal actions. In CVPR , pages 9777–9786, 2021.\n[47] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman\nCohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. In\nICLR , 2025.\n[48] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, et al. Mlvu: A comprehensive benchmark\nfor multi-task long video understanding. In CVPR , 2025.\n[49] Longvideobench: A benchmark for long-context interleaved video-language understanding , 2025.\n[50] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, et al. Movqa: A benchmark of\nversatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817 , 2023.\n12\n--- Page 13 ---\n[51] Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny.\nInfinibench: A comprehensive benchmark for large multimodal models in very long video understanding.\narXiv preprint arXiv:2406.19875 , 2024.\n[52] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan\nChen, et al. Mmvu: Measuring expert-level multi-discipline video understanding. arXiv preprint\narXiv:2501.12380 , 2025.\n[53] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu.\nVideo-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint\narXiv:2501.13826 , 2025.\n[54] Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. Env-qa: A video question answering benchmark for\ncomprehensive understanding of dynamic environments. In ICCV , pages 1675–1685, 2021.\n[55] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui\nLiu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models.\nCoRR , 2023.\n[56] Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: A benchmark for\nmultimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447 ,\n2024.\n[57] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi\nMeng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in\nperception-cognition-action chain. arXiv preprint arXiv:2402.15527 , 2024.\n[58] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark\nfor very long-form video language understanding. In NeurIPS , 2023.\n[59] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar,\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of\negocentric video. In CVPR , pages 18995–19012, 2022.\n[60] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are\nmllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765 , 2025.\n[61] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision:\nThe epic-kitchens dataset. In ECCV , pages 720–736, 2018.\n[62] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego:\nA large-scale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626 , 2018.\n[63] Francesco Ragusa, Antonino Furnari, Salvatore Livatino, and Giovanni Maria Farinella. The meccano\ndataset: Understanding human-object interactions from egocentric videos in an industrial-like domain. In\nWACV , 2021.\n[64] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction\ntuning with synthetic data. arXiv preprint arXiv:2410.02713 , 2024.\n[65] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi\nCao, Yuxian Gu, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468 ,\n2024.\n[66] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shang-\nhang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to\ncomprehend what you want. In ICLR , 2025.\n[67] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\nunleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023.\n13\n--- Page 14 ---\nAppendix\nIn this document, we offer additional details about our benchmark. The Appendix is organized as\nfollows:\n• § A: Additional details on EOCBench;\n• § B: More experimental analysis;\n• § C: Experimental setup;\n• § D: Additional dataset analysis;\n• § E: Limitations and broader impacts;\n• § F: Asset license and consent;\n• § G: More exemplar visualizations.\nA Additional Details on EOCBench\nA.1 Human Error Analysis for Evaluation Metrics\nTo accurately evaluate the Open-Ended Questions ( OQ) task, we have developed a novel metric,\nMulti-Scale Temporal Accuracy (MST A ) for comprehensive temporal perception, as introduced in\nthe Section 3.2.4 of the main paper. Here, we provide additional details on the choice of dynamic\nerror margins in MST A through the carefully designed human error analysis. Specially, we first\nasked three volunteers to answer this type of question, and then analyzed the error ratio compared to\nthe ground truth, expressed as r= (Tpred−Tgt)/Tgt. We compared all rvalues from all questions\nand created a histogram of these results, as shown in Fig. 5-(a).\nIt can be observed that the error ratios are primarily concentrated around 0, with absolute values rarely\nexceeding 30%. We then conduct a quantile analysis of |r|, selecting quantiles at 50%, 75%, 90%,\n95%, with corresponding error ratio being 1.4%, 11.9%, 20% and 30%, respectively. Based on these\nanalyses, we set the threshold for dynamic error margins at {1%, 10%, 20%, 30%} for subsequently\nscale-adaptive boundaries as described in the main paper. A 1% threshold demands near-exact\nalignment, signifying extreme precision, whereas a 30% threshold caters to greater variability in\nresponses, accommodating almost all human answers within this margin. Fig. 5-(b) depicts the\nstatistical distribution of human error ratio. This dynamic threhold scheme balances strictness and\nflexibility, ensuring that our framework captures the spectrum of human error while maintaining\nstringent evaluation criteria.\nDensityError Ratio (%)(a)(b)\n<1%1%-10%10%-20%20-30%>30%\nHistogram of HumanError Ratio\nFigure 5: Statistics distribution of human error ratio. (a) displays a histogram depicting the\ndensity and spread of human error ratios; (b) presents a pie chart categorizing the error ratios into\nquantitative segments.\n14\n--- Page 15 ---\nA.2 Additional Annotation Details\nA.2.1 Human-in-the-Loop Annotation Pipeline\nTo ensure the high quality of our benchmark, we employ a rigorous human-in-the-loop annotation\npipeline comprising two stages: initial annotation stage, followed by cross-checking and verification\nstage. Additional details are provided below:\nInitial annotation. In the initial annotation phase, each annotator is assigned questions belonging\nto a specific category, such as Past, Present, or Future, along with a set of randomly selected videos to\nensure the data diversity and a relatively uniform distribution. To ensure a thorough understanding of\neach category, annotators are provided with a detailed guide for each category. During the annotation\nprocess, annotators are permitted to pause and examine any frame within the video.\nCross-check and verification. During the cross-check procedure, each annotator reviews the work\nof another, focusing primarily on two key aspects : the quality of the question-answer pairs and the\naccuracy of the annotated visual prompts. If a visual prompt, like point, box and mask, is of low\nquality or missing, the annotator can either carefully reannotate it or discard it; 185 object prompts\nwere reannotated and 34 were discarded. For question-answer pairs that are deemed low quality, the\nreviewer must definitely discuss the issues with the original annotator to finalize the annotation. In\ntotal, 76 questions required collaborative resolution during this verification phase.\nBesides, we provide brief biographies of all 10 annotators who participated in the annotation process\nin the Table 5.\nID Academic Status Field of Study\n1 First-Year Master’s Student Computer Science\n2 First-Year Master’s Student Computer Science\n3 First-Year Master’s Student Robotics\n4 Second-Year Master’s Student Computer Science\n5 Second-Year Master’s Student Computer Science\n6 Second-Year Master’s Student Robotics\n7 Recent Master’s Graduate Robotics\n8 First-Year Ph.D. Student Computer Science\n9 Third-Year Ph.D. Student Computer Science\n10 Recent Ph.D. Graduate Computer Science\nTable 5: Brief biographies of the 10 human annotators in EOC-Bench .\nB More Experimental Analysis\nB.1 Performance of Various Visual Object Prompts\nTo validate the effectiveness of visual prompts for object referencing, we conduct additional experi-\nments on the representative MLLMs with various visual prompts, including point, box, and mask.\nThe comparison results are presented in Table 6. In the main paper, we employ box prompt as the\ndefault setting. Boxes, compared to masks, are easier to obtain in practical applications. Additionally,\ncompared to points, boxes offer more precise references.\nB.2 Quantitative Error Analysis in EOC-Bench\nTo quantify and identify the primary challenges of our EOC-Bench , we perform a comprehensive\nerror analysis on representative MLLMs, examining both choice-based and open-ended questions.\nB.2.1 Choice-based Questions\nFor choice-based questions, we conduct analysis on the top-performing MLLM, GPT-4o [ 3]. Specially,\nwe randomly sampled 300 choice-based erroneous QAs from EOC-Bench , with 30 QAs for each task.\nWe then meticulously examined these errors and categorized them into four primary types :\n15\n--- Page 16 ---\nPoint Box Mask\nModel Mean Past Present Future Mean Past Present Future Mean Past Present Future\nInternVL2.5-8B [20] 41.83 36.35 45.48 47.51 42.05 33.45 49.70 45.36 42.12 35.04 48.07 46.15\nQwen2.5VL-7B [1] 41.21 31.81 49.18 46.35 43.13 31.38 53.93 47.35 41.31 30.64 52.08 42.60\nVideoLLaMA3-7B [21] 45.24 37.04 52.89 47.93 46.04 35.00 56.01 50.49 46.10 35.41 55.93 49.94\nLLaV A-Video-7B [64] 41.13 32.82 49.04 43.39 39.50 32.67 48.96 43.39 40.91 32.40 48.96 43.39\nTable 6: Performance of representative MLLMs with different visual prompt inputs.\n•Perception Error. This type of error involves issues with perception in the current frame, including\ninterference from previous frames, insufficient attention to finer details, counting errors, and\nintra-frame interferences.\n•Memory Error. This error type reflects incorrect observation or recall of information from previous\nframes, including interference from current frames and missing observations, suggesting that the\n32 sampled frames are insufficient to answer the memory-related questions.\n•Relational Reasoning Error. This type of error involves difficulties in perceiving or inferring\nsimple relationships between objects.\n•Knowledge Error. This category encompasses errors in reasoning, common sense, and calculation.\nOSRORELRPFIORISRAPTMPSCPDRPDistribution of Error Types by Task \n020406080100\n61%\n22%\n7%\n10%\n93%\n3%\n4%\n16%\n59%\n19%\n6%\nPerception ErrorCurrent Visual Perception ErrorPrevious Frame InterferenceDetailErrorCountingErrorIntra-frame InterferenceMemory ErrorHistorical Frame ErrorCurrent Frame InterferenceMissing ObservationRelational Reasoning ErrorKnowledge ErrorReasoning ErrorCalculation ErrorCommon Sense ErrorPastPresentFuture%\nFigure 6: Quantitative error analysis by type for choice-based questions in EOC-Bench .\nIn the Past category, as illustrated in Fig. 6, memory errors are predominant, accounting for 93% of the\nerrors. These are primarily due to insufficient processing of historical frames (73%) and interference\nfrom current frame (17%). The remaining 10% are missing observation errors, which highlight the\ninherent constraints of fixed-frame sampling strategies. These findings point to a significant weakness\nof GPT-4o [ 3] in temporal context modeling, particularly its difficulty in effectively retaining and\nusing cross-frame information for video understanding tasks.\nIn the Present category, perception errors account for 61%, followed by knowledge errors (22%) and\nmemory errors (7%). Notably, intra-frame interference constitutes a significant portion of perception\nerrors, revealing the model’s limitations in regional-level visual perception and its susceptibility\nto hallucinatory artifacts. These observations suggest that spatial perception remains a persistent\nchallenge.\nIn the Future category, approximately 59% of errors are knowledge-related issues, indicating\nlimitations in reasoning abilities and common sense understanding.\n16\n--- Page 17 ---\nError Ratio (%)DensityComparison of Time Perception DeviationsHumanGPT-4oLLaV A-Video-72BVideoLLaMA3-7BQwen2.5-VL-72BNVILA-8BHumanGPT-4oLLaVA-Video-72B\nVideoLLaMA3-7BNVILA-8BQwen2.5-VL-72BAccuracy\nAccuracy\nAccuracyAccuracy\nAccuracy\nAccuracyThresholdThresholdThreshold\nThresholdThresholdThresholdFigure 7: Quantitative error analysis for open-ended questions in EOC-Bench .Left: Density analysis\nof temporal perception deviations (error ratio) among humans and models. Right: Model accuracy\nacross different time thresholds for dynamic error margins.\nModel Frames API Checkpoint / HF Checkpoint Do Max New Temp. Top-P\nSample Tokens\nProprietary Multimodal Foundation Models\nGPT-4o-mini [3] 32 gpt-4o-mini-2024-07-18 1024 0 1\nGPT-4o [3] 32 gpt-4o-2024-08-06 1024 0 1\nGemini-2.0-Flash [4] 32 gemini-2.0-flash 1024 0 1\nOpen-Source Multimodal Foundation Models\nInternVL2.5-8B [20] 32 OpenGVLab/InternVL2_5-8B False 1024\nInternVL2.5-38B [20] 32 OpenGVLab/InternVL2_5-38B False 1024\nInternVL2.5-78B [20] 32 OpenGVLab/InternVL2_5-78B False 1024\nLongV A-7B [31] 32 lmms-lab/LongVA-7B False 1024\nLLaV A-Video-7B [64] 32 lmms-lab/LLaVA-Video-7B-Qwen2 False 1024\nLLaV A-Video-72B [64] 32 lmms-lab/LLaVA-Video-72B-Qwen2 False 1024\nLLaV A-OneVision-7B [27] 32 lmms-lab/llava-onevision-qwen2-7b-ov False 1024\nLLaV A-OneVision-72B [27] 32 lmms-lab/llava-onevision-qwen2-72b-ov-sft False 1024\nQwen2.5-VL-3B [1] 1fps Qwen/Qwen2.5-VL-3B-Instruct False 1024\nQwen2.5-VL-7B [1] 1fps Qwen/Qwen2.5-VL-7B-Instruct False 1024\nQwen2.5-VL-72B [1] 1fps Qwen/Qwen2.5-VL-72B-Instruct False 1024\nVideoLLaMA2.1-7B [25] 16 DAMO-NLP-SG/VideoLLaMA2.1-7B False 1024\nVideoLLaMA2-72B [25] 32 DAMO-NLP-SG/VideoLLaMA2-72B False 1024\nVideoLLaMA3-2B [21] 1fps DAMO-NLP-SG/VideoLLaMA3-2B False 1024\nVideoLLaMA3-7B [21] 1fps DAMO-NLP-SG/VideoLLaMA3-7B False 1024\nNVILA-8B [65] 32 Efficient-Large-Model/NVILA-8B-Video False 1024\nVideoLLaV A-7B [28] 8 LanguageBind/Video-LLaVA-7B False 1024\nVideoRefer-7B [12] 16 DAMO-NLP-SG/VideoRefer-7B False 1024\nViP-LLaV A-7B [23] 1 llava-hf/vip-llava-7b-hf False 1024\nOsprey-7B [22] 1 sunshine-lwt/Osprey-Chat-7b False 1024\nSPHINX-V-13B [66] 1 Afeng-x/SPHINX-V-Model False 1024\nTable 7: Model configurations for evaluating mainstream MLLMs in EOCBench (Temp.: tempera-\nture).\nB.2.2 Open-Ended Questions\nTo assess open-ended questions related to temporal perception accuracy, we conducted a density-\nbased analysis of deviations between ground-truth timestamps and model-generated responses, as\nvisualized in Fig. 7-(Left). The distribution of human responses exhibits a pronounced peak followed\nby rapid decay, suggesting that most human answers achieve minimal error ratios, with only sporadic\ninstances of higher inaccuracies. In contrast, the five top-performing models–GPT-4o [ 3], LLaV A-\nVideo-72B [ 64], VideoLLaMA3-7B [ 21], Qwen2.5-VL-72B [ 1] and NVILA-8B [ 65]–demonstrate\nflatter distributions with broader spreads. This pattern suggests that these models exhibit greater\nvariability in temporal perception, frequently producing larger errors in specific cases.\nThe observed disparity highlights a substantial discrepancy between current MLLMs and human-level\ntemporal perception, suggesting that some model predictions rely on haphazard estimation rather\nthan precise temporal understanding. As illustrated in Fig. 7-(Right), the figure also presents model\naccuracy across various time thresholds, specifically 0.01, 0.1, 0.2, and 0.3.\n17\n--- Page 18 ---\nC Experimental Setup\nC.1 Model Configurations\nThe configurations of the mainstream MLLMs we evaluate, including the official checkpoints, the\nnumber of frame samples, and details regarding the “Do Sample”, “Max New Tokens”, “Temperature”\nand “Top-P” parameters, are provided in Table 7.\nC.2 Additional Implementation Details\nWe utilize the official repository of each MLLMs to perform evaluations on our EOC-Bench bench-\nmark. Pre-sampled images from 1-frame, 8-frame, 16-frame, 32-frame, and 1 fps sequences serve\nas input for the corresponding models according to their default settings. Besides, for proprietary\nMLLMs, including GPT-4o, GPT-4o-mini and Gemini, we introduce timestamps prior to each frame\nto enhance the model’s temporal awareness. The open-source models are evaluated with their default\nsettings, and all evaluations are conducted using NVIDIA A100 GPUs.\nC.3 Carefully Crafted Prompts\nVisual Prompts. We employ the SoM [ 67] method to overlay various spatial markers onto the images\nin the final frame of the video. For a single object, only its visual prompt is highlighted in red on\nthe last frame. In the case of multiple objects, we overlay both their identifying numbers and visual\nprompts in various colors to facilitate differentiation. An example is presented in Fig. 8.\nText Prompts. The text prompts used for inference are consistent across all models and are as\nfollows:\nSystem Prompt: I have overlaid the box on the last frame of the video , <\nobject 1>: red ; <object 2>: blue , <object 3>: green ; <object 4>:\nyellow ; <object 5>: purple ; <object 6>: orange ;\nSingle choice\nUSER: {Question} Options : {Options} Answer directly using the letters of\nthe options given and wrap your response in <choice ></ choice >. For\nexample , if the answer is A, then output <choice >A </ choice >.\nMulti choice\nUSER: {Question} Options : {Options} Answer directly using the letters of\nthe options given . There are multiple answers , so wrap your response\nin <choice ></ choice >. For example , if the answer is A and B, then\noutput <choice >A, B </ choice >; if the answer is A, B and C, then output\n<choice >A, B, C </ choice >.\nOpen ended\nUSER: {Question} Please output the answer directly in seconds .\nSingle object (box)Multiple objects (box)\nSingle object (mask)\nMultiple objects (point)\nFigure 8: Illustrative examples of annotation formats for visual prompts.\nD Additional Dataset Analysis\nFig. 9 provides an additional statistical analysis through the form word cloud , capturing the range of\nquestions and answers encompassed in EOC-Bench . The predominance of terms related to dynamics\n18\n--- Page 19 ---\nand changes—such as “ change ”, “changed ”, “seconds ”, and “ happened ”—indicates a substantial\nfocus on the temporal and transformational aspects within the dataset. These terms are essential for\nassessing the memory capabilities of robots, as they require understanding and recalling sequences of\nevents and alterations over time.\nMoreover, the word cloud highlights the significance of spatial understanding, with frequent terms\nlike “ relative ”, “relative position ”, and “ relationship ”. These words underscore the importance of\ncomprehending the spatial dynamics between objects. Analyzing these spatial relationships allows\nthe model to infer how objects are positioned relative to each other, providing insights essential for\neffective planning and execution.\nE Limitations and Broader Impacts\nLimitations. Our EOC-Bench demonstrates the commendable assessment of embodied object\ncognition, yet certain limitation remain. The video inputs for EOC-Bench are limited to durations of\nunder six minutes, which may not adequately evaluate the cognitive abilities of MLLMs in terms of\nprolonged visual memory.\nIn our future work, we are dedicated to progressing our research by collecting video resources of\nlonger durations. We aims to explore the effects of increasing video input durations, particularly in\nterms of the models’ ability to retain prolonged visual information.\nBroader Impacts. As a benchmark specifically designed for evaluating in the domain of embodied\nego-centric cognition, EOC-Bench is set to draw considerable interest from researchers keen on\nexamining cognitive processes related to focusing on specific objects. Moreover, EOC-Bench aims to\nassist contemporary MLLMs in transcending the limitations inherent in images, videos, and texts\nalone, by shifting their focus toward the visual prompt inputs encountered in real-world scenarios.\nF Asset License and Consent\nIn our EOC-Bench , we utilize four open-source datasets: EPIC-KITCHENS [ 61], Ego4D [ 59],\nCharades-ego [ 62] and MECCANO [ 63]. All datasets are publicly accessible and freely available for\nacademic research. Table 8 provides a detailed list of the resources used in this research work, along\nwith their respective licenses.\nDataset License URL\nEPIC-KITCHENS [61] CC BY-NC 4.0 https://epic-kitchens.github.io/2025\nEgo4D [59] MIT license https://github.com/facebookresearch/Ego4d\nCharades-ego [62] Non-Commercial Use https://prior.allenai.org/projects/charades-ego\nMECCANO [63] CC BY-NC 4.0 https://iplab.dmi.unict.it/MECCANO/\nTable 8: Open-source resources used in this work.\nG More Exemplar Visualizations\nG.1 Failure Case Studies\nFig. 10 displays representative cases from top-performing GPT-4o [ 3] on our EOC-Bench . These cases\nsystematically demonstrate the model’s failure patterns across multiple error categories, including\ncurrent visual perception errors, common sense errors, and historical frame errors, while covering\ndiverse question types from EOC-Bench .\nG.2 Visual Samples Across Tasks\nTo intuitively illustrate the characteristics of our EOC-Bench , we further showcase samples spanning\n11 tasks, organized as follows:\n• Object State Retrospection (Fig. 11)\n19\n--- Page 20 ---\nFigure 9: Worldcloud of questions and answers in EOC-Bench .\n• Object Location Retrospection (Fig. 12)\n• Object Relationship Evolution (Fig. 13)\n• Absolute Time Perception (Fig. 14)\n• Immediate State Recognition (Fig. 15)\n• Object Relationship (Fig. 16)\n• Purpose and Function Inference (Fig. 17)\n• Anomaly Perception (Fig. 18)\n• Trajectory and Motion Prediction (Fig. 19)\n• State Change Prediction (Fig. 20)\n• Dynamic Relationship Prediction (Fig. 21)\n20\n--- Page 21 ---\nQ: Are <object 0> and <object 1> made of the same material?\nA: Yes, they are all made of ceramic.\nB: Yes, they are all made of plastic.\nC: No, <object 0> is made of plastic while <object 1> is made of ceramic.\nD: No, <object 0> is made of ceramic while <object 1> is made of plastic.\nGT: C GPT-4o: DAnomaly Perception\nCurrent Visual Perception Errors\nQ: What is the purpose of this tool<object 0>? \nA: Remove rust \nB: Mounting screws \nC: Clean the wheels \nD: Dismantle the car cobalt \nGT: A,C GPT-4o: A,BPurpose and Function Inference\nCommon Sense Errors\nQ: If I took <object 0> away, what would happen to <object 1>? \nA: Remain stationary \nB: May slip on the table \nGT: B              GPT-4o: ADynamic Relationship Prediction\nRelational Reasoning Errors\nWhat will the temperature of <object 0> change? \nA: It will increase \nB: It will decrease \nC: It will remain unchanged \nGT: B       GPT-4o: AState Change Prediction\nHistorical Frame Errors\nAnomaly Perception\nQ: Where is <object 0>?\nA: On the floor\nB: On the desk\nC: On the bed\nD: Under the desk\nGT: B              GPT-4o: APrevious Frame Interference\nLocation Retrospection\nQ: Where was the <object 0> originally?\nA: On the placemat\nB: In the sink\nC: On the desk\nD: On the floor\nGT: A              GPT-4o: BCurrent Frame Interference\nQ: How much dough is on the plate now <object 0>?\nA: 15\nB: 13\nC: 14\nD: 11\nGT: B                 GPT-4o: A Counting Errors\nImmediate State Recognition\n Object State Retrospection\nQ: Has the status of <object 0> changed?\nA: No\nB: Yes, it was sliced\nC: Yes, it was cut into strips\nD: Yes, it was cleaned\nGT: A              GPT-4o: B Intra -frame Interference\nFigure 10: Failure cases of the top-performing GPT-4o on EOC-Bench .\n21\n--- Page 22 ---\nObject State Retrospection\nVideo Source: Epic -kitchen Question: Has the amount of flour in <object 0> changed? \nA: Yes, it has decreased a bit \nB: Yes, it is already empty \nC: No\nAnswer: A Single -choice\nObject State Retrospection\nVideo Source: Epic -kitchen Question: Has the state of <object 0> changed before being moved?\nA: It was polished\nB: It's still unaltered\nC: It was coated with butter\nD: It was moved from the chopping board to the plate\nAnswer: C, D Multi -choice\nObject State Retrospection\nVideo Source: Charades -ego Question: What changes have occurred in the state of the <object 0>?\nA: It was cleaned\nB: It has less water in it\nC: It got stained\nD: It has more water in it\nAnswer: A, BMulti -choice\nFigure 11: Visualization of samples in Object State Retrospection (Past) .\n22\n--- Page 23 ---\nObject Location Retrospection\nVideo Source: Epic -kitchenQuestion: Where was the <object 0> originally?\nA: On the placemat\nB: In the sink\nC: In the cabinet\nD: On the shelf\nAnswer: ASingle -choice\nObject Location Retrospection\nVideo Source: Ego4DQuestion: Where was the <object 0> originally?\nA: On the hand\nB: On the floor\nC: On a bowl\nD: On the counter\nAnswer: D Single -choice\nObject Location Retrospection\nVideo Source: Charades -egoQuestion: What was the initial position of the <object 0>?\nA: On the shelf\nB: In hand\nC: In the wardrobe\nD: On the floor\nAnswer: A,C Multi -choice\nFigure 12: Visualization of samples in Location Retrospection (Past) .\n23\n--- Page 24 ---\nObject Relationship Evolution\nVideo Source: Epic -kitchenQuestion: What was the relationship between the <object 0> and the <object 1> before?\nA: They were stacked together\nB: They were placed in parallel\nC: They scattered on the floor\nD: They were used to stir the content in the same pot\nAnswer: B, D Mult i-choice\nObject Relationship Evolution\nVideo Source: Epic -kitchenQuestion: Which of the two was moved first in this video, <object 0> or <object 1>?\nA: <object 0>\nB: <object 1>\nC: Both of them were moved at the same time\nD: They didn't move\nAnswer: BSingle -choice\nObject Relationship Evolution\nVideo Source: Charades -egoQuestion: What happened to the relative position of <object 0> and <object 1>?\nA: From uncorrelated to stacked\nB: Kept uncorrelated\nC: From stacked to uncorrelated\nD: Kept stacked\nAnswer: ASingle -choice\nFigure 13: Visualization of samples in Object Relationship Evolution (Past) .\n24\n--- Page 25 ---\nAbsolute Time Perception\nVideo Source: Epic -kitchen Question: How many seconds ago did the <object 0> stop heating?\nAnswer: 17sTemporal Accuracy\n0.00s 2.00s 4.00s 6.00s 10.00s\n21.32s 19.00s 17.00s 15.00s 13.00s\nAbsolute Time Perception\nVideo Source: Ego4D Question: How many seconds has <object 0> been picked up?\nAnswer: 5s Temporal Accuracy\n5.00s 10.00s 15.00s 20.00s 25.00s\n51.80s 45.00s 40.00s 35.00s 30.00s\nAbsolute Time Perception\nVideo Source: Ego4DQuestion: When many seconds ago was the <object 0> picked out from the refrigerator?\nAnswer: 40s Temporal Accuracy\n0.00s 3.00s 6.00s 12.00s 17.00s\n46.63s 40.00s 34.00s 27.00s 22.00sFigure 14: Visualization of samples in Absolute Time Perception (Past) .\n25\n--- Page 26 ---\nImmediate State Recognition\nVideo Source: Ego4DQuestion: What's in the <object 0>?A: WaterB: MeatC: CauliflowerD: CabbageAnswer: CSingle-choice\nImmediate State Recognition\nVideo Source: Ego4DQuestion: What is the state of <object 0>?A: ClosedB: OpenC: In repairD: RunningAnswer: DSingle-choice\nImmediate State Recognition\nVideo Source: Ego4DQuestion: Has <object 0> been heated?A: YesB: NoC: Not sureAnswer: BSingle-choice\nFigure 15: Visualization of samples in Immediate State Recognition (Present) .\n26\n--- Page 27 ---\nObject Relationship\nVideo Source: Epic-kitchenQuestion: What is the relationship between the <object 0> and <object 1>?A: CrossB: On the same planeC: PerpendicularD: ParallelAnswer: A, CMulti-choiceObject Relationship\nVideo Source: Ego4DQuestion: Which is bigger? <object 0> or <object 1>?A: LeftB: RightC: SameD: Not sureAnswer: CSingle-choice\nObject Relationship\nVideo Source: Epic-kitchenQuestion: What is the relationship between the <object 0> and <object 1>?A: The latter is resting against the former.B: The former is beside the latter.C: The two are not in contact.D: The former is on top of the latter.Answer: A, BMulti-choice\nFigure 16: Visualization of samples in Object Relationship (Present) .\n27\n--- Page 28 ---\nPurpose and Function Inference\nVideo Source: Ego4DQuestion: What is <object 0> used for ?A: Measure the widthB: Measure the thicknessC: Measure the lengthD: Fix the planksAnswer: BSingle-choice\nVideo Source: Ego4DQuestion: What is <object 0> used for?A: DrinkB: Wash handC: GargleD: Wash the penAnswer: DSingle-choicePurpose and Function Inference\nVideo Source: Epic-kitchenQuestion: What is the purpose of the <object 0>?A: To mix ingredientsB: To cut foodC: To roll doughD: To measure ingredientsAnswer: CSingle-choice\nPurpose and Function Inference\nFigure 17: Visualization of samples in Purpose and Function Inference (Present) .\n28\n--- Page 29 ---\nAnomaly Perception\nVideo Source: Self-recordingQuestion: Where is <object 0>?A: In the cupB: On the bedC: Near the phoneD: Near the soapboxAnswer: B, C, DMulti-choice\nVideo Source: Self-recordingQuestion: Where is <object 0>?A: On the chairB: On the deskC: On the Mahjong tableD: On the floorAnswer: CSingle-choiceAnomaly Perception\nVideo Source: Self-recordingQuestion: Are <object 0> and <object 1> have the same function?A: Yes, they are all used for writingB: Yes, they are all used for makeupC: No, the first one is used for writing while the latter is used for makeupD: No, the first one is used for makeup while the latter is used for writingAnswer: CSingle-choiceAnomaly Perception\nFigure 18: Visualization of samples in Anomaly Perception (Present) .\n29\n--- Page 30 ---\nTrajectory and Motion Prediction\nVideo Source: MeccanoQuestion: How will <object 0> move?A: It will keep stillB: It will rotate both clockwise and anticlockwiseC: It will rotate clockwiseD: It will rotate anticlockwiseAnswer: BSingle-choice\nTrajectory and Motion Prediction\nVideo Source: Self-recordingQuestion: If I press <object 0>, will it fix firmly?A: YesB: NoAnswer: BTrue/False\nTrajectory and Motion Prediction\nVideo Source: Self-recordingQuestion: Will <object 0> fall on to the ground?A: YesB: NoAnswer: ATrue/False\nFigure 19: Visualization of samples in Trajectory and Motion Prediction (Future) .\n30\n--- Page 31 ---\nState Change Prediction\nVideo Source: Epic-kitchenSingle-choiceState Change Prediction\nVideo Source: Ego4DQuestion: If <object 0> fall on the ground, will it broken?A: YesB: NoC: Not sureAnswer: ASingle-choice\nQuestion: If it is kept on the table for an hour, how will <object 0>'s temperature change?A: The temperature will increaseB: The temperature will decreaseC: The temperature will remain almost unchangedAnswer: B\nState Change Prediction\nVideo Source: Self-recordingQuestion: What will happen if I press <object 0>?A: The air conditioning will turn onB: The air conditioning will turn offC: The temperature will increaseD: The temperature will decreaseAnswer: BSingle-choice\nFigure 20: Visualization of samples in State Change Prediction (Future) .\n31\n--- Page 32 ---\nDynamic Relationship Prediction\nVideo Source: Epic-kitchenTrue/FalseDynamic Relationship Prediction\nVideo Source: Self-recordingQuestion: Will these <object 0> and <object 1> collide?A: YesB: NoAnswer: BTrue/FalseQuestion: If I take away <object 0>, will it affect <object 1>?A: YesB: NoAnswer: B\nDynamic Relationship Prediction\nVideo Source: Epic-kitchenQuestion: If I take away <object 0>, what will happen to <object 1>?A: Remain stationaryB: Slip on the tableC: Spilled over onto the tableD: Move slightly on the tableAnswer: ASingle-choice\nFigure 21: Visualization of samples in Dynamic Relationship Prediction (Future) .\n32",
  "text_length": 83278
}