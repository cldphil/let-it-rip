{
  "id": "http://arxiv.org/abs/2505.24871v1",
  "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
  "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
  "authors": [
    "Yiqing Liang",
    "Jielin Qiu",
    "Wenhao Ding",
    "Zuxin Liu",
    "James Tompkin",
    "Mengdi Xu",
    "Mengzhou Xia",
    "Zhengzhong Tu",
    "Laixi Shi",
    "Jiacheng Zhu"
  ],
  "published": "2025-05-30T17:59:38Z",
  "updated": "2025-05-30T17:59:38Z",
  "categories": [
    "cs.CV",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24871v1"
}