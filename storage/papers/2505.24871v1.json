{
  "id": "http://arxiv.org/abs/2505.24871v1",
  "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
  "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
  "authors": [
    "Yiqing Liang",
    "Jielin Qiu",
    "Wenhao Ding",
    "Zuxin Liu",
    "James Tompkin",
    "Mengdi Xu",
    "Mengzhou Xia",
    "Zhengzhong Tu",
    "Laixi Shi",
    "Jiacheng Zhu"
  ],
  "published": "2025-05-30T17:59:38Z",
  "updated": "2025-05-30T17:59:38Z",
  "categories": [
    "cs.CV",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24871v1",
  "full_text": "arXiv:2505.24871v1 [cs.CV] 30 May 2025MoDoMoDo: Mul ti-Do main Data Mixtures for Mul timodal LLM Reinforcement Learning Yiqing Liang1∗, Jielin Qiu2, Wenhao Ding3, Zuxin Liu2, James Tompkin1, Mengdi Xu4, Mengzhou Xia5, Zhengzhong Tu6, Laixi Shi7, Jiacheng Zhu8∗ 1Brown University2Salesforce AI Research3NVIDIA Research 4Carnegie Mellon University5Princeton University6Texas A&M University 7California Institute of Technology8MIT CSAIL Project Website:https://modomodo-rl.github.io/ Abstract Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reason- ing. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and con- sequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model’s accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline. 1 Introduction Multimodal large language models (MLLMs)[ 46] are significantly expanding the frontiers of artificial intelligence. They integrate diverse data types like images and audio with text to achieve a more comprehensive understanding and enable richer, human-natural AI interactions, unlocking advanced capabilities such as cross-modal reasoning and spatial intelligence [ 50,1,31,42]. Effective post- training of multimodal LLMs remains challenging as they must integrate diverse data modalities, whose data is orders of magnitude scarcer than text-only LLMs [ 59] and present fundamentally different problem features. They translate into a broad spectrum of distinct capabilities—spatial 1* corresponding: yiqing_liang@brown.edu, zjc@mit.edu Preprint. Dataset #1 Dataset #2 Dataset #3 … Dataset #MMixture Sampler Reward Model #1 Reward Model #2 Reward Model #N… RLVR Dataset #1 Dataset #2 Dataset #3 … Dataset #MReward Model #1 Reward Model #2 Reward Model #N… RLVR Figure 1: MoDoMoDo is a framework that combines Multi-Domain Data Mixtures with Multimodal LLM Reinforcement Learning, enabling generalizable performance gain across diverse VL tasks. Models trained with our estimated optimal mixtures can outperform those trained with naive mixtures on in-domain and out-of-domain benchmarks. reasoning, fine -grained recognition, chart interpretation, and more. In response, researchers have de- veloped and adapted various fine-tuning techniques for MLLMs, including instruction following [ 32], Reinforcement Learning from Human Feedback (RLHF) [ 47,59], Direct Preference Optimization (DPO)[51, 61], and online reinforcement learning (RL) approaches [15, 56, 7]. Recently, reinforcement learning with verifiable rewards (RLVR)[ 55,44] has emerged as a powerful post-training paradigm for text-based language models, delivering state-of-the-art performance in domains with programmatically checkable answers, such as mathematics and code. By replacing noisy human preferences with rule-based signals, RLVR yields stable optimization and strong generalization[ 55,15,44]. However, using RLVR to enable MLLMs presents significant challenges due to the relative scarcity of accessible verifiable datasets compared to the huge demand for data driven by inherently richer and heterogeneous multimodal tasks. Each verifiable reward tackles a slice of that capability spectrum, leaving large gaps in multimodal reasoning. Existing effective attempts that transfer vision tasks into verifiable reward signals for RL fine-tuning often only focus on one specific multimodal task domain (e.g., VQA [ 7], object detection [ 56], math [ 49,26], and video [ 14]). Such single -domain focus is inadequate for achieving the wide range of capabilities required by MLLMs. To address this, it is beneficial to incorporate multiple datasets from diverse multimodal task do- mains during post-training, rather than relying on a single dataset, to ensure broad coverage and generalization across a wide range of capabilities. This is particularly important for multimodal LLMs, given the vast number of distinct tasks arising from multimodal compositions. However, using multiple training datasets introduces challenges, including potential conflicting objectives resulting from interactions among diverse datasets, as well as corresponding unstable behaviors during training. These challenges underscore the need to optimize dataset mixture strategies. Notably, data mixture has been extensively investigated and has demonstrated its effectiveness in various scenarios, such as supervised deep learning, pre-training LLMs, and supervised fine-tuning. Here we seek to answer: (Q)How to mix diverse datasets in RLVR to achieve the wide range of multimodal capabilities? To enable efficient multi-dataset multimodal RLVR for multimodal LLM, our main contributions are: • We develop a novel multimodal RLVR framework formulti-dataset post-training, designed to optimize the strategies for mixing these datasets to achieve desired capabilities. This includes the creation of five image-text datasets with different verifiable rewards, as well as the RLVR framework for training on these datasets (Sec. 3.2). •We propose a multi-domain data mixture optimization strategy, which learns a surrogate function to predict the outcome of RL finetuning given the mixture distribution (Sec. 2.3). We show that a multivariate quadratic function can capture the counterfactual interplay among different datasets (Fig 6). •We comprehensively evaluate the performance on various evaluation sets, including MMMU, MathVista, InfoVQA, and ChartQA. The predicted optimal data mixture distribution drives the finetuned model to have strong and generalizable reasoning capability, compared with any other standalone or naive mixture recipes (Sec. 3.3). 2 2 Multimodal RLVR with Data Mixtures In this section, we first present the problem formulation of RLVR with data mixtures, framed as a bi-level optimization problem. Subsequently, we propose an approach to optimize the data mixture strategy using a two-step procedure, incorporating a prior model for the objective of the inner optimization problem. 2.1 Reinforcement Learning with Verifiable Reward (RLVR) with Data Mixtures Multiple multimodal datasets. Suppose there are mdomains {Di:= (Si, ri)}m i=1, where Si= {xi,1,..., x i,n} ∼Ps,iare prompt samples draw from distribution Ps,i. Given a prompt xi,n∼Si and an LLM policy model πθ(y|x), the reward model ri:X × Y 7→ { 0,1}labels each prompt and generation pairs (xi,n, yi,n)with binary score as per verifiable criteria. Multimodal RLHF. Reinforcement Learning from Human Feedback (RLHF) [ 77,47] is a powerful and scalable strategy to align LLMs with human values, and has become a major technique for LLM post-training. Typically, pipelines includes: (1) Multimodal Supervised Fine-Tuning (SFT), where a MLLM is fine-tuned on an instruction following dataset using token-level supervision [ 32]. (2) Multimodal Preference Modeling, which seeks to maximize the difference between the reward for the chosen response and that for the rejected response[ 61]. (3) Reinforcement Learning. A policy model π:7→∆(A), usually initialized through multimodal SFT checkpoint [ 48], is trained to generate appropriate responses by maximizing the reward signal given by a reward model r:X × A 7→ [0,1] during the post-training stage. Reinforcement Learning with Verifiable Reward (RLVR) Group Relative Policy Optimization (GRPO) was proposed [ 16] as a group-based extension of the widely-used RL algorithm PPO [ 54,60] to fine-tune the reference policy. It facilitates a variance-reduced and robust fine-tuning process, achieving excellent results in LLM reasoning. Assume the data distribution used in post-training is µ. Mathematically, GRPO maximizes the expected reward of the policy with respect to a group of Gactions from the current policy, regularized to ensure that it does not deviate significantly from the reference policy πref. Let the policy network be parameterized by θ, and denote the policy at the current iteration as πold. GRPO aims to optimize max θEx∈µ,ai∼πold(x)\u00141 GGX i=1\u0010 min\u0012πθ(ai|x) πθold(ai|x)Ai,clip\u0012πθ(ai|x) πθold(ai|x),1−ϵ,1 +ϵ\u0013 Ai\u0013 −βD KL(πθ(ai|x)∥πref(ai|x)\u0011\u0015, (1) where the normalized reward feedback Ai:=ri−mean{r1,···,rG} std{r1,···,rG}, and DKL(·)represents the Kull- back–Leibler (KL) divergence. Typically, there is one verifiable structured reward function rassociated with each dataset sample, which is either (1) verifiable correctness, (2) verification via execution, or (3) verifiable constraints. It has been shown that verifiable rewards balance simplicity and bias and are thought to be less prone to reward hacking than reward models learned from preference data. However, the above verifiable rewards are carefully designed for certain types of datasets. In practice, we want to fine-tune LLM on a number of datasets to cover as large a domain as possible, to enable a generalizable performance across tasks. 2.2 Data Mixture Modeling for Multidomain RLVR Suppose there are mdomains {Di:= (Si, ri)}m i=1, where Si={xi,1,..., x i,n} ∼Ps,iare prompt samples draw from distribution Ps,i. Given a prompt xi,n∼Siand an LLM policy model πθ(y|x), the reward model ri:X × Y 7→ { 0,1}labels each prompt and generation pairs (xi,n, yi,n)with binary score as per verifiable criteria. Domain weights w∈∆mspecifies a probability distribution over the mdomains, and consequently a distribution over the training data: Pw=Pk i=1wi·Ps,i. Here, without loss of generality we 3 assume Ps,ito be uniform distributions unif(S) =1 |S|P x∈Sδxwhere δx(x′)is the Dirac function at location x′. Consider a formal setting for MLLM RL finetuning on multi-domain data mixture, We define an RL algorithm Rthat takes training data subject to distribution Pwand output a fine-tuned model. Note this RL finetune process is not deterministic. Then, given a fixed input prompt xwe can define: fR(x, Pw):= the outcome of training a model on Pwusing Rand evaluated on input x, where the outcome is a measure of the performance of the finetuned model on some task, e.g., pass rate at answering vision QA questions. Our objective is to find the optimal mixture distribution that maximizes the outcome on an unseen testing distribution Ptest ˆw:= arg max wEx∼Ptest[fR(x, Pw)]. (2) Here, the complex yet stochastic function fR(x,·)captures how the training data mixture distribution yields the final outcome, but its brute force estimation is implausibly expensive. To solve this problem, we aim to find a simple surrogate datamodel [ 20] function g(·)which approximate fR(x,·)efficiently. Specifically, we transform the challenge of estimating fR(x,·)into a supervised learning problem, where the training examples are the distribution simplex w′and corresponding ‘labels’ are given by fR(x, w′). This can be computed by fine-tuning a base model on data Pw′using RL algorithm R. Then, our goal is to fit a parametric function gθthat maps the training data mixture Pwto the outcome of fine-tuning a model using this distribution. gθ: ∆m7→Rwhere θ= arg min θ1 nnX i=1L(gθ(wi), fR(x, wi)), (3) andL(·,·)is a fixed loss function (e.g., RMSE). Achieving this goal would reduce the challenge of scrutinizing fR(x,·)—and more generally the map from training data distribution to model responses through RL algorithm R—to the hopefully easier optimization objective. Afterwards, we seek a distribution of weights that achieves the desired performance by searching for the mixture that maximizes the expected criteria: w∗= arg max w[gθ∗(w)] (4) Should we have a good estimate gθ, we can find the optimal data mixture distribution defined in Eq.(2) by solving the optimization problem in Eq.(4). 2.3 Constructing data mixture models The complex characteristics of fR(x, Pw)pose additional challenges in solving Eq.(3), since gθ is actually predicting the outcome of fine-tuning. And it contains the following steps: (a) We need to parameterize gθas a class of function. (b) Sample a collection of {w0 i}n0 i=1as \"seed\" samples. (c) Performing n0RLVR training runs and curate a collection of training samples {(w0 1, fR(x, Pw0 1)),(w0 2, fR(x, Pw0 2)),...,}. (d) Split these samples into training and validation, and fit gθthrough the empirical risk minimization (ERM) objective. (e) (Optional) Using the esti- mated gˆθ, seek the predicted optimal data mixture ˆwand collect more training samples to refine gθ(f) Finally, output the optimal weight w∗via optimizing Eq.(4). 1.Collecting such dataset is expensive as it requires doing a full cycle of RL fine-tuning and evaluation. Thus, we carefully select the seed distributions {w0 i}n0 i=1to maximize the information, e.g.w0 1= [1,0,0,0,0]⊤,w0 2= [0,1,0,0,0]⊤, and w0 1= [0,0.25,0.25,0.25,0.25]⊤. This is motivated by hypothesis that these different domains might have independent or counterfactual effects on evaluation tasks. 2.While we start from parameterizing gθ:{a,b}:=b+a⊤was a linear function, we also try gθ:{b,a,C}=b+a⊤w+1 2wTC w to capture the non-linear counterfactual relationship between different domains. 3 Experiments We begin by describing the experimental setup, including dataset curation, reasoning mode, reward models, training and testing strategies, data sampling, and evaluation metrics in Sec. 3.1. Subse- quently, Sec. 3.2 explains the design of 3groups of data mixture strategies examined in our paper, i.e. 4 Table 1: Training and Testing datasets. The top section shows Training datasets processed for RLVR Post-Training with Data Mixture. The bottom two sections show Testing datasets containing training datasets’ in-distribution test sets ( In) and common VLM test datasets ( Out). Type Dataset Domain Answer Type Rewards # samplesTrainingCOCO  Object Recognition 2D Bounding Box IoU, Format 5997 LISA-train  Referring Expression 2D Bounding Box IoU, Format 1326 GeoQA V  Math VQA Multiple Choice Acc, Format 1969 SAT-train  Spatial VQA Natural Language Acc, Format 15000 ScienceQA-train  Science VQA Multiple Choice Acc, Format 6218Testing InLISA-test  Referring Expression 2D Bounding Box 3397 SAT-test  Spatial VQA Natural Language 1928 ScienceQA-test  Science VQA Multiple Choice 2017OutChartQA  Chart VQA Natural Language 2500 InfoVQA  Infographics VQA Natural Language 2801 MathVista  Math VQA Multiple Choice 1000 MMMU  General VQA Multiple Choice 900 seed, heuristic, and model-based. Finally, Sec. 3.3 presents a comprehensive analysis of the results, demonstrating the importance of data mixture in Multimodal LLM RLVR and the efficacy of different data mixture strategies. 3.1 Experimental Setup Question: In this food chain, the diatom is a producer. Why? A. It eats another organism. B. It makes its own food. C. It eats a consumer. Answer: B Question: In this food chain, the diatom is a producer. Why? A. It eats another organism. B. It makes its own food. C. It eats a consumer. Output the thinking process in <think> </think> and final answer (option) in <answer> </answer> tags. Answer: <think> In this food chain, the diatom is a producer because it makes its own food. The diatom uses carbon dioxide, water, and sunlight to make its own food. </think> <answer> B </answer> Figure 2: Demonstration of a General Question- Answer Pair With and Without Reasoning.Data Curation To expose our model to a broad range of vision-language competencies— from scientific reasoning to fine-grained visual grounding—we assemble a diverse mixture of training corpora where each dataset’s question- answer pairs are verifiable: the ground-truth answer can be deterministically checked either by exact-string comparison (for multiple-choice letters or short free-form text) or by straight- forward programmatic evaluation (for numeric responses). This guarantees an unambiguous reward signal during RLVR Post-Training. In addition to training datasets’ in-distribution test splits (if exist), we adopt out-of-distribution benchmarks that have been widely used to eval- uate Multimodal LLMs. Statistics for all training and evaluation sets appear in Tab. 1. Pre-Processing of Individual Datasets (some training datasets have corresponding testing datasets ): 1.COCO: we adopt the full object-category subset used by Liu et al.. 2.LISA: each image-bounding-box pair may be associated with multiple questions; we treat every question as an independent sample and shuffle the flattened list. Images whose longer side exceeds 640 pixels are resized while maintaining the aspect ratio. The same pipeline is applied to both the official train and test splits, yielding our LISA-train/test sets. 3.GeoQA V: starting from the 8k-example math subset, we remove corrupted items by retaining only those whose answers match the expected multiple-choice regex pattern; oversized images are resized as in LISA. 4.SAT: we use the original train split for GRPO Post-Training and val split for evaluation. 5.ScienceQA: we keep only problems that reference an image. All answer options are concatenated with the original question into a single multiple-choice prompt, and the ground truth answer is converted to its corresponding letter label for style matching. Reasoning Mode Rather than permitting the multimodal LLM to emit an answer immediately after observing the image-question pair, we place it in a two-step reasoning mode: (1) generate a free-form 5 chain-of-thought, and (2) commit to a concise final answer. We trigger this mode by appending a specialised reasoning prompt to every question. Sec. 3.1 is an example. LISASAT ScienceQAChartQA InfoVQAMathVistaMMMU0.00.10.20.30.40.50.60.70.8Score 0.150.25 0.050.240.310.390.380.480.570.70 0.480.47 0.43 0.41Base Model All Datasets In-domain test (seen during training) Figure 3: Model Performance before / after GRPO training on Alldata mixture.Reward Models After the above steps, we extract the final answer via a regular-expression parse. ❶Format: if the regex succeeds, this reward yields 1; otherwise, {For- mat, Acc, IoU} are all set to zero. ❷Accuracy ( Acc): a binary reward indicating an exact match. ❸Intersection- over-Union ( IoU): for bounding boxes, a real-valued re- ward in [0,1]. The specific combination of rewards applied to each training dataset is summarised in Tab. 1. Train/Test Strategy To stay within 4×NVIDIA A100 GPUS’ budget while retaining adequate representational power, we select Qwen2-VL-2B-Instruct [ 63] as the base for our GRPO experiments. To further improve efficiency, 1GPU runs a vLLM  server that handles all forward generation and periodically sync weight with other pro- cesses; the remaining GPUs execute policy update loop. Each of 3training GPUs processes 2samples in parallel; 2gradient accumulation steps give an effective batch of 12trajectories per update. This provides the k= 6 generations needed for GRPO reward while maximizing throughput without exceeding memory limits. The vision encoder is frozen; only the LLM-part parameters are updated. We use AdamW optimizer with β1=0.9, β2=0.999, ε=10−8. The learning rate follows a linear schedule: 10% warm-up to a peak ηmax= 1×10−6, then linear decay to zero. Training is performed in bfloat16 with deterministic seed 42. Regarding GRPO-specific hyperparameters, we use KL coefficient 0.04, reward weights 2.0forAcc andIoU, and 1.0for Format. We train each experiment for 1epoch and report metrics after 2000 steps of training. If a certain mixture training ends early, we use the last checkpoint for evaluation. We also compute metrics after 500and1500 steps of training for certain data mixture prediction strategies. Training Data Sampling Letw1,..., w mdenote the weights for the mavailable training sets, withPm i=1wi= 1. For each training step, we perform a two-stage draw: (1) sample a domain (Di) according to (Pr(Di) =wi); (2) sample 1unseen example from the training split of (Di). This design aims to force the model to see training samples proportional to the data mixture. If all training samples from a certain dataset have been seen, the training stops. Evaluation Metrics We compute a score in the range [0,1]for every test dataset. For LISA-test, we use the mean intersection-over-union (IoU) metric between predicted and ground-truth boxes. For all others, we use the fraction of samples where the answer exactly matches the reference as the score. To provide a concise summary of in/out-of-distribution, we additionally report 2aggregations. ❶ In-Score: the weighted average over datasets that might be seen during training, and ❷Out-Score: the weighted average over datasets that were not used at train time. Weights are proportional to the number of test samples in each dataset (see Tab. 1), ensuring that larger benchmarks contribute more to the overall score. 3.2 Data Mixture Implementation Seed data mixture We begin with a suite of seed mixtures in which every participating dataset is equally sampled at each training step. Three variants are considered: •Single: train on one dataset only ( αj= 1for the chosen dataset, 0otherwise). •Exclude-One: the mixtures uniformly over all but one dataset; the held-out set’s weight is 0. •All: a mixture uniformly over the complete collection ( αi= 1/mfor every dataset). Heuristic data mixture After the seed mixture strategies, we now explore heuristic approaches that make assumptions about how each dataset impacts test performance. These data-driven strategies predict weights using empirical performance scores observed in our seed mixture experiments. We employ three heuristic methods: 6 •Alpha-family: These approaches assume independence of datasets, and track each dataset’s contribution to performance with a hyperparameter αto balance In-Score and Out-Score priorities: –Ain(α= 1.0) /Aout(α= 0.0): Preferred for In-Score / Out-Score –Abal(α= 0.5): Balanced weighting between In-Score and Out-Score •Collinearity-Aware Regression (Coli): Employs ridge regression and assumes that variance inflation factor (VIF) correction can account for statistical dependencies between datasets. •Leave-One-Out Normalization (Norm): Assumes that the performance gap when a dataset is excluded compared to Allis related to its importance in the final mixture. Model-based data mixture Moving beyond heuristic approaches, we introduce a Covariance Ma- trix Adaptation Evolution Strategy (CMA-ES) framework that fits a parametric-model to approximate the mapping from data mixtures to performance scores. LISASAT ScienceQAChartQA InfoVQAMathVistaMMMU0.00.10.20.30.40.50.60.70.8ScoreLISA-only SAT-only ScienceQA-only All Datasets In-domain test (seen during training) Figure 4: Model Performance Comparison after GRPO training using All data mixture and 3Single data mixtures that have in-distribution test set.After fitting empirical observations of both seed and heuristic mixture experiments, the parametric model enables the prediction of performance for any potential mixture without requiring additional empirical observations. By sampling from regions centered on observed mixtures and ranking mixtures based on model estimation, we can discover mixtures of potential that heuris- tic methods could not reach, particularly when complex synergies exist between datasets. 3.3 Evaluation Results and Analysis We begin by showing the generalization potential of basic seed mixtures for multimodal LLM post- training using GRPO. Next, we explore the inherent complexities of combining diverse datasets. Lastly, we justify our parametric model choice for the model-based strategy, and compare the overall efficacy of seed, heuristic, and model-based-yielded data mixtures. LISASAT ScienceQAChartQA InfoVQAMathVistaMMMU Out-Score0.100 0.075 0.050 0.025 0.0000.0250.0500.0750.100Score Benefit Exclude COCO Exclude LISA Exclude GeoQAVExclude SAT Exclude ScienceQA Figure 5: Model Performance Comparison after GRPO train- ing using Alldata mixture and 5Exclude-One data mixtures.Data Mixture Helps Generaliza- tion (Figure 3). Even a simple ap- proach like seed data mixture can yield improvements over the base model, particularly in out-of-domain generalization. GRPO post-training with a seed mixture of all datasets ( All) enhances model performance across in-domain test sets LISA, SAT, and ScienceQA, where the mixture leads to scores of 0.48,0.57, and 0.70re- spectively, up from the base model’s 0.15,0.25, and 0.05. More impor- tantly, this trend of improvement ex- tends to out-of-domain datasets including ChartQA ( 0.24→0.48), InfoVQA ( 0.31→0.47), Math- Vista ( 0.39→0.43), and MMMU ( 0.38→0.41). This indicates that mixture over diverse datasets, even without sophisticated weighting, can enhance both specialized and generalized capabilities. 7 Intricate Nature of Data Mixture (Figures 4 and 5). While the above seed mixture shows improvement on all test benchmarks, finding optimal data mixtures is far from straightforward. First, naive inclusion of more data does not guarantee superior results. For example, Fig 4 shows that All’s performance on ChartQA and InfoVQA is lower than 2single-dataset mixtures. Furthermore, Fig 4 illustrates that each dataset might have a distinct impact on model performance. For instance, LISA- only is beneficial for generalization on InfoVQA but hurts ScienceQA score, while ScienceQA-only yields the highest score on the same-domain test set but is less useful for InfoVQA. When there’s more than one data source, the interplay between them makes the situation more complicated. Figure 5 displays this complexity by comparing Allwith Exclude-One mixtures. The targeted removal of certain datasets might not negatively affect, or could even enhance, performance on particular tasks or overall. For instance, the exclusion of ScienceQA significantly lowers scores on the ScienceQA and ChartQA benchmarks, while its effects elsewhere are mixed. Additionally, excluding one dataset is consistently helpful for overall Out-Score, i.e., useful for out-domain generalization. This variability underscores the need for more intelligent and adaptive data mixture strategies. Out-ScoreSeed Heuristic Model-based Single Exclude-One AllAin Aout AbalColi NormModel Base Score Figure 6: Data Mixture Strategies’ Comparison. Heuristic ( middle ) and model-based ( right ) achieve higher min- ima and medians than Seed ( middle ). Model-based further reduces variance.Selection of an Effective Parametric data mixture Model (Figure 7). As the seed mixture shows promise but is likely suboptimal due to the aforementioned com- plexities, we investigated heuristic and model-based strate- gies to predict optimal data mixture. For model-based strategies, the choice of an appropriate mathematical model to approximate the function from mixture to Out- Score is crucial, as depicted in Fig 7. A linear (1- dimensional) optimization model, for example, fails to ac- curately map mixture to performance outcomes even when using all samples for fitting, as evidenced by the signifi- cant deviation between \"Actual\" and \"Predicted Scores\". Similarly, an analysis using Principal Component Analysis (PCA) on the mixture vectors suggests that the data points are not readily separable in a linearly reduced dimensional space, further pointing to underlying non-linearities. In contrast, a quadratic optimization function is more adept. When employing cross-validation to pick top- 1out of 5 shuffles, the quadratic model (rightmost panel) demon- strates improved ability to fit the training data and, crit- ically, to generalize to unseen test sets. We hypothesize that the advantage of using a quadratic function over 1- dimensional linear function comes from the former’s han- dling of the nuanced dataset interactions. Now, we adopt the quadratic paradigm to guide the search for effective data mixture. Comparison of Mixture Strategies (Figure 6). Our investigation of data mixture strategies is summarized in Fig 6, which presents an aggregated view of out-of-domain scores (Out-Score), grouped by strategy. This figure shows that, as groups, the predicted data mixtures of heuristic and model-based strategies both offer advantages over naive seed mixtures by having higher median and minimum scores. This reinforces the idea that thoughtful data mixing is the key to unlocking better generalization. When comparing heuristic methods against our model-based approach (which leverages the afore- mentioned quadratic optimization), the model-based strategy achieves a higher median, a higher minimum, and a competitive maximum. Moreover, the model-based approach exhibits smaller variance compared to both seed and heuristic methods, which is a sign of not only an improvement in average performance but also a gain in reliability. 4 Related Work RL for aligning language models. Reinforcement Learning from Human Feedback (RLHF) guides LLMs with rewards learned from human-labeled data, most visibly in InstructGPT [ 47]. Recent variants reduce human effort, e.g., Constitutional AI [ 3], RLAIF [ 24], and log-likelihood-based 8 0.5 0.0 0.5 PC 10.2 0.00.20.40.60.81.0PC 2PCA of Mixture Vectors 0.45 0.50 Predicted Score0.420.440.460.480.500.52ActualLinear Optimization (All Data) 0.45 0.50 Predicted Score0.420.440.460.480.500.52ActualQuadratic Optimization Training Set (1@5 Splits) 0.45 0.50 Predicted Score0.420.440.460.480.500.52ActualQuadratic Optimization Test Set (1@5 Splits) Actual ErrorFigure 7: Model-based mixture optimization. Left: PCA on mixture vectors reveals lack of linear separability, motivating nonlinear modeling. Middle-left: Linear regression fails to fit Out-Score accurately, even with all data. Right: Quadratic regression fitted on training folds could generalize better to held-out folds, capturing nonlinear interactions critical for mixture prediction. SimPO [ 43], or replace value networks with groupwise comparisons (GRPO) and other verifiable- reward methods that markedly improve multi-step reasoning [4, 5, 21]. R1-style multimodal reasoning. DeepSeek-Math/R1 show that GRPO can boost textual reasoning without an explicit reward model [ 55]. A fast-growing line of work ports this idea to vision–language models, demonstrating domain-specific gains in math, fine-grained recognition, counting, and spatial tasks (e.g., MAYE [ 39], Visual-RFT [ 35], R1-V [ 7]). Follow-up studies refine rewards, curricula, and exploration strategies, achieving further stability and efficiency; yet how to allocate training across heterogeneous multimodal tasks remains open. Data-mixture strategies. Classic multilingual pretraining up-samples rare languages with tempera- ture sampling [ 13]. Proxy-model approaches such as DoReMi [ 66] and RegMix [ 33] learn mixture weights in two stages, while ODM [ 2] updates them online. Very recent evidence suggests that even simple heuristics rival these sophisticated schemes [ 18]. We extend this discussion to vision–language reasoning, treating task-level sampling as a first-class design choice. Overall, prior studies show that (i) lightweight, verifiable rewards can reliably sharpen reasoning, and (ii) mixture policies critically shape downstream performance. Our work unifies these threads by jointly optimising data mixture and GRPO-style reinforcement for multimodal reasoning. For a full discussion, please refer to the supplementary materials Appendix A. 5 Conclusion We introduced MoDoMoDo, a framework that couples multi-domain data mixtures with reinforce- ment learning under verifiable rewards (RLVR) for multimodal large language models. Leveraging five complementary image–text datasets equipped with rule-based reward functions, we train a lightweight quadratic surrogate to predict post-training performance as a function of the mixture distribution. This surrogate lets us identify an optimal mix after only a small set of pilot runs. Models fine-tuned with the learned mixture surpass those trained on any individual dataset or on uniform combinations. Limitations and Future Work. Our analysis is limited to image–text settings; extending MoD- oMoDo to audio, video, and 3-D modalities, as well as to tasks where verifiable signals are sparse or noisy, remains open. On the algorithmic side, exploring surrogate models that incorporate dataset similarity, curriculum schedules, or uncertainty estimates could further reduce pilot-run cost. Finally, connecting our empirical findings to a unified multi-objective RL theory would deepen understanding of why mixture-optimized RLVR generalizes so well. 9 References J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716–23736, 2022. A. Albalak, L. Pan, C. Raffel, and W. Y. Wang. Efficient online data mixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. H. Chen, Y. Feng, Z. Liu, W. Yao, A. Prabhakar, S. Heinecke, R. Ho, P. L. Mui, S. Savarese, C. Xiong, and H. Wang. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self- rewarding. OpenReview (ICLR), 2024. H. Chen, H. Tu, F. Wang, H. Liu, X. Tang, X. Du, Y. Zhou, and C. Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models, 2025. L. Chen, L. Li, H. Zhao, Y. Song, and Vinci. R1-v: Reinforcing super generalization ability in vision- language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025- 02-02. X. Chen, W. Li, C. Liu, C. Xie, X. Hu, C. Ma, F. Zhu, and R. Zhao. On the suitability of reinforcement fine-tuning to visual tasks, 2025. Y. Chen, Y. Ge, R. Wang, Y. Ge, L. Qiu, Y. Shan, and X. Liu. Exploring the effect of reinforcement learning on video understanding: Insights from seed-bench-r1. ArXiv Preprint, 2025. H. W. Chung, N. Constant, X. Garcia, A. Roberts, Y. Tay, S. Narang, and O. Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv Preprint, 2025. Y. Deng, H. Bansal, F. Yin, N. Peng, W. Wang, and K.-W. Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. ArXiv Preprint, 2025. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171–4186, 2019. K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue. Video-r1: Reinforcing video reasoning in mllms. ArXiv Preprint, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Wu, Z.-F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Chen, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, J. Li, H. Li, J. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, L. Yang, L. Zhao, and C. Xiong. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. 10  W. Held, B. Paranjape, P. S. Koura, M. Lewis, F. Zhang, and T. Mihaylov. Optimizing pretraining data mixtures with llm-estimated utility. arXiv preprint arXiv:2501.11747, 2025. J. Hu, Y. Zhang, Q. Han, D. Jiang, X. Zhang, and H.-Y. Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. A. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022. A. Kazemnejad, M. Aghajohari, E. Portelance, A. Sordoni, S. Reddy, A. Courville, and N. L. Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. OpenReview (ICLR), 2024. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia. Lisa: Reasoning segmentation via large language model. ArXiv Preprint, 2023. H. Lee, S. Phatale, H. Mansoor, T. Mesnard, J. Ferret, K. Lu, C. Bishop, E. Hall, V. Carbune, A. Rastogi, and S. Prakash. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2024. J. Leng, C. Huang, L. Huang, B. Y. Lin, W. W. Cohen, H. Wang, and J. Huang. Crosswordbench: Evaluating the reasoning capabilities of llms and lvlms with controllable puzzle generation, 2025. B. Li, K. Zhang, and A. Marafioti. Multimodal open r1. https://github.com/EvolvingLMMs-Lab/ open-r1-multimodal, 2025. Accessed: 2025-02-08. Z. Li, X. Wu, H. Du, H. Nghiem, and G. Shi. A survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. ArXiv Preprint, 2025. W. Liang, W. Gao, W. Yu, and W. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, European Conference on Computer Vision (ECCV), 2014. Z. Lin, Y. Gao, X. Zhao, Y. Yang, and J. Sang. Mind with eyes: from language reasoning to multimodal reasoning. ArXiv Preprint, 2025. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892–34916, 2023. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. Q. Liu, X. Zheng, N. Muennighoff, G. Zeng, L. Dou, T. Pang, J. Jiang, and M. Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. X. Liu, J. Ni, Z. Wu, C. Du, L. Dou, H. Wang, T. Pang, and M. Q. Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation, 2025. Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. ArXiv Preprint, 2025. Z. Liu, Y. Zhang, F. Liu, C. Zhang, Y. Sun, and J. Wang. Othink-mr1: Stimulating multimodal generalized reasoning capabilities through dynamic reinforcement learning. ArXiv Preprint, 2025. P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Y. Ma, S. Chern, X. Shen, Y. Zhong, and P. Liu. Rethinking rl scaling for vision language models: A transparent, from-scratch framework and comprehensive evaluation scheme. ArXiv Preprint, 2025. 11  A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1697–1706, January 2022. B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, A. Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304–323. Springer, 2024. Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with a reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Y. Mroueh. Reinforcement learning with verifiable rewards: Grpo’s effective loss, dynamics, and success amplification. arXiv preprint arXiv:2503.06639, 2025. NVIDIA. Cosmos-reason1: From physical common sense to embodied reasoning. ArXiv Preprint, 2025. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022. Y. Peng, Chris, X. Wang, Y. Wei, J. Pei, W. Qiu, A. Jian, Y. Hao, J. Pan, T. Xie, L. Ge, R. Zhuang, X. Song, Y. Liu, and Y. Zhou. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought, 2025. URL https://huggingface.co/Skywork/Skywork-R1V-38B. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PmLR, 2021. R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. A. Ray, J. Duan, R. Tan, D. Bashkirova, R. Hendrix, K. Ehsani, A. Kembhavi, B. A. Plummer, R. Krishna, K.-H. Zeng, and K. Saenko. Sat: Spatial aptitude training for multimodal language models. ArXiv Preprint, 2024. S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. H. Shen, P. Liu, J. Li, C. Fang, Y. Ma, J. Liao, Q. Shen, Z. Zhang, K. Zhao, Q. Zhang, R. Xu, and T. Zhao. Vlm-r1: A stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Y. Song, T. Ou, Y. Kong, Z. Li, G. Neubig, and X. Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge, 2025. Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 12  H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. F. Wang, W. Zhou, J. Y. Huang, N. Xu, S. Zhang, H. Poon, and M. Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024. H. Wang, C. Qu, Z. Huang, W. Chu, F. Lin, and W. Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. ArXiv Preprint, 2025. P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. ArXiv Preprint, 2024. X. Wang, Z. Yang, C. Feng, H. Lu, L. Li, C.-C. Lin, K. Lin, F. Huang, and L. Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. ArXiv Preprint, 2025. W. Xiao, L. Gan, W. Dai, W. He, Z. Huang, H. Li, F. Shu, Z. Yu, P. Zhang, H. Jiang, and F. Wu. Fast-slow thinking for large vision-language model reasoning. ArXiv Preprint, 2025. S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. S. Liang, Q. V. Le, T. Ma, and A. W. Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:69798–69818, 2023. W. Xiong, H. Zhang, C. Ye, L. Chen, N. Jiang, and T. Zhang. Self-rewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025. W. Xu, J. Wang, W. Wang, Z. Chen, W. Zhou, A. Yang, L. Lu, H. Li, X. Wang, X. Zhu, W. Wang, J. Dai, and J. Zhu. Visulogic: A benchmark for evaluating visual reasoning in multi-modal large language models. ArXiv Preprint, 2025. J. Yan, Y. Li, Z. Hu, Z. Wang, G. Cui, X. Qu, Y. Cheng, and Y. Zhang. Learning to reason under off-policy guidance. ArXiv Preprint, 2025. Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, B. Zhang, and W. Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. ArXiv Preprint, 2025. J. Ye, P. Liu, T. Sun, J. Zhan, Y. Zhou, and X. Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. E. Yu, K. Lin, L. Zhao, J. Yin, Y. Peng, H. Wei, J. Sun, C. Han, Z. Ge, X. Zhang, D. Jiang, J. Wang, and W. Tao. Perception r1: Pioneering perception policy with reinforcement learning. ArXiv Preprint, 2025. X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, Y. Yue, S. Song, and G. Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? ArXiv Preprint, 2025. K. Zhang, B. Li, P. Zhang, F. Pu, J. A. Cahyono, K. Hu, S. Liu, Y. Zhang, J. Yang, C. Li, and Z. Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv. org/abs/2407.12772. H. Zhou, X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh. R1-zero’s \"aha moment\" in visual reasoning on a 2b non-sft model. ArXiv Preprint, 2025. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 13 A Related Work A.1 Reinforcement Learning for LLM Reinforcement Learning from Human Feedback (RLHF) has become a foundational technique for aligning LLM outputs with human preferences. Early implementations, such as InstructGPT [ 47], utilized Proximal Policy Optimization (PPO) guided by reward models trained on human-labeled data. Subsequent approaches like Constitutional AI [ 3] introduced self-supervised objectives to reduce reliance on human feedback. To further minimize human involvement, Reinforcement Learning from AI Feedback (RLAIF) [ 24] leverages AI-generated feedback. Simplifying the RLHF pipeline, SimPO [ 43] replaces explicit reward models with implicit rewards derived from the model’s own log-likelihood, streamlining training while maintaining performance. Building upon these foundations, recent research has focused on enhancing LLM reasoning capa- bilities through RL via verifiable rewards. DeepSeek’s R1 models [ 4,28] employ Group Relative Policy Optimization (GRPO), which eschews value networks in favor of comparing groups of outputs to reinforce correct reasoning patterns. Similarly, LaTRO [ 5] frames reasoning as a latent-variable optimization problem, enabling self-improvement without external feedback. [ 17] compare several RL algorithms (Expert Iteration, PPO, return-conditioned RL) using sparse correctness rewards or learned reward models, finding broadly similar gains in multi-step reasoning. VinePPO [ 21] addresses credit assignment in multi-step reasoning by utilizing unbiased Monte Carlo returns, and Xiong et al. train models to iteratively generate, verify, and refine their answers using rule-based self-rewards. show that a minimalistic PPO pipeline steadily increases response length and accuracy. On the systems side, VeRL [ 57] provides a flexible RL execution framework: it combines single- and multi-controller designs to support complex LLM training dataflows (PPO, GRPO, etc.) and reports 1.5×-20×throughput improvements over prior systems. Together, these advances demonstrate that relatively simple RL setups with rule-based correctness rewards can substantially enhance LLM reasoning when scaled effectively. A.2 R1-Style Multimodal Reasoning Large language models (LLMs) are usually pre-trained for next token generation and then post-trained to follow instructions and reason. The dominant post-training recipe combines supervised fine-tuning (SFT) with RLHF, where PPO is steered by a reward model built from human judgements [ 48]. Recently, DeepSeek-Math [ 55] and DeepSeek-R1 [ 11] skip the costly reward model training and replace PPO with Group-Relative Policy Optimisation (GRPO), achieving stronger reasoning at lower compute. While DeepSeek-Math/R1 targeted language-only models, a wave of concurrent studies now adapts similar idea to multimodal large language models (MLLMs) to improve reasoning abilities [27, 30]. Single-domain MLLM Reasoning The first wave of multimodal R1 work asks a focused question: can GRPO turn a general-purpose MLLM into a domain specialist? Math reasoning is explored by MAYE [ 39], R1-OneVision [ 70], and Multimodal-Open-R1 [ 26].Fine-grained/Open-vocabulary recognition is addressed by Visual-RFT [ 35] and VLM-R1 [ 56]. R1-V [ 7] targets visual counting and geometric reasoning, and VisualThinker-R1-Zero [ 76] pioneers spatial reasoning without any SFT warm-up [ 52]. Collectively, these studies demonstrate that GRPO is task-portable: by pairing a verifiable reward with a modest domain-specific corpus, one can reliably lift MLLM reasoning within that domain. Reward engineering and algorithmic variants. Once a core stack exists, researchers turn to how the policy is optimised, devising alternative rewards, KL schedules, and curricula. OThink- MR1 [ 36] introduces GRPO-D, an adaptive KL penalty that balances exploration and imitation for counting and geometry. NoisyRollout [ 34] injects image augmentations during roll-outs to strengthen mathematical reasoning, and ThinkLite-VL [ 64] shows that a small but hard subset can rival large-scale runs. FAST  further adapts the reasoning length to task difficulty on-the-fly. Stability, efficiency and exploration strategies. As reward designs grow more sophisticated, training can suffer from vanishing gradients and sample inefficiency, motivating techniques that explicitly stabilise and accelerate GRPO. [ 8] proposes normalized length reward to mitigate instability 14 caused by compeletion length. VL-Rethinker [ 62] combats gradient issues via selective sample replay and forced self-reflection, whereas LUFFY [ 69] blends off-policy imitation with on-policy exploration using regularised importance sampling. Empirical analyses and large-scale deployments. A complementary thread conducts systematic ablations or scales GRPO to production-grade MLLMs, clarifying its real-world impact. VLAA- Thinking [ 6] argues that SFT alone induces “pseudo-reasoning” and that mixed-reward GRPO restores genuine logical skill; Limit-of-RLVR [ 74] notes that GRPO mainly lifts top-1 rather than top-kperformance; and Perception-R1 [ 72] measures transfer to pure perception. In practice, Skywork-R1V [ 49] and Cosmos-Reason1 [ 45] add a GRPO stage to large production models, while OpenVLThinker [ 12] alternates SFT and GRPO over progressively harder datasets. The recipe is already migrating to video—Video-R1 [ 14], SEED-Bench-R1 [ 9] and VideoChat-R1—highlighting its modality-agnostic appeal. To track progress, new visual puzzle suites probe the logical depth of MLLMs [58, 25, 68]. Open problem. Despite the rapid proliferation of R1-style methods in MLLM Reasoning, data mixture strategies —how to allocate training resources across heterogeneous reasoning tasks—remain unexplored. Our work fills this gap and serves as a first step for future studies. A.3 Data Mixture for LLM Existing approaches to mixing heterogeneous data sources for pretraining have largely fallen into two categories: language-centric up-sampling and model-guided data mixture. In the multilingual setting, BERT [ 13] up-samples under-represented languages by raising sampling probabilities with a temperature hyperparameter, but this often causes the lowest-resource languages to be repeated excessively. To address this imbalance problem, UniMax [ 10] introduces a heuristic that dynamically adjusts language weights to achieve a fairer distribution across languages. In parallel, methods such as data mixing laws [ 71] learn analytic functions over sample mixtures to predict model performance on unseen combinations without requiring full training runs. DoReMi [ 66] and RegMix [ 33] pursue a similar two-stage strategy: a small proxy model is first trained – via GroupDRO [ 53] in DoReMi or by evaluating multiple mixture configurations in RegMix – to identify optimal mixture weights, before resampling and training a full-scale model. Although effective, these proxy-based methods suffer from reduced efficiency due to multiple training passes, and the weights they learn often fail to generalize across different models. Online Data Mixing (ODM) [ 2] builds on DoReMi by updating data weights during full-model training, but still relies on the same underlying minimax estimation. More recent findings suggest that even simple heuristics, such as token-count proportions [ 18], can outperform both manual and learned data mixture schemes, calling into question the necessity of complex weight estimation. Motivated by this insight, [ 18] proposes UtiliMax and Model Estimated Data Utility (MEDU), which blend lightweight utility estimates with model-informed adjustments to strike a balance between simplicity and adaptivity. While the aforementioned techniques focus primarily on monolingual or multilingual text corpora drawn from sources like Wikipedia, books, web text, and code, our study shifts attention to the domains of vision-language reasoning tasks. In these multimodal settings, dataset relationships are governed not only by raw sample frequency but also by the semantic and reasoning objectives unique to each task, necessitating novel strategies for sampling and weighting across diverse VLM reasoning benchmarks. 15 Table 2: Overall Performance Comparison. Seed experiments use digits to denote training datasets: 1: COCO, 2: LISA, 3: GeoQA V, 4: SAT, 5: ScienceQA. ExperimentIn Tests Out TestsIn-Score Out-Score LISA SAT ScienceQA ChartQA InfoVQA MathVista MMMU Base 0.1525 0.2479 0.0486 0.236 0.3144 0.391 0.3789 0.149 0.3059SeedSingle 1 0.4015 0.5047 0.0258 0.41 0.5357 0.436 0.3811 0.3254 0.4589 2 0.4433 0.4035 0.0253 0.3704 0.476 0.426 0.3922 0.318 0.4219 3 0.0835 0.2547 0.4284 0.4936 0.5086 0.405 0.3989 0.2232 0.4753 4 0.0551 0.5949 0.063 0.5476 0.5075 0.414 0.3722 0.199 0.4915 5 0.0279 0.3786 0.7828 0.494 0.3787 0.426 0.3867 0.3274 0.4263 Exclude-one 2345 0.4393 0.5897 0.7313 0.5644 0.5319 0.443 0.4022 0.559 0.5146 1345 0.4064 0.5902 0.7288 0.4488 0.5471 0.429 0.4011 0.5432 0.4783 1245 0.4728 0.6011 0.7283 0.5424 0.4875 0.437 0.4022 0.5767 0.4889 1235 0.4727 0.4824 0.7313 0.498 0.4858 0.423 0.4122 0.5463 0.4721 1234 0.4493 0.5856 0.4259 0.5108 0.5387 0.406 0.3978 0.4787 0.493 All 12345 0.4778 0.5737 0.6991 0.4816 0.4681 0.435 0.41 0.5638 0.4609Heuristic-basedStep-averaged Ain 0.4687 0.5928 0.7476 0.5524 0.5111 0.434 0.4 0.5779 0.5008 Aout 0.4616 0.5788 0.7179 0.488 0.5125 0.422 0.3989 0.5628 0.4772 Abal 0.4755 0.5892 0.7338 0.552 0.5179 0.441 0.4144 0.5763 0.5061 Coli 0.4494 0.5773 0.7055 0.5304 0.4851 0.432 0.3978 0.5533 0.4825 Norm 0.4573 0.5664 0.705 0.564 0.5083 0.467 0.4133 0.554 0.51 2000-step Ain 0.4415 0.5939 0.7581 0.522 0.5036 0.417 0.4133 0.5685 0.4867 Aout 0.4075 0.5799 0.71 0.4788 0.5089 0.429 0.3911 0.5359 0.4726 Abal 0.4676 0.5877 0.7382 0.5548 0.5063 0.436 0.4078 0.5735 0.5011 Coli 0.4678 0.5783 0.6083 0.4076 0.5035 0.422 0.4089 0.5354 0.4471 Norm 0.4732 0.5762 0.7372 0.5764 0.5156 0.449 0.4022 0.5728 0.5133Heuristic-based (No 1)Step-averaged Ain 0.4498 0.402 0.7129 0.5456 0.5065 0.428 0.4089 0.5095 0.497 Aout 0.0873 0.5856 0.7015 0.4692 0.4932 0.421 0.4022 0.3869 0.4635 Abal 0.4666 0.5892 0.7447 0.5572 0.5307 0.427 0.3967 0.5752 0.5088 Coli 0.448 0.5923 0.704 0.4836 0.5002 0.402 0.3933 0.5562 0.4674 Norm 0.4662 0.5794 0.711 0.5072 0.507 0.42 0.41 0.5632 0.4829 2000-step Ain 0.4257 0.4414 0.7204 0.5096 0.4308 0.403 0.3956 0.5108 0.4499 Aout 0.0556 0.5731 0.6896 0.488 0.5053 0.415 0.4022 0.3657 0.4739 Abal 0.4637 0.5529 0.7248 0.4904 0.4847 0.429 0.3922 0.5589 0.4674 Coli 0.4815 0.5897 0.7105 0.514 0.4978 0.42 0.4067 0.5728 0.4812 Norm 0.4065 0.5892 0.7169 0.5596 0.5158 0.424 0.3944 0.5398 0.5031Model-based001 0.2274 0.5871 0.7387 0.5196 0.5246 0.453 0.3911 0.4623 0.4962 002 0.2674 0.5762 0.7288 0.5536 0.525 0.438 0.3956 0.4752 0.5067 003 0.2225 0.5747 0.7427 0.514 0.5005 0.446 0.4044 0.4579 0.4856 004 0.438 0.597 0.7154 0.5344 0.5016 0.424 0.3844 0.556 0.4876 005 0.1219 0.6006 0.7313 0.4844 0.5256 0.414 0.4178 0.415 0.4823 006 0.4293 0.5949 0.7402 0.5528 0.5126 0.427 0.3867 0.5582 0.4989 007 0.2382 0.5669 0.7234 0.5412 0.5269 0.443 0.4033 0.4578 0.5048 008 0.3843 0.5887 0.7214 0.516 0.5067 0.448 0.3989 0.5306 0.4883 009 0.3955 0.5773 0.7343 0.5648 0.5258 0.434 0.3789 0.5363 0.5082 010 0.3925 0.5742 0.7338 0.5668 0.5184 0.446 0.4 0.534 0.5104 16 Figure 8: Comparison of Grounding Question-Answer Pairs With and Without Reasoning. Question: Output the bounding box in the image corresponding to the in- struction: In a football match, which position primarily focuses on guarding the goal and stopping the opposing team from scoring? Answer: [422, 781, 464, 926] Question: Output the bounding box in the image corresponding to the in- struction: In a football match, which position primarily focuses on guarding the goal and stopping the opposing team from scoring? Output the thinking process in <think> </think> and final answer in <an- swer> </answer> tags.The output answer format should be as follows: <think>... </think> <answer>[’Position’: [x1, y1, x2, y2], ’Confidence’: number,...]</answer> Please strictly follow the format. Answer: <think>... </think> <answer>[’Position’: [422, 781, 464, 926], ’Confidence’: 1]</answer> 17 Algorithm 1 Heuristic-Alpha: Balanced, In-optimized and Out-optimized Require: D={1,..., m}: Set of mdataset indices R: Set of performance records from experiments αsingle∈[0,1]: Scaling factor for single datamix baselines α∈[0,1]: Trade-off parameter balancing In and Out set performance Ensure: Dataset weights {wi}i∈DwhereP i∈Dwi= 1 1:Sin←1..m ▷Initialize In Score sums 2:Sout←1..m ▷Initialize Out Score sums 3:foreach record {sin r, sout r} ∈Rdo 4: Identify datasets Dr⊆ D used in recipe r 5: if|Dr|= 1then ▷Single-dataset 6: sin r←αsingle·sin r 7: sout r←αsingle·sout r 8: end if 9: foreach dataset i∈Drdo 10: Sin[i]←Sin[i] +sin r 11: Sout[i]←Sout[i] +sout r 12: end for 13:end for 14:Min-max normalization: 15:ˆSin←MinMax (Sin) ▷ˆSin i=Sin i−minjSin j maxjSin j−minjSin j 16:ˆSout←MinMax (Sout) ▷ˆSout i=Sout i−minjSout j maxjSout j−minjSout j 17:Combine scores with trade-off parameter: 18:foreach dataset i∈ D do 19: Ci←α·ˆSin[i] + (1−α)·ˆSout[i] 20:end for 21:Normalize to obtain final weights: 22:Csum←P i∈DCi 23:foreach dataset i∈ D do 24: wi←Ci/Csum 25:end for 26:return {wi}i∈D 18 Algorithm 2 Heuristic-Colinearity; Variance Inflation Factor (VIF) quantifies multicollinearity. Require: D={1,..., m}: Set of mdataset indices R: Set of performance records from experiments λ∈R+: Ridge regularization parameter (default 10−3) Ensure: Dataset weights {wi}i∈DwhereP i∈Dwi= 1 1:X←[ ] ▷Initialize design matrix 2:y←[ ] ▷Initialize target vector 3:foreach record {sin r, sout r} ∈Rdo 4: Identify datasets Dr⊆ D used in recipe r 5: xr←1..m ▷Initialize feature vector for this record 6: foreach dataset i∈Drdo 7: xr[i]←1 ▷Set indicator to 1 for datasets used in recipe 8: end for 9: X←X∪ {xr} ▷Add feature vector to design matrix 10: y←y∪ {sout r} ▷Add score to target vector 11:end for 12:Fit ridge regression without intercept: 13:β←arg min β∥y−Xβ∥2 2+λ∥β∥2 2 ▷Ridge regression coefficients 14:Compute Variance Inflation Factor (VIF) correction: 15:Im←Identity matrix of size m×m ▷m×midentity matrix 16:XTXreg←XTX+λ·Im ▷Regularized Gram matrix 17:(XTXreg)−1←Inverse (XTXreg) ▷Inverse of Gram matrix 18:VIF←diag((XTXreg)−1) ▷Diagonal elements give VIF 19:Adjust coefficients using VIF: 20:foreach dataset i∈ D do 21: si←βi/VIFi ▷Adjust coefficient by its variance inflation 22:end for 23:Apply non-negativity constraint: 24:foreach dataset i∈ D do 25: si←max(0, si) ▷Replace negative values with zero 26:end for 27:Normalize to obtain final weights: 28:ssum←P i∈Dsi 29:foreach dataset i∈ D do 30: wi←si/ssum 31:end for 32:return {wi}i∈D 19 Algorithm 3 Heuristic-Normalize: Leave-One-Out Dataset Weighting Require: D={1,..., m}: Set of mdataset indices R: Set of performance records from experiments C: Set of leave-one-out experiments Ensure: Dataset weights {wi}i∈DwhereP i∈Dwi= 1 1:SC←[ ] ▷Initialize scores for combinations 2:foreach record {sin r, sout r} ∈Rdo 3: Extract combination ID crfrom leading digits in r 4: Extract out score sout r 5: ifcr∈Cthen ▷If this is a leave-one-out combination 6: SC←SC∪ {(cr, sout r)} ▷Store combo and score 7: end if 8:end for 9:Normalize out scores: 10:smax←max (cr,sr)∈SCsr ▷Maximum out score 11:smin←min (cr,sr)∈SCsr ▷Minimum out score 12:foreach(cr, sr)∈SCdo 13: ˆsr←sr−smin smax−smin▷Min-max normalization 14: s′ r←0.2−(0.1·ˆsr) ▷Transform: higher score = lower weight 15:end for 16:Derive dataset weights from leave-one-out scores: 17:wraw←1..m ▷Initialize raw weights 18:fori∈ {1..m}do 19: wraw[i]←s′ rwhere cris the combination missing dataset i 20:end for 21:Normalize to obtain final weights: 22:wsum←Pm i=1wraw[i] 23:fori∈ {1..m}do 24: wi←wraw[i]/wsum 25:end for 26:return {wi}i∈D 20 Algorithm 4 QuadSurface: Quadratic Response Surface Optimization Require: D={1,..., m}: Set of mdataset indices R: Set of performance records from experiments nsamples: Number of candidate points to evaluate k: Number of top mixtures to return Ensure: Topkdataset mixtures {wi}i∈Doptimized for performance 1:Data preparation: 2:X←[ ] ▷Initialize matrix of mixture weights 3:y←[ ] ▷Initialize vector of performance scores 4:foreach record r∈Rdo 5: Extract weight vector wrand performance score sr 6: X←X∪ {wr} 7: y←y∪ {sr} 8:end for 9:Cross-validation splits: 10:Perform nsplits= 5random train-test splits of (X, y) ▷80% train, 20% test 11:foreach split ido 12: (Xtrain, ytrain),(Xtest, ytest)←Spliti(X, y) 13: Fit quadratic response surface model: 14: Ftrain←[1, Xtrain, Xtrain⊗Xtrain] ▷Design matrix with quadratic terms 15: βi←arg min β∥ytrain−Ftrainβ∥2 2 ▷Ordinary least squares 16: (bi, Wi, Qi)←Extract (βi) ▷Extract intercept, linear, and quadratic terms 17: Evaluate model: 18: ˆytrain←bi+XtrainWi+ (Xtrain⊗Xtrain)Qi 19: ˆytest←bi+XtestWi+ (Xtest⊗Xtest)Qi 20: R2 train,i, R2 test,i←ComputeR2(ˆytrain, ytrain,ˆytest, ytest) 21:end for 22:Select best model: 23:i∗←arg max iR2 test,i ▷Choose model with best test performance 24:(b∗, W∗, Q∗)←(bi∗, Wi∗, Qi∗) 25:Fit GMM to observed mixtures: 26:GMM ←FitGaussianMixture (X, n components = 1) ▷Fit single-component GMM 27:Generate candidate mixtures via GMM: 28:Xraw←SampleFromGMM (GMM, nsamples ) 29:Xvalid← {x∈Xraw|xi≥0for all i} ▷Filter non-negative points 30:Xcandidates ← {x/P ixi|x∈Xvalid} ▷Normalize to sum to 1 31:Predict performance for all candidates: 32:ˆycandidates ←b∗+Xcandidates W∗+ (Xcandidates ⊗Xcandidates )Q∗ 33:Select top-performing mixtures: 34:indices top←Argsort (ˆycandidates )[−k:] ▷Indices of top kscores 35:Xtop←Xcandidates [indices top] ▷Topkmixtures 36:return Xtop 21",
  "text_length": 68317
}