{
  "id": "http://arxiv.org/abs/2506.01121v1",
  "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded,\n  Robust, and Safe Generation",
  "summary": "Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution.",
  "authors": [
    "Jacob K. Christopher",
    "Michael Cardei",
    "Jinhao Liang",
    "Ferdinando Fioretto"
  ],
  "published": "2025-06-01T18:58:59Z",
  "updated": "2025-06-01T18:58:59Z",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01121v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01121v1  [cs.LG]  1 Jun 2025Proceedings of Machine Learning Research vol 288:1–26, 2025 2nd International Conference on Neuro-symbolic Systems (NeuS)\nNeuro-Symbolic Generative Diffusion Models for Physically\nGrounded, Robust, and Safe Generation\nJacob K. Christopher CSK4SR@VIRGINIA .EDU\nMichael Cardei NTR2RM@VIRGINIA .EDU\nJinhao Liang NJS4NU@VIRGINIA .EDU\nFerdinando Fioretto FIORETTO @VIRGINIA .EDU\nDepartment of Computer Science, University of Virginia\nEditors: G. Pappas, P. Ravikumar, S. A. Seshia\nAbstract\nDespite the remarkable generative capabilities of diffusion models, their integration into safety-\ncritical or scientifically rigorous applications remains hindered by the need to ensure compliance\nwith stringent physical, structural, and operational constraints. To address this challenge, this pa-\nper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps\nwith symbolic optimization, enabling the generation of certifiably consistent samples under user-\ndefined functional and logic constraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both continuous (e.g., images and\ntrajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with\nconstraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety , in\nthe context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data\nscarcity , in domains such as drug discovery and materials engineering; and (3) Out-of-domain\ngeneralization , where enforcing symbolic constraints allows adaptation beyond the training distri-\nbution.\nKeywords: Diffusion Models, Controllable Generation, Differentiable Optimization\n1. Introduction\nDiffusion models (Ho et al., 2020) are a class of generative AI models at the forefront of high-\ndimensional data creation and form the backbone of many state-of-the-art image and video gener-\nation systems (Rombach et al., 2022; Betker et al., 2023; Liu et al., 2024). This potential has also\nbeen recently extended to the context of discrete outputs, which is suitable for language modeling\nor combinatorial structure design, like chemical compounds or peptide design (Zheng et al., 2024;\nLou et al., 2024; Shi et al., 2024). Diffusion models operate by progressively introducing controlled\nrandom noise into the original content and learning to reverse the process to reconstruct statistically\nplausible samples. This approach has shown transformative potential for engineering, automation,\nand scientific research through applications including generating trajectories for robotic agents in\ncomplex, high-dimensional environments or synthesizing new molecular structures with improved\nstrength, thermal resistance, or energy efficiency (Carvalho et al., 2023; Watson et al., 2023).\nHowever, as opposed to standard image synthesis tasks, scientific applications of diffusion mod-\nels need to be controlled by precise mechanisms or properties that must be imposed on generations.\nWhile diffusion models produce statistically plausible outputs, they are simultaneously unable to\ncomply with fundamental physics, safety constraints, or user-imposed specifications (Motamed\net al., 2025). Violations of established principles and constraints not only undermine the utility\n© 2025 J. Christopher, M. Cardei, J. Liang & F. Fioretto.\n--- Page 2 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nof generative models, but also erode their trustworthiness in high-stakes domains. For example,\nembodied agents powered by generative AI such as drones or robotic arms are susceptible to adver-\nsarial manipulations that bypass safety protocols (Robey et al., 2024). To date, these models have\nstruggled to produce even simple trajectories that satisfy basic collision avoidance, let alone more\nstringent safety requirements in complex environments (Power et al., 2023). Similarly, in scientific\nand industrial applications such as autonomous bio-labs, systems may improperly model specifica-\ntions or even react to adversarial triggers, potentially leading to synthesis of hazardous compounds\n(Wittmann, 2024). Thus, there is a pressing need for generative models to satisfy physical, opera-\ntional, and structural constraints that govern large-scale scientific and engineering challenges.\nRecently, Christopher et al. (2025) observed that a class of generative models can ground their\ninduced distributions to a specific property. Inspired by this observation, this paper provides a step\ntowards addressing the challenge of constraining generative models and developing a novel integra-\ntion of symbolic optimization with generative diffusion models. The resulting framework, called\nNeuro-Symbolic Diffusion (NSD), enables the generation of outputs that are certifiably consistent\nwith user-defined properties, ranging from continuous constraints, such as structural properties for\nmaterial science applications or collision avoidance in motion-planning environments, to discrete\nconstraints, including the prevention of toxic substructures in molecule generation tasks.\nContributions. The contributions of this study are as follows.\n1. It develops a novel methodology for integrating functional and logic constraints within genera-\ntive diffusion models . The core concept involves a tight integration of differentiable constraint\noptimization within the reverse steps of diffusion process and ensures that each generated sample\nrespects user-imposed or domain-specific properties.\n2. It shows that this approach is not only viable for generation within continuous subspaces but\nalso effective in constraining token generation for discrete modalities, including domain-specific\nsequence generation for scientific discovery and open-ended language generation.\n3. It provides theoretical grounding to demonstrate when and why constraint adherence can be\ncertified during the neuro-symbolic generative process.\n4. It presents an extensive evaluation across three key challenges: (1) Safety , demonstrated through\nnon-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity , with\napplications in drug discovery and materials engineering; and (3) Out-of-domain generalization ,\nwhere enforcing symbolic constraints enables adaptation beyond the training distribution.\nThese advances bring forward two key features that are important for the development of generative\nmodels for scientific applications: Improved assurance , e.g., the models can implement safety pred-\nicates needed in the domain of interest, such as natural language generation where prompts could be\nengineered to elicit harmful outputs, and Improved generalization , e.g., the imposition of knowledge\nand symbolic constraints dramatically improves the model generalizability across domains.\n2. Preliminaries: Generative Diffusion Models\nDiffusion models define a generative process by learning to reverse a forward stochastic transfor-\nmation that progressively corrupts structured data into noise. The generative model then approxi-\nmates the inverse of this transformation to restore the original structure, thereby allowing sampling\nfrom the learned distribution. The forward noising process {xt}T\nt=0progressively corrupts data in\na Markovian process, starting from x0∼pdata(x0)and culminating into noise xT∼p(xT). Here,\npdata(x0)represents the distribution induced by real data samples, and p(xT)is, by design, some\n2\n--- Page 3 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nknown distribution. The reverse process starts from xT∼p(xT)and produces samples x0that\nfollow pdata.\nIn this study, we consider two settings: (1) continuous diffusion models (Ho et al., 2020; Song\net al., 2020) for data in Rd(e.g., images or trajectories), and (2) discrete diffusion models (Lou et al.,\n2024; Sahoo et al., 2024), which were recently introduced to handle discrete data (e.g., sequences\nof tokens representing natural language or molecular structures).\nDiffusion models for continuous data. For data in Rd, the forward diffusion process is often\nmodeled as a stochastic differential equation (SDE) of the form dxt=−1\n2β(t)xtdt+p\nβ(t)dB(t),\nwhere B(t)denotes standard Brownian motion and β(t)defines a noise schedule. As t→T, the\nprocess asymptotically transforms data into an isotropic Gaussian distribution. The reverse process ,\nwhich recovers the original data, follows a time-reversed SDE underlying Langevin dynamics :\ndxt=\u0014\n−1\n2β(t)xt+∇xtlogp(xt)\u0015\ndt+p\nβ(t)dB(t). (1)\nHowever, since exact integration of this process is intractable, in practice, it is discretized into a\nfinite-step Markov chain :\nxt−∆=xt+γtsθ(xt, t) +p\n2γtϵ, (2)\nwhere sθis a neural network that approximates the gradient of the log data distribution ∇xtlogp(xt),\ncalled the score function , and is used to guide the model toward high-density regions. Additionally,\nγtis the step size and ϵis a Gaussian perturbation. In the deterministic limit (i.e., γt→0), this\nbecomes a pure gradient ascent update on logp(xt).\nDiscrete diffusion models. For discrete data such as text tokens, each sample is a sequence x0=\n(x1\n0, . . . ,xL\n0)where each token xi\n0∈RVis represented as a one-hot vector over a vocabulary of\nsizeV. The forward process progressively corrupts the sequence by replacing tokens with noise,\nthe marginal of which is defined as: q(xt|x0) = Cat ( xt; (1−β(t))x0+β(t)ν),where\nβ(t)∈[0,1]is a schedule that increases with t, so that tokens are increasingly replaced by noise, and\nCat(·;z)denotes a categorical distribution parameterized by probability vector z∈ΣV, where ΣV\ndenotes the V-dimensional simplex. Finally, νis a fixed categorical distribution, often concentrated\non a special token, such as [MASK] (as in MDLM from Sahoo et al. (2024)) or chosen uniformly\n(as in UDLM from Schiff et al. (2024)). It models a process akin to that induced by the isotropic\nGaussian in the continuous counterpart. As tincreases, each token in xtbecomes less correlated\nwith its original value, and approaches the noise distribution. The reverse process is represented as\nxt−∆=\n\nCat\u0000\nxt−∆;xt\u0001\n, ifxt̸=ν,\nCat\u0010\nxt−∆;β(t−∆)ν+(β(t)−β(t−∆))\nβ(t)sθ(xt, t)\u0011\n,ifxt=ν,(3)\nSincex0is unknown at inference, it is approximated with sθ(xt, t). Here, xtrepresents a vector of\nprobability distributions over tokens at each position in the sequence. The paper denotes with x⋆\nt=\nargmax( xt)as the selected output sequence, where the argmax operator is applied independently\nto each member xi\ntof the sequence xt.\n3. Related Work and Limitations\nDespite their success, existing diffusion models struggle to enforce structured constraints. An ap-\nproach developed to address this issue relies on sampling a conditional distribution pdata(x0|c),\nwhere cconditions the generation. This approach transforms the denoising process via\n3\n--- Page 4 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\n0.0 0.1 0.2 0.3 0.4\nDiffusion Timestep t010203040Constraint ViolationLimitations of Conditional Models\nNeuro-Symbolic Diffusion\nConditional Diffusion\nFigure 1: Conditional models fail to converge\nto feasible states while Neuro-Symbolic Dif-\nfusion produces no violations.classifier-free guidance:\nˆsθdef=λ×sθ(xt, t,c) + (1 −λ)×sθ(xt, t,⊥),\nwhere λ∈(0,1)is the guidance scale and⊥is a\nnull vector representing non-conditioning (Ho and Sal-\nimans, 2022). These methods have demonstrated effec-\ntiveness in capturing physical design properties (Wang\net al., 2023), positional awareness (Carvalho et al.,\n2023), and motion dynamics (Yuan et al., 2023). How-\never, while conditioning can guide the generation pro-\ncess, it offers no reliability guarantees. This issue is\nillustrated in Figure 1 (red curve), which shows the magnitude of constraint violations observed in a\nphysics-informed motion experiment simulating the dynamics of an object under a force-field influ-\nence (more details provided in Appendix E.4). Here, the conditional model fails to adhere to motion\nconstraints as the diffusion steps evolve (t→0). Additionally, conditioning in diffusion models\noften necessitates the training of auxiliary classification or regression models, which requires addi-\ntional labeled data. This is highly impractical in many scientific contexts of interest to this paper,\nwhere sample collection is expensive or extremely challenging.\nAn alternative approach involves applying post-processing steps to correct deviations from de-\nsired constraints in the generated samples. This correction is typically implemented in the last noise\nremoval stage (Giannone et al., 2023; Power et al., 2023; Maz ´e and Ahmed, 2023). However, this\napproach present two main limitations. First, the objective does not align with optimizing the dif-\nfusion model score function, and thus does not guide the model towards high-density regions. This\ninherently positions the diffusion model’s role as ancillary, with the final synthesized data often re-\nsulting in a significant divergence from the learned (and original) data distributions. Second, these\nmethods are reliant on a limited and problem specific class of objectives and constraints, such as spe-\ncific trajectory “constraints” or shortest path objectives which can be integrated as a post-processing\nstep (Giannone et al., 2023; Power et al., 2023).\nTo overcome these gaps and handle arbitrary symbolic constraints, our approach casts the re-\nverse diffusion process as a differentiable constraint optimization problem, which is then solved\nthrough the application of repeated projection steps. The next section focuses on continuous mod-\nels for clarity, but this reasoning extends naturally to discrete models, as shown in Appendix D.\n4. Reverse Diffusion as Constrained Optimization\nIn traditional diffusion model sampling, the reverse process transitions a noisy sample xTtox0by\nreversing the stochastic differential equation in (1), which is discretized into an iterative Langevin\ndynamics update in (2). The key enabler for the integration of constraints in the diffusion process\nis the realization that each reverse step can be framed as an optimization problem . As shown in Xu\net al. (2018), under appropriate regularity conditions, Langevin dynamics converges to a stationary\ndistribution p(xt), effectively maximizing logp(xt)(Christopher et al., 2025). As t→0,the\nvariance schedule decreases and the noise term√2γtϵvanishes, causing the update step to become\ndeterministic gradient ascent on logp(xt). This perspective reveals that the reverse process can be\nviewed as minimizing the negative log-likelihood of the data distribution . The proposed method for\nconstraining the generative process relies on this interpretation.\n4\n--- Page 5 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nIn traditional score-based models, at any point throughout the reverse process, xtis uncon-\nstrained. When these samples are required to satisfy certain constraints, the objective remains\nunchanged, but the solution to this optimization must fall within a feasible region C. Thus, the\noptimization problem formulation becomes:\nMinimize\nxt:t∈(0,T]ZT\nt=0−logp\u0000\nxt|x0\u0001\nsubject to xt∈C,∀t∈(0, T]. (4)\nIn practice, Cis defined by the intersection of multiple ( n) functional expressions or logic predi-\ncates: Cdef=Vn\ni=1ϕi(x), where each ϕi(x)is a predicate that returns 1 if xsatisfies a condition\nand 0 otherwise. For example, these may be a series of properties that are required for a generated\nmolecule to be a non-toxic chemical compound. The paper uses ϕ1:nto denote the subset of n\nconstraints in C.\nIn discrete diffusion models, the score function can be similarly modeled through Concrete\nScore Matching as expounded on in Appendix D (Meng et al., 2022). Hence, the underlying op-\ntimization interpretation remains consistent: each denoising update seeks to move xtcloser to the\nhigh-density region of the learned distribution while respecting the constraints C.\n5. Neuro-Symbolic Generative Models\nThe score network sθ(xt, t)directly estimates the first-order derivatives of Equation (4) (excluding\nthe constraints) and provides the necessary gradients for iterative updates defined in Equations (2)\nand (3). In the presence of constraints, however, an alternative iterative method is necessary to guar-\nantee feasibility. This section illustrates how projected guidance can augment diffusion sampling\nand transform it into a constraint-aware optimization process. First, it formalizes the notion of a\nprojection operator PC, which finds the nearest feasible point to the input x:\nPC(x)def= argmin\ny\n\n||x−y||2\n2 s.t.y∈C, ifxis continuous ,\nDKL(x∥y)s.t.y⋆= argmax( y)∈C,ifxis discrete .(5)\nBecause continuous diffusion operates in a multi-dimensional real space, x∈Rd, whereas discrete\ndiffusion represents samples as x∈ΣV, the notion of proximity must be adapted accordingly. In\ncontinuous settings, Euclidean distance provides a natural measure of deviation from feasibility. At\nthe same time, for discrete models, the underlying representations correspond to probability distri-\nbutions; thus, the Kullback–Leibler (KL) divergence is chosen to quantify the minimal adjustment\nneeded to satisfy the constraints. The optimization objective in Equation (5) defines a projection\noperator that minimizes a cost function, which we refer to as the cost of projection . In the contin-\nuous setting, this corresponds to the squared Euclidean distance, ∥y−x∥2\n2, while in the discrete\nsetting, it is determined by the KL divergence. More generally, we denote this projection cost as\nDcost(x,y), representing the modality-specific distance minimized in the projection step.\nTo ensure that feasibility is maintained throughout the reverse process of the diffusion model,\nthe sampling step is updated as:\nxt−∆=\n\nPC\u0010\nxt+γtsθ(xt, t) +√2γtϵ\u0011\nifxis continuous ,\nPC\u0010\nCat\u0000\n·;πθ(xt, t)\u0001\u0011\nifxis discrete .(6)\n5\n--- Page 6 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nwhere γt>0is the step size, ϵ∼ N (0, I), and πθ(xt, t)is the predicted probability vector,\nas generalized from Equation (3). Hence, at each step of the Markov chain, a gradient update is\napplied to minimize the objective in Equation (4), while interleaved projections ensure feasibility\nthroughout the sampling process. Importantly, convergence is guaranteed for convex constraint sets\n(see Section 6), and empirical results in Section 7 demonstrate the effectiveness of this approach,\neven in highly non-convex settings. Notably, the projection operators can be warm-started across\niterations, providing a practical solution for efficiently handling regions with complex constraints.\nAlgorithm 1 Augmented Lagrangian Projection\nInput: xt,λ,µ,γ,α,δ\ny←xt\nwhile ˜ϕ(y)< δdo\nforj←1tomax inner iterdo\nLALM←Dcost\u0000\nxt,y\u0001\n+λ˜ϕ(y)+µ\n2˜ϕ(y)2\ny←y−γ∇yLALM\nend\nλ←λ+µ˜ϕ(y);µ←min\u0000\nαµ, µ max\u0001\nend\nxt−∆←y\nreturn xt−∆Augmented Lagrangian Projection. To solve the\nprojection subproblem PC(xt)in each sampling step,\nthe paper uses a Lagrangian dual method (Boyd,\n2004), where the constraints are incorporated into\na relaxed objective by using Lagrange multipliers λ\nand a quadratic penalty term µ. The augmented La-\ngrangian function is defined as\nLALM\u0000\ny, λ, µ\u0001\n=Dcost\u0000\nxt,y\u0001\n+λ˜ϕ(y)+µ\n2˜ϕ(y)2,\nwhere ˜ϕdenotes a differentiable residual or con-\nstraint violation of the original (potentially non-\ndifferentiable) constraint function ϕ1:n. For example, consider a linear constraint ϕ=Ay≤b,\nthen ˜ϕ= max(0 , Ay−b). The iterative update follows a dual ascent strategy, where the vari-\nablesyare optimized via gradient step on ∇yLALM, while the dual variables λare updated by\nλ→λ+µ˜ϕ(y).Additionally, the penalty coefficients µare increased adaptively by αto tighten\nconstraint enforcement. This procedure continues until ˜ϕ(y)< δ or the maximum iteration count\nis reached, returning a feasible yasxt−∆, as illustrated in Algorithm 1. Note that for convex\nconstraint sets, the augmented Lagrangian method provides strong theoretical guarantees for exact\nconvergence to the projection onto the feasible set (Boyd, 2004). Specifically, if Slater’s condition\nholds (i.e., there exists a strictly feasible point), then the method guarantees convergence to the pri-\nmal solution satisfying the constraints. This feature is key for several applications of interest to this\nwork (see Section 7).\nNotice that, for discrete variables, the projection PC(xt)must be imposed on the decoded\nsequence y⋆= argmax( y). Because the argmax operator is not differentiable, this paper adopts\na Gumbel-Softmax relaxation (Jang et al., 2016) to preserve gradient-based updates. Details on this\nrelaxation are provided in Appendix C, and further technical aspects of the augmented Lagrangian\nscheme are discussed in Appendix B.\nBy incorporating constraints throughout the sampling process, the interim learned distributions\nare steered to comply with these specifications. The effectiveness of this approach is empirically\nevident from Figure 1 (blue curve): remarkably, as the reverse process unfolds, constraint violations\nsteadily approach 0and a theoretical justification for the validity of this approach is provided in\nthe next section. A key distinction of this method, in contrast to prior approaches (Giannone et al.,\n2023; Power et al., 2023), is that it optimizes the negative log-likelihood as the primary sampling\nobjective , maintaining consistency with standard unconstrained diffusion models while enforcing\nverifiable constraints. This provides a fundamental advantage: it maximizes the probability of gen-\nerating samples that conform to the data distribution while ensuring feasibility . In contrast, existing\nbaselines prioritize external constraints at the expense of distributional fidelity, often leading to sig-\nnificant deviations from the learned distribution, as shown in Section 7.\n6\n--- Page 7 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\n6. Effectiveness of Neuro-Symbolic Generation: A Theoretical Justification\nThis section focuses on two key outcomes of incorporating iterative projections during diffusion\nsampling: (1)As the sample xttransitions toward the minimizer of the negative log-likelihood\n(the primary diffusion objective), each projection step needs only a small adjustment to maintain\nfeasibility. Thus, the projected sampling remains closely aligned with the unconstrained score-\nbased dynamics, causing minimal deviation from the main objective. (2)By keeping the sample\nnearPC(xt)throughout the sampling trajectory, any subsequent or “final” projection step becomes\nless costly (e.g., smaller Euclidean or KL distance).\nProofs for all theorems are provided in Appendix F and additional technical details in Appendix\nD. The analysis assumes a convex feasible region Cand unifies results for both continuous (Eu-\nclidean) and discrete (KL-based) metrics (Christopher et al., 2025). Below, we detail the theoretical\nunderpinnings using the update notation xt→xt−∆in place of traditional iterative indexing.\nConsider an update step that transforms a sample xtat diffusion time tintoxt−∆at time t−∆.\nNext, we use the update operator Uθ(xt)def=Eq. (1) if xtis continuous or Eq. (3) if categorical.\nWe first establish a convergence criterion on the proximity to the optimum, showing that as\ndiffusion progresses, the projected updates remain close to the highest-likelihood regions of the\ndata distribution while respecting constraints.\nTheorem 1 (Convergence Proximity) Iflogpdata(x0)is convex, then there exists a minimum iter-\nation ¯tsuch that, for all t≤¯t, the following inequality holds: ∥Uθ(xt)−Φ∥2≤ ∥ρ−Φ∥2,where\nρis the closest point to Φ, the global optimum of logpdata(x0), which can be reached via a single\ngradient step from any point in C.\nNext, we show that incorporating projections systematically reduces the cost of enforcing feasibility,\nmaking the projection steps increasingly efficient as sampling progresses.\nTheorem 2 (Error Reduction via Projection) LetPCbe the projection operator onto C. For all\nt≤¯t, as defined by Theorem 1. Then,\nE[Error (Uθ(xt),C)]≥E[Error (Uθ(PC(xt)),C)],\nwhere Error (·,C)quantifies the cost of projection.\nIn essence, performing an update starting from a projected xtyields a sample xt−∆that is, in\nexpectation, closer to the feasible set than an update without projection. A direct consequence is:\nCorollary 3 (Convergence to Feasibility) For any arbitrarily small ξ >0, there exists a time t\nsuch that after the update\nError\u0000\nUθ(PC(xt)),C\u0001\n≤ξ.\nThis result leverages the fact that the step size γtstrictly decreases as tdecreases, and thus, both\nthe gradient magnitude and noise diminish. Consequently, the projection error approaches zero,\nimplying that the updates steer the sample toward the feasible subdistribution of pdata(x0).\nTogether, Theorem 2 and Corollary 3 explain why integrating the projection steps into the re-\nverse update xt→xt−∆produces samples that conform closely to the imposed constraints.\nFeasibility Guarantees. For an arbitrary density function, NSD provides feasibility guarantees\nfor convex constraint sets. This assurance is critical in applications of interest to this paper, including\nthe material design explored in Section 7.3 and physics-based simulations (Appendix E.4).\n7\n--- Page 8 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nO=CC1CC23CC2C3O1Toxic𝜙!: Prevent ‘Aldehyde Flag’\nOCC1CC23CC2C3O1Non-Toxic\nCC12C3OC(=O)CC1C32Not Novel𝜙\": All valid molecules are novelCC12C3CC(=O)CC1C32Novel \n❌ \n✅\n✅\n❌ Molecule GenerationModel NovelNovel &\nNon-ToxicViol (%)\nϕ1:novelty ϕ2:6:non-toxic\nAR 10.3±2.3 5 .3±1.4 99.0±0.2 40 .2±10.9\nMDLM 260.7±16.4 108 .0±9.753.9±3.1 35 .3±0.5\nUDLM 279.7±22.7 132 .3±3.770.8±2.4 38 .1±3.3\nNSD BRENK 451.7±19.5 392 .0±16.751.2±2.0 0.0±0.0\nNSD BRENK + Novel 533.3±8.7 474 .3±5.71.4±0.3 0.0±0.0\nFigure 2: Results for Molecule Generation experiments constrained to be novel and non-toxic. On the left,\nwe provide examples of the projection operators; on the right, the table outlines specific results.\n7. Experiments and Evaluation\nThe evaluation of the symbolic diffusion approach focuses on three primary tasks designed to stress-\ntest compliance with challenging constraints, with focus on safety ,data scarcity robustness , and\nout-of-domain generalization . In all cases, we compare our method (NSD) against state-of-the-art\nbaseline diffusion models and relevant ablations, as assessed by domain-specific qualitative met-\nrics (i.e., path length for motion planning and FID scores for image generation) and frequency of\nconstraint violations. Complimenting any domain specific baselines, the evaluation also includes,\nwhere applicable, a conditional diffusion model (Cond) , where constraints are applied as condi-\ntioning variables of the models and a post-hoc correction (Post+)approach (projecting the final\noutput only) to illustrate the importance of integrating constraints during sampling. We use iden-\ntical neural network architectures and training procedures for the diffusion model across methods;\nthus, differences in performance can be attributed to constraint implementation rather than model\ncapabilities. Due to space constraints, we fully elaborate all domain specifications in Appendix E.\nTo demonstrate the broad applicability of NSD, the experimental settings showcase its capability in:\n1.Enabling safe, non-toxic molecular generation and out-of-domain discovery (§7.1).\n2.Handling safety-critical settings and highly non-convex constraints for motion planning (§7.2).\n3.Facilitating microstructure design in data scarce settings for out-of-domain discovery (§7.3).\nIn addition, we test the ability of NSD to generate ODE-governed videos for out-of-distribution\ntasks (Appendix E.4), to prevent harmful text generation for natural language modeling (Appendix\nE.5), and to constrain supplementary out-of-domain molecule generation properties (Appendix E.1).\n7.1. Molecule Generation for Drug Discovery (Safety and Domain Generalization)\nIn drug discovery, ensuring the chemical safety and quality of output molecules is critical. This\nexperiment generates molecules in SMILES format (Weininger, 1988) using a uniform discrete dif-\nfusion model finetuned on the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014). For\nthis task, NSD is compared with MDLM (Sahoo et al., 2024) and UDLM (Schiff et al., 2024), the\ncurrent state-of-the-art discrete diffusion models, and an autoregressive (AR) baseline with identical\narchitecture and size to our diffusion model backbone.\nThe experiment enforces two key constraints: a novelty constraint ( ϕ1) that ensures generated\nmolecules do not appear in the training set, and five BRENK substructure filters ( ϕ2:6) that identify\nundesirable molecular fragments (e.g., aldehydes, three-membered heterocycles) linked to toxicity\nand the absence of drug-like characteristics. Critically, molecules flagged by BRENK often exhibit\ntoxicity, reactivity, or other liabilities making them unsuitable for drug discovery (Brenk et al.,\n2008). Thus, for this study, we define ‘non-toxic’ molecules as those passing all five of the chosen\nBRENK filters (Appendix E.1). An illustration of the NSD correction mechanism employed to\nproject the generated molecules is presented in Figure 2 (left).\n8\n--- Page 9 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\n0.00.51.0Success Rate0.79 0.88 0.94 0.90 N/A N/A 0.93 1.06 N/A N/A 0.93 1.08Shelf\nCond MPD MMD NSD0.00.51.0Success RateN/A 1.18 1.27 1.19 N/A N/A 1.27 1.30 N/A N/A N/A 1.34Room3 Robots 6 Robots 9 Robots\n(a) Shelf Maps.\n (b) Room Maps.\nFigure 3: Evaluation on practical maps with three different numbers of robots. On the left, we assess failure\nrates (depicted by the gray regions of the bars) and average path length (values on top of the bars). On the\nright, we provide visualizations of the practical maps tested on.\nTogether, these constraints serve two purposes: (1) Out-of-Distribution Generation: the nov-\nelty constraint promotes out-of-distribution generation , which is essential for discovering new chem-\nical compounds, and (2) Safety-Critical Outputs: the BRENK filters ensure the sampled molecules\nare safe , thereby improving their likelihood of success in downstream drug-development pipelines.\nFigure 2 (right) reports the number of novel and non-toxic molecules generated, along with\nconstraint violations (expressed as the percentage of generations that do not conform to the imposed\nrequirements). While the diffuison baselines report substantial improvements with respect to the AR\nmodel, they frequently violate constraints. In contrast, NSD achieves perfect adherence to safety\nconstraints , while also increasing the frequency of molecule generations that are novel, valid, and\nnon-toxic by over 3.5 ×, a remarkable improvement over the current state-of-the-art. Additionally,\nas detailed in Appendix E.1, we provide an evaluation of settings where NSD generations comply\nwith strict thresholds on synthetic accessibility, the ease with which the generated molecules can be\nsynthesized, further improving the practical utility.\n7.2. Motion Planning for Autonomous Agents (Safety)\nNext, we examine how NSD enforces collision avoidance in autonomous multi-agents settings using\nacontinuous diffusion model. Two main challenges arise: (1) Safety-Critical Outputs: In real-\nworld deployments, robots must avoid restricted or hazardous areas to ensure safe navigation in\ncluttered or dynamic environments. (2) Highly Non-Convex Constraints: Furthermore, the under-\nlying problem is characterized by a large number of non-convex and temporal constraints, rendering\nthe problem extremely challenging (more details in Appendix E.2).\nWe compare NSD to a conditional baseline and current state-of-the-art methods for multi-agent\npathfinding: (1) Motion Planning Diffuion (MPD) originally designed for single-robot motion plan-\nning (Carvalho et al., 2023), and adapted here to multi-agent tasks for comparison and (2) Multi-\nrobot Motion Planning (MMD), a recent method that integrates diffusion models with classical\nsearch techniques (Shaoul et al., 2024). Figure 3 (left) highlights the results on practical maps that\nfeature multiple rooms connected by doors or constrained pathways for robot navigation. These sce-\nnarios require robots to not only avoid collisions ( ϕ1) but also coordinate globally to find feasible\nroutes through shared spaces ( ϕ2). NSD sets a new state-of-the-art in feasibility and scalability. In\nFigure 3( a), MPD and MMD achieve 60% success with three robots but degrade significantly with\nmore agents, with MMD dropping to 45% for nine robots. In contrast, NSD maintains high success\nrates: 100% for three robots, 96% for six, and 93% for nine. On room maps (Figure 3( b)), NSD\n9\n--- Page 10 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nachieves perfect success rates for up to six robots and over 95% for nine robots, whereas MPD and\nMMD fail entirely as complexity increases. This trend persists across other environments (see Ap-\npendix E.2). This is remarkable, as NSD can enable effective coordination among multiple robots\nin shared, constrained spaces, ensuring collision-free, kinematically feasible trajectories.\n7.3. Morphometric Property Specification (Data Scarcity and Domain Generalization)\nGround P(%)Generative Methods\nNSD Cond Post+Cond+\n10\n30\n50\nFID: 30.7±6.8 31.7±15.6 41.7 ±12.8 46.4 ±10.7\nViol.>5%: 0.0 94.2 0.0 0.0\nFigure 4: Samples and results from the morphometric\nproperty specification experiments.Finally, this experiment focuses on a mi-\ncrostructure design task. Here, achieving spe-\ncific morphometric properties is crucial for\nexpediting the discovery of structure-property\nlinkages. We consider an inverse-design prob-\nlem in which a target porosity percentage, de-\nnoted by P(%), is desired. Here, the porosity is\na measure of the percentage of ‘damaged’ pix-\nels in the microstructure. This setting provides\ntwo particular challenges: (1) Data Scarcity:\nA key consideration in this context is the ex-\npense of generating training data, making data\naugmentation an important application of this\nproblem. Obtaining real material microstruc-\nture images is expensive and time-consuming, with limited control over attributes such as porosity,\ncrystal sizes, and volume fraction, often requiring a trial-and-error approach. Provided these costs,\nour training data regime is very low ; we subsample a single 3,000 ×3,000 pixel image to compose\nthe dataset. (2) Out-of-Distribution Constraints Given the low amounts of data available, often\nthedesired porosity levels are unobserved in the training set . Recall that NSD guarantees strict\nadherence to specified porosity constraints . Figure 4 illustrates the effectiveness of our method\nin generating microstructures with precise porosity levels as attempted by prior works employing\nconditional models (Chun et al., 2020). This demonstrates that our approach not only provides\nthe highest fidelity to the training distribution but also outperforms baselines in producing valid\nmicrostructures as assessed by domain-specific heuristic metrics (Figure 6, bottom and Appendix\nE.3).\n8. Conclusion\nThis paper presented a novel framework that integrates symbolic optimization into the diffusion pro-\ncess, ensuring generative models can meet stringent physical, structural, or operational constraints.\nBy enforcing these constraints continuously, rather than relying on post-processing approaches or\nsoft guidance schemes, the proposed neuro-symbolic generative approach shows a unique ability\nto handle safety-critical tasks, cope with limited or skewed data, and generalize to settings be-\nyond the original training distribution. Empirical evaluations across domains including toxic com-\npound avoidance, motion planning, and inverse-design for material science illustrate this ability\nand provide a new state-of-the-art in utility and constraint adherence. As evidenced by this work,\nthe capability to embed and process diverse symbolic knowledge and functional constraints into\ndiffusion-based models paves the way for more trustworthy and reliable applications of generative\nAI in scientific, engineering, and industrial contexts.\n10\n--- Page 11 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nAcknowledgments\nThis research is partially supported by NSF grant RI-2334936 and NSF CAREER Award 2401285.\nThe authors acknowledge Research Computing at the University of Virginia for providing compu-\ntational resources that have contributed to the results reported within this paper. The views and\nconclusions of this work are those of the authors only.\nReferences\nKareem Ahmed, Eric Wang, Kai-Wei Chang, and Guy Van den Broeck. Neuro-symbolic entropy\nregularization. In Uncertainty in Artificial Intelligence , pages 43–53. PMLR, 2022.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023.\nStephen Boyd. Convex optimization. Cambridge UP , 2004.\nRuth Brenk, Alessandro Schipani, Daniel James, Agata Krasowski, Ian Hugh Gilbert, Julie Frear-\nson, and Paul Graham Wyatt. Lessons learnt from assembling screening libraries for drug discov-\nery for neglected diseases. ChemMedChem: Chemistry Enabling Drug Discovery , 3(3):435–444,\n2008.\nKrysia B Broda, A d’Avila Garcez, and D Gabbay. Neural-symbolic learning system: foundations\nand applications, 2002.\nJoao Carvalho, An T Le, Mark Baierl, Dorothea Koert, and Jan Peters. Motion planning diffusion:\nLearning and planning of robot motions with diffusion models. In 2023 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) , pages 1916–1923. IEEE, 2023.\nJacob K Christopher, Stephen Baek, and Nando Fioretto. Constrained synthesis with projected\ndiffusion models. Advances in Neural Information Processing Systems , 37:89307–89333, 2025.\nSehyun Chun, Sidhartha Roy, Yen Thi Nguyen, Joseph B Choi, HS Udaykumar, and Stephen S\nBaek. Deep learning for synthetic microstructure generation in a materials-by-design framework\nfor heterogeneous energetic materials. Scientific reports , 10(1):13307, 2020.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason\nYosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled\ntext generation. arXiv preprint arXiv:1912.02164 , 2019.\nPeter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like\nmolecules based on molecular complexity and fragment contributions. Journal of cheminfor-\nmatics , 1:1–11, 2009.\nFerdinando Fioretto, Pascal Van Hentenryck, Terrence W. K. Mak, Cuong Tran, Federico Baldo, and\nMichele Lombardi. Lagrangian duality for constrained deep learning. In European Conference\non Machine Learning , volume 12461 of Lecture Notes in Computer Science , pages 118–135.\nSpringer, 2020. doi: 10.1007/978-3-030-67670-4 \\8. URL https://doi.org/10.1007/\n978-3-030-67670-4_8 .\n11\n--- Page 12 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Real-\ntoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint\narXiv:2009.11462 , 2020.\nGiorgio Giannone, Akash Srivastava, Ole Winther, and Faez Ahmed. Aligning optimization trajec-\ntories with diffusion models for constrained design generation. arXiv preprint arXiv:2305.18470 ,\n2023.\nYingqing Guo, Hui Yuan, Yukang Yang, Minshuo Chen, and Mengdi Wang. Gradient guidance for\ndiffusion models: An optimization perspective. arXiv preprint arXiv:2404.14743 , 2024.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598 , 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems , 33:6840–6851, 2020.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144 , 2016.\nGreg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Riccardo Vianello,\nsriniker, Peter Gedeck, Gareth Jones, NadineSchneider, Eisuke Kawashima, Dan Nealschneider,\nAndrew Dalke, Matt Swain, Brian Cole, Samo Turk, Aleksandr Savelev, tadhurst cdd, Alain\nVaucher, Maciej W ´ojcikowski, Ichiru Take, Vincent F. Scalfani, Rachel Walker, Kazuya Uji-\nhara, Daniel Probst, Juuso Lehtivarjo, Hussein Faara, guillaume godin, Axel Pahl, and Jeremy\nMonat. rdkit/rdkit: 2024 095 (q3 2024) release, January 2025. URL https://doi.org/\n10.5281/zenodo.14779836 .\nYixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue\nHuang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on back-\nground, technology, limitations, and opportunities of large vision models, 2024. URL https:\n//arxiv.org/abs/2402.17177 .\nAaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios\nof the data distribution. In Forty-first International Conference on Machine Learning , 2024.\nFranc ¸ois Maz ´e and Faez Ahmed. Diffusion models beat gans on topology optimization. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence (AAAI), Washington, DC , 2023.\nChenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Gener-\nalized score matching for discrete data. Advances in Neural Information Processing Systems , 35:\n34532–34545, 2022.\nSaman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative\nvideo models learn physical principles from watching videos?, 2025. URL https://arxiv.\norg/abs/2501.09038 .\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nInInternational conference on machine learning , pages 8162–8171. PMLR, 2021.\n12\n--- Page 13 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia\nGlaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language mod-\nels.arXiv preprint arXiv:2202.03286 , 2022.\nThomas Power, Rana Soltani-Zarrin, Soshi Iba, and Dmitry Berenson. Sampling constrained tra-\njectories using composable diffusion models. In IROS 2023 Workshop on Differentiable Proba-\nbilistic Robotics: Emerging Perspectives on Robot Learning , 2023.\nRaghunathan Ramakrishnan, Pavlo Dral, Matthias Rupp, and Anatole von Lilienfeld. Quantum\nchemistry structures and properties of 134 kilo molecules. Scientific Data , 1, 08 2014. doi:\n10.1038/sdata.2014.22.\nAlexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, and George J. Pappas.\nJailbreaking llm-controlled robots, 2024. URL https://arxiv.org/abs/2410.13691 .\nR Tyrrell Rockafellar. Convergence of augmented lagrangian methods in extensions beyond non-\nlinear programming. Mathematical Programming , 199(1):375–420, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages 10684–10695, 2022.\nLars Ruddigkeit, Ruud Deursen, Lorenz Blum, and Jean-Louis Reymond. Enumeration of 166\nbillion organic small molecules in the chemical universe database gdb-17. Journal of chemical\ninformation and modeling , 52, 10 2012. doi: 10.1021/ci300415d.\nSubham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T\nChiu, Alexander Rush, and V olodymyr Kuleshov. Simple and effective masked diffusion lan-\nguage models. arXiv preprint arXiv:2406.07524 , 2024.\nYair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre,\nBernardo P de Almeida, Alexander Rush, Thomas Pierrot, and V olodymyr Kuleshov. Simple\nguidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193 , 2024.\nYorai Shaoul, Itamar Mishani, Shivam Vats, Jiaoyang Li, and Maxim Likhachev. Multi-robot motion\nplanning with diffusion models. arXiv preprint arXiv:2410.03072 , 2024.\nIman Sharifi, Mustafa Yildirim, and Saber Fallah. Towards safe autonomous driving policies using a\nneuro-symbolic deep reinforcement learning approach. arXiv preprint arXiv:2307.01316 , 2023.\nYifei Shen, Xinyang Jiang, Yifan Yang, Yezhen Wang, Dongqi Han, and Dongsheng Li. Under-\nstanding and improving training-free loss-based diffusion guidance. Advances in Neural Infor-\nmation Processing Systems , 37:108974–109002, 2025.\nJiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K Titsias. Simplified and gener-\nalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329 , 2024.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribu-\ntion. Advances in neural information processing systems , 32, 2019.\n13\n--- Page 14 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 , 2020.\nVikram V oleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffu-\nsion for prediction, generation, and interpolation. Advances in Neural Information Processing\nSystems , 35:23371–23385, 2022.\nTsun-Hsuan Wang, Juntian Zheng, Pingchuan Ma, Yilun Du, Byungchul Kim, Andrew Spiel-\nberg, Joshua Tenenbaum, Chuang Gan, and Daniela Rus. Diffusebot: Breeding soft robots with\nphysics-augmented generative diffusion models. arXiv preprint arXiv:2311.17053 , 2023.\nJoseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eise-\nnach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of\nprotein structure and function with rfdiffusion. Nature , 620(7976):1089–1100, 2023.\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to method-\nology and encoding rules. Journal of chemical information and computer sciences , 28(1):31–36,\n1988.\nMaximilian Wittmann. Exploring the effect of anthropomorphic design on trust in industrial robots:\nInsights from a metaverse cobot experiment. In 2024 21st International Conference on Ubiqui-\ntous Robots (UR) , pages 118–124. IEEE, 2024.\nPan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynam-\nics based algorithms for nonconvex optimization. Advances in Neural Information Processing\nSystems , 31, 2018.\nHaotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Y\nZou, and Stefano Ermon. Tfg: Unified training-free guidance for diffusion models. Advances in\nNeural Information Processing Systems , 37:22370–22417, 2025.\nYe Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human\nmotion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 16010–16021, 2023.\nKaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked\ndiffusion models are secretly time-agnostic masked models and exploit inaccurate categorical\nsampling. arXiv preprint arXiv:2409.02908 , 2024.\n14\n--- Page 15 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nAppendix A. Extended Related Work\nNeuro-symbolic frameworks. This paper’s novel integration of symbolic constraints with genera-\ntive models builds on foundational work in hybrid AI systems, blending the pattern recognition of\nneural networks with symbolic reasoning’s structured constraints. Early approaches like coopera-\ntive architectures (Broda et al., 2002) established iterative feedback loops between neural and sym-\nbolic components, as seen in autonomous driving systems where visual detectors refine predictions\nvia spatial logic rules (Sharifi et al., 2023). Parallel efforts in compiled architectures embedded\nsymbolic operations directly into neural activations, enabling dynamic constraint enforcement in\ndomains such as finance, where neurons encoded regulatory thresholds into credit scoring models\n(Ahmed et al., 2022).\nTraining-free correction. An alternative approach to enforcing desired properties in diffusion mod-\nels is through training-free correction via gradient-based guidance. Inspired by methods such as\nPlug and Play Language Models (PPLM) (Dathathri et al., 2019), these techniques compute gra-\ndients from an external objective or constraint function at sampling time. Rather than relying on\nadditional classifier training or extensive data labeling, the method directly adjusts the score esti-\nmates during the sampling process. Specifically, a loss function encoding the desired property is\ndefined over the generated sample, and its gradient with respect to the sample is computed. Un-\nlike model conditioning, which augments the score with a fixed conditioning signal, training-free\ncorrection dynamically refines the generation by continuously monitoring and correcting deviations\nfrom the target behavior (Guo et al., 2024; Shen et al., 2025). Such methods provide an alterna-\ntive to existing conditioning approaches, but generally report worse performance than conditioning\nmethods, due to inaccuracies in their gradients when the sample is at higher noise levels (Ye et al.,\n2025).\nAppendix B. Augmented Lagrangian Method\nSince ˜ϕis typically nonlinear and hard to enforce directly, we adopt an augmented Lagrangian\napproach (Boyd, 2004), which embeds the constraint ˜ϕ(y)≈0into a minimization objective with\nmultipliers λand a quadratic penalty µ. LetUθ(xt)be the sample after applying the denoising step\nat time t. We introduce a projected sample ythat we iteratively refine to reduce violations of ˜ϕ\nwhile remaining close to Uθ(xt)under Dcost. The augmented Lagrangian is:\nLALM\u0000\ny, λ, µ\u0001\n=Dcost\u0000\nxt,y\u0001\n+λ˜ϕ(y) +µ\n2˜ϕ(y)2.\nMinimizing LALM yields a lower-bound approximation to the original projection. Its Lagrangian\ndual solves:\narg max\nλ,µ\u0010\narg min\nyLALM\u0000\ny, λ, µ\u0001\u0011\n.\nWe optimize iteratively, updating yvia gradient descent and adjusting λandµas follows\nFioretto et al. (2020):\ny←y−γ∇yLALM\u0000\ny, λ, µ\u0001\n, (7a)\nλ←λ+µ˜ϕ(y), (7b)\nµ←min(αµ, µ max), (7c)\n15\n--- Page 16 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nwhere γis the gradient step size, α > 1increases µover iterations, and µmaxis an upper bound.\nThis drives yto satisfy ˜ϕ(y)≈0while staying close to Uθ(xt). Noting that ˜ϕmay be computed\nusing a surrogate network, this optimization can be further grounded by directly using ϕ1:n(y) = 1\nas the termination condition; hence, assuming strong convergence properties (which are encouraged\nby the inclusion of the quadratic term (Rockafellar, 2023)), the projected sample will strictly satisfy\nthe symbolic constraints as assessed by the reasoning test.\nAppendix C. Discrete Sequence Relaxations\nAn important challenge to imposing gradient-based projections on discrete data sequences is provid-\ning a differentiable relaxation of our constraint satisfaction metric. This arises because we impose\nthe constraint over the decoded version of the probability distributions, which is inherently discrete,\nmaking it not naturally differentiable. This poses a significant obstacle when one needs to backprop-\nagate errors through operations that select discrete tokens or decisions. To overcome this limitation,\nwe leverage a Gumbel-Softmax relaxation of the arg max operator, denoted as ψ, which effectively\nbridges the gap between discrete and continuous representations.\nMore specifically, given a probability vector of token i,xi\nt, where each component xi\nt(v)rep-\nresents the probability assigned to token vfrom a vocabulary of size V, we approximate the hard,\ndiscrete decision of the arg max function by constructing a continuous, differentiable approxima-\ntion:\nψ(xi\nt)(v) =exp\u0010\nlogxi\nt(v)+gv\nTsample\u0011\nPV\nv′=1exp\u0010logxi\nt(v′)+gv′\nTsample\u0011.\nHere, gvis a random variable drawn independently from a Gumbel (0,1)distribution for each token\nv. The introduction of the Gumbel noise gvperturbs the log-probabilities, thereby mimicking the\nstochasticity inherent in the discrete sampling process. The parameter T sample >0serves as a\ntemperature parameter that governs the degree of smoothness of the resulting distribution. Lower\ntemperatures make the approximation sharper and more similar to the original arg max operator,\nwhile higher temperatures yield a smoother distribution that is more amenable to gradient-based\noptimization.\nThis relaxation not only facilitates the propagation of gradients through the projection step but\nalso maintains a close approximation to the original discrete decision process. By incorporating the\nGumbel-Softmax technique, we can integrate the arg max operation into our model in a way that is\ncompatible with gradient descent, ultimately enabling the end-to-end training of models that require\ndiscrete token decisions without sacrificing the benefits of differentiability (Jang et al., 2016).\nAppendix D. Score Matching for Discrete Diffusion\nRecall that in Section 2 we introduce the Euler discretized update step for Langevin dynamics:\nxt−∆=xt+γt∇xtlogp(xt) +p\n2γtϵ\nThis directly allows us to formulate the objective of the reverse process from the given update rule,\nas shown in Equation (4). This representation of the diffusion sampling procedure is fundamental\nto our theoretical analysis. While our discussion focuses on continuous diffusion models, as noted\nearlier, the framework can be naturally extended to discrete diffusion models as well.\n16\n--- Page 17 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nParticularly, we highlight that Langevin dynamics sampling algorithms used by continuous\nscore-based diffusion models, whether applied directly (Song and Ermon, 2019) or through predictor-\ncorrector frameworks (Song et al., 2020), can be utilized by discrete diffusion models. Notably,\nscore-based discrete diffusion models leverage a discrete generalization of the score function, re-\nferred to as the Concrete score (Meng et al., 2022), to approximate the gradient of the probability\ndensity function logpt(xt). As opposed to continuous score-based diffusion, where the gradient is\ndirectly applied to the representation of the sample (e.g., for image data the gradient will directly\nchange pixel values), discrete models apply this gradient to the probability distributions which are\nsampled from to predict the final, discrete sequence. Despite this discrepancy, Concrete Score\nMatching provides an approach which mirrors continuous Score Matching in that the estimated gra-\ndients of the probability density function are used to guide the sample to high density regions of the\ntarget distribution.\nAs a final note, while many works do not explicitly adopt Concrete Score Matching as done by\nprevious literature (Meng et al., 2022; Lou et al., 2024), the score function is often still implicitly\nmodeled by the denoiser. For example, Sahoo et al. (2024) provide theoretical results demonstrating\nequivalence to a score-based modeling, supporting the extrapolation of our theoretical framework\nto models which employ simplified derivations of the Concrete Score Matching training objective.\nAppendix E. Extended Experimental Results\nE.1. Molecule Generation for Drug Discovery (Safety and Domain Generalization)\nIn this section further explain the setting for constrained molecular generation. We use UDLM\n(Schiff et al., 2024) as our underlying diffusion model architecture for NSD for this application.\nThe task is to generate representations of molecule structures using SMILES sequences (Weininger,\n1988), human readable strings that can be directly mapped to molecule compounds. We begin\nwith an overview of the domain-specific benchmarks used in our evaluation. Then, we provide an\nextended version of Figure 2 (right), where we detail the violations for each specific BRENK test\nthat is corrected by our projection operator. We then discuss the violations, their corresponding\nsymbolics tests, and projection operators in detail. Finally, we introduce and explain the setting and\nresults for constraining the synthetic accessibility of the molecules generated.\nAdditional benchmarks. To supplement our evaluation, we compare to several domain specific\napproaches:\n1.Autoregressive LLM (AR) : An autoregressive transformer-based model, trained for molecule\ngenerations and sized to be comparable with the other diffusion-based architectures (100M pa-\nrameters).\n2.Conditional Masked Diffusion Model (MDLM) : A conditional masked discrete diffusion model\nimplementation from Schiff et al. (2024) with guidance schemes in the subscript if applicable.\n3.Conditional Uniform Diffusion Model (UDLM) : A conditional uniform discrete diffusion\nmodel implementation from Schiff et al. (2024) with guidance schemes in the subscript if ap-\nplicable.\nSymbolic test. For the purpose of generating safer, higher quality, and novel molecules we im-\nplement a total of six symbolic tests each corresponding with its own correction; in practice, our\nprojection operator composes these corrections to find the nearest feasible point at the intersection\nof these constraints, x∈C.\n17\n--- Page 18 ---\nCHRISTOPHER CARDEI LIANG FIORETTOMolecule GenerationModelNovel &\nNon-ToxicViol (%)\nϕ1:novelty ϕ2:Aldehydes ϕ3:Three-membered heterocycles ϕ4:Imines ϕ5:Triple bonds ϕ6:Isolated alkene\nAR 5.3±1.4 99.0±0.2 20.2±6.2 9.3±7.6 21.2±13.9 10.9±2.6 2.6±4.1\nMDLM 108.0±9.753.9±3.1 9.5±1.4 22.2±2.1 11.0±1.9 9.0±1.5 10.3±0.9\nUDLM 132.3±3.770.8±2.4 11.3±1.4 16.9±2.1 10.9±2.6 8.0±2.9 11.1±1.6\nNSD BRENK 392.0±16.751.2±2.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0\nNSD Novel + BRENK 474.3±5.7 1.4±0.3 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0\nTable 1: Extended results from Figure 2 (right).\n1.Novelty ϕ1: As defined in (Schiff et al., 2024), a generate molecule is considered valid if it can be\nparsed by the RDKit library (Landrum et al., 2025), and a molecule is novel if its valid, unique,\nand not present in the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014). The test\nfunction ϕ1determines whether the current discretized molecular representation x⋆is within the\ntraining set D, thus ϕ1def=x⋆/∈ D. Ifϕ1is not satisfied, the corresponding projection operation\nuses a best-first search to systematically flip tokens in a sequence, based on minimal probability\n“flip costs” to generate new sequences not yet in the dataset. Once a novel sequence is found, it is\nfinalized, added to the dataset, and the model’s token probabilities are adjusted to maintain high\nlikelihood for the newly generated sequence. Specifically, we seek a novel sequence x⋆/∈ D by\nminimally altering the top-ranked tokens and flip cost denoted as Dcost. We find\nPC(x)def= argmin\ny⋆/∈DDcost(y,x),\nvia a best-first search. Once a novel sequence is found, it is added to D, and the distribution is\nupdated so that xbecomes the new top-ranked path, avoiding duplicates in future generations.\n2.Substructure Violations ϕ2:6: In order to generate safer and less toxic molecules we use the\nblackbox BRENK filter (Brenk et al., 2008) provided by RDKit (Landrum et al., 2025) which\noffers various violation alerts which lead to a BRENK flag. While there are many of these poten-\ntial substructure violations, we cover the five most frequent. For these violation we use RDKit\nto identify and flag these substructures. Thus, we can define B={x⋆|BRENK Flag(x⋆) =\nTrue},where Bis the set of molecules that the BRENK filter flags. Now, we can define the tests\nas:ϕ2:6def=x⋆/∈Bwith each specific ϕidescribed below.\n(a)Aldehydes ϕ2: Aldehydes feature a carbonyl group ( C=O) in which the carbon is also\nbonded to at least one hydrogen (i.e., R--CHO ). In SMILES notation, this typically ap-\npears as C=O where the carbon atom carries a hydrogen. In drug discovery, aldehydes are\nconsidered undesirable owing to their high reactivity and potential toxicity.\nTo address flagged aldehydes, our method proceeds as follows:\ni.Transform : Identify the aldehyde ( C=O with a hydrogen on the carbon) and attempt to\nconvert it into either an alcohol ( R--CH 2--OH ) or a methyl ketone ( R--C(=O)CH 3).\nii.Fallback : If neither transformation produces a valid molecule, directly modify the car-\nbonyl bond (e.g., force C=O toC--OH ) or remove the oxygen entirely, thus eliminating\nthe problematic functionality.\nThese operations yield a molecular sequence that no longer violates the aldehyde-related\nBRENK filter.\n(b)Three-membered heterocycles ϕ3: These are small ring systems composed of three atoms,\nat least one of which is a heteroatom (e.g., nitrogen, oxygen, etc.). Molecules containing\n18\n--- Page 19 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\nsuch rings are considered undesirable due to their high ring strain, reactivity, and potential\ntoxicity. Typically, they are detected by filters that look for three-member rings containing\na heteroatom.\nTo correct a flagged three-membered heterocycle, we use a stepwise approach:\ni.Ring expansion : Insert a new carbon atom into one of the ring bonds, creating a larger,\nless-strained ring.\nii.Bond breaking : If expansion fails to produce a valid, non-flagged molecule, break one\nof the ring bonds to open the ring.\niii.Complete removal : If neither of the previous steps works, remove all bonds in the\noriginal three-membered ring entirely.\nAfter each step, we check for validation and against the BRENK filter. The result is a\nstructure that no longer violates the “three-membered heterocycle” constraint.\n(c)Imines ϕ4: An imine is a functional group containing a carbon–nitrogen double bond\n(C=N). These groups are often flagged due to potential instability and reactivity.\nThe corresponding operation employs a two-stage procedure:\ni.Initial fix : Convert the double bond into a single bond and add a hydrogen atom to the\nnitrogen.\nii.Fallback fix : If the first approach fails or yields an invalid structure, remove or break\ntheC=N bond entirely so that no imine remains.\n(d)Triple bonds ϕ5: Molecules containing triple bonds (denoted by #in SMILES) can be\nflagged due to concerns about reactivity, metabolic stability, or synthetic difficulty. To ad-\ndress such cases, we apply a simple transformation that replaces the triple bond character\n(#) with either a double bond ( =) or a single bond ( -), thus reducing the likelihood of\nreactivity or instability.\n(e)Isolated alkene ϕ6: Alkenes, represented by ( =) can be flagged if they appear in unde-\nsired or isolated positions that may lead to reactivity or instability issues. To address this,\nwhen flagged, our method replaces the double bond character ( =) with a single bond ( -),\neffectively saturating the alkene. This ensures that the final structure does not violate the\nisolated-alkene BRENK constraint.\nNext, to supplement the evaluations provided in the main text, we provide an additional setting\nwhere we constrain synthetic accessibility (SA) of the generated molecules below a strict threshold.\nFor this setting, we consider that many of the generated molecules, while potentially novel and\nvalid generations, cannot be directly synthesized due to the complexity of these compounds (Ertl\nand Schuffenhauer, 2009). Hence, we impose a constraint on the permissible synthetic accessibility\nof these outputs and compare to a series of conditional models (Table 2). Notably, our model yields\na 0% violation rate, despite generating a competitive number of valid molecules and exhibiting the\nhighest drug-likeness scores (referred to in the table as QED). These results demonstrate how the\ninclusion of constraint projection operators ensure that generated molecules not only scores well\nin property optimization but also adhere to synthetic feasibility requirements as determined by an\nindependent, external standard.\n19\n--- Page 20 ---\nCHRISTOPHER CARDEI LIANG FIORETTOMolecules (Synthetic Accessibility)ModelValid Novel QED Viol (%)\n[↑] [ ↑] [ ↑]τ= 3.0τ= 3.5τ= 4.0τ= 4.5\nAR 1023 11 0.46 91.6 78.4 62.3 42.0\nAR FUDGE γ=7 925 13 0.48 11.1 9.8 9.6 9.2\nMDLM 596 271 0.45 85.9 73.7 61.1 44.0\nMDLM (D-CFG: γ= 3) 772 53 0.41 87.8 73.9 54.2 22.5\nMDLM (D-CBG: γ= 3) 436 21 0.37 50.5 48.6 46.1 44.7\nUDLM 895 345 0.47 89.4 88.0 58.1 37.8\nUDLM (D-CFG: γ= 5) 850 69 0.47 80.6 58.6 35.9 13.9\nUDLM (D-CBG: γ= 10 )896 374 0.47 90.1 77.8 58.6 37.7\nNSD τ=3.0 353 36 0.63 0.0 0.0 0.0 0.0\nNSD τ=3.5 863 91 0.62 - 0.0 0.0 0.0\nNSD τ=4.0 936 108 0.61 - - 0.0 0.0\nNSD τ=4.5 938 121 0.58 - - - 0.0\nTable 2: Molecule generation constrained to strict synthetic accessibility thresholds.\nE.2. Motion Planning for Autonomous Agents (Safety)\nFor this task, we begin by assigning start and goal states for a series of agents in each respective\nenvironment. For practical maps (Figure 3), these positions fall in predefined zones that reflect real-\nworld constraints, such as designated pickup and drop-off locations in a warehouse. For random\nmaps (Figure 5), we assign these start and goal state randomly, constraining these to be collision-free\nlocations, ensuring feasible solutions exist. To further ensure this, we discretize the environments\nand apply multi-agent pathfinding algorithms to verify the existence of collision-free solutions. If\nno valid assignment can be found, we regenerate the configuration.\nOur results are evaluated by success rate (bars included in the figures), the frequency at which\nstate-of-the-art methods find feasible solutions, and path length (at the top of the bars), a metric\nfor the optimality of the solutions. Experiments are conducted with three, six, and nine robots,\ngenerating ten test cases per configuration. In addition to evaluation on practical map environments\nillustrated in Figure 3, we provide additional evaluation on random maps in Figure 5. Again, we\nsee that NSD dramatically outperforms the baselines in its ability to generate feasible motion tra-\njectories. This is particularly exasperated as we scale the number of agents and obstacles. Of the\nbaselines, only MMD is able to ever provide feasible solutions for nine agents, although we note\nthat it has much more frequent constraint violations than NSD on non-empty maps.\nAdditional benchmarks. To supplement our evaluation, we compare to several domain specific\napproaches:\n1.Conditional Diffusion Model (Cond) : A matching diffusion model implementation to NCS,\nfine-tuned on benchmark trajectories to address autonomous motion planning problems (Nichol\nand Dhariwal, 2021).\n2.Motion Planning Diffusion (MPD) : The previous state-of-the-art for single-robot motion plan-\nning (Carvalho et al., 2023), this approach is extended to handle multi-agent settings for com-\nparative analysis.\n3.Multi-robot Motion Planning Diffusion (MMD) : A recently proposed method that integrates\ndiffusion models with classical search techniques, generating collision-constrained multi-agent\npath planning solutions (Shaoul et al., 2024).\n20\n--- Page 21 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\n0.00.51.0Success Rate1.00 1.02 1.11 1.01 N/A 2.59 1.11 1.18 N/A N/A 1.12 1.19Empty\n0.00.51.0Success Rate0.99 1.03 1.13 1.01 N/A N/A 1.13 1.16 N/A N/A 1.14 1.22Basic\nCond MPD MMD NSD0.00.51.0Success RateN/A 1.06 1.16 1.03 N/A N/A 1.17 1.18 N/A N/A 1.18 1.36Dense3 Robots 6 Robots 9 Robots\n(a) Basic Maps.\n (b) Dense Maps.\nFigure 5: Evaluation on random maps with three different numbers of robots. Gray bars represents the failure\nrate, and values on top of the bars indicate average path length per robot.\nSymbolic test. We model two types of collision constraint tests. The first, ϕ1corresponds to\ncollisions between agents, whereas ϕ2captures collisions between agents and obstacles in the map.\nWe can express the collision-avoidance constraints as follows. First, for inter-agent separation, we\nrequire that for every pair of distinct agents iandi′and at every time step j, their positions are\nseparated by at least a minimum distance dmin(which is defined as the sum of their radii):\nϕ1(x)def=∀i, i′(i̸=i′),∀j∥pj\ni−pj\ni′∥2≥dmin.\nSecond, to ensure agents do not collide with obstacles, we require that for each agent iat each time\nstepjand for every obstacle kwith radius rk, the agent’s position is at least rkaway from the\nobstacle’s center ok:\nϕ2(x)def=∀i,∀j,∀k∥pj\ni−ok∥2≥rk.\nHere, iandi′index the agents, jdenotes the time steps, pj\niis the position of agent iat time j, and\nokis the position of obstacle k. As these constraints can be expressed in closed-form, we model\n∆ϕdirectly from this, employing the augmented Lagrangian method to solve this projection (due\nto the highly non-convex nature of these constraints).\nE.3. Morphometric Property Specification (Data Scarcity and Domain Generalization)\nAs mentioned in the main text, our dataset is generated by subsampling a single 3,000 ×3,000 pixel\nimage into patches of size 64 ×64. We then upscale these patches to 256 ×256 images to increase\nthe resolution for our generation task. The data is obtained from Chun et al. (2020), and we reiterate\nit contains only a small range of porosity values fall within the desired porosity ranges (e.g., at the\nporosity level P(%) 50, only 7% of the training data falls within a generous five percent error margin\nto either side), contributing to the challenging nature of this setting.\nIn the analysis of both natural and synthetic materials, heuristic metrics are commonly used\nto quantify microstructure features such as crystal shapes and void distributions. These measures\nprovide qualitative insights into the fidelity of the synthetic samples relative to the training data.\nHere, we present the distributions of three microstructure descriptors, following the approach of\nChun et al.\n21\n--- Page 22 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nThe results demonstrate that the explicit constraint enforcement in NSD yields microstructures\nthat more faithfully replicate the ground truth. In contrast, the conditional model tends to produce\ncertain features at frequencies that do not align with the training distribution. By integrating porosity\nand related constraints during the sampling process, NSD is able to generate a set of microstructures\nthat is both more representative and accurate.\n0.00 0.25 0.50 0.75 1.00 1.25\nDvoid(m)\n01020304050\n0 10 20 30 40 50\nAR0.00.20.40.60.81.0\n0 20 40 60 80\n(°)\n0.000.050.100.150.20\nNSD\nCond\nGround\nFigure 6: Morphometric parameter distributions comparing ground truth microstructures with those generated\nby the NSD andCond models, evaluated using heuristic analysis.\nAdditional benchmarks. To supplement our evaluation, we compare to several domain specific\napproaches:\n1.Conditional Diffusion Model (Cond) : A conditional diffusion model implementation modeled\nfrom Chun et al. (2020).\n2.Post-Processing (Post+): A matching implementation to our diffusion model for NSD, with the\nprojection steps omitted from the sampling process, except after the final step.\n3.Conditional + Post-Processing (Cond+): The Cond model, but with the addition of a post-\nprocessing projection after the final step.\nSymbolic test. We define a test function ϕthat measures the porosity of an image:\nϕ(x)def=\u0000nX\ni=1mX\nj=1\u0000\nxi,j<0\u0001 \u0001\n=K,\nwhere xi,j∈[−1,1]is the pixel value at row iand column j. Our desired constraint is that the\nporosity of the generated image xmust equal a target value K. In our framework, this condition is\nused as a test that triggers the projection: if ϕ(x)̸= 1, a projection operator is applied to minimally\nadjust xso that the constraint is satisfied. This can be constructed using a top-k algorithm to return,\nPC(x) = arg min\nyi,jX\ni,j∥yi,j−xi,j∥\ns.t.yi,j∈[−1,1],nX\ni=1mX\nj=1\u0010\nyi,j<0\u0011\n=K\nwhere Kis the number of pixels that should be “porous”. Because this program is convex, it serves\nas a certificate that the generated images comply with the prescribed porosity constraint.\n22\n--- Page 23 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELS\ntEarth (in distribution) Moon (out of distribution)\nGround NSD Post+Cond+Ground NSD Post+Cond+\n1\n3\n5\nNSD Post+Cond Cond+\nFID: 26.5±1.7 52.5 ±1.0 22.5±0.1 53.0±0.3\nViol. (%): 0.0 0.0 100.0 0.0\n0 5 10 15\nError T olerance (Pixels)020406080100Satisfaction (%)Positional Constraint Satisfaction\nCond (In Distribution)\nCond (Out of Distribution)\nFigure 7: Physics-informed motion experimental results.\nE.4. Physics-informed Motion (Data Scarcity and Domain Generalization)\nFor this setting, we generate a series of video frames depicting an object accelerating due to grav-\nity. Here, the object’s motion is governed by a system of ordinary differential equations (Eq. (9)),\nwhich our method directly integrates into the constraint set ( ϕ). In addition to the complexity\nof the constraints, two key challenges are posed: (1) Data Scarcity: Our training data is based\nsolely on Earth’s gravity, yet our model is tested on gravitational forces from the Moon and other\nplanets, where there are no feasible training samples provided , and, consequentially, (2) Out-of-\nDistribution Constraints: the imposed constraints are not represented in the training.\nFigure 7 (left) highlights the results of our experiments; standard conditional diffusion models\noften produce objects that are misplaced within the frame, as evidenced by white object outlines in\nthe generated samples and the reported constraint violations on the right side of the figure. Post-\nprocessing approaches correct positioning at the cost of significant image degradation. In contrast,\nour method guarantees satisfaction of physical constraints while maintaining high visual fidelity ,\nproducing samples that fully satisfy positional constraints. These results demonstrate that our ap-\nproach generalizes to out-of-distribution physical conditions while ensuring strict compliance with\ngoverning physical laws.\nFor training, we begin by generating a dataset uniformly sampling various object starting points\nwithin the frame size [0,63]. For each data point, six frames are produced, depicting the objects\nmovement as governed by the ODE in Equation (9). The velocity is initialized to v0= 0. The\ndiffusion models are trained on 1000 examples from this dataset, using a 90/10 training and testing\nsplit. The conditional model is implemented following V oleti et al. (2022), where we provide two\nframes illustrating the motion as conditioning. The model then infers future frames from these to\nproduce the final videos.\nAdditional benchmarks. To supplement our evaluation, we compare to several domain specific\napproaches:\n1.Conditional Diffusion Model (Cond) : A conditional diffusion model implementation as out-\nlined by V oleti et al. (2022).\n23\n--- Page 24 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\n2.Post-Processing (Post+): A matching implementation to our diffusion model for NSD, with the\nprojection steps omitted from the sampling process, except after the final step.\n3.Conditional + Post-Processing (Cond+): The Cond model, but with the addition of a post-\nprocessing projection after the final step.\nSymbolic test. We define a test function ϕthat checks whether the object’s position in a frame\nmeets the prescribed positional constraints given by Equations:\npt=pt−1+\u0012\nvt+\u0012\n0.5×∂vt\n∂t\u0013\u0013\n(9a) vt+1=∂pt\n∂t+∂vt\n∂t, (9b)\nHence, we define our test function:\nϕ(x)def=\u0000\nobject position in xequals pt)\nIn other words, the generated frame xis considered feasible if the object’s position exactly matches\nthe target position ptcomputed by the dynamics model.\nWhen ϕ(x)̸= 1, a projection operator is triggered to enforce the positional constraint. This\nprojection proceeds in two steps. First, the object’s current location is detected and its pixels are\nset to the maximum intensity (i.e., white), effectively removing the object from its original position\nwhile storing the indices of the object’s structure. Second, the object is repositioned by mapping\nits stored pixel indices onto the center point corresponding to pt. If the frame already satisfies\nthe positional constraint (i.e., ϕ(x) = 1 ), the projection leaves the image unchanged. Since this\nprojection process is well-defined and convex, it provides a certificate that the generated frames\ncomply with the prescribed positional constraints.\nE.5. Safe Text Generation (Safety)\nAs language models become widely adopted for commercial and scientific applications, it is neces-\nsary that generated text adheres to safety constraints, preventing the production of harmful or toxic\ncontent. Hence, it is essential to provide (1) Safety-Critical Outputs: the adoption of constraint-\naware methods is essential for these applications, especially considering recent examples of toxic\noutputs encouraging self-harm or providing information which could be used to harm others (Perez\net al., 2022).\nTable 3 highlights the results of our experiments, which evaluate language models on toxicity\nmitigation using prompts from the RealToxicityPrompts dataset (Gehman et al., 2020). Our method\nsignificantly improves control over generated content by enforcing strict toxicity constraints during\ninference. Compared to baseline models, such as GPT-2 and Llama 3.2, which exhibit high violation\nrates, NSD achieves perfect constraint satisfaction across all toxicity thresholds . Furthermore, our\napproach scales effectively, ensuring robust toxicity mitigation even at increasingly strict thresholds\n(denoted as τ).\nAmong the baselines, GPT-2 (124M) and LLaMA (1B) achieve the lowest perplexity scores;\nhowever, they frequently generate toxic content, leading to high violation rates. While GPT-2 +\nPPLM (345M) Dathathri et al. (2019) reduces violations across all toxicity thresholds, it fails to\nconsistently prevent toxic generations and suffers from increased perplexity. MDLM (110M) ex-\nhibits higher perplexity than GPT-2, with a median PPL of 39.8, and although it moderately reduces\ntoxicity violations compared to GPT-2, the rates remain significant. In contrast, NSD achieves\nperfect constraint satisfaction across all toxicity thresholds while maintaining sentence fluency.\n24\n--- Page 25 ---\nNEURO -SYMBOLIC GENERATIVE DIFFUSION MODELSSentence ToxicityModel SizePPL Viol (%)\nMean Median τ= 0.25τ= 0.50τ= 0.75\nGPT2 124M 19.1 17.6 36.3 23.8 16.2\nGPT2 PPLM 345M 47.6 37.2 15.2 8.1 4.3\nGPT2 FUDGE λ=2 124M 26.46 18.79 31.5 19.7 12.7\nGPT2 FUDGE λ=9 124M 81.84 19.22 30.6 19.6 11.7\nLlama 3.2 1B 15.7 14.6 34.9 27.8 23.1\nMDLM 110M 46.7 39.8 32.1 23.2 17.2\nNSD τ=0.25(Ours) 110M 61.6 45.4 0.0 0.0 0.0\nNSD τ=0.50(Ours) 110M 59.4 44.2 – 0.0 0.0\nNSD τ=0.75(Ours) 110M 54.9 43.2 – – 0.0\nTable 3: Results for safe text generation at various toxicity levels τ.\nAdditional benchmarks. To supplement our evaluation, we compare to several domain specific\napproaches:\n1.GPT2 : Our model uses a GPT2 tokenizer and is roughly the same size as GPT2, so we add this\nas a point of comparison.\n2.Llama 3.2 : For comparison to state-of-the-art autoregressive models, we employ Llama 3.2,\nnoting that this model is an order of magnitude larger than our diffusion model.\n3.Plug and Play Language Model (GPT2 PPLM ): We utilize gradient-based guidance as proposed\nby Dathathri et al. (2019) to condition autoregressive generation against producing toxic outputs.\n4.Masked Diffusion Model (MDLM) : A masked discrete language diffusion model implementa-\ntion from Schiff et al. (2024).\nSymbolic test. Asϕcannot be explicitly modeled for general text toxicity quantification, we\ntrain a surrogate model to provide a differentiable scoring metric ∆ϕ. Hence, the constraint is\nassessed with respect to this learned metric, such that: ∆ϕ(x⋆)≤τ,where τis a tunable threshold\nthat controls the degree of toxicity that’s permissible (lower values resulting in less toxic output\nsequences). As the surrogate model for toxicity task, we use a GPT-Neo (1.3B) model, adapted for\nbinary classification. We finetune this model on the Jigsaw toxicity dataset which includes multiple\ntoxicity-related labels such as toxic, severe toxic, insult, etc. We consolidates these columns into a\nsingle binary target (toxic vs. non-toxic).\nAppendix F. Missing Proofs\nProof (Theorem 1) By optimization theory of convergence in a convex setting, provided an arbitrary\nnumber of update steps t,xtwill reach the global minimum. Hence, this justifies the existence of ¯t\nas at some iteration as T− → ∞ ,\n∥Uθ(xt)−Φ∥2≤ ∥ρ−Φ∥2\nwhich will hold for every iteration thereafter.\nProof (Theorem 2) For any update taken after convergence, consider a gradient update without the\nstochastic noise. There are two cases:\n25\n--- Page 26 ---\nCHRISTOPHER CARDEI LIANG FIORETTO\nCase 1: Suppose that Uθ(xt)is closer to the optimum than ρ. By the definition of ρ, this implies\nthatxtis infeasible. Moreover, a gradient step taken from an infeasible point will yield an update\nthat is closer to the optimum than any point achievable from the feasible set. Hence, we obtain:\nError\u0010\nUθ(xt)\u0011\n>Error\u0010\nPC\u0000\nUθ(xt)\u0001\u0011\n. (10)\nCase 2: Suppose instead that Uθ(xt)is equally close to the optimum as ρ. In this situation, either\n(1)xtis already the closest feasible point to the optimum (i.e., xt=PC(xt)), so that the error\nterms are equal, or (2) xtis infeasible. In the latter case, the gradient step from xtis equivalent in\nmagnitude to that from the nearest feasible point, but, by convexity, the triangle inequality ensures\nthat the error from starting at an infeasible point exceeds that from starting at the feasible projection.\nThus, Equation (10) holds in all cases. Finally, when the stochastic noise (sampled from a zero-mean\nGaussian) is incorporated, taking the expectation over the update yields:\nEh\nError\u0010\nUθ(xt)\u0011i\n≥Eh\nError\u0010\nPC\u0000\nUθ(xt)\u0001\u0011i\n.\n26",
  "text_length": 81455
}