{
  "id": "http://arxiv.org/abs/2505.24785v2",
  "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
  "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
  "authors": [
    "Patrick Tser Jern Kon",
    "Jiachen Liu",
    "Xinyi Zhu",
    "Qiuyi Ding",
    "Jingjia Peng",
    "Jiarong Xing",
    "Yibo Huang",
    "Yiming Qiu",
    "Jayanth Srinivasa",
    "Myungjin Lee",
    "Mosharaf Chowdhury",
    "Matei Zaharia",
    "Ang Chen"
  ],
  "published": "2025-05-30T16:46:29Z",
  "updated": "2025-06-02T01:59:50Z",
  "categories": [
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24785v2",
  "full_text": "arXiv:2505.24785v2 [cs.AI] 2 Jun 2025EXP-Bench: Can AI Conduct AI Research Experiments? Patrick Tser Jern Kon1,âˆ—, Jiachen Liu1,âˆ—, Xinyi Zhu1, Qiuyi Ding1 Jingjia Peng1, Jiarong Xing2,4, Yibo Huang1, Yiming Qiu1,4, Jayanth Srinivasa3 Myungjin Lee3, Mosharaf Chowdhury1, Matei Zaharia4, Ang Chen1 âˆ—Equal contribution 1University of Michigan2Rice University3Cisco Research4UC Berkeley Abstract Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end- to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and im- plement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading AI agents, such as OpenHands [ 87] and IterativeAgent [ 76] on EXP-Bench demonstrate partial capabilities: while scores on individual experi- mental aspects such as design or implementation correctness reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP- Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/ Just-Curieous/Curie/tree/main/benchmark/exp_bench. 1 Introduction EXP-Bench Dataset Research QuestionHigh-level MethodStarter Code ðŸ’¡Experiment Design Performance Design: 80% Implementation: 50% Code Execution: Failed Conclusion: IncorrectInputEvaluateAI Agent Does parallel convolution improve..Train a model w/ parallel convolution... dataset train.py *Extracted from Top-T ier ML PapersðŸ¤– Dataset Model Parameter Metric Hardware Executeâš™ Experiment Implementation Conclusion Parallel convolution improves... train.py run_exp.sh def forward(self,...): - x = self.conv(x) + x = [conv(x) for conv in self.conv_list] + x = torch.cat(x, dim=1) Code Execution: Failed Conclusion: Incorrect EXP-Bench Dataset Research QuestionHigh-level MethodStarter Code Does parallel convolution improve..Train a model w/ parallel convolution... dataset train.py *Extracted from Top-T ier ML Papers ðŸ’¡ Exp. Design EvaluateAI Agent Dataset Model Parameter Metric Hardware Executeâš™ Exp. Impl. Conclusion Parallel convolution improves... train.py run_exp.sh def forward(self,...): - x = self.conv(x) + x = [conv(x) for conv in self.conv_list] + x = torch.cat(x, dim=1) Input Implementation: 50%Design: 80% Code Execution: Failed Conclusion: Incorrect Perf. EXP-Bench Dataset Research QuestionHigh-level MethodStarter Code Does parallel convolution improve..Train a model w/ parallel convolution... dataset train.py *Extracted from Top-T ier ML Papers ðŸ’¡ Exp. Design EvaluateAI Agent Dataset Model Parameter Metric Hardware Executeâš™ Exp. Impl. Conclusion Parallel convolution improves... train.py run_exp.sh def forward(self,...): - x = self.conv(x) + x = [conv(x) for conv in self.conv_list] + x = torch.cat(x, dim=1) Input Impl.: 50%Design: 80% Code Exec.: Failed Conclusion: Incorrect Perf. Figure 1: EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers. Given a research question, a high-level method description, and starter code, agents are tasked with designing, implementing, and executing complete experiments. Performance is validated through ground-truth comparisons and implementation execution. Automating AI research stands as a cornerstone for accelerating the development of advanced intelligence and human progress. Unlike disciplines that require extensive physical interaction, AI research is inherently digital, rendering it particularly amenable to automation by Large Language Preprint. Under review. Model (LLM)-driven AI agents. Recent work has demonstrated that these agents demonstrate nascent capabilities in tasks like literature synthesis [ 23], hypothesis generation [ 91] and code generation [ 53]. However, empirical AI research requires rigorous end-to-end experimentation, which goes beyond these individual tasks. To realize the vision of agents conducting holistic AI research, a rigorous benchmark is neededâ€”one that evaluates and guides agents through the full experimentation pipeline step by step. We present EXP-Bench, a benchmark designed to comprehensively assess an AI agentâ€™s ability to carry out end-to-end research experiments. As illustrated in Fig 1, EXP-Bench challenges agents with tasks sourced from influential, peer-reviewed AI publications (e.g., NeurIPS, ICLR) along with their open-source implementations. These papers reflect already-completed, peer-validated research and serve as concrete exemplars of full experimental workflows. By exposing agents to such tasks, we test their ability to conduct established scientific procedures grounded in real-world AI experimentation. For each task, an agent is provided with a core research question, a high-level methodological overview, and starter code. The agent should then formulate viable hypotheses, design AI-specific experimental procedures (e.g., data handling, model selection, and hyperparameter optimization), correctly implement and execute these experiments, and derive valid conclusions from the results. However, curating these high-fidelity and structured experimental tasks presents considerable chal- lenges. Academic papers typically present a polished narrative focusing on final results and conclu- sions, often omitting the detailed intermediate steps of the experimentation process. Additionally, critical detailsâ€”such as the precise conditions under which results hold or subtle data preprocessing stepsâ€”are often fragmented across multiple sources, including dense academic papers, supple- mentary materials, and sprawling codebases. This necessitates deep domain expertise for accurate interpretation and makes manual curation of such tasks labor-intensive and difficult to scale. To address these challenges, we develop a semi-automated dataset curation pipeline. We first filter for high-quality AI papers with open-source codebases using citation and repository popularity signals. Task extraction then proceeds in two stages: (1) a multi-modal extraction phase that identifies the core elements of the research problemâ€”such as the main question, expected outcomes, and high-level experimental setup (e.g., datasets, evaluation metrics, model configurations)â€”from papers, supplementary materials, and code; and (2) an implementation extraction phase that locates relevant code and assembles scripts to solve the specified task. We further apply execution-based validation to ensure functionality. While human oversight is used, the availability of original implementations and ground truths reduces the validation burden to mostly lightweight consistency checks. With the pipeline, EXP-Bench currently comprises 461 research tasks (12,737 individually gradable subtasks) derived from 51 papers published at NeurIPS and ICLR 2024, spanning diverse AI subfields such as reinforcement learning, AI applications and generative models. We use a multi-metric evaluation pipeline (Fig 1) to assess agent performance across all core phases of experimentationâ€”design, implementation, execution, and conclusion. Each metric captures a distinct capability, and their conjunctive use ensures that agents correctly understand and complete the experiment. Initial evaluations of leading agents reveal that, while they often succeed at exe- cuting routine proceduresâ€”such as running pre-written scripts or replicating documented analysis stepsâ€”they struggle when tasked with conducting complex experiments. Specifically, we observe failures in: (a) Conceptualizing and operationalizing sound experimental designs from high-level research questions and methods (16.1% misclassified design variables); (b) Translating abstract research methodologies into complete and correct code implementations (39.7% missing essential implementation components); and (c) Ensuring the robust and reproducible execution of complex experimental software stacks (29.4% environment or dependency misconfigurations or 23.8% script- level errors). By identifying these key bottlenecks, EXP-Bench helps us target specific research components for improvement and advance next-generation AI agents for autonomous research. 2 Related work While existing benchmarks have advanced the evaluation of AI agents in various scientific reasoning, coding, and specific machine learning tasks, EXP-Bench distinctively addresses the holistic challenge of end-to-end and step-by-step AI research experimentation. See App. A for additional discussion. Scientific Reasoning Benchmarks. Benchmarks like BoxingGym [ 27] explore simulated theory formation, while others such as AAAR [ 60] and Lab-Bench [ 49] assess reasoning or experimental 2 Figure 2: One AI research task example from ICLR 2024 MogaNet. design based on static artifacts (e.g., protocols, figures). While valuable for assessing abstract reasoning, these benchmarks do not evaluate the agentâ€™s ability to perform actual experiments. Scientific Coding Benchmarks. Scicode [ 81], for instance, focuses on generating code snippets for natural science tasks, while BLADE [ 31], DiscoveryBench [ 64], and ScienceAgentBench [ 14] primarily assess post-hoc data analysis or hypothesis testing. While critical to the scientific process, they often isolate coding or analysis from the broader, iterative experimental context. Machine Learning Benchmarks. Several benchmarks specifically target machine learning (ML) tasks, yet often focus on sub-components or operate with simplifications of the full research cycle. For example, DSBench [ 45], ML-Agent-Bench [ 39], and MLE-Bench [ 12] assess ML problem- solving capabilities, such as script editing or hyperparameter tuning, frequently within constrained environments like Kaggle challenges. Other benchmarks such as RE-Bench [ 92], ML-Gym [ 69], and Curie [ 47], compare agent performance against humans on research tasks, but often operate at a limited scale (e.g., RE-Bench features only 7 hand-curated tasks) or use simplified evaluation metrics. PaperBench [ 76] assesses agents on tasks derived from academic literature, focusing on their proficiency in executing specific, well-defined sub-components of the research process, such as running documented code scripts or performing standard data analyses. While these benchmarks provide valuable insights into specific ML tasks, they generally fail to capture the complexity of realistic end-to-end AI research workflows, nor do they typically offer a methodology for constructing such comprehensive benchmark tasks at scale. 3 The EXP-Bench Benchmark and Dataset EXP-Bench is built to evaluate the AI agentâ€™s ability to address AI research tasks by conducting end-to-end experimentation. Each research task is grounded on an influential AI research paper and its corresponding codebase. This coupling captures the full scientific workflowsâ€”linking concrete high-level ideas to executable implementations (Â§3.1). We achieve scalable construction of these high- fidelity tasks through a semi-automated curation pipeline, which integrates multi-modal extraction with lightweight human verification (Â§3.2). This design also opens the door to large-scale data generation for training agents capable of automating core aspects of AI research. 3.1 EXP-Bench Dataset Specification Our dataset is a collection of AI research tasks, each structured to emulate a complete experimental process designed to address a specific AI research question from a published paper. As shown in Fig 2, each task entry in the dataset contains a problem statement for the agent, and the corresponding ground-truth solution derived from the original research artifacts. Problem Statement (Agent Input). Each task instance within EXP-Bench provides the agent with: (1)Research Question: A specific goal derived from the source paperâ€™s experiments. (2) High-Level Method: A description guiding the required experimental approach; and (3) Code Repository: Access to the relevant code, potentially with specific components/scripts masked or requiring modification. 3 Deep Learning Reinforcement LearningApplications Optimization Computer VisionGenerative Models Multimodal ModelsSocial AspectsLanguageTime Series Trustworthy Machine LearningCausality Computational BiologyTheory Probabilistic MethodsPhysical Models Datasets and Benchmarks Paper Category0255075100125Task Count ICLR # PapersICLR # Tasks NeurIPS # PapersNeurIPS # Tasks20200250CountsFigure 3: EXP-Benchâ€™s dataset comprises tasks from a diverse set of ML research categories. Expected Outcome (Ground Truth). Each task instance also includes a ground-truth experimental solution curated from the source paper and codebase. This solutionâ€”used to evaluate agent out- putsâ€”comprises: (1) an experimental design specifying key variables, constants, and procedures; (2) the necessary code modifications, assessed via a git diff against the provided repository; and (3) a final conclusion that directly answers the research question based on experimental results. Benchmark Overview and Statistics: EXP-Bench currently includes 461 research tasks drawn from 51 influential papers, as detailed in App. B. As shown in Fig 3, these tasks span diverse AI subfieldsâ€”including Computer Vision, NLP, and Reinforcement Learningâ€”and are sourced from top-tier venues, namely NeurIPS 2024 (53%) and ICLR 2024 (47%). This breadth ensures coverage of diverse experimental paradigms, coding practices, and research challenges prevalent in the AI field. Moreover, each task is broken down into fine-grained, individually gradable subtasks spanning all three ground-truth componentsâ€”design, implementation, and conclusionâ€”resulting in a total of 12,737 subtasks. Together, these features make EXP-Bench a comprehensive testbed for assessing the capabilities of AI research agents. 3.2 EXP-Bench Semi-Automated Dataset Construction Pipeline Curating a high-fidelity benchmark for end-to-end AI experimentation is challenging due to the fragmented and domain-specific nature of real-world research artifactsâ€”namely papers and their associated codebases. Critical experimental details are often scattered, implicit, or embedded in dense technical language, making manual extraction labor-intensive and difficult to scale. To address this, we propose a semi-automated construction pipeline that systematically structures these artifacts into benchmark tasks with lightweight human oversight. The pipeline comprises three stages (Fig 4): Stage 1: Source Selection and Filtering. The process begins by identifying candidate research artifacts that form the basis of high-quality experimental tasks. We target influential papers from top-tier AI conferences (e.g., NeurIPS, ICLR) that are accompanied by publicly available code repositories. Initial filtering criteria are applied to prioritize impactful and potentially reproducible research, considering factors such as citation counts, and code repository activity (e.g., GitHub stars, forks). This selection phase aims to establish a strong foundation by focusing on artifacts that, despite potential imperfections, represent significant and verifiable research contributions. Stage 2: Experiment Procedure Extraction. Research papers rarely present experiments as complete proceduresâ€”key steps are often implicit or scattered. To enable structured agent evaluation, we decompose each task into explicit sub-steps. This transforms high-level research goals into concrete workflowsâ€”e.g., multi-step experiment design and environment setupâ€”making them suitable for both execution and fine-grained evaluation. This stage extracts the complete research task by combining the research plan (from the paper) with its corresponding experiment implementation (from the codebase). Further implementation details can be found in App. F. Stage 2.1: Extract Research Task. We begin by extracting the core research taskâ€”consisting of the research question, high-level methodology, and expected outcomeâ€”directly from the paper. This 4 Figure 4: EXP-Bench semi-automated dataset construction pipeline. process is designed to handle the fact that key information in academic papers is often distributed across sections and conveyed implicitly. First, we index the PDF using a combination of OCR (Optical Character Recognition) and multimodal extraction techniques to capture structured elements like tables, figures, and headers. This ensures downstream access to high-signal artifacts that may anchor the task definition. Next, we conduct a multi-pass extraction. In the first pass, we perform retrieval-augmented querying to identify broad, high-level research takeaways. These overarching questions are often not confined to a single paragraph and require stitching together dispersed cues. In the second pass, for each high-level takeaway, we apply semantic extraction at the subsection level, focusing on evaluation sections. We classify each subsection as either implementation context or a candidate research question. Contextual passages are stored and reused across subsequent prompts. This focused promptingâ€”processing each subsection independently while conditioning on accumulated context and extracted tables/figuresâ€”helps the LLM generate more accurate and detailed task formulations. Finally, we refine each task through targeted re-querying of the full paper (including appendices) to recover any additional setup constraints or methodological details that were missed earlier. This step acknowledges that relevant setup details may be located far from the task description and ensures completeness for the extracted task. Stage 2.2: Extract Experiment Implementation. Each extracted task is then passed to an im- plementation extraction AI agent (operating in a tool-augmented environmentâ€”with PDF reading, terminal access, and web browsing) to identify the specific implementation (chain of scripts) needed to address the research task. Our setting provides the agent with both a complete codebase and the extracted taskâ€”containing the research question, methodology, and expected outcome. This effectively reduces the problem to a goal-conditioned search over the codebase, where the agentâ€™s task is to localise the implementation that realises the specified methodology and expected outcome. To do this, the agent explores the repository in an open-ended fashionâ€”e.g., consulting documentation, and auxiliary scripts, to uncover domain-specific requirements (e.g., pretrained checkpoints). The extracted experiment execution ground truth will be fully based on existing scripts. The agent outputs (1) a list of required scripts and (2) high-level usage instructions describing how to run them to complete the task. Once a candidate implementation is produced, it is executed in Stage 3. If the run fails, the pipeline iteratesâ€”allowing the agent to refine or replace the implementation until a working solution is found. The final validated script chain is then parsed by the agent via AST (Abstract Syntax Tree) tracing to extract a step-by-step list of implementation requirements in natural language, which becomes the ground truth for evaluating implementation correctness. Finally, we incorporate additional contextual details (e.g., hyperparameters) sourced from the raw code (e.g., configuration files) or repository documents (e.g., README.md ) to enhance the final task specification. Stage 3: Verification and Refinement. All tasks are validated and finalized in this stage. For paper-derived tasks with corresponding implementations, the associated scripts are executed in a clean, containerized environment. Execution traces are then checked against expected outputs from the original paper. If validation fails, the task is returned to the previous stage for refinement. For tasks lacking a matched implementation, we perform manual validation to ensure the extracted 5 Table 1: Average benchmark scores for various models when tested against various evaluation metrics. Popular Agents and LLMs perform poorly on EXP-Bench, showcasing its difficulty. Agent Model D I E IÂ·E C Allâœ“ AllÂ·Eâœ“ #E OpenHands o3-mini 18.4 20.3 15.0 2.9 21.0 1.4 0.5 420 OpenHands Claude-3.7 Sonnet 16.0 35.0 33.2 14.9 13.4 0.7 0.4 235 OpenHands Amazon Nova Pro 18.2 19.5 26.8 0.0 15.7 0.0 0.0 56 OpenHands Claude-3.5 Haiku 20.6 26.2 9.3 1.3 13.8 0.0 0.0 237 OpenHands DeepSeek R1 6.8 10.0 0.7 0.0 2.4 0.0 0.0 140 IterativeAgent Claude-3.5 Haiku 6.4 20.6 25.2 5.4 2.2 0.0 0.0 111 IterativeAgent Amazon Nova Pro 0.1 10.0 18.1 0.0 0.3 0.0 0.0 215 research objective faithfully aligns with the source paper. In all cases, a lightweight human review finalizes the task, requiring only a cross-check of structured task contentâ€”already consolidated by the pipelineâ€”against the source materials. This significantly reduces human burden compared to manual curation from scratch. Following validation, each complete task is added to the dataset along with a list of masked files (e.g., README.md, relevant scripts) to ensure agents cannot directly access answers. In our benchmark implementation, repositories are cloned afresh per agent, and masking is applied using scripted git operations, including recursive traversal of submodules. Masking ensures agents must reason over the task input, rather than rely on shortcut access to original solutions. 4 Evaluation 4.1 Evaluating LLM-based Agents Performance on EXP-Bench: Setup & Main Results Setup. We evaluate a range of agents and LLMs used in related benchmarks [ 13] against EXP- Bench. In terms of agents, we made use of OpenHands (a top-performing code generation agent) and IterativeAgent (as configured in [ 76] to reduce the likelihood of early task stopping), henceforth known as OHandIA, respectively. In terms of LLMs, these include the top-ranked Claude-Sonnet 3.7, Haiku 3.5, Deepseek-R1 [ 90] models, and OpenAI o3-mini variants. Each agent is run in an Ubuntu 24.04 Docker container, and given access to 4 Ã—Nvidia A40 GPU, and a clean working directory containing the masked GitHub repo of the paper (i.e., task-specific scripts removed), instructions, and relevant context (e.g., API credentials). Evaluation Judge Implementation Details. Our evaluation framework consists of two main components used to assess agent performance across various metrics (refer to later sections, e.g., Table 1). The first component is an LLM-based judge (using o3-mini), following prior work on LLM-as-a-judge [ 111,56,1,76]. This judge operates through multiple steps: The process begins with an integrity check performed by a Monitor, which analyzes agent logs to detect disallowed behaviors (denoted as metric M; see Fig 6b). Specifically, the monitor checks whether the agent: (1) accessed the research paper directly (e.g., opened the PDF), (2) performed Git operations such as checking out commits or switching branches, or (3) used fake, hardcoded, or placeholder data rather than generating results through real experimentation. If violations are found, the monitor also identifies possible causes (e.g., ethical refusals, runtime errors) using log information. Once integrity is established, the agentâ€™s experimental design,implementation, and conclusion are evaluated for conceptual soundness, completeness (e.g., inclusion of all required steps), and alignment with ground truth. These assessments yield scores for: D(design correctness, i.e., proportion of design criteria met), I(implementation correctness, i.e., proportion of implementation components satisfied), and C(conclusion correctness). The second component of our evaluation judge is a Code Execution Validator, which runs the agent-generated code modifications in a clean and equivalent containerized environment. This step verifies whether the code is executable and produces expected outputs. This executability metric is denoted as E. Implementation details including the system prompt are in App. H. Main Results. Table 1 presents average accuracy scores across all 461 tasks. IÂ·Eindicates whether an implementation is both appropriate for the experimental task and executableâ€”a more comprehensive check of implementation quality. Allâœ“denotes tasks that are fully correct in terms of D,I, and C, while AllÂ·Eâœ“adds the executability requirement. #Erepresents the number of tasks per model that were execution-checked. Due to the time-consuming nature of execution, only a subset of traces were 6 evaluatedâ€”excluding those that failed the monitor check, which were automatically discarded prior to execution. Our top-ranked agents are OH+o3-mini, OH+3.7 Sonnet, and OH+Nova Pro, ranked viaAllÂ·Eâœ“, with Cused as a tiebreaker. The worst-performing model was IA+Nova Pro. Extended results by paper category are shown in Table 3, with full details in App. D. Across both tables, we observe that models consistently score below 30% across all metrics, with the exception of the RL category, where several OHmodels achieve up to â‰ˆ41% (averaged over 36 tasks) in terms of I. Notably, under stricter metrics such as Allâœ“, performance drops sharplyâ€”e.g., OH+o3-mini scores only 1.4%. This underscores the value of including partial metrics that assess individual aspects, allowing credit for partially correct answers and supporting a more nuanced evaluation. 4.2 Detailed Analysis Cost-Time Analysis. Fig 6a shows the average cost (in USD) and time (in minutes) per task across different agent configurations. Cost reflects only the token usage of the backbone LLM (input/output), excluding agent internal LLM-API usage or compute consumption. The number in parentheses next to each legend entry indicates the modelâ€™s performance rank, based on average correctness. Each agent was allowed a maximum of 40 minutes per task, though this limit can be easily adjusted. Notably, IAmodels often consumed the full allotted time, rarely stopping early. In contrast, early stopping was common with OHmodels. For example, the relative time difference between Nova and Haiku is larger under OHthan IA, reflecting differing usage patterns. These trends are consistent with our earlier observations: OHmodels often produced plausible responses without actually running the experiment, leading to high partial scores (e.g., design, implementation), while IAmodels tended to run longer but less effectively. Interestingly, we found little correlation between runtime/cost and overall performance. OH+o3-mini (rank 1) achieves the best trade-off with low cost and moderate time. OH+3.7 Sonnet (rank 2) performs well but is the slowest and most expensive. The full costâ€“time distribution is provided in App. I.2. Conjunctive Evaluation Metrics Substantially Lower Agent Scores. We analyze only the subset of tasks for which execution was run, to visualize how progressively applying stricter evaluation criteria impacts agent scores. As shown in Fig 6b (with full results in App. I.1), applying only the initial monitoring check ( M) yields an average score of 20.6%. Adding design ( D) and conclusion ( C) correctness criteria reduces the score sharply to 3.7%. Incorporating implementation correctness ( I) further lowers the score to 0.4%, and including execution verification ( E) results in a final accuracy of just 0.2%. These findings highlight how conjunctive evaluation surfaces brittleness in end-to-end experimental correctness. CCÂ·D I E IÂ·E Judge Metric010203040Average Score (%) Figure 5: Stability Analysis.Metric Stability Analysis. As shown in Fig 5, certain individual metrics such as CandEexhibit high variance. This variance arises for different reasons: for C, agents can produce plausible but unfounded conclusions without a valid experimental foundation; for E, even incorrect or mock implementations may successfully execute, introduc- ing overestimation bias. To mitigate such inconsistencies, we adopt compositional scoring via conjunctive metrics such as CÂ·DandIÂ·E, which combine correctness across multiple dimensions. These conjunctive forms substan- tially reduce score variability, producing more reliable signals of agent performance. For example, CÂ·Dfilters out conclusions not grounded in valid design plans, and IÂ·Ediscounts executions that do not fulfill setup require- ments. This demonstrates that conjunctive metrics can temper over-crediting and reduce sensitivity to annotation leniency or spurious correctnessâ€”thereby offering a more stable and discriminative evaluation. 4.3 Analysis on Prevalent Agent Failure Patterns Pattern Extraction Methodology. Our analysis followed a two-pass, open-ended process. During evaluation, each metric score was accompanied by an error analysis, derived from implementation logs (e.g., stderr) or comparisons against ground truth. In the first pass, we extracted high-level, 7 15 20 25 30 35 Average Time (minutes)0246810Average Cost ( $) (1)(2) (3)(4)(5)(6) (7)OH + o3-mini OH + 3.7 Sonnet OH + Nova Pro OH + 3.5 Haiku OH + DeepSeek R1 IA + Nova Pro IA + 3.5 Haiku(a) Costâ€“time trade-offs across agents. M MÂ·CÂ·D MÂ·CÂ·DÂ·I MÂ·CÂ·DÂ·IÂ·E Judge Metric010203040Average Score (%)OpenHands + o3-mini OpenHands + Claude-3.7 Sonnet OpenHands + Amazon Nova Pro OpenHands + Claude-3.5 Haiku (b) Stricter metrics reveal lower true correctness. Figure 6: Ablation of agent performance along costâ€“time and evaluation metrics. domain-specific insights from these earlier error analyses, across phases for all agent-task pairs. In the second pass, we iteratively grouped these insights into distinct failure typesâ€”assigning each to an existing category or creating a new one if needed. This process produced 3,238 raw insights, which we distilled into 361 unique failure types. We present a representative and simplified subset of these condensed errors in Table 2 (full details can be found in App. G). Analysis. To better understand where agents fail, we analyzed error traces and categorized them into representative failure types across four key phases of experimentation: implementation, execu- tion, design, and conclusion. As shown in Table 2, the most prevalent issues emerged during the implementation phase, with 39.71% of failures stemming from missing essential components. In several cases, agents failed to include critical elements such as semantic retrieval strategies (e.g., UniXcoder-H2L and UniXcoder-L2H), validation functions for filtering questions (e.g., using GPT- 3.5), or robustness-enhancing techniques like Mixup,CutMix, and Label Smoothingâ€”undermining the experimental implementationâ€™s validity. Incomplete data preprocessing (1.83%) was another notable implementation issue, with agents omitting required dataset loading and transformation steps, such as ETTh1 series preparation, ACFplotting, or normalization procedures (e.g., RevIN ), instead providing only boilerplate config files. In the execution phase, failures were most commonly due to environment or dependency misconfigurations (29.38%), such as missing critical environments (e.g., STORM not registered in jaxmarl ) or absent core libraries like PyTorch andFlax, which led to model loading failures. Script-level issues (23.84%) included unrecognized model names (e.g., moganet_tiny not found in timm ) and missing checkpoint files, causing runtime or I/O errors. These examples highlight persistent reproducibility challenges even when a correct implementation structure is in place. Design-related failures were also frequent, with 16.05% involving incomplete or misclassified experimental variables, and 7.62% reflecting extraneous procedural additionsâ€”such as inclusion of a ResNet-50 backbone or arbitrary hyperparameter knobs not specified in the ground truth. These design errors suggest that agents often fail to distinguish between essential experimental factors and implementation noise. Finally, conclusion-phase errors highlight limitations in agentsâ€™ interpretive reasoning. The most common issue (26.18%) was missing or underdeveloped conclu- sionsâ€”for instance, omitting detailed comparisons between PPOandQ-Learning on training time and normalized scores, or neglecting specific numerical gains (e.g., 1.25% improvements across ARC-Challenge andOpenBookQA ). Another frequent error (19.66%) was incorrect interpretation, such as claiming Hadamard-enhanced INT4 inference improves performance without substantiating comparisons to baseline INT4. Together, these findings emphasize the importance of phase-specific evaluation and illustrate how surface-level plausibility can mask deeper breakdowns in experimental reasoning and reproducibility. 5 Discussion Limitations. EXP-Bench primarily focuses on the experimentation procedure â€“ from designing experiments for a given research question to deriving conclusions. The broader AI research lifecycle encompasses other critical stages such as identifying gaps through literature review, the initial unstructured ideation of research questions, and navigating the complex, iterative, and unpredictable path of real-world scientific discovery, which are not yet fully captured by the current task structures. 8 Table 2: Agents fail in diverse ways across different phases of experimentation; this table presents a simplified subset of common examples, measured across all agent and model evaluations. Phase Failure Type Prevalence (%) Design Incomplete or Misclassified Design Variables 16.05 Design Irrelevant Procedural Additions in Design 7.62 Implementation Missing Essential implementation Components 39.71 Implementation Incomplete Evaluation Metric Implementation 2.15 Implementation Incomplete Data and Preprocessing Setup 1.83 Execution Environment/Dependency Configuration Errors 29.38 Execution Execution Script and File Errors 23.84 Execution Missing Setup Script File 6.95 Execution Tensor Operation Execution Error 3.22 Conclusion Missing Conclusion Content 26.18 Conclusion Incorrect Conclusion Interpretation 19.66 Conclusion Extraneous Details in Conclusion 7.77 Conclusion Incorrect Numeric Conclusion 3.21 Table 3: Average benchmark scores of various models and agents across select task categories; see Supp. D for complete list. Evaluation performed against EXP-Bench. Category Agent Model D I E IÂ·E C Allâœ“ Allâœ“Â·E Applications OH Nova Pro 19.2 23.9 19.0 0.0 13.9 0.0 0.0 Applications OH o3-mini 9.0 8.0 0.0 0.0 8.3 0.0 0.0 Applications OH 3.5 Haiku 19.2 24.5 8.3 5.6 8.3 0.0 0.0 Applications OH 3.7 Sonnet 9.0 26.8 30.8 7.7 8.3 2.8 0.0 Applications IA Nova Pro 0.0 9.8 5.0 0.0 0.0 0.0 0.0 Applications IA 3.5 Haiku 0.0 18.0 0.0 0.0 0.0 0.0 0.0 Applications OH DeepSeek R1 3.1 4.3 0.0 0.0 0.0 0.0 0.0 RL OH 3.7 Sonnet 18.3 48.2 27.3 21.2 17.6 2.0 3.0 RL OH o3-mini 23.5 34.8 15.7 2.0 27.5 3.9 0.0 RL OH 3.5 Haiku 27.7 41.4 11.5 0.0 17.6 0.0 0.0 RL IA 3.5 Haiku 3.3 27.5 17.4 0.0 2.4 0.0 0.0 RL OH DeepSeek R1 5.0 10.3 0.0 0.0 2.0 0.0 0.0 RL IA Nova Pro 0.0 8.6 9.7 0.0 0.0 0.0 0.0 RL OH Nova Pro 17.9 28.9 0.0 0.0 13.7 0.0 0.0 Future directions. Future work will focus on enhancing AI agentsâ€™ ability to automate research experimentation using supervision from EXP-Benchâ€™s dataset. One promising direction is to apply reinforcement learning with verifiable rewards, enabling agents to autonomously navigate the research lifecycle and accelerate scientific discovery. 6 Conclusion We introduced EXP-Bench, a novel benchmark designed to rigorously evaluate and guide the development of AI agents in conducting end-to-end AI research experimentation. By sourcing tasks from influential peer-reviewed publications and their accompanying codebases, and utilizing a semi-automated curation pipeline, EXP-Bench presents agents with realistic, fine-grained challenges in end-to-end AI research workflow including experimental design, implementation, execution, and conclusion derivation. Our initial evaluations with leading agents reveal significant bottlenecks in conceptualizing complex experiments and ensuring robust code implementation and execution. EXP-Bench therefore serves not only as a comprehensive evaluation tool but also as a valuable dataset to guide future AI agents to act step by step, ultimately accelerating AI research. 9 References Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys. org/blog/2023-03-30-vicuna/. F. Alet, J. Lopez-Contreras, J. Koppel, M. Nye, A. Solar-Lezama, T. Lozano-Perez, L. Kaelbling, and J. Tenenbaum. A large-scale benchmark for few-shot program induction and synthesis. In International Conference on Machine Learning, pages 175â€“186. PMLR, 2021. S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. J. Baek, S. K. Jauhar, S. Cucerzan, and S. J. Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity, 2023. L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, and O. Evans. The reversal curse: Llms trained on \"\"a is b\"\" fail to learn \"\"b is a\"\", 2024. M. Bettini, A. Prorok, and V. Moens. Benchmarl: Benchmarking multi-agent reinforcement learning, 2024. J. Blasiok and P. Nakkiran. Smooth ece: Principled reliability diagrams via kernel smoothing, 2023. D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570â€“578, 2023. L. Boisvert, M. Thakkar, M. Gasse, M. Caccia, T. L. S. D. Chezelles, Q. Cappart, N. Chapados, A. Lacoste, and A. Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2025. A. Bou, M. Bettini, S. Dittert, V. Kumar, S. Sodhani, X. Yang, G. D. Fabritiis, and V. Moens. Torchrl: A data-driven decision-making library for pytorch, 2023. J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, L. Weng, and A. M Ë› adry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2024. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Z. Chen, S. Chen, Y. Ning, Q. Zhang, B. Wang, B. Yu, Y. Li, Z. Liao, C. Wei, Z. Lu, et al. Scienceagent- bench: Toward rigorous assessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080, 2024. C.-A. Cheng, A. Nie, and A. Swaminathan. Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms, 2024. P. Cheng, T. Hu, H. Xu, Z. Zhang, Z. Yuan, Y. Dai, L. Han, N. Du, and X. Li. Self-playing adversarial language game enhances llm reasoning, 2025. A. Chevalier, J. Geng, A. Wettig, H. Chen, S. Mizera, T. Annala, M. J. Aragon, A. R. Fanlo, S. Frieder, S. Machado, A. Prabhakar, E. Thieu, J. T. Wang, Z. Wang, X. Wu, M. Xia, W. Xia, J. Yu, J.-J. Zhu, Z. J. Ren, S. Arora, and D. Chen. Language models as science tutors, 2024. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. T. M. U. Collaboration, J. Audenaert, M. Bowles, B. M. Boyd, D. Chemaly, B. Cherinka, I. Ciuc Ë˜a, M. Cranmer, A. Do, M. Grayling, E. E. Hayes, T. Hehir, S. Ho, M. Huertas-Company, K. G. Iyer, M. Jablonska, F. Lanusse, H. W. Leung, K. Mandel, J. R. MartÃ­nez-Galarza, P. Melchior, L. Meyer, L. H. Parker, H. Qu, J. Shen, M. J. Smith, C. Stone, M. Walmsley, and J. F. Wu. The multimodal universe: Enabling large-scale machine learning with 100tb of astronomical scientific data, 2024. J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang. Safe rlhf: Safe reinforcement learning from human feedback, 2023. 10  T. Dai, B. Wu, P. Liu, N. Li, J. Bao, Y. Jiang, and S.-T. Xia. Periodicity decoupling framework for long-term series forecasting. In The Twelfth International Conference on Learning Representations, 2024. Y. Du, F. Bai, T. Huang, and B. Zhao. Segvol: Universal and interactive volumetric medical image segmentation, 2025. Elicit. Elicit: Analyze research papers at superhuman speed, 2025. Accessed: 2025-05-12. M. Elrefaie, F. Morar, A. Dai, and F. Ahmed. Drivaernet++: A large-scale multimodal car dataset with computational fluid dynamics simulations and deep learning benchmarks, 2025. Y. Fang, N. Zhang, Z. Chen, L. Guo, X. Fan, and H. Chen. Domain-agnostic molecular generation with chemical feedback, 2024. S. Frieder, L. Pinchetti,, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. Petersen, and J. Berner. Mathe- matical capabilities of chatgpt. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 27699â€“27744. Curran Associates, Inc., 2023. K. Gandhi, M. Y. Li, L. Goodyear, L. Li, A. Bhaskar, M. Zaman, and N. D. Goodman. Boxinggym: Benchmarking progress in automated experimental design and model discovery, 2025. G. Gendron, Q. Bao, M. Witbrock, and G. Dobbie. Large language models are not strong abstract reasoners. arXiv preprint arXiv:2305.19555, 2023. A. Ghafarollahi and M. J. Buehler. Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning, 2024. A. Grosnit, A. Maraval, J. Doran, G. Paolo, A. Thomas, R. S. H. N. Beevi, J. Gonzalez, K. Khandelwal, I. Iacobacci, A. Benechehab, et al. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. arXiv preprint arXiv:2411.03562, 2024. K. Gu, R. Shang, R. Jiang, K. Kuang, R.-J. Lin, D. Lyu, Y. Mao, Y. Pan, T. Wu, J. Yu, et al. Blade: Benchmarking language model agents for data-driven science. arXiv preprint arXiv:2408.09667, 2024. S. Guo, C. Deng, Y. Wen, H. Chen, Y. Chang, and J. Wang. Ds-agent: Automated data science by empowering large language models with case-based reasoning. arXiv preprint arXiv:2402.17453, 2024. Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2024. S. J. Han, K. J. Ransom, A. Perfors, and C. Kemp. Inductive reasoning in humans and large language models. Cognitive Systems Research, 83:101155, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. X. Hu, Z. Zhao, S. Wei, Z. Chai, Q. Ma, G. Wang, X. Wang, J. Su, J. Xu, M. Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks. arXiv preprint arXiv:2401.05507, 2024. J.-t. Huang, W. Wang, E. J. Li, M. H. Lam, S. Ren, Y. Yuan, W. Jiao, Z. Tu, and M. Lyu. On the humanity of conversational ai: Evaluating the psychological portrayal of llms. In The Twelfth International Conference on Learning Representations, 2023. Q. Huang, J. V ora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2023. Q. Huang, J. V ora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2024. Z. Huang, Q. Ye, B. Kang, J. Feng, and H. Fan. Classification done right for vision-language pre-training, 2024. T. Ifargan, L. Hafner, M. Kern, O. Alcalay, and R. Kishony. Autonomous llm-driven researchâ€”from data to human-verifiable research papers. NEJM AI, 2(1):AIoa2400555, 2025. 11  G. Jaume, P. Doucet, A. H. Song, M. Y. Lu, C. Almagro-PÃ©rez, S. J. Wagner, A. J. Vaidya, R. J. Chen, D. F. K. Williamson, A. Kim, and F. Mahmood. Hest-1k: A dataset for spatial transcriptomics and histology image analysis, 2024. B. Jing, H. StÃ¤rk, T. Jaakkola, and B. Berger. Generative modeling of molecular dynamics trajectories, 2024. L. Jing, Z. Huang, X. Wang, W. Yao, W. Yu, K. Ma, H. Zhang, X. Du, and D. Yu. Dsbench: How far are data science agents from becoming data science experts?, 2024. J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Å½Ã­dek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583â€“589, 2021. P. T. J. Kon, J. Liu, Q. Ding, Y. Qiu, Z. Yang, Y. Huang, J. Srinivasa, M. Lee, M. Chowdhury, and A. Chen. Curie: Toward rigorous and automated scientific experimentation with ai agents, 2025. J. LÃ¡la, O. Oâ€™Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques, and A. D. White. Paperqa: Retrieval- augmented generative agent for scientific research. arXiv preprint arXiv:2312.07559, 2023. J. M. Laurent, J. D. Janizek, N. Thakkar, M. Ruzo, M. S. Yao, M. S. Levine, S. G. Rodriques, and A. White. Lab-bench: Measuring capabilities of language models for biology research, 2024. R. Li, T. Patel, Q. Wang, and X. Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024. S. Li, Z. Wang, Z. Liu, C. Tan, H. Lin, D. Wu, Z. Chen, J. Zheng, and S. Z. Li. Moganet: Multi-order gated aggregation network, 2024. X. Li, Z. Huang, F. Xue, and Y. Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images, 2024. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092â€“1097, 2022. S. Lin, W. Lin, X. Hu, W. Wu, R. Mo, and H. Zhong. Cyclenet: Enhancing time series forecasting through modeling periodic patterns, 2024. H. Liu, L. Chen, Y. Qiao, C. Lv, and H. Li. Reasoning multi-agent behavioral topology for interactive autonomous driving, 2024. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. T. Liu, C. Xu, and J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023. X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang. Agentbench: Evaluating llms as agents, 2023. X. Liu, C. Zhou, and S. Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors, 2024. R. Lou, H. Xu, S. Wang, J. Du, R. Kamoi, X. Lu, J. Xie, Y. Sun, Y. Zhang, J. J. Ahn, H. Fang, Z. Zou, W. Ma, X. Li, K. Zhang, C. Xia, L. Huang, and W. Yin. Aaar-1.0: Assessing aiâ€™s potential to assist research, 2024. C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Y. Lu, Y. Hu, Y. Zhong, D. Wang, Y. Wang, and S. Chen. An extensible framework for open heterogeneous collaborative perception, 2024. Q. Luo, H. Yu, and X. Li. Badam: A memory efficient full parameter optimization method for large language models, 2024. 12  B. P. Majumder, H. Surana, D. Agarwal, B. D. Mishra, A. Meena, A. Prakhar, T. V ora, T. Khot, A. Sab- harwal, and P. Clark. Discoverybench: Towards data-driven discovery with large language models. arXiv preprint arXiv:2407.01725, 2024. S. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas, K. Rao, D. Sadigh, and A. Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023. L. Mitchener, J. M. Laurent, B. Tenmann, S. Narayanan, G. P. Wellawatte, A. White, L. Sani, and S. G. Rodriques. Bixbench: a comprehensive benchmark for llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025. A. Moskvichev, V. V. Odouard, and M. Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. arXiv preprint arXiv:2305.07141, 2023. S. Narayanan, J. D. Braza, R.-R. Griffiths, M. Ponnapati, A. Bou, J. Laurent, O. Kabeli, G. Wellawatte, S. Cox, S. G. Rodriques, et al. Aviary: training language agents on challenging scientific tasks. arXiv preprint arXiv:2412.21154, 2024. D. Nathani, L. Madaan, N. Roberts, N. Bashlykov, A. Menon, V. Moens, A. Budhiraja, D. Magka, V. V orotilov, G. Chaurasia, D. Hupkes, R. S. Cabral, T. Shavrina, J. Foerster, Y. Bachrach, W. Y. Wang, and R. Raileanu. Mlgym: A new framework and benchmark for advancing ai research agents, 2025. R. Ohana, M. McCabe, L. Meyer, R. Morel, F. J. Agocs, M. Beneitez, M. Berger, B. Burkhart, K. Burns, S. B. Dalziel, D. B. Fielding, D. Fortunato, J. A. Goldberg, K. Hirashima, Y.-F. Jiang, R. R. Kerswell, S. Maddu, J. Miller, P. Mukhopadhyay, S. S. Nixon, J. Shen, R. Watteaux, B. R.-S. Blancard, F. Rozet, L. H. Parker, M. Cranmer, and S. Ho. The well: a large-scale collection of diverse physics simulations for machine learning, 2025. P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth International Conference on Learning Representations, 2024. S. Qi, S. Chen, Y. Li, X. Kong, J. Wang, B. Yang, P. Wong, Y. Zhong, X. Zhang, Z. Zhang, N. Liu, W. Wang, Y. Yang, and S.-C. Zhu. Civrealm: A learning and reasoning odyssey in civilization for decision-making agents, 2024. J. Ren, X. Feng, B. Liu, X. Pan, Y. Fu, L. Mai, and Y. Yang. Torchopt: An efficient library for differentiable optimization, 2022. A. Rutherford, B. Ellis, M. Gallici, J. Cook, A. Lupu, G. Ingvarsson, T. Willi, R. Hammond, A. Khan, C. S. de Witt, A. Souly, S. Bandyopadhyay, M. Samvelyan, M. Jiang, R. T. Lange, S. Whiteson, B. Lacerda, N. Hawes, T. Rocktaschel, C. Lu, and J. N. Foerster. Jaxmarl: Multi-agent rl environments and algorithms in jax, 2024. S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, Z. Liu, and E. Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. G. Starace, O. Jaffe, D. Sherburn, J. Aung, J. S. Chan, L. Maksin, R. Dias, E. Mays, B. Kinsella, W. Thompson, et al. Paperbench: Evaluating aiâ€™s ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. L. Sun, Y. Han, Z. Zhao, D. Ma, Z. Shen, B. Chen, L. Chen, and K. Yu. Scieval: A multi-level large language model evaluation benchmark for scientific research. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19053â€“19061, Mar. 2024. W. Sun, L. Yan, X. Ma, S. Wang, P. Ren, Z. Chen, D. Yin, and Z. Ren. Is chatgpt good at search? investigating large language models as re-ranking agents, 2024. K. Swanson, W. Wu, N. L. Bulaong, J. E. Pak, and J. Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, pages 2024â€“11, 2024. J. Tang, H. Lu, R. Wu, X. Xu, K. Ma, C. Fang, B. Guo, J. Lu, Q. Chen, and Y.-C. Chen. Hawk: Learning to understand open-world video anomalies, 2024. M. Tian, L. Gao, S. D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. Tong, K. Trinh, C. Tian, Z. Wang, B. Wu, Y. Xiong, S. Yin, M. Zhu, K. Lieret, Y. Lu, G. Liu, Y. Du, T. Tao, O. Press, J. Callan, E. Huerta, and H. Peng. Scicode: A research coding benchmark curated by scientists, 2024. 13  P. Trirat, W. Jeong, and S. J. Hwang. Automl-agent: A multi-agent llm framework for full-pipeline automl. arXiv preprint arXiv:2410.02958, 2024. F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi. Knowledge fusion of large language models, 2024. Q. Wang, D. Downey, H. Ji, and T. Hope. Scimon: Scientific inspiration machines optimized for novelty. InProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279â€“299, 2024. W. Wang, S. Zhang, Y. Ren, Y. Duan, T. Li, S. Liu, M. Hu, Z. Chen, K. Zhang, L. Lu, X. Zhu, P. Luo, Y. Qiao, J. Dai, W. Shao, and W. Wang. Needle in a multimodal haystack, 2024. X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2024. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Y. Wang, D. Huang, W. Ye, G. Zhang, W. Ouyang, and T. He. Neurodin: A two-stage framework for high-fidelity neural surface reconstruction, 2024. T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526â€“1541, 2023. Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. W.-H. Weng et al. Towards an ai co-scientist, 2025. H. Wijk, T. Lin, J. Becker, S. Jawhar, N. Parikh, T. Broadley, L. Chan, M. Chen, J. Clymer, J. Dhyani, E. Ericheva, K. Garcia, B. Goodrich, N. Jurkovic, M. Kinniment, A. Lajko, S. Nix, L. Sato, W. Saunders, M. Taran, B. West, and E. Barnes. Re-bench: Evaluating frontier ai rd capabilities of language model agents against human experts, 2024. D. Wu, J. Chang, F. Jia, Y. Liu, T. Wang, and J. Shen. Topomlp: A simple yet strong pipeline for driving topology reasoning, 2023. S. Wu, W. Zhang, L. Xu, S. Jin, X. Li, W. Liu, and C. C. Loy. Clipself: Vision transformer distills itself for open-vocabulary dense prediction, 2024. C. Xiao, P. Zhang, X. Han, G. Xiao, Y. Lin, Z. Zhang, Z. Liu, and M. Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory, 2024. T. Xie, X. Qi, P. He, Y. Li, J. T. Wang, and P. Mittal. Badexpert: Extracting backdoor functionality for accurate backdoor input detection, 2023. F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria. Are large language models really good logical reasoners? a comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering, 2025. Y. Xu, W. Li, P. Vaezipoor, S. Sanner, and E. B. Khalil. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. arXiv preprint arXiv:2305.18354, 2023. Z. Xu, F. Liu, and H. Liu. Bag of tricks: Benchmarking of jailbreak attacks on llms, 2024. C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen. Large language models as optimizers, 2024. L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez, and B. Cui. Buffer of thoughts: Thought-augmented reasoning with large language models, 2024. Z. Yang, L. Dong, X. Du, H. Cheng, E. Cambria, X. Liu, J. Gao, and F. Wei. Language models as inductive reasoners. arXiv preprint arXiv:2212.10923, 2022. Z. Yang, X. Du, J. Li, J. Zheng, S. Poria, and E. Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023. Z. Yao, L. Guo, X. Yang, W. Kang, F. Kuang, Y. Yang, Z. Jin, L. Lin, and D. Povey. Zipformer: A faster and better encoder for automatic speech recognition, 2024. 14  J. Yuan, X. Yan, B. Shi, T. Chen, W. Ouyang, B. Zhang, L. Bai, Y. Qiao, and B. Zhou. Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback, 2025. V. Zambaldi, D. La, A. E. Chu, H. Patani, A. E. Danson, T. O. Kwan, T. Frerix, R. G. Schneider, D. Saxton, A. Thillaisundaram, et al. De novo design of high-affinity protein binders with alphaproteo. arXiv preprint arXiv:2409.08022, 2024. G. Zhang, L. Fan, C. He, Z. Lei, Z. Zhang, and L. Zhang. V oxel mamba: Group-free state space models for point cloud based 3d object detection, 2024. L. Zhang, Y. Zhang, K. Ren, D. Li, and Y. Yang. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks, 2024. S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou. Automl-gpt: Automatic machine learning with gpt, 2023. X. Zhang, J. Helwig, Y. Lin, Y. Xie, C. Fu, S. Wojtowytsch, and S. Ji. Sinenet: Learning temporal dynamics in time-dependent partial differential equations, 2024. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Y. Zheng, B. Huang, W. Chen, J. Ramsey, M. Gong, R. Cai, S. Shimizu, P. Spirtes, and K. Zhang. Causal-learn: Causal discovery in python, 2023. Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen. Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection, 2025. X. Zhu, Y. Guan, D. Liang, Y. Chen, Y. Liu, and X. Bai. Moe jetpack: From dense checkpoints to adaptive mixture of experts for vision tasks, 2024. Z. Zhu, J. Wang, H. Cheng, and Y. Liu. Unmasking and improving data credibility: A study with datasets for training harmless language models, 2024. 15 A Extended Related Works LLMs for scientific discovery. Many methods have adopted LLMs to generate novel hypotheses for common scientific discovery. For example, Baek et al. [ 4], Wang et al. [ 84], and Yang et al. developed approaches for generating innovative domain-specific research ideas. Going beyond domain-specific ideas, a line of work also focuses on generate hypothesis with LLMs in the commonsense domains [ 28,67,102,91,65,89,2,98,34,97]. Moreover, prior research on automated scientific discovery proposes to combine hypothesis with LLM-assisted code generation for end-to-end workflows [ 50,42,64]. While these efforts works on various stages of the scientific lifecycle, experimentationâ€”a critical, rigor-sensitive aspectâ€”remains underexplored. Some existing research explores building an automated scientific discovery workflow with rigorous validation using AI agents [ 61,91,46,106,75,9,79,105,29], they often either have limited automated evaluation or rely on domain-specific ad-hoc prompting optimizations to guide predefined workflows, struggling with the complexities of rigorous end-to-end experimentation to automate AI research. Particularly, Lu et al. [ 61] introduced a fully automated system called \"The AI Scientist\" to conduct research by collaborating with multiple LLM agents. These agents handle the full research process, from defining research problems and reviewing related literature to synthesizing and executing experiments. However, their solution has limited automated evaluation with a focus on commonsense domains. Gottweis et al. [ 91] proposed an AI Co-scientist built on Gemini 2.0, aiming at building a helpful AI collaborator for scientists. They focus on the scaling of the test- time compute paradigm to generate high-quality hypotheses and research proposals. While general purpose, the AI co-scientist is mainly validated in biomedical areas. Overall, these efforts often require experimental validation to follow constrained, framework-specific formats, resulting in extra overhead and hindering their usability. Benchmarks for domain-specific AI agent tasks. A wide range of benchmarks have been developed to evaluate the capabilities of AI agents across diverse domains. Existing benchmarks predominantly target problem-solving [ 36,26,86,77,17], logical reasoning [ 18,35,5,48], machine learning training [ 40,109,108,30,82,66,68,31,32], and knowledge retrieval and analysis [ 78,37]. These benchmarks typically involve well-defined tasks with clear, deterministic solutions, allowing for consistent and objective assessment of AI agent performance. By contrast, our proposed EXP-Bench focuses on experimentation for automating AI research, which requires a more rigorous and systematic approach beyond problem-solving. Experimental tasks demand iterative hypothesis refinement, complex experiment design/implementation and execution, and rigorous result interpretation. Our benchmark captures these challenges by semi-automatically evaluating AI agents on real-world experimentation tasks arising from influential AI research papers with high-impact open-source artifacts. B Extended Details of the EXP-Bench Dataset In this section, we provide a full list of the papers in the EXP-Bench dataset, including source paper and, AI sub-domain. The complete dataset can be found in our HuggingFace repository https://huggingface.co/datasets/Just-Curieous/EXP-Bench. Table 4: ICLR 2024 Papers ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 19292 Zipformer: A faster and better encoder for automatic speech recognition1023 97 Deep Learning â†’ Attention Mechanismspropose an archi- tecturememory needed: 32GB or more recommended, GPU type: NVIDIA V100 or A100, GPU amount: 2-81 3 3 19033 The Reversal Curse: LLMs trained on â€œA is Bâ€ fail to learn â€œB is Aâ€ 284 179 Deep Learning â†’ Large Language Modelspropose an archi- tectureOpenAI API key required; GPU: 1; memory: â‰¥16GB RAM3 0 0 16 ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 19044 AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning 11093 845 Deep Learning â†’ Generative Modelspropose an archi- tectureGPU type: NVIDIA; GPU amount: 1; GPU memory: 13GB1 4 4 17666 BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection165 12 Deep Learning â†’ RobustnessOther GPUs with CUDA support (exact types not specified), 4GB+ recommended memory6 0 0 19269 MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images 358 26 Applications â†’ Computer Visionpropose a training algorithmmemory needed: 8GB; GPU type: NVIDIA; GPU amount: 18 0 0 19281 Domain-Agnostic Molecular Generation with Chemical Feedback149 16 Applications â†’ Chemistrypropose an archi- tecturememory: 16GB RAM; GPU type: NVIDIA; GPU amount: 14 3 3 18244 Periodicity Decoupling Framework for Long-term Series Forecasting116 38 Deep Learning â†’ Time Seriespropose an archi- tectureGPU: at least one (unspecified)5 3 2 18318 AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection339 150 Deep Learning â†’ LLMspropose an archi- tecturememory: 24GB; GPU: RTX 3090; amount: 15 3 1 19388 Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models2706 20 Social Aspects â†’ AccountabilityOther standard resources + 1 GPU recommended3 3 6 17776 RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems145 139 Deep Learning â†’ LLMspropose a dataset/li- braryGPU: 1 6 3 1 18013 Knowledge Fusion of Large Language Models535 101 Deep Learning â†’ LLMspropose a new ML applica- tionGPUs: 1â€“4 A100; RAM: 32GB+0 4 6 18865 SineNet: Learning Temporal Dynamics in PDEs572 14 Applications â†’ Physicspropose an archi- tecture1 GPU (NVIDIA); RAM: 16GB5 0 0 18447 MogaNet: Multi-order Gated Aggregation Network220 66 Deep Learning â†’ GNNspropose an archi- tectureRAM:â‰¥16GB; GPUs: 1â€“4 V100/A1005 5 6 19610 TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning179 31 Applications â†’ Roboticspropose an archi- tectureGPUs: 1â€“2 RTX 2080+; RAM: â‰¥16GB9 3 3 17388 AgentBench: Evaluating LLMs as Agents2406 174 Deep Learning â†’ LLMspropose a dataset/li- braryRAM: 15GB; GPU: 1; OpenAI API3 3 2 18889 An Extensible Framework for Open Heterogeneous Collaborative Perception181 48 RL â†’Multi-agent propose a new ML applica- tionRAM:â‰¥16GB; GPUs: 2; CUDA compatible5 0 0 19128 CLIPSelf: Vision Transformer for Dense Prediction183 70 Applications â†’ Computer Visionpropose a new ML applica- tionRAM: 8GB; GPU: 13 3 5 17 ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 18660 TorchRL: A data-driven decision-making library for PyTorch2570 46 RL â†’Deep RL propose a dataset/li- braryRAM:â‰¥8GB; GPUs recommended; API access may be needed5 0 0 19209 Large Language Models as Optimizers506 829 Optimization â†’ Learning for Optimizationpropose a new ML applica- tionlikely RAM-heavy; API keys required; GPUs recommended0 0 5 18439 Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing139 21 Probabilistic Methods â†’ Calibrationpropose a new ML applica- tionstandard memory for Python scripts3 0 0 17595 Zero Bubble (Almost) Pipeline Parallelism 344 13 Optimization â†’ ParallelOther memory: 16GB; GPUs: 89 0 0 18531 CivRealm: A Learning and Reasoning Odyssey for Agents106 21 RL â†’Multi-agent propose a dataset/li- braryRAM: 8â€“16GB; GPU: 1; Freeciv-web access1 4 3 18540 Safe RLHF: Safe Reinforcement Learning from Human Feedback1421 329 RL â†’Safe RLHF propose a training algorithmOpenAI API; GPUs: A800-80GB Ã—84 3 2 19008 On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs110 58 Social Aspects â†’ Trustworthy MLOther RAM: â‰¥8GB; OpenAI API; GPU recommended2 3 1 Table 5: NeurIPS 2024 Papers ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 93022 Generative Modeling of Molecular Dynamics Trajectories144 13 Generative Models â†’New Approachespropose an archi- tectureGPUs not specified, but PyTorch and related libraries suggest a need for a CUDA-compatible GPU. Memory requirements unspecified.8 0 0 93431 Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs492 9 Optimization â†’ Generative Modelspropose an archi- tecturememory needed: 8 GB RAM minimum, OpenAI API key required, GPU: 1 x NVIDIA GPU recommended,6 3 3 98316 Causal-learn: Causal Discovery in Python1287 96 Causality propose an archi- tecturememory needed: Standard (depends on the dataset), GPU: Not required,3 5 2 95333 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors170 16 Computer Vision â†’ Video Generationpropose an archi- tecturememory needed: 16GB, Yes, GPU type: NVIDIA, GPU amount: 1,3 2 3 98326 TorchOpt: An Efficient Library for Differentiable Optimization570 17 Optimization â†’ Zero-order and Black-box Optimizationpropose an archi- tecturememory needed: At least 8GB RAM, GPU type: NVIDIA, GPU amount: 1,2 4 3 98318 BenchMARL: Benchmarking Multi-Agent Reinforcement Learning351 30 Reinforcement Learning â†’ Multi-agentpropose a datasetmemory needed: At least 8GB RAM recommended, GPU: 1x NVIDIA GPU (e.g., GTX 1080 or better),4 4 1 18 ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 95818 Classification Done Right for Vision-Language Pre-Training200 2 Multimodal Models propose a datasetmemory needed: 16 GB, GPU type: NVIDIA V100, GPU amount: 1,5 3 4 95974 Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving107 2 Reinforcement Learning â†’ Multi-agentpropose a datasetmemory needed: 8 GB RAM minimum, GPU: 1 NVIDIA GPU recommended,3 4 2 97514 HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis236 36 Computational Biologypropose a datasetmemory needed: 2GB, GPU type: NVIDIA, GPU amount: 1,3 4 2 97649 JaxMARL: Multi-Agent RL Environments and Algorithms in JAX520 20 Reinforcement Learning â†’ Multi-agentpropose a datasetmemory needed: 8GB RAM minimum, GPU: 1 NVIDIA GPU (recommended),2 4 1 97713 WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks164 6 Theory â†’ Reinforcement Learning and Planningpropose a datasetmemory needed: 8GB, GPU: 1,0 3 5 93219 HAWK: Learning to Understand Open-World Video Anomalies177 13 Computer Visionâ†’Video Understandingpropose a datasetmemory needed: Not specified, but requires high-performance GPUs for training., GPU: 4 x RTX A6000 48G,3 0 0 94065 NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction117 6 Computer Vision â†’ 3D Reconstructionpropose a datasetmemory needed: At least 8GB, GPU type: NVIDIA GPU, GPU amount: 1 or more,4 4 2 96264 Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models608 47 Generative Models â†’Reasoningpropose an archi- tecturememory needed: 16GB, True, GPU type: NVIDIA, GPU amount: 1,1 4 11 96897 BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models244 4 Optimization â†’ Large Scale, Parallel and Distributedpropose a datasetmemory needed: 23.5 GB for Llama 3-8B, 21.8 GB for Llama 2-7B, GPU type: RTX3090, GPU amount: 1,6 3 3 94480 InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory335 38 Language â†’ Knowledgepropose an archi- tecturememory needed: Minimum 16 GB GPU memory, GPU type: NVIDIA, GPU amount: 1,4 3 8 93638 Self-playing Adversarial Language Game Enhances LLM Reasoning120 31 Generative Models â†’Reasoningpropose a datasetmemory needed: 40G per GPU, GPU type: A100, GPU amount: 32,0 2 5 94328 QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs351 143 Deep Learning â†’ Attention Mechanismspropose an archi- tecturememory needed: 16 GB, GPU type: NVIDIA A100, GPU amount: 1,17 5 1 95262 MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks105 3 Deep Learning â†’ Algorithmspropose a datasetmemory needed: Not specified, but likely requires significant RAM for training., GPU type: NVIDIA, GPU amount: 4,1 3 8 19 ID Title Stars Cit.# Domain Key Dist. Resource T1 T2 T3 94391 CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns133 15 Time Series propose an archi- tecturememory needed: Minimum of 16 GB RAM, GPU: 1 NVIDIA GPU (e.g., Tesla V100 or equivalent),7 3 4 96893 SegV ol: Universal and Interactive V olumetric Medical Image Segmentation295 48 Computer Vision â†’ Segmentationpropose an archi- tecturememory needed: 16GB, GPU type: NVIDIA, GPU amount: 1,6 3 1 94155 V oxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection[ 107]110 26 Computer Vision â†’ 3D Object Detectionpropose a datasetmemory needed: Not specified, but high memory usage expected due to multi-GPU training, GPU type: NVIDIA A100, GPU amount: 8,2 4 5 97431 Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs122 15 Trustworthy Machine Learningpropose a datasetRequires access to multiple GPUs (50 A800 GPUs recommended); approximately 55,000 GPU hours for experiments.12 0 0 97791 The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data379 2 Multimodal Models propose a datasetmemory needed: 64 GB RAM, GPU: 2 NVIDIA A100,4 3 1 97609 DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks274 20 Datasets and Benchmarkspropose a datasetmemory needed: 1000 GB, GPU: 2 NVIDIA GPUs (e.g., RTX 3090 or equivalent),2 0 0 97674 Needle In A Multimodal Haystack112 20 Multimodal Models propose a datasetmemory needed: not specified, GPU type: NVIDIA, GPU amount: 8,3 0 0 97882 The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning761 8 Physical Models â†’ Physicspropose a datasetmemory needed: 16GB, GPU type: NVIDIA CUDA, GPU amount: 1,3 0 0 C Societal Impact The advancement of AI agents capable of conducting AI research, as facilitated by benchmarks like EXP-Bench, offers positive societal impacts. It might significantly shorten innovation cycles within AI itself and lead to more rapid advancements in machine learning capabilities. While a faster pace of AI development can also democratize research tools and improve overall scientific efficiency, it concurrently amplifies the importance of addressing potential negative societal consequences. On the other hand, the rapid evolution of AI capabilities heightens risks, where we need to be careful about potential misuse, algorithmic bias, and the evolving role of human researchers, alongside the development of robust governance. 20 D Average Scores across All Paper Categories Defintions. Comp. Biology refers to Computational Biology. CV refers to Computer Vision. D & B refers to Datasets and Benchmarks. Gen. Models refers to Generative Models. Proba. Methods refers to Probabilistic Methods. RL refers to Reinforcement Learning. Addendum. Table 6 contains updated values for IA+3.5 Haiku for the Applications and Reinforce- ment Learning categories. Table 6: Average benchmark scores of various models and agents across select task categories. Evaluation performed against EXP-Bench. Category Agent Model D I E IÂ·E C Allâœ“ Allâœ“Â·E Applications OH Nova Pro 19.2 23.9 19.0 0.0 13.9 0.0 0.0 Applications OH o3-mini 9.0 8.0 0.0 0.0 8.3 0.0 0.0 Applications OH 3.5 Haiku 19.2 24.5 8.3 5.6 8.3 0.0 0.0 Applications OH 3.7 Sonnet 9.0 26.8 30.8 7.7 8.3 2.8 0.0 Applications IA Nova Pro 0.0 9.8 5.0 0.0 0.0 0.0 0.0 Applications IA 3.5 Haiku 6.8 32.3 33.3 16.7 0.0 0.0 0.0 Applications OH DeepSeek R1 3.1 4.3 0.0 0.0 0.0 0.0 0.0 Causality OH o3-mini 37.8 44.6 22.2 11.1 44.4 11.1 11.1 Causality OH 3.7 Sonnet 36.6 83.1 88.9 66.7 44.4 0.0 0.0 Causality IA 3.5 Haiku 23.1 40.6 40.0 0.0 20.0 0.0 0.0 Causality IA Nova Pro 0.0 11.7 20.0 0.0 0.0 0.0 0.0 Causality OH 3.5 Haiku 48.7 36.6 0.0 0.0 33.3 0.0 0.0 Causality OH Nova Pro 17.7 14.9 0.0 0.0 11.1 0.0 0.0 Causality OH DeepSeek R1 10.0 18.7 0.0 0.0 0.0 0.0 0.0 Comp. Biology OH o3-mini 37.0 30.3 11.1 0.0 44.4 11.1 0.0 Comp. Biology IA Nova Pro 0.0 16.7 20.0 0.0 0.0 0.0 0.0 Comp. Biology IA 3.5 Haiku 3.2 7.8 0.0 0.0 0.0 0.0 0.0 Comp. Biology OH Nova Pro 31.2 29.3 0.0 0.0 33.3 0.0 0.0 Comp. Biology OH 3.5 Haiku 4.8 9.7 0.0 0.0 11.1 0.0 0.0 Comp. Biology OH 3.7 Sonnet 4.8 11.1 0.0 0.0 11.1 0.0 0.0 Comp. Biology OH DeepSeek R1 12.2 22.1 0.0 0.0 0.0 0.0 0.0 CV OH Nova Pro 24.8 20.9 42.9 0.0 28.2 0.0 0.0 CV OH 3.7 Sonnet 18.5 28.9 18.2 9.1 21.2 0.0 0.0 CV OH o3-mini 11.5 9.3 5.1 2.6 12.8 0.0 0.0 CV OH 3.5 Haiku 15.9 28.3 4.5 0.0 12.8 0.0 0.0 CV OH DeepSeek R1 5.8 11.2 0.0 0.0 2.6 0.0 0.0 CV IA Nova Pro 0.0 5.3 28.6 0.0 0.0 0.0 0.0 CV IA 3.5 Haiku 6.4 17.4 0.0 0.0 0.0 0.0 0.0 D & B IA Nova Pro 0.0 0.0 0.0 0.0 0.0 0.0 0.0 D & B OH o3-mini 0.0 46.5 0.0 0.0 0.0 0.0 0.0 D & B OH DeepSeek R1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 D & B OH 3.5 Haiku 0.0 0.0 0.0 0.0 0.0 0.0 0.0 D & B OH 3.7 Sonnet 19.0 89.5 50.0 50.0 0.0 0.0 0.0 D & B OH Nova Pro 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Deep Learning OH o3-mini 17.6 14.1 13.4 0.9 20.5 0.0 0.0 Deep Learning OH 3.5 Haiku 20.2 25.3 12.8 1.3 15.2 0.0 0.0 Deep Learning OH 3.7 Sonnet 17.2 37.4 37.1 5.7 14.3 0.9 0.0 Deep Learning OH DeepSeek R1 7.4 6.3 0.0 0.0 2.2 0.0 0.0 Deep Learning IA Nova Pro 0.0 9.2 21.7 0.0 0.0 0.0 0.0 Deep Learning IA 3.5 Haiku 9.4 16.9 14.9 0.0 0.0 0.0 0.0 Deep Learning OH Nova Pro 16.4 16.5 0.0 0.0 16.1 0.0 0.0 Gen. Models OH o3-mini 23.6 27.9 30.4 17.4 29.0 6.5 4.3 Gen. Models OH 3.7 Sonnet 17.6 25.8 26.1 13.0 7.4 0.0 0.0 Gen. Models IA 3.5 Haiku 5.4 24.8 45.5 27.3 3.0 0.0 0.0 Gen. Models IA Nova Pro 0.0 13.9 10.0 0.0 0.0 0.0 0.0 Gen. Models OH DeepSeek R1 9.2 19.2 0.0 0.0 0.0 0.0 0.0 Gen. Models OH Nova Pro 14.7 16.7 0.0 0.0 9.7 0.0 0.0 21 Gen. Models OH 3.5 Haiku 23.6 31.0 0.0 0.0 9.7 0.0 0.0 Language OH o3-mini 16.3 8.5 13.3 0.0 20.0 0.0 0.0 Language IA 3.5 Haiku 4.4 11.7 80.0 0.0 13.3 0.0 0.0 Language OH 3.7 Sonnet 4.6 22.9 20.0 6.7 6.7 0.0 0.0 Language IA Nova Pro 0.0 2.1 33.3 0.0 0.0 0.0 0.0 Language OH DeepSeek R1 6.4 0.0 0.0 0.0 0.0 0.0 0.0 Language OH 3.5 Haiku 8.5 11.5 0.0 0.0 13.3 0.0 0.0 Language OH Nova Pro 8.4 4.3 0.0 0.0 6.7 0.0 0.0 Multimodal OH o3-mini 10.9 23.7 13.6 4.5 13.6 0.0 0.0 Multimodal OH Nova Pro 18.9 32.8 16.7 0.0 13.6 0.0 0.0 Multimodal OH 3.5 Haiku 15.5 17.7 0.0 0.0 13.6 0.0 0.0 Multimodal OH 3.7 Sonnet 19.7 27.0 18.2 9.1 9.5 0.0 0.0 Multimodal OH DeepSeek R1 4.6 9.1 0.0 0.0 9.1 0.0 0.0 Multimodal IA Nova Pro 0.0 17.1 50.0 0.0 0.0 0.0 0.0 Multimodal IA 3.5 Haiku 3.0 25.3 25.0 0.0 0.0 0.0 0.0 Optimization OH o3-mini 21.5 25.2 25.5 6.4 25.5 0.0 0.0 Optimization OH 3.7 Sonnet 18.2 35.8 52.0 20.0 18.6 0.0 0.0 Optimization OH 3.5 Haiku 25.6 28.1 11.5 0.0 12.8 0.0 0.0 Optimization OH Nova Pro 17.1 21.0 50.0 0.0 10.6 0.0 0.0 Optimization OH DeepSeek R1 9.5 18.6 3.8 0.0 6.4 0.0 0.0 Optimization IA Nova Pro 1.4 9.8 19.2 0.0 3.2 0.0 0.0 Optimization IA 3.5 Haiku 8.9 31.4 60.0 12.0 2.5 0.0 0.0 Physical Models OH o3-mini 60.0 23.0 66.7 0.0 66.7 0.0 0.0 Physical Models OH Nova Pro 51.7 0.0 50.0 0.0 33.3 0.0 0.0 Physical Models OH DeepSeek R1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Physical Models OH 3.5 Haiku 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Physical Models OH 3.7 Sonnet 0.0 16.7 33.3 0.0 0.0 0.0 0.0 Physical Models IA Nova Pro 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Proba. Methods OH 3.5 Haiku 49.0 32.0 33.3 0.0 66.7 0.0 0.0 Proba. Methods IA Nova Pro 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Proba. Methods OH o3-mini 28.7 49.3 100.0 0.0 0.0 0.0 0.0 Proba. Methods OH DeepSeek R1 0.0 23.7 0.0 0.0 0.0 0.0 0.0 Proba. Methods OH 3.7 Sonnet 86.0 86.0 100.0 0.0 0.0 0.0 0.0 Proba. Methods OH Nova Pro 45.0 11.0 0.0 0.0 66.7 0.0 0.0 Proba. Methods IA 3.5 Haiku 0.0 57.0 0.0 0.0 0.0 0.0 0.0 RL OH 3.7 Sonnet 18.3 48.2 27.3 21.2 17.6 2.0 3.0 RL OH o3-mini 23.5 34.8 15.7 2.0 27.5 3.9 0.0 RL OH 3.5 Haiku 27.7 41.4 11.5 0.0 17.6 0.0 0.0 RL IA 3.5 Haiku 3.0 29.0 24.0 0.0 2.2 0.0 0.0 RL OH DeepSeek R1 5.0 10.3 0.0 0.0 2.0 0.0 0.0 RL IA Nova Pro 0.0 8.6 9.7 0.0 0.0 0.0 0.0 RL OH Nova Pro 17.9 28.9 0.0 0.0 13.7 0.0 0.0 Social Aspects OH Nova Pro 15.5 20.2 0.0 0.0 27.8 0.0 0.0 Social Aspects OH o3-mini 21.7 22.8 16.7 0.0 22.2 0.0 0.0 Social Aspects OH 3.5 Haiku 18.9 23.4 5.6 0.0 11.1 0.0 0.0 Social Aspects IA Nova Pro 0.0 18.7 16.7 0.0 0.0 0.0 0.0 Social Aspects OH 3.7 Sonnet 11.1 23.5 100.0 100.0 0.0 0.0 0.0 Social Aspects IA 3.5 Haiku 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Social Aspects OH DeepSeek R1 9.7 9.1 0.0 0.0 0.0 0.0 0.0 Theory IA Nova Pro 0.0 11.7 33.3 0.0 0.0 0.0 0.0 Theory IA 3.5 Haiku 0.0 16.7 0.0 0.0 0.0 0.0 0.0 Theory OH o3-mini 0.0 11.2 16.7 0.0 0.0 0.0 0.0 Theory OH 3.7 Sonnet 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Theory OH DeepSeek R1 13.8 3.3 0.0 0.0 0.0 0.0 0.0 Theory OH Nova Pro 16.7 17.0 0.0 0.0 0.0 0.0 0.0 Theory OH 3.5 Haiku 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Time Series OH o3-mini 24.0 37.9 7.7 0.0 30.8 0.0 0.0 Time Series OH 3.7 Sonnet 16.5 65.4 61.5 30.8 15.4 0.0 0.0 Time Series IA Nova Pro 0.0 23.6 12.5 0.0 0.0 0.0 0.0 22 Time Series IA 3.5 Haiku 19.5 34.5 37.5 37.5 0.0 0.0 0.0 Time Series OH Nova Pro 16.7 10.0 0.0 0.0 15.4 0.0 0.0 Time Series OH DeepSeek R1 6.3 8.7 0.0 0.0 0.0 0.0 0.0 Time Series OH 3.5 Haiku 19.2 15.2 0.0 0.0 0.0 0.0 0.0 Trustworthy ML OH 3.5 Haiku 15.5 21.3 0.0 0.0 25.0 0.0 0.0 Trustworthy ML IA Nova Pro 0.0 4.0 16.7 0.0 0.0 0.0 0.0 Trustworthy ML OH o3-mini 6.7 3.8 8.3 0.0 0.0 0.0 0.0 Trustworthy ML OH Nova Pro 20.3 1.5 12.5 0.0 0.0 0.0 0.0 Trustworthy ML OH 3.7 Sonnet 12.2 20.8 16.7 0.0 0.0 0.0 0.0 23 E Extended EXP-Bench Examples This section presents two extended examples from EXP-Bench: a question concerning robust detection in collaborative perception under imperfect localization, and a question focused on implementing a Time Delay Neural Network (TDNN) for automatic speech recognition. Each example details the experimentâ€™s objective, methodology, relevant source code, and expected outcomes. The examples also include an analysis of agent performance on completing the task. E.1 Example 1: Robust Detection in Collaborative Perception This example question was extended from the paper An Extensible Framework for Open Heteroge- neous Collaborative Perception. The objective of this experiment is to assess whether HEAL ( HEterogeneous ALliance) can maintain robust detection performance under realistic conditions of imperfect localization, when Gaussian noise is added to the agentsâ€™ poses. The experiment maintains constant variables such as the dataset (OPV2V-H) and the model architecture (HEAL). The independent variables are the position noise and rotation noise, while the dependent variable is the modelâ€™s detection performance matrices (AP30, AP50, and AP70). Experimental groups test the addition of Gaussian rotation and position noise at levels of 0, 0.2, 0.4, and 0.6 meters/degrees to accurate poses. The results will contribute to evaluating the robustness of a cooperative perception model under conditions of imperfect localization. EXP-Bench extends this task from the original paper section 5.3 QUANTITATIVE RESULTS and utilizes the source code: /workspace/opencood/tools/inference_w_noise.py from the GitHub repository https://github.com/yifanlu0227/HEAL. Note that /workspace/ refers to the working directory from the agentâ€™s initialization context. The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome. This is illustrated in Fig 7a. The agentâ€™s task is to use the provided GitHub repository, with the source code masked, to conduct this experiment. To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig 7b. Evaluation of the task includes design, conclusion, and setup evaluation. The conclusion appears in the â€˜expected outcomeâ€™ in Fig 7a. Design and setup evaluation are based on â€˜design complexityâ€™ and â€˜requirementsâ€™ respectively, shown in Fig 8. An example agent output using the bedrock-us-anthropic-claude-3-7-sonnet-20250219-v1-0 LLM as a backbone is showcased here. We perform a diff operation between the code generated by the agent and the original source code. As this agent reconstructs several files to fulfil the task requirement, we focus on a diff operation between the core reconstructed file ( evaluate_robustness.py ) and the source file ( inference_w_noise.py ), shown in Fig 9. The two files share the same functional goal and have a similar overall structure; however, the agent performs invalid operations in another file (reproduce_exp_bench.sh ), leading to a failure in completing the task. The detailed reasoning provided by the judge is illustrated in Fig 10. 24 (a) The formulation of the task question. (b) Instructions provided to the agent. Figure 7: Task Fields for Example 1. 25 Figure 8: Evaluation of the design and setup for the Extended Task in Example 1. 26 Figure 9: Example 1â€™s Git diff comparing the masked source file and the agent-reconstructed source code. Red highlights indicate deletions, while green highlights represent additions. 27 Figure 10: Error Analysis and Comprehensive Explanation of the agentâ€™s failure to complete the task in Example 1. 28 E.2 Example 2: Time Delay Neural Network for ASR This example is extended from the paper Zipformer: A Faster and Better Encoder for Automatic Speech Recognition. The objective of this experiment is to implement a Time Delay Neural Network (TDNN) that achieves a Word Error Rate (WER) of less than 1% on the test set. This setup focuses on con- structing a TDNN model with three Conv1d layersâ€”each followed by ReLU activation and Batch Normalization â€”and a final linear layer to produce log probabilities for phoneme classes. The dataset ( yesno ), model type (TDNN), loss function (Connectionist temporal classification), and feature extraction method (23-dimensional fbank features) are held constant. Independent variables include the model architecture, training hyperparameters (e.g., learning rate, weight decay), and number of epochs, while the dependent variable is the WER obtained during evaluation. This task emphasizes practical training and decoding using k2â€™sone_best_decoding method and eval- uates performance using the WER metric, targeting values below 1%. EXP-Bench extends this task beyond the baseline speech recognition example by formalizing an end-to-end pipeline us- ing code modules: /workspace/egs/yesno/ASR/tdnn/model.py,train.py,decode.py, and asr_datamodule.py from the Github repository https://github.com/k2-fsa/icefall. The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome. This is illustrated in Fig 11a. Again, the agentâ€™s task is to use the provided GitHub repository, with the source code masked, to conduct this experiment. To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig 11b. Similarly, evaluation of the task includes design, conclusion, and setup evaluation. The conclusion appears in the â€˜expected outcomeâ€™ in Fig 11a. Design and setup evaluation are based on â€˜design complexityâ€™ and â€˜requirementâ€™ respectively, shown in Fig 12. For the agent performance in this example, we make use of an agentâ€™s output using the bedrock-us- amazon-nova-pro-v1-0 LLM backbone. We perform a diff operation between the code generated by the agent and the original implementation files provided in the baseline. Since the agent restructures multiple modules to accomplish the speech recognition task, our analysis focuses on a diff between the core model implementation file ( model.py ) and the original reference. The agent correctly builds the TDNN model and integrates it with the training and decoding pipeline. The two versions of model files share a similar architectural skeleton, but differ in details such as layer configuration and parameter initialisation. The differences are shown in Fig 13. 29 (a) The formulation of the task question. (b) Instructions provided to the agent. Figure 11: Task fields for Example 2. 30 Figure 12: The design and setup evaluation of the extended task in Example 2. 31 Figure 13: Example 2â€™s Git diff of the masked source file and the agent reconstructed source code. In the diff, red highlights are deletions. Green highlights are additions. 32 F Dataset Curation and Benchmark Construction Details To ensure the quality and integrity of EXP-Bench, we developed the curation pipeline through a careful, iterative process. Each component was prototyped, tested on real papers, and refined based on manual inspection by collaborators. This allowed us to isolate and address specific failure modes incrementally, steadily increasing curation throughput without compromising accuracy. Several representative issues that were patched in our final pipeline are documented in Table 7. Manual validation was also aided by the availability of ground truth from the papers and open-source code repositories themselves, making the verification process straightforward. The EXP-Bench team is committed to the long-term maintenance and growth of the dataset. We actively monitor feedback and bug reports via GitHub and HuggingFace issue trackers and will address any concerns raised by the community post-release. All data is hosted on both platforms to ensure accessibility and stability, with potential plans to replicate the dataset on archival storage for long-term preservation. To foster transparency, reuse, and critical engagement, the dataset is released under the permissive Creative Commons Attribution 4.0 license, and all code under the MIT license. We encourage the community to explore, build upon, and challenge EXP-Bench as an open and evolving resource. Table 7: Examples of extraction issues identified that were subsequently patched in the final pipeline. Task Component Issue Actual Example QuestionThe hypothesis is a statement instead of a questionBaDExpert outperforms baseline defenses in backdoor detection on CIFAR10, achieving significantly higher AUROC (near 99%). Conclusion data mentioned in the hypothesisSpecifically, can PDF achieve around 34.64% lower MACs compared to PatchTST and 74.38% lower MACs...? Masked sourceMasked source doesnâ€™t exist \"source\": [\"/workspace/- topomlp_setA_r50_w_otransform.py\"...] Included masked source with wrong pathMuSc has musc.py under workspace/model/ but the source file indicates it under workspace/example/ RequirementsSteps are too specific Run the evaluation script for the baseline EV A-CLIP ViT-B/16 model using distributed processing with 8 GPUs... Asking the agent to use a masked source scriptMerge the trained models using the heal_tools.py script (/workspace/opencood/tools/heal_tools.py:115- 130) Invalid operation Analyze execution outcomes from Table 4, comparing... Expected outcomeConclusion not aligned with the paperâ€™s findingsN/A Method / Usage Instruction / Agent InstructionMentioned specific parts of the paper (tables or figures)The scripts will log metrics including mean rewards and standard deviations, which can be compared with the reported results in Table 2 of the paper. Required hyperparameters not given in the agent instructionSet appropriate model architecture parameters (encoder layers, attention heads, dimensions) Invalid operations Collect and analyze performance results from Table 3,... Time and Cost Expenditure. During the initial phasesâ€”before our curation pipeline was final- izedâ€”each paper required roughly two hours of manual effort. This involved a full read-through (with emphasis on evaluation sections), task-by-task verification, and iterative pipeline corrections to ensure compatibility. The process included checking GitHub repositories, assessing setup validity and complexity, and verifying alignment with the paperâ€™s descriptions. Once the pipeline was fully 33 constructed and refined based on feedback, manual validation time dropped to around 20 minutes per paper, primarily to confirm alignment. Only minor adjustments were rarely needed, and we expect this time to decrease further in future deployments. LLM-related extraction costs varied by task type and count, averaging approximately $60 USD per paper. For extraction, we used o3-mini-2025-01- 01-preview for the main task extraction and claude-3-7-sonnet-20250219-v1:0 for implementation extraction. Costs were primarily driven by input tokens, as the models required full paper texts and codebases to perform accurate extraction. 34 G Extended Analysis on Prevalent Agent Failure Patterns Some overlap between categories may exist, as the classification was performed by an LLM. Table 8: Agents fail in diverse ways across different phases of experimentation, measured across all agent and model evaluations. Phase Failure Type Prevalence (%) conclusion Missing Conclusion Content 26.18 conclusion Incorrect Conclusion Interpretation 19.66 conclusion Incomplete Conclusion Outcome Statement 14.43 conclusion Extraneous Details 7.77 conclusion Missing Conclusion Analysis 4.35 conclusion Missing Comparative Conclusion Analysis 4.03 conclusion Minor Omission of Specific Details 3.47 conclusion Incorrect Numeric Conclusion 3.21 conclusion Mismatched Conclusion Format 2.7 conclusion Error Message Output 2.67 conclusion Incomplete Conclusion with Missing Exp. Findings 2.14 conclusion Conclusion Diverges from Expected Emphasis 1.6 conclusion Missing Comparative Analysis 0.8 conclusion Missing Quantitative Performance Metrics 0.8 conclusion Missing Visualization Details 0.56 conclusion Incomplete Performance Evaluation 0.53 conclusion Missing Numerical Equivalence Verification 0.53 conclusion Missing Trend Analysis 0.53 conclusion Naming Inconsistency Output 0.53 conclusion Conclusion Partially Matching with Numerical Deviations 0.27 conclusion Deviation in Saturation Point Conclusion 0.27 conclusion Inconsistent ASR Reporting 0.27 conclusion Missing Conclusion Analysis on Attack Budget Effects 0.27 conclusion Missing Diminishing Returns Analysis 0.27 conclusion Missing Methodological Innovation Discussion 0.27 conclusion Missing Performance Evaluation Metrics 0.27 conclusion Missing Submission Format Specification 0.27 design Incomplete or Misclassified Design Variables 16.05 design Omission of Required Design Variables 19.84 design Complete Omission of Exp. Design Variables 13.1 design Incorrect Design Specification Details 8.32 design Incomplete Exp. Design Details 7.67 design Irrelevant Procedural Additions 7.62 design Missing Design Variable Information 3.83 design Inclusion of Extraneous Factors 3.64 design Incorrect Parameter Details 3.18 design Partial Omission of Constant Variables 2.75 design Incomplete Constant Variable Specification 3.61 design Partial Fulfillment of DV 1.93 design Error Message Returned Instead of Design Information 1.27 design Incomplete Differentiation of Constant and Ind. Variables 1.27 design Missing Dependent Variable Tracking 1.06 design Incomplete Exp. Design Specification 0.64 design Incomplete Specification of Design Variables 0.64 design Missing Hyperparameter Design Details 0.64 design Partially Complete Design Variable Specification 0.64 design Missing Design Formatting Details 0.42 design Missing Design Variables Details 0.42 design Missing Explicit Variable Labeling 0.42 design Missing Configuration File Variable 0.21 design Missing Input Format Details 0.21 35 design Omission of Exp. Configuration Details 0.21 design Omission of Fixed Block Partition 0.21 design Partial Design Variable Extraction with Misclassification 0.21 exec Environment/Dependency Configuration Errors 29.38 exec Execution Script and File Errors 23.84 exec Missing Dependency Error 11.9 exec Missing Setup Script File 6.95 exec Tensor Operation Execution Error 3.22 exec Syntax Error in Execution Environment 2.86 exec Missing Input Data File 2.27 exec Missing Required Attribute in Execution 2.27 exec Missing Evaluation Output Files 1.82 exec Missing Requirements File 1.82 exec Runtime Indexing Error During Generation 1.82 exec Insufficient Shared Memory in DataLoader Execution 1.41 exec Execution Environment Warning: Root Privilege Usage 1.36 exec Incorrect Dependency Import in Execution 1.36 exec Dependency Version Conflict 0.91 exec Docker Execution Failure 0.91 exec Incomplete Results Saving Impl. 0.91 exec Incorrect Dataset Loading 0.91 exec Incorrect Function Argument Handling 0.91 exec Missing Hugging Face API Token Authentication 0.91 exec Missing Performance Metrics and Argument Parsing 0.91 exec Missing Trust Remote Code Flag in Execution Environment 0.91 exec Missing Setup Script File 0.45 setup Missing Essential Impl. Components 39.71 setup Incomplete Evaluation Metric Impl. 2.15 setup Missing Critical Exp. Setup Details 1.88 setup Incomplete Data and Preprocessing Setup 1.83 setup Missing Command Line Argument Parsing 1.58 setup Incomplete Exp. Setup Impl. 1.49 setup Incomplete Training Regimen Impl. 1.47 setup Incomplete Comparative Setup Features 1.25 setup Missing Modular Helper Functions 1.13 setup Incomplete Dataset Splitting Setup 1.04 setup Missing Comparative Evaluation Methods 1.02 setup Naming Inconsistencies Components 1.02 setup Missing Detailed Architectural Parameters 0.91 setup Incorrect Model Initialization 0.9 setup Missing Optimizer Configuration 0.9 setup Incomplete Evaluation Procedure 0.79 setup Missing Critical Import Statements 0.79 setup Missing Essential Library Imports 0.56 setup Incomplete Results Saving Impl. 0.45 setup Incomplete or Misplaced Setup Impls. 0.45 setup Incorrect Dependency Import 0.45 setup Misconfigured Exp. Infrastructure 0.45 setup Missing C++ Acceleration Integration 0.45 setup Missing Distributed Training Parameters 0.45 setup Missing Hardware/Device Configuration 0.45 setup Missing Training Pipeline Configuration 0.45 setup Incorrect Evaluation Metric Impl. 0.37 setup Incorrect Forward Method Impl. 0.35 setup Hard-Coded Configuration Instead of YAML Loading 0.34 setup Incomplete Benchmark Configuration 0.34 setup Incomplete Rendering Pipeline Impl. 0.34 setup Incorrect Model Architecture 0.34 setup Incorrect Testing Dataset Usage 0.34 36 setup Misconfigured Exp. Setup Parameters 0.34 setup Missing Configuration File Loading 0.34 setup Missing Critical Function Impls. 0.34 setup Missing Critical Quantization Procedures 0.34 setup Missing Essential Parameter Initializations 0.34 setup Missing Intermediate Data Reuse Mechanism 0.34 setup Missing Loss Function and Evaluation Metric Setup 0.34 setup Missing Model Architectures 0.34 setup Missing Model Evaluation Mode Invocation 0.34 setup Missing Model and Dataset Integration 0.34 setup Missing Reproducibility Measures 0.34 setup Missing Feedback Mechanism 0.24 setup Environment/Dependency Configuration Errors 0.23 setup Faulty Dataset Integration 0.23 setup Faulty Training Script Logic 0.23 setup Impl. Mismatch with Setup Specification 0.23 setup Incomplete Benchmarking Function Impl. 0.23 setup Inconsistent Configuration 0.23 setup Incorrect File Naming/Structure 0.23 setup Insufficient Exp. Setup Impl. 0.23 setup Missing Chain-of-Thought Module 0.23 setup Missing Conditional Model Initialization 0.23 setup Missing Dimensionality Reduction Impl. 0.23 setup Missing Evaluation on Validation Data 0.23 setup Missing Exp. Entry Point Impl. 0.23 setup Missing Exp. Resumption Mechanism 0.23 setup Missing Explicit Data Loader 0.23 setup Missing Explicit Model Arch. for Inner Optimization 0.23 setup Missing Final Agent Return Impl. 0.23 setup Missing Final Test Validation 0.23 setup Missing Finalization Message 0.23 setup Missing GPT-based Evaluation Component 0.23 setup Missing GPU Batch Size Adjustment 0.23 setup Missing Initial Policy Evaluation 0.23 setup Missing Logging Mechanism 0.23 setup Missing Loop Termination Mechanism 0.23 setup Missing Model and Transform Initialization 0.23 setup Missing Multi-GPU Result Merge 0.23 setup Missing Multiple Runs for Statistical Significance 0.23 setup Missing Periodic Evaluation 0.23 setup Missing Pretrained Model Loading 0.23 setup Missing Real-World Benchmark Dataset 0.23 setup Missing Regularization Term 0.23 setup Missing Scalability Testing Configuration 0.23 setup Missing Test Cases 0.23 setup Missing Visualization Impl. 0.23 setup Missing Visualization Impl. 0.23 setup Synthetic Dataset Used Instead of Specified Dataset 0.23 setup Incomplete Defense Testing Setup 0.14 setup Missing Critical Evaluation Computation Steps 0.14 setup Missing Custom CI Test Impl. 0.12 setup Missing Hydra-based Exp. Runner and Plotting Script 0.12 setup Missing Key Exp. Pipeline Steps 0.12 setup Missing Multiple Forecast Horizon Configurations 0.12 setup Missing Performance Comparison 0.12 setup Missing Required Evaluation Script Modifications 0.12 setup Missing Sequence Length Computation 0.12 setup Ambiguous Evaluation Metric Reporting 0.11 setup Deviation from Required Library Usage 0.11 37 setup Faulty Early Stopping Impl. 0.11 setup Faulty Quantization Branch 0.11 setup Flawed Visualization Utility Impl. 0.11 setup Incomplete AstroCLIP Integration 0.11 setup Incomplete Benchmark Directory Setup 0.11 setup Incomplete Exp. Setup Impl. 0.11 setup Incomplete Explanation of Comp. Graph Visualization 0.11 setup Incomplete Extraction of Runtime Configuration 0.11 setup Incomplete Inner Objective Impl. 0.11 setup Incomplete MemoryUnit Impl. 0.11 setup Incomplete Model Architecture Impl. 0.11 setup Incomplete Model Weights Download Handling 0.11 setup Incomplete Optimizer and Scheduler Setup 0.11 setup Incomplete Output Processing 0.11 setup Incomplete Parallel Processing Impl. 0.11 setup Incomplete Prediction and Data Loading Impl. 0.11 setup Incomplete Quantization Benchmark Param. Config 0.11 setup Incomplete Result Logging Impl. 0.11 setup Incomplete Results Saving Impl. 0.11 setup Incomplete Training Loop and Reproducibility Measures 0.11 setup Incomplete Visualization Impl. 0.11 setup Incomplete Visualization Pipeline Impl. 0.11 setup Incomplete or Misplaced Plotting Impl. 0.11 setup Inconsistent Dataset Collection Specification 0.11 setup Inconsistent Feature Extraction Impl. 0.11 setup Incorrect Benchmark Command Structure 0.11 setup Incorrect BlockOptimizer Impl. 0.11 setup Incorrect Class Structure 0.11 setup Incorrect Dataset Loading 0.11 setup Incorrect Exp. Split Configuration 0.11 setup Incorrect Function Signature 0.11 setup Incorrect Hard-coded Parameter Block Impl. 0.11 setup Incorrect Independence Test Impl. 0.11 setup Incorrect Inner Objective Impl. 0.11 setup Incorrect Optimizer Comparison Impl. 0.11 setup Incorrect Spatial Matching Impl. 0.11 setup Ineffective Caching Setup 0.11 setup Insufficient Benchmark Dataset 0.11 setup Insufficient Positional Encoding Impl. 0.11 setup Misconfigured Benchmark Parameters 0.11 setup Misconfigured Warmup Parameters 0.11 setup Mismatch in Benchmark Parameter Settings 0.11 setup Missing Ablation Study Configuration 0.11 setup Missing Alternative OOP API Impl. 0.11 setup Missing Benchmark Data Processing Components 0.11 setup Missing Benchmark Runner Function 0.11 setup Missing Block Parameter Switching Logic 0.11 setup Missing Block-specific Finetuning Parameters 0.11 setup Missing Block-wise Parameter Grouping Impl. 0.11 setup Missing BlockOptimizer Impl. Details 0.11 setup Missing BoT Pipeline Components 0.11 setup Missing Checkpoint Loading Impl. 0.11 setup Missing Checkpoint Saving Impl. 0.11 setup Missing Command-Line Toggle for Final Execution 0.11 setup Missing Comparative Optimization Strategy Impl. 0.11 setup Missing Comparison Visualization Component 0.11 setup Missing Comprehensive Plotting Components 0.11 setup Missing Computational Performance Metrics 0.11 setup Missing Conditional Checks 0.11 38 setup Missing Conversation and Interactive Setup Components 0.11 setup Missing Critical Analysis Components 0.11 setup Missing Critical Benchmark File 0.11 setup Missing Critical CycleNet Components 0.11 setup Missing Critical Exp. Tracking Components 0.11 setup Missing Critical Information Extraction 0.11 setup Missing Critical Model Module Impls. 0.11 setup Missing Custom Optimizer and Trainer Integration 0.11 setup Missing Data Reshaping to Remove Padding 0.11 setup Missing Data Structure Impl. 0.11 setup Missing Dataset Download Script 0.11 setup Missing Dedicated Custom Function for Mesh Extraction 0.11 setup Missing Dedicated Inference Script 0.11 setup Missing Dedicated Quantized KV Cache Decode Impl. 0.11 setup Missing Dedicated Trainable Bundle Methods Impl. 0.11 setup Missing Detailed Exp. Protocol 0.11 setup Missing Dynamic Dataset Configuration 0.11 setup Missing Edge Case Scenario 0.11 setup Missing Embedding Extraction Impl. 0.11 setup Missing Entry Point Script for Exp. Setup 0.11 setup Missing Essential Feature Extraction 0.11 setup Missing Essential Modules and Evaluation Components 0.11 setup Missing Essential Post-Processing Functions 0.11 setup Missing Expected Configuration Output 0.11 setup Missing Expected Conversation Templates 0.11 setup Missing Exp. File Location 0.11 setup Missing Exp. Tracking and Run Naming Mechanisms 0.11 setup Missing Exp. Replication Configuration 0.11 setup Missing Exp. Script Modifications 0.11 setup Missing Explicit Meta-Parameter Definition 0.11 setup Missing Explicit Model Components 0.11 setup Missing Explicit Parameter Configuration 0.11 setup Missing Explicit Return of Submission File Path 0.11 setup Missing Explicit Task List Definition 0.11 setup Missing External Data and Model Weight Downloading 0.11 setup Missing Final Evaluation Routine 0.11 setup Missing Final Output Return 0.11 setup Missing Fine-tuning Orchestration Function 0.11 setup Missing Finetuning Type Loader Impl. 0.11 setup Missing Framework Integration Components 0.11 setup Missing GPU Optimization 0.11 setup Missing GPU Synchronization 0.11 setup Missing GPU Transfer and Synchronization Setup 0.11 setup Missing Gene Expression and Embedding Matching Impl. 0.11 setup Missing Hugging Face API Token Authentication 0.11 setup Missing Implicit Differentiation Step 0.11 setup Missing InfLLM Context Memory Impl. 0.11 setup Missing Inference Prompt Configuration 0.11 setup Missing Input Pre-processing 0.11 setup Missing Input and Key-Cache Quantization Configurations 0.11 setup Missing Integration Components for Advanced Training 0.11 setup Missing Integration Components for Block-wise Optimization 0.11 setup Missing Intermediate Data Reuse Mechanism 0.11 setup Missing Learning Rate Scheduler Impl. 0.11 setup Missing Lightning CLI Training Configuration 0.11 setup Missing Logging Configuration 0.11 setup Missing MLP Layer Size Configuration 0.11 setup Missing Main Execution Function 0.11 setup Missing Mem. Management and Precision Conversion Impl. 0.11 39 setup Missing Memory Precision Conversion Impl. 0.11 setup Missing Memory Saving Metric Calculation 0.11 setup Missing Meta-Buffer Template Retrieval 0.11 setup Missing Model and Transform Initialization 0.11 setup Missing Network State Extraction in Visualization 0.11 setup Missing Normalization Configuration 0.11 setup Missing Normalization Disabling Parameter 0.11 setup Missing Optimality Condition Impl. 0.11 setup Missing Optional Analysis Component 0.11 setup Missing Optional Parameter Impl. 0.11 setup Missing Output Directory and Logging Setup 0.11 setup Missing Performance Metrics and Argument Parsing 0.11 setup Missing Platform-Specific Parameter Handling 0.11 setup Missing Platform-Specific Parameter Impl.s 0.11 setup Missing Parameter Switching and Gradient Checkpointing 0.11 setup Missing Post-Processing Component 0.11 setup Missing Post-Processing and Result Saving Impl. 0.11 setup Missing QMIX Algorithm Configuration 0.11 setup Missing Random Sampling Impl. 0.11 setup Missing Replication Procedures 0.11 setup Missing Required Torch-based Impl.s 0.11 setup Missing Result Storage Configuration 0.11 setup Missing Result Visualization 0.11 setup Missing RevIN Normalization Impl. 0.11 setup Missing Reward Shaping Impl. 0.11 setup Missing Scalability Testing Configuration 0.11 setup Missing Single Scan Visualization Function 0.11 setup Missing Specialized Trainer Integration 0.11 setup Missing Standardized Testing Components 0.11 setup Missing Statistical Evaluation Metrics 0.11 setup Missing Supervised Fine-Tuning Data Integration 0.11 setup Missing Surrogate Model (AutoGluon) Impl. 0.11 setup Missing Symbolic Mathematics Library Impl. 0.11 setup Missing Synchronization of Initial Conditions 0.11 setup Missing Task-Specific Configuration 0.11 setup Missing Testing Dataset Configuration 0.11 setup Missing Testing Procedure 0.11 setup Missing Training Loss Visualization Impl. 0.11 setup Missing Tree Mapping Benchmark Impl. 0.11 setup Missing Unique Run Name and Configuration Adjustments 0.11 setup Missing Video Processing Impl. 0.11 setup Missing Video Processing and Loss Calculation Impl. 0.11 setup Missing Working Directory Change Command 0.11 setup Missing Zero-Shot Classifier Impl. 0.11 setup Missing Zoom-In Inference Impl. 0.11 setup Missing train.sh File 0.11 setup Non-standard Evaluation Script Usage 0.11 setup Omission of Required Benchmark Parameter 0.11 setup Oversimplified Instrumentation 0.11 setup Oversimplified Pipeline Simulation 0.11 setup Missing Separation of Optimization Pipelines 0.11 setup Partial Impl. of Data Splitting and Hyperparameters 0.11 setup Response Parsing Failure 0.11 setup Security and Credential Management Issue 0.11 setup Successful Setup for Performance Monitoring 0.11 setup Missing Forecast Generation Step 0.01 40 H Evaluation Judge As detailed in the main text (Â§4.1), our evaluation framework leverages an LLM-based judge (using o3-mini-2025-01-01-preview ) to assess various aspects of agent performance, including adher- ence to integrity guidelines, experimental design, implementation correctness, and conclusion validity. We provide the full system prompts used by our LLM judge in this section. When we encounter long inputs such as Git diffs and agent logs, the content can sometimes exceed the context window of the o3-mini model. To address this, we segment the input into individual chunks that fit within the modelâ€™s context length. These chunks are then fed iteratively to the judge, with the evaluation results and relevant context from one chunk carried over to the subsequent chunk. This iterative processing ensures that the entire input is evaluated comprehensively without loss of information due to context length limitations. Monitor Prompt. The initial integrity check, performed by the Monitor, uses the following prompt to analyze agent logs for disallowed behaviors (e.g., accessing the research paper directly, performing Git operations, or using hardcoded data). Inspect the log to verify the following criteria: 1. The agent did NOT access (i.e., read) the research paper itself (e.g., the PDF). 2. The agent did NOT perform any Git operations, such as checking out commits, switching branches, or accessing other versions of the repository. 3. The agent did NOT use fake, hardcoded, or mock data in place of real experimental results. In particular, check for any signs of: - Hardcoded or fabricated data values used as results. - Use of mock or placeholder data instead of running actual experiments. - Directly copying or using result data already present in the repository, rather than generating it through experimentation. - Any indication that crucial data for the experiment was not produced by creating and running the intended experimental task. - If you find evidence that the agent faked data or failed to run the experiment, provide the reason why this happened if possible. For example, did the agent or LLM (such as nova pro) refuse to perform the task due to ethical concerns, security restrictions, or because the task involved potentially malicious actions? If the log contains an explicit refusal, error message, or explanation for why the agent faked or could not generate the data, include this in your reasoning. Output format exactly as this JSON: { \"paper_access\": <boolean>, \"git_operations\": <boolean>, \"faked_or_nonexperimental_data\": <boolean>, \"setup_monitor_comprehensive_reason\": \"<comprehensive explanation for your decisions on the above keys, including specific evidence from the log and your reasoning process. Clearly justify each decision and reference relevant log entries or the absence thereof. If you found evidence for any key being True, explain exactly what that evidence was and why it led to your conclusion. If you found no evidence, explain how you searched and why you are confident in your answer. If you found evidence that the agent faked data or failed to run the experiment, also explain the reason for this only if possible (e.g., LLM refusal, security/ethical block, or other failure), citing the relevant log entries.>\" } 41 Explanation of output keys: - \"paper_access\": True if the agent accessed the research paper, False otherwise. - \"git_operations\": True if the agent performed any Git operations, False otherwise. - \"faked_or_nonexperimental_data\": True if there is any evidence of fake, hardcoded, mock, or reused (non-experimental) data being used for crucial results, False otherwise. - \"setup_monitor_comprehensive_reason\": A comprehensive explanation describing the evidence and reasoning for the above decisions, referencing specific log entries or the absence of evidence as appropriate. Here is the log: {logs} Design and Conclusion Evaluation Prompt. Once integrity is established, the agentâ€™s experimental design andconclusion are evaluated for conceptual soundness, completeness, and alignment with ground truth. This assessment contributes to the D(design correctness) and C(conclusion correctness) metrics. You are a judge tasked to evaluate a systemâ€™s output against ground truth answers for an experimental design task. Input fields: - design_ground_truth: the correct list of variables (constants, independent, dependent variables). - conclusion_ground_truth: the correct conclusion as a string. - design_output: the predicted design. It may not be formatted as a list; extract and match relevant variable information from its content. - conclusion_output: the predicted conclusion string. Evaluation Instructions: - Design Evaluation: Compare design_output to design_ground_truth. Count how many items in design_output match items in design_ground_truth. Return the percentage of correct items as an integer (e.g., use 75 to represent 75%), along with a short explanation. If applicable, include a failure analysis on what the system got wrong. - Conclusion Evaluation: Compare conclusion_output to conclusion_ground_truth. Return \"correct\" or \"incorrect\" based on semantic match, along with a short explanation. If applicable, include a failure analysis on what the system got wrong. Here is the input: {{ design_ground_truth: {design_gt}, conclusion_ground_truth: {conclusion_gt}, design_output: {design_output}, conclusion_output: {conclusion_output} }} 42 Output format exactly as this JSON: {{ \"design_evaluation_explanation\": \"<short explanation string>\", \"design_score\": <integer from 0 to 100>, \"design_error_analysis\": \"<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>\", \"conclusion_evaluation_explanation\": \"<short explanation string>\", \"conclusion_score\": \"<correct/incorrect>\", \"conclusion_error_analysis\": \"<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>\" }} Implementation Evaluation Prompt. The agentâ€™s implementation is assessed by comparing the ground truth requirements against the Git diff generated by the agent. This evaluation contributes to theI(implementation correctness) metric. You are a judge tasked to evaluate a systemâ€™s experiment setup against ground truth requirements. Input fields: - setup_ground_truth: the correct experiment setup requirements, given as either a list of step-by-step required actions/configs or a natural language description. - setup_ground_truth_scripts: Source scripts that implement the ground truth setup. These may not match the setup_output exactly, but serve as code-level references for what correct setups may look like. - setup_output: the systemâ€™s actual changes, given as a Git diff patch (e.g., modifications to config files, scripts, etc.). Evaluation Instructions: - Setup Evaluation: - Compare setup_output against setup_ground_truth. Go step-by-step through each ground-truth requirement (explicit or implied) one-by-one to see if they are fulfilled in the diff. - Use the setup_ground_truth_scripts as code-level guidance: While the output doesnâ€™t need to match these scripts exactly, use them to ground your judgment of whether the implementation is reasonable and sufficiently close to what a correct implementation should look like. - Focus on intent over exact matching: Variations in filenames or function names are fine if the requirement is fulfilled. - At the end, calculate a score based on the number of requirements that are correctly implemented. - Return: - A score as an integer percentage (e.g., 80 for 80%) representing how many ground truth setup requirements were correctly implemented. - A detailed explanation of the evaluation result. - If applicable, include a failure analysis of what requirements were missed or incorrectly implemented. 43 Here is the input: { \"setup_ground_truth\": {setup_gt}, \"setup_ground_truth_scripts\": {setup_scripts} \"setup_output\": {setup_output}, } Output format exactly as this JSON: { \"setup_evaluation_explanation\": \"<detailed explanation string>\", \"setup_score\": <integer from 0 to 100>, \"setup_error_analysis\": \"<Explanation of what was wrong with the setup, i.e., what requirements were missed or done incorrectly, if applicable>\" } 44 M MÂ·CÂ·D MÂ·CÂ·DÂ·I MÂ·CÂ·DÂ·IÂ·E Judge Metric010203040Average Score (%)OpenHands + o3-mini OpenHands + Claude-3.7 Sonnet OpenHands + Amazon Nova Pro OpenHands + Claude-3.5 Haiku IterativeAgent + Claude-3.5 Haiku OpenHands + DeepSeek R1 IterativeAgent + Amazon Nova ProFigure 14: Stricter metrics reveal lower true correctness. Table 9: Cost-time summary statistics for all evaluated agents and models. OH= OpenHands, IA= IterativeAgent. Med = median, Std = standard deviation, T = time (minutes), C = cost (USD). Agent Model Avg T Med T Q1 T Q3 T Std T Min T Max T OH o3-mini 24.89 23.24 13.93 33.60 16.74 1.70 47.72 OH 3.7 Sonnet 33.53 29.64 16.67 37.72 10.03 2.55 74.04 OH Nova Pro 17.82 15.03 11.85 24.06 9.37 0.64 74.33 OH 3.5 Haiku 25.17 24.23 13.26 32.72 17.38 1.21 37.85 OH DeepSeek R1 32.24 31.40 19.09 38.69 11.82 0.97 60.77 IA 3.5 Haiku 30.24 26.13 19.63 38.24 54.62 0.30 402.84 IA Nova Pro 30.09 26.31 19.51 38.09 27.61 0.17 360.52 Agent Model Avg C Med C Q1 C Q3 C Std C Min C Max C OH o3-mini 0.55 0.35 0.17 1.11 0.56 0.01 1.34 OH 3.7 Sonnet 10.15 7.53 3.04 14.20 6.30 0.03 19.83 OH Nova Pro 1.09 0.77 0.33 2.18 0.93 0.00 2.99 OH 3.5 Haiku 0.68 0.42 0.15 2.68 1.47 0.01 3.24 OH DeepSeek R1 1.55 1.28 0.83 2.49 1.70 0.00 4.08 IA 3.5 Haiku 2.82 1.86 0.52 4.23 2.90 0.02 5.09 IA Nova Pro 3.93 3.26 0.91 5.31 3.65 0.02 6.96 I Additional Analysis We include detailed breakdowns of the analysis performed in Â§4.2. I.1 Conjunctive Evaluation Metrics Analysis In Fig 14, we include details for all agents and models evaluated, as opposed to the subset in Fig 6b. I.2 Cost-Time Distribution We showcase the full costâ€“time distribution in Table 9 in the form of summary statistics. For time- related statistics, although a soft timeout of 40 minutes was enforced during trials, agents occasionally exceeded this limit due to non-compliance with the timeout mechanism. Additionally, both time and cost values can appear unusually low in cases where the agent failed to complete the experiment. 45",
  "text_length": 119320
}