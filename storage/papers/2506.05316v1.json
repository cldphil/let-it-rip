{
  "id": "http://arxiv.org/abs/2506.05316v1",
  "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through\n  Difficulty-targeted Online Data Selection and Rollout Replay",
  "summary": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.",
  "authors": [
    "Yifan Sun",
    "Jingyan Shen",
    "Yibin Wang",
    "Tianyu Chen",
    "Zhendong Wang",
    "Mingyuan Zhou",
    "Huan Zhang"
  ],
  "published": "2025-06-05T17:55:43Z",
  "updated": "2025-06-05T17:55:43Z",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05316v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05316v1  [cs.LG]  5 Jun 2025Improving Data Efficiency for LLM Reinforcement\nFine-tuning Through Difficulty-targeted Online Data\nSelection and Rollout Replay\nYifan Sun‚àó\nUIUCJingyan Shen‚àó\nNew York UniversityYibin Wang‚àó\nUIUC\nTianyu Chen\nUniversity of Texas at AustinZhendong Wang\nMicrosoftMingyuan Zhou\nUniversity of Texas at Austin\nHuan Zhang\nUIUC\nAbstract\nReinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning capabilities.\nHowever, RL fine-tuning remains highly resource-intensive, and existing work has\nlargely overlooked the problem of data efficiency. In this paper, we propose two\ntechniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted\nonline data selection androllout replay . We introduce the notion of adaptive diffi-\nculty to guide online data selection, prioritizing questions of moderate difficulty\nthat are more likely to yield informative learning signals. To estimate adaptive\ndifficulty efficiently, we develop an attention-based framework that requires roll-\nouts for only a small reference set of questions. The adaptive difficulty of the\nremaining questions is then estimated based on their similarity to this set. To\nfurther reduce rollout cost, we introduce a rollout replay mechanism that reuses\nrecent rollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to65% to reach the same level of performance\nas the original GRPO algorithm.\n1 Introduction\nReinforcement learning (RL) has emerged as a promising and increasingly adopted paradigm for\nfine-tuning large language models (LLMs) toward stronger reasoning capabilities [ 9,27,15,50].\nDespite a steady stream of algorithmic improvements [ 48,25,1,47], relatively little attention has\nbeen paid to improving the data efficiency of LLM RL fine-tuning. This gap is particularly concerning\ngiven that RL fine-tuning for LLMs is notoriously computationally expensive1.\nIn this paper, we present two simple yet effective techniques to improve the data efficiency for\nLLM RL fine-tuning: difficulty-targeted online data selection androllout replay . Our goal is to\n‚àóEqual contribution. Correspondence to Yifan Sun < yifan50@illinois.edu > and Huan Zhang\n<huan@huan-zhang.com >.\n1For example, Luo et al. [27] report that training a relatively small 1.5B-parameter model on just 40K\nsamples required over 3,800 A100 GPU hours‚Äîequivalent to approximately $4,500 in compute cost‚Äîeven\nbefore scaling to larger models or longer training horizons.\nPreprint. Under review.\n--- Page 2 ---\nTraining\nQuestionsPredicted\nAdaptive\nDifficultyReference Questions\nQuestions closest to difficulty of 0.5Difficulty -targeted \nOnline Data Selection\nAdaptive\nDifficulty\nUnlabeled QuestionsRollout Replay\nSelectFIFO \nReplay Buffer\nDifficulty \nPredictor\nPolicy\nModel GRPO\nUpdate Current RolloutsRecent Rollouts\nCurrent RolloutsRecent Rollouts\nRollout\nRolloutWrite InRetrieveFigure 1: Overview of our framework combining difficulty-targeted online data selection and\nrollout replay. At each training step, the online data selection module selects training questions with\nadaptive difficulty near 0.5, requiring rollouts on only a small reference set (¬ß4.1, ¬ß4.2). The rollout\nreplay module combines current rollouts with retrieved recent rollouts from a FIFO buffer, and the\ncurrent rollouts are stored into the buffer for future use (¬ß4.3).\nreduce both (1) the number of training steps required to match the performance of the original GRPO\nalgorithm, and (2) the per-step computational cost.\nTotal RL fine-tuning time =Number of training steps| {z }\nReduced by difficulty-targeted online data selection√ó Time per step|{z}\nReduced by rollout replay\nDifficulty-targeted Online Da taSelection ( DOTS )In RL, tasks that are too easy or too difficult\noften provide limited learning signal [ 7,17]. Moreover, since the policy evolves during training, it is\ncrucial to adopt an online and adaptive mechanism for selecting informative data [ 31,34]. To this\nend, we introduce the notion of adaptive difficulty , which measures how likely the current policy\nis to fail on a given question. At each training step, we prioritize questions of moderate adaptive\ndifficulty, as these are most likely to yield meaningful learning signals.\nHowever, computing adaptive difficulty exactly requires executing multiple rollouts per question,\nwhich is computationally expensive. To address this, we propose an attention-based adaptive\ndifficulty prediction framework that efficiently estimates difficulty without generating full rollouts\nfor all questions. At each training step, we generate rollouts only for a small reference set and\ncompute their ground-truth adaptive difficulty. The difficulty of the remaining questions is estimated\nby comparing them to the reference set using similarity-based attention.\nRollout Replay ( RR)To further reduce the cost of rollout generation, we introduce a simple rollout\nreplay mechanism. At each training step, we generate a smaller number of new rollouts and reuse\npast rollouts from recent steps. A bounded First-In-First-Out (FIFO) buffer is maintained to store\nrecent rollouts, from which we retrieve samples to complete each training batch. Although this makes\nGRPO slightly off-policy, our modified GRPO loss ensures stability, and RReffectively reduces\nper-step training time without degrading model performance.\nOur key contributions are summarized as follows:\n‚Ä¢We propose a novel attention-based adaptive difficulty prediction framework that efficiently esti-\nmates how likely a question will be answered incorrectly by the current policy, without requiring full\nrollouts for all questions.\n‚Ä¢Guided by this difficulty prediction framework, we introduce an adaptive difficulty-targeted online\ndata selection mechanism ( DOTS ) for RL fine-tuning, supported by theoretical justifications. DOTS\nprioritizes questions of moderate difficulty relative to the current policy, accelerating convergence.\n‚Ä¢We develop a rollout replay ( RR) mechanism that reuses recently generated rollouts. With a modified\nGRPO training loss, RRremains stable and effectively reduces per-step rollout cost.\n‚Ä¢Extensive experiments on six LLM‚Äìdataset combinations show that our method reduces RL fine-\ntuning time by 25% to65% while achieving the same performance as the original GRPO algorithm.\n2\n--- Page 3 ---\n2 Related Work\nOnline Data Selection Data selection seeks to accelerate training by focusing computation on the\nmost informative examples [ 2]. A key limitation of static data selection methods is their assumption\nthat the importance of samples remains fixed throughout training. Online methods instead periodically\nreselect data during training to reflect the model‚Äôs evolving state [ 43,49,26,16,18]. Such adaptability\nis particularly important in RL, where non-stationary policy updates and environmental interactions\nnecessitate continuous re-evaluation of data utility [31, 44, 33].\nExperience Replay On-policy algorithms such as Proximal Policy Optimization (PPO) [ 38] and\nGroup Relative Policy Optimization (GRPO) [ 39] have become standard in online RL for LLM\nreasoning tasks [ 10]. However, their reliance on freshly collected rollouts for each policy update leads\nto substantial data inefficiency and computational overhead [ 24,4]. Experience replay mitigates this\nby maintaining a fixed-size buffer of recent transitions collected by the policy. Instead of discarding\ndata after a single use, the buffer enables multiple passes over past rollouts, thereby improving sample\nefficiency and stabilizing training [6, 51, 36].\n3 Problem Setup\nGRPO We focus on GRPO algorithm [ 39] with verifiable rewards. For each question q, a group\nofGindividual responses {oi}G\ni=1are sampled from the old policy œÄold. The advantage of the i-th\nresponse is calculated by normalizing the group-level rewards {ri}G\ni=1:\nÀÜAi=ri‚àímean( {ri}G\ni=1). (1)\nCompared to the original formulation proposed by [ 39], we remove the standard deviation normaliza-\ntion, as it has been shown to introduce bias into the optimization process [ 25]. Based on this, the\nGRPO objective can be formulated as:\nJGRPO(Œ∏) =Eq‚àºD,{oi}G\ni=1‚àºœÄŒ∏old(¬∑|q)\n\"\n1\nGGX\ni=11\n|oi||oi|X\nt=1\u0010\nmin\u0010\nri,t(Œ∏)ÀÜAi,clip(ri,t(Œ∏),1‚àíœµ,1 +œµ)ÀÜAi\u0011\n‚àíŒ≤D KL(œÄŒ∏‚à•œÄref)\u0011#\n.\nThe first term represents a clipped policy update, where the ratio term ri,t(Œ∏) =œÄŒ∏(oi,t|q,oi,<t)\nœÄŒ∏old(oi,t|q,oi,<t)\nrepresents the probability ratio between the current and old policies. A KL penalty DKL(œÄŒ∏‚à•œÄref)is\napplied with respect to a fixed reference policy œÄref, weighted by a scalar coefficient Œ≤.\nProblem Formulation LetD={qi}N\ni=1denote the full dataset of Nquestions. In standard GRPO,\neach policy update uses a batch of questions uniformly sampled from D. However, not all questions\ncontribute equally to learning progress. In particular, questions that are either too easy or too hard\nrelative to the current policy‚Äôs capability may yield weak gradient signals, slowing convergence.\nTo address this, we consider an online data selection setting [ 49]. At each step t, a batch Bt‚äÇ D\nof fixed size Bis selected based on the current policy œÄt. Unlike static data pruning, this selection\nis repeated throughout training and adapts to the evolving policy. While more frequent selection\nallows better adaptation, it also increases computational overhead. In practice, when policy updates\nare relatively stable, it is often more efficient to perform data selection every ¬µ(e.g., 2,4,8...) steps,\nselecting a sequence of ¬µbatches to be used in the subsequent updates [26, 41].\n4 Method\nOur proposed method is two-fold: (1) difficulty-targeted online data selection , which reduces the\nnumber of training steps needed to achieve the same performance as the original GRPO algorithm\nby prioritizing questions of moderate difficulty, and (2) rollout replay , which reduces the per-step\ncomputational cost by reusing recent rollouts. Full pseudocode is provided in Algorithm 1.\n3\n--- Page 4 ---\nSolve arcsinùë•ùë•+arcsin2ùë•ùë• =ùúãùúã\n3.What is the value of (2ùë•ùë•+5)2when ùë•ùë•= 3?\nIf ùëìùëì(3)=1 and ùëìùëì(2ùë•ùë•)=2ùëìùëì (ùë•ùë•) for all ùë•ùë•, find \nùëìùëì‚àí1(64).0\n0.875Find the distance between the foci of the \nhyperbola ùë•ùë•2- 6ùë•ùë•- 4ùë¶ùë¶2- 8ùë¶ùë¶= 27.0.375\n1Reference Question\nUnlabeled Question\nSolve arcsin sinùë•ùë•=ùë•ùë•\n2.Predicted\nDifficulty\n0.060.01\n0.41=+0.04\n0.927\nAttention\nScoreGround Truth  \nDifficultyFigure 2: Illustration of our attention-based adaptive difficulty prediction framework. For\neach unlabeled question, we compute its embedding and attend to reference questions with known\ndifficulty. The predicted difficulty is obtained by computing a similarity-weighted average over the\nreference difficulties. In this example, the unlabeled question involves inverse trigonometric functions.\nThe model assigns high attention to a reference question that tests a closely related concept and has a\ndifficulty of 1.0. As a result, the predicted difficulty is also close to 1.0. All difficulty values shown\ncorrespond to adaptive difficulty scores computed at the same step.\nWe propose using adaptive difficulty to guide online data selection. The adaptive difficulty of a\nquestion is defined with respect to the current policy and reflects how challenging the question is for\nthe policy at the current stage of training. Formally, at step t, for each question q, we sample a group\nofGresponses {o(t)\ni}G\ni=1from the current policy and obtain their corresponding rewards {r(t)\ni}G\ni=1.\nThe adaptive difficulty at step tis then computed as:\nd(t)\nq=1\nGGX\ni=1(1‚àír(t)\ni). (2)\nThis value represents the average failure rate under the current policy, with higher values indicating\ngreater difficulty. Unlike static difficulty measures, adaptive difficulty evolves with the policy and\nprovides a dynamic signal for selecting informative training samples.\nChallenge: How to estimate adaptive difficulty efficiently? A key challenge in using adaptive\ndifficulty is that computing it requires executing multiple rollouts to obtain rewards, which is one of\nthe most expensive components in LLM RL fine-tuning2. This raises the question: can we estimate\nadaptive difficulty efficiently without generating rollouts for all questions? To address this, we\npropose a lightweight attention-based adaptive difficulty prediction framework that generates rollouts\nfor only a small reference subset of questions. The adaptive difficulty of the remaining questions\nis then estimated by comparing them to reference questions with known difficulty values using\nsimilarity-based attention, thereby avoiding full rollouts. See Fig. 2 for an illustration.\n4.1 Attention-based Adaptive Difficulty Prediction Framework\nAt each step t, given the full training dataset D, we first sample a small subset of Kquestions (e.g.,\n128 or 256) uniformly at random to form the reference set Dref. For each question in the reference set,\nwe execute rollouts and compute its adaptive difficulty at step t, denoted by {d(t)\ni}K\ni=1using Eq. 2.\nFor the remaining N‚àíKquestions, we aim to estimate their adaptive difficulty without performing\nrollouts. To this end, we employ a lightweight embedding model EŒ∏to encode questions and capture\nsimilarity. We first compute the embeddings {zi=EŒ∏(qi)}K\ni=1for all reference questions. Then,\nfor each unlabeled question q, we compute its embedding zq=EŒ∏(q)and use similarity-weighted\naveraging to estimate its adaptive difficulty:\nai=exp(z‚ä§\nqzi/‚àö\nh)\nPK\nj=1exp(z‚ä§qzj/‚àö\nh),ÀÜd(t)\nq=KX\ni=1aid(t)\ni,\n2For instance, generating rollouts for a batch of 512 samples with maximum sequence length 3072 takes\n109.83 seconds on 8 L40S GPUs, nearly half of the total step time.\n4\n--- Page 5 ---\nwhere his the embedding dimension.\nCalibration To improve the prediction performance, we apply Platt scaling [ 32] that utilizes\nthe information of mean and standard deviation of the reference set difficulties. Specifically, let\n¬µ(t)=1\nKPK\ni=1d(t)\niandœÉ(t)=q\n1\nKPK\ni=1(d(t)\ni‚àí¬µ(t))2denote the mean and standard deviation\nof the reference difficulties at step t. These two statistics are passed through a lightweight MLP\nto produce scale and bias parameters (w(t), b(t)) =MLP([¬µ(t), œÉ(t)]). We then apply a calibrated\ntransformation to the predicted difficulty:\nÀÜd(t)\nq,cal=œÉ \nw(t)¬∑logÀÜd(t)\nq\n1‚àíÀÜd(t)\nq+b(t)!\n,\nwhere œÉ(¬∑)denotes the sigmoid function. The MLP is optimized using binary cross-entropy loss. Full\ntraining details can be found in ¬ß5.1 and Appendix D.1.\n4.2 Adaptive Difficulty-targeted Online Data Selection\nAt each training step, with the adaptive difficulty prediction framework, we now efficiently obtain\nthe adaptive difficulty for all questions in the dataset. Inspired by prior work on goal curriculum in\nRL [7, 45], we prioritize questions whose predicted difficulty is closest to 0.5.\nThis selection strategy selects questions that are neither too easy nor too hard for the current policy,\nas these are intuitively the most informative for learning. Moreover, in GRPO, when all sampled\nrewards for a question are either 0or1, the group-normalized advantage becomes identically zero,\nresulting in no gradient signal. By focusing on questions with predicted difficulty near 0.5, we avoid\nsuch degenerate cases and ensure each update contributes meaningfully to policy gradients, thereby\naccelerating optimization convergence. We formalize this intuition in Theorem 1, which shows that\nthe expected gradient magnitude is maximized when the reward success rate is 0.5(i.e., the adaptive\ndifficulty is also 0.5). A complete proof is provided in Appendix C.\nTheorem 1 (Maximal Gradient Signal at 50% Success Rate) .Consider a single question q, where\nGresponses {oi}G\ni=1are sampled independently from the current policy œÄŒ∏(¬∑ |q). Each response\nreceives a binary reward ri‚àà {0,1}, sampled i.i.d. from a Bernoulli (p)distribution, where p\nrepresents the reward success rate. Define the group-relative advantage ÀÜAias in Eq. 1. We consider\nthe unclipped policy gradient estimator for this question without KL penalty:\ng=GX\ni=1ÀÜAi‚àáŒ∏logœÄŒ∏(oi|q).\nAssume that the likelihood gradients ‚àáŒ∏logœÄŒ∏(oi|q)are independent of the reward distribution\nparameter pand have bounded variance. Then, the expected squared norm of the gradient satisfies:\nE[‚à•g‚à•2]‚àùp(1‚àíp)¬∑(1‚àí1/G),\nand is maximized when p= 0.5.\n4.3 Rollout Replay\nTo further improve data efficiency, we aim to reduce the time cost of each training step . Since\nrollout generation is one of the most expensive components, we adopt a rollout replay mechanism.\nSpecifically, at each training step, we generate new rollouts for only a fraction Œ¥Bof the batch, where\nŒ¥‚àà(0,1], and fill the remaining (1‚àíŒ¥)Bsamples using recent rollouts sampled from a FIFO replay\nbufferDreplay with capacity C.\nHowever, naively reusing past rollouts introduces bias into the policy gradient estimation, as the\ndata is no longer drawn from the current policy. This mismatch can lead to unstable training and\nperformance degradation [ 28]. Inspired by off-policy variants of PPO [ 35], we propose a modified\nGRPO loss using importance sampling with respect to the behavior policy œÄbehavior under which the\n5\n--- Page 6 ---\nAlgorithm 1 GRPO with DOTS andRR\nRequire: Initial policy model œÄŒ∏, reward model rœÜ, math question dataset D, target difficulty Œ±,\nbatch size B, total steps T, reference set size K, sampling temperature œÑ, adaptive difficulty\nprediction framework D IFFPRED (¬ß4.1), fresh rollout fraction Œ¥‚àà(0,1], buffer capacity C\n1:Initialize replay buffer R ‚Üê ‚àÖ\n2:forstep= 1, . . . , T do\n// Adaptive difficulty prediction\n3: Sample reference set Dref‚äÇ D uniformly at random, where |Dref|=K\n4: foreachq‚àà D refdo\n5: Generate Goutputs {oq\ni}G\ni=1‚àºœÄŒ∏old(¬∑ |q)\n6: Compute rewards rq\ni=rœÜ(oq\ni)fori‚àà[G]and difficulty score dq=1\nGPG\ni=1(1‚àírq\ni)\n7: end for\n8: Predict adaptive difficulty ÀÜdq‚Ä≤=DIFFPRED(Dref,{dq}, q‚Ä≤)for all q‚Ä≤‚àà D \\ D ref.\n9: Sample rollout batch Brollout of size Œ¥BfromDaccording to:\nP(q) =exp\u0010\n‚àí|ÀÜdq‚àíŒ±|/œÑ\u0011\nP\nq‚Ä≤‚ààDexp\u0010\n‚àí|ÀÜdq‚Ä≤‚àíŒ±|/œÑ\u0011\n10: foreachq‚àà B rollout do\n11: Generate Goutputs {oq\ni}G\ni=1‚àºœÄŒ∏old(¬∑ |q)\n12: Compute rewards rq\ni=rœÜ(oq\ni)fori‚àà[G]and group average reward ¬Ørq=1\nGPG\ni=1rq\ni\n13: Obtain advantages ÀÜAq\niand policy probabilities œÄŒ∏old(oq\ni|q)fori‚àà[G]\n14: end for\n// Rollout Replay and Update\n15: Sample (1‚àíŒ¥)Bsamples from buffer Rto complete batch B\n16: Update policy œÄŒ∏using modified GRPO objective on batch B\n// Store informative rollouts in buffer\n17: Add(q, oq\ni,ÀÜAq\ni, œÄŒ∏old(oq\ni|q))toRforq‚àà B rollout,¬Ørq/‚àà {0,1}, i‚àà[G]\n18: Remove the oldest samples from Rif buffer is full until |R| ‚â§ C\n19:end for\nrollouts stored in the buffer were originally collected :\nJGRPO-RR (Œ∏) =Eq‚àºD replay,{oi}G\ni=1‚àºœÄbehavior (¬∑|q)\n\"\n1\nGGX\ni=11\n|oi||oi|X\nt=1(min (Àú ri,t(Œ∏)Ai,clip(Àúri,t(Œ∏),1‚àíœµ,1 +œµ)Ai)‚àíŒ≤D KL(œÄŒ∏‚à•œÄref))#\n,\nwhere Àúri,t(Œ∏) =œÄŒ∏(oi,t|q,oi,<t)\nœÄbehavior (oi,t|q,oi,<t). By appropriately controlling the buffer size C, we empirically\nshow that rollout replay improves sample efficiency while maintaining training stability.\nFor each newly generated rollout, if the group average reward is neither 0 nor 1 ( i.e., the sample\nyields a non-zero gradient signal), we store the question, its sampled rollouts, computed advantages,\nand policy probabilities into the buffer. When the buffer is full, the oldest samples are discarded.\n5 Experiments\n5.1 Experimental Setup\nLLMs and RL training datasets We perform GRPO training on three model scales: Qwen2.5-\nMath-1.5B, Qwen2.5-3B, and Qwen2.5-Math-7B [ 46]. We adopt four open-source datasets for\ntraining: MATH [ 13], DeepScaleR-40K [ 27], Open-Reasoner-Zero-57K (ORZ) [ 15] and DeepMath-\n103K dataset [ 12]. For MATH, we include all level 3‚Äì5 questions. For the other three datasets, we\nsample 8K to 10K subsets to construct the training pools. These datasets span diverse mathematical\ndomains and difficulty levels. In total, we experiment with six LLM-training dataset combinations to\nassess the effectiveness of our framework.\n6\n--- Page 7 ---\n1 3 4 244\n046485052Avg Accuracy (%)Qwen2.5 -Math-1.5B+MATH\n1 3 4 244\n046485052Avg Accuracy (%)Qwen2.5 -Math-1.5B+DeepScaleR\n0 1 3 4444648505254Avg Accuracy (%)Qwen2.5 -Math-1.5B+ORZ\n0 2 6 4\nTraining Time (Hours)44464850Avg Accuracy (%)Qwen2.5 -3B+DeepScaleR\n0 2 6 44546474849Avg Accuracy (%)Qwen2.5 -3B+DeepMath\n0 2 8 10 4 6586062646668Avg Accuracy (%)Qwen2.5 -Math-7B+DeepScaleR\nDOTS+RR  (Ours) Original GRPO32.1% ‚Üì 46.9 %‚Üì 29.2 %‚Üì\n35.1 %‚Üì 64.6 %‚Üì 24.9 %‚Üì\nTraining T ime (Hours) Training Time (Hours)Figure 3: Average accuracy curves of our method and original GRPO under various\nLLM‚Äìdataset combinations. The curves show average performance aggregated over four bench-\nmarks with exponential smoothing for visualization. Although both methods are trained for the same\nnumber of steps (60), our curve is shorter in duration because RRreduces the wall-clock time per step.\nOur method consistently outperforms the original GRPO throughout training and reduces the time\nrequired to match the original GRPO‚Äôs final accuracy after 60 training steps by an average of 38.8%.\nImplementation details for adaptive difficulty prediction framework In practice, we observe\nthat off-the-shelf pretrained embedding models struggle to capture fine-grained similarity between\nmath questions. To address this, we freeze the Qwen2.5-Math-1.5B-Instruct backbone [ 46] and train\na 3-layer MLP adapter with a calibration head, using binary cross-entropy loss. We fix the reference\nset size to 256. Additional implementation details are provided in Appendix D.1 and ablation results\non key components are presented in Appendix F.1.\nImplementation details for RL training We employ the verl framework [ 40] to perform GRPO\ntraining. We use a batch size of 512, generate 8 rollouts per prompt, and train for 60 steps in all\nexperiments. The maximum rollout length is set to 3072 tokens for the Qwen2.5-Math series models\n(due to max position embedding limits) and 4096 tokens for Qwen2.5-3B. For reward computation,\nwe use a simple rule-based function based solely on answer correctness, without incorporating any\nformat-related signals. The 1.5B and 3B models are trained on 8 L40S GPUs, while the 7B model is\ntrained on 8 A100 GPUs. For DOTS , data selection is performed every 2 steps. For RR, we choose the\nfresh rollout ratio Œ¥as0.5and buffer capacity C‚àà {256,512}. All RL training hyperparameters are\ndetailed in Appendix E.2.\nEvaluation We adopt the official Qwen2.5-Math evaluation implementation [ 46], setting the\nmaximum generation length to 3072 tokens for Qwen2.5-Math series models and 4096 tokens for\nQwen2.5-3B. Following [ 9,27,25], we evaluate RL model performance on standard mathematical\nreasoning benchmarks, including GSM8K [ 3], MATH500 [ 23], Minerva Math [ 20] and Olympiad-\nBench [ 11]. Accuracy is measured using a sampling temperature of 0.6, top-p of 0.95, and the\nstandard prompt template, consistent with [ 9]. We exclude benchmarks with very few questions,\nsuch as AIME 24 (30 questions) and AMC 23 (40 questions), as their small size results in high\nevaluation variance and unreliable performance comparisons on smaller models [14]. We report the\nfinal performance as the average accuracy across the four benchmarks to mitigate benchmark-specific\nvariance. As the baseline, we use the original GRPO algorithm with uniform batch selection.\n5.2 Main Results\nThe total training costs can be decomposed into two components: the number of steps required to\nreach a target performance and the average wall-clock time per step. Each training step consists of\n7\n--- Page 8 ---\nTable 1: Percentage of training steps saved, per-step time saved, and total training time saved.\nAll metrics are averaged over four mathematical reasoning benchmarks and reported relative to the\noriginal GRPO baseline. All timing measurements are conducted on the same computational devices.\nModel Dataset Steps Saved (%) Time Saved/Step (%) Total Time Saved (%)\nQwen2.5-Math-1.5BMATH 23.33 11.71 32.15\nDeepScaleR 40.00 11.69 46.90\nORZ 20.00 11.66 29.20\nQwen2.5-3BDeepScaleR 26.67 11.52 35.10\nDeepMath 60.00 11.35 64.60\nQwen2.5-Math-7B DeepScaleR 13.33 13.39 24.94\nprocessing a fixed-size batch, primarily including rollout generation and policy update. To ensure a\nfair comparison, each set of experiments is run on the same type and number of GPU devices.\nOur method reaches the same performance as original GRPO with fewer steps. Tab. 1 reports\nthe number of training steps required by DOTS+RR to match the final performance of the original\nGRPO at 60 steps. Across all LLM‚Äìdataset combinations, our method consistently reaches the same\nperformance with substantially fewer steps, achieving reductions ranging from 13.33% to 60.00%.\nThese results demonstrate that DOTS significantly accelerates convergence by prioritizing informative\ntraining samples.\nOur method reduces per-step cost. In our experiments, rollout generation accounts for approxi-\nmately 47%, 46%, and 54% of the total per-step time for the 1.5B, 3B, and 7B models, respectively,\nwith the remaining time primarily spent on policy updates3. By reducing the number of rollouts per\nstep, our RRstrategy leads to a 11%‚Äì13% reduction in per-step training time, as shown in Tab. 1.\nOur method significantly reduces total training cost. As shown in Fig. 3, DOTS+RR (orange)\nconsistently outperforms the original GRPO (blue) throughout training, maintaining higher accuracy\nat almost every step. Across all six settings, DOTS+RR reduces total training time by an average of\n38.8%, with the largest improvement observed on Qwen2.5-3B trained on DeepMath (64.6%).\n5.3 Effectiveness of Adaptive Difficulty Prediction Framework\nTo better understand why our method accelerates training effectively, we examine whether the\nattention-based prediction framework can accurately estimate adaptive difficulty and consistently\nprioritize informative training signals throughout learning.\nTable 2: Average Pearson correlation ( œÅ) be-\ntween predicted and ground-truth adaptive\ndifficulties. Reported as mean ¬±standard de-\nviation over 60 training steps.\nModel Dataset œÅ\nQwen2.5-Math-1.5BMATH 0.7843 ¬±0.0243\nDeepScaleR 0.7244 ¬±0.0318\nORZ 0.7153 ¬±0.0257\nQwen2.5-3BDeepScaleR 0.7789 ¬±0.0191\nDeepMath 0.7029 ¬±0.0082\nQwen2.5-Math-7B DeepScaleR 0.7076 ¬±0.0195The adaptive difficulty prediction aligns with\nevolving training dynamics. To assess the fit-\nness of online predictions, we collect ground-\ntruth adaptive difficulty labels from training\nbatches and compute the Pearson correlation\nbetween these labels and the predicted difficulty\nscores. As shown in Tab. 2, our framework\nconsistently achieves strong Pearson correlation\n(œÅ >0.7) across settings, demonstrating its abil-\nity to effectively track policy behavior through-\nout training. Additional qualitative examples\nare provided in Appendix D.2 for further insight\ninto our attention-based prediction mechanism.\nOur prediction framework effectively filters out uninformative samples. Based on the discussion\nin ¬ß4.2, questions with adaptive difficulty 0or1correspond to cases where all rollouts receive the\nsame reward. In such cases, the group-normalized advantage becomes zero, yielding no gradient\nsignal. We define effective questions as those with adaptive difficulty strictly between 0 and 1. As\nshown in Fig. 4, on average across all LLM-dataset combinations, DOTS selects 25.4% more effective\n3In practice, for longer generation lengths, such as 8K and 16K, rollout time increases substantially, making\nit the dominant computational bottleneck. In such settings, our rollout replay mechanism can yield even greater\nwall-clock savings.\n8\n--- Page 9 ---\n0.20.40.60.81.0Effective Question Ratio+30.4%Qwen2.5-Math-1.5B+MATH\n+29.4%Qwen2.5-Math-1.5B+DeepScaleR\n+19.0%Qwen2.5-Math-1.5B+ORZ\n10 20 30 40 50 600.20.40.60.81.0Effective Question Ratio+33.9%Qwen2.5-3B+DeepScaleR\n10 20 30 40 50 60+19.6%Qwen2.5-3B+DeepMath\n10 20 30 40 50 60+19.2%Qwen2.5-Math-7B+DeepScaleR\nDOTS (Ours) Original GRPOTraining Steps Training Steps Training StepsFigure 4: Ratio of effective questions ( i.e., questions with adaptive difficulties strictly between 0\nand 1) during training across various LLM-training dataset combinations. Annotated percent-\nages indicate the per-step increase in effective question ratio achieved by DOTS compared to original\nGRPO, averaged over the training process. Our adaptive prediction framework consistently selects\nmore informative samples throughout training.\nquestions than the original GRPO, demonstrating a clear advantage in selecting more informative\nquestions throughout training, thereby accelerating convergence.\nOur prediction framework incurs minimal computational overhead and scales efficiently to\nlarge datasets. By caching question embeddings and using a lightweight encoder, our prediction\nframework remains highly efficient‚Äîprocessing 10K samples in just 1.71 seconds at deployment.\n5.4 Analysis and Discussion\nWe further investigate two important questions: ( Q1) What are the individual contributions of DOTS\nandRRto training efficiency? ( Q2) How does DOTS compare to a online data selection method based\nonexternal difficulty labels?\nDOTS accelerates convergence, while RRreduces per-step cost. As shown in Fig. 5(a), training\nguided by DOTS alone yields a steeper learning curve compared to the original GRPO algorithm.\nFig. 5(b) shows that incorporating RRfurther reduces total training time by approximately 20%\nwithout sacrificing performance. These results show that DOTS andRRimprove RL training efficiency\nincomplementary ways.\nDOTS outperforms online data selection method based on external difficulty labels. We compare\nDOTS with an online data selection baseline that relies on external difficulty annotations ( e.g., anno-\ntated by GPT-4o), where training questions are selected at different stages based on static difficulty\nlabels, gradually shifting from easier to harder questions over time. Results in Appendix F.2 show\nthatDOTS consistently outperforms this baseline. Moreover, such methods require expensive external\nlabeling and offer limited adaptability, as they typically follow hand-crafted curricula that demand\nextensive manual design and tuning. In contrast, by leveraging adaptive difficulty, DOTS automatically\nadjusts to the model‚Äôs learning progress without relying on external supervision, enabling more\nscalable and efficient training.\n6 Conclusion\nIn this paper, we propose two techniques to improve the data efficiency of LLM RL fine-tuning:\ndifficulty-targeted online data selection and rollout replay. We hope these effective techniques\nencourage future work to explore data-centric approaches to improving LLM RL fine-tuning.\n9\n--- Page 10 ---\nAvg Accuracy (%)Qwen2.5 -Math -1.5B+MATH Qwen2.5 -Math -1.5B+DeepScaleR Qwen2.5 -Math -1.5B+ORZ\n0 20 40Avg Accuracy (%)Qwen2.5 -Math -1.5B+MATH\n60 0 20 40Qwen2.5 -Math -1.5B+DeepScaleR\n60 0 20 40 60Qwen2.5 -Math -1.5B+ORZ\nDOTS+RR DOTS  Original GRPOSimilar performance,\nbut ~ 20%\n less  time!Similar performance,\nbut ~ 20%\n less  time!Similar performance,\nbut ~ 20%\n less  time!(a)\n(b)444648505254\n444648505254\nTraining Steps Training Steps Training StepsFigure 5: Average accuracy curves of (a) DOTS vs. Original GRPO, and (b) DOTS+RR vs.DOTS\non the Qwen2.5-Math-1.5B model. The curves show average performance aggregated over four\nbenchmarks with exponential smoothing. Note that the x-axis is the number of steps (rather than\ntime). (a) DOTS consistently outperforms the original GRPO and leads to faster convergence. (b)\nIncorporating RRreduces training time by 20% while preserving the performance of DOTS .\nReferences\n[1]Pranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with reinforcement\nlearning. arXiv preprint arXiv:2503.04697 , 2025.\n[2]Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas\nMuennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language\nmodels, 2024. URL https://arxiv. org/abs/2402.16827 , 2022.\n[3]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168 , 2021.\n[4]Nicholas E. Corrado and Josiah P. Hanna. On-policy policy gradient reinforcement learning without\non-policy sampling, 2024. URL https://arxiv.org/abs/2311.08290 .\n[5]Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu\nYu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint\narXiv:2502.01456 , 2025.\n[6]William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland,\nand Will Dabney. Revisiting fundamentals of experience replay. In International conference on machine\nlearning , pages 3061‚Äì3071. PMLR, 2020.\n[7]Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for rein-\nforcement learning agents. In International conference on machine learning , pages 1515‚Äì1528. PMLR,\n2018.\n[8]Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang\nChen, Runxin Xu, et al. Omni-math: A universal olympiad level mathematic benchmark for large language\nmodels. arXiv preprint arXiv:2410.07985 , 2024.\n[9]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 , 2025.\n[10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948 , 2025.\n10\n--- Page 11 ---\n[11] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with\nolympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 3828‚Äì3850, 2024.\n[12] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen\nLiang, Wenxuan Wang, et al. Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable\nmathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456 , 2025.\n[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth\nConference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) .\n[14] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias\nBethge. A sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv\npreprint arXiv:2504.07086 , 2025.\n[15] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-\nreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv\npreprint arXiv:2503.24290 , 2025.\n[16] Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger,\nGauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating deep learning by\nfocusing on the biggest losers. arXiv preprint arXiv:1910.00762 , 2019.\n[17] Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian\nRisi. Illuminating generalization in deep reinforcement learning through procedural level generation. arXiv\npreprint arXiv:1806.10729 , 2018.\n[18] Angelos Katharopoulos and Fran√ßois Fleuret. Not all samples are created equal: Deep learning with\nimportance sampling. In International conference on machine learning , pages 2525‚Äì2534. PMLR, 2018.\n[19] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T \\\" ulu 3: Pushing frontiers in open\nlanguage model post-training. arXiv preprint arXiv:2411.15124 , 2024.\n[20] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving\nquantitative reasoning problems with language models. In Advances in Neural Information Processing\nSystems , 2022.\n[21] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif\nRasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume\nLample, and Stanislas Polu. Numinamath. https://huggingface.co/AI-MO/NuminaMath-CoT , 2024.\nReport available at https://github.com/project-numina/aimo-progress-prize/blob/main/\nreport/numina_dataset.pdf .\n[22] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint\narXiv:2502.11886 , 2025.\n[23] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. In The Twelfth International\nConference on Learning Representations , 2023.\n[24] Qisai Liu, Zhanhong Jiang, Hsin-Jung Yang, Mahsa Khosravi, Joshua Russell Waite, and Soumik\nSarkar. HP3o: Hybrid-policy proximal policy optimization with best trajectory, 2025. URL https:\n//openreview.net/forum?id=PgR6fziYmJ .\n[25] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.\nUnderstanding r1-zero-like training: A critical perspective, 2025. URL https://arxiv. org/abs/2503.20783 .\n[26] Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. arXiv\npreprint arXiv:1511.06343 , 2015.\n[27] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y . Tang, Manan Roongta, Colin Cai, Jeffrey\nLuo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview\nwith a 1.5b model by scaling rl, 2025. Notion Blog.\n11\n--- Page 12 ---\n[28] Wenjia Meng, Qian Zheng, Gang Pan, and Yilong Yin. Off-policy proximal policy optimization. In\nProceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 9162‚Äì9170, 2023.\n[29] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang,\nXiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: A reproduction report on\nslow-thinking reasoning systems. arXiv preprint arXiv:2412.09413 , 2024.\n[30] Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpo‚Äôs effective loss, dynamics, and\nsuccess amplification. arXiv preprint arXiv:2503.06639 , 2025.\n[31] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum\nlearning for reinforcement learning domains: A framework and survey. Journal of Machine Learning\nResearch , 21(181):1‚Äì50, 2020.\n[32] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood\nmethods. Advances in large margin classifiers , 10(3):61‚Äì74, 1999.\n[33] R√©my Portelas, C√©dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum\nlearning of deep rl in continuously parameterized environments. In Conference on Robot Learning , pages\n835‚Äì853. PMLR, 2020.\n[34] R√©my Portelas, C√©dric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Automatic\ncurriculum learning for deep rl: A short survey. arXiv preprint arXiv:2003.04664 , 2020.\n[35] James Queeney, Yannis Paschalidis, and Christos G Cassandras. Generalized proximal policy optimization\nwith sample reuse. Advances in Neural Information Processing Systems , 34:11909‚Äì11919, 2021.\n[36] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox,\nand R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran As-\nsociates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/\nfa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf .\n[37] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2016. URL\nhttps://arxiv.org/abs/1511.05952 .\n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n[39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language\nmodels. arXiv preprint arXiv:2402.03300 , 2024.\n[40] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin\nLin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256 ,\n2024.\n[41] Hwanjun Song, Minseok Kim, Sundong Kim, and Jae-Gil Lee. Carpe diem, seize the samples uncertain\"\nat the moment\" for adaptive batch selection. In Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management , pages 1385‚Äì1394, 2020.\n[42] Milan V ojnovic and Se-Young Yun. What is the alignment objective of grpo? arXiv preprint\narXiv:2502.18548 , 2025.\n[43] Jiachen Tianhao Wang, Tong Wu, Dawn Song, Prateek Mittal, and Ruoxi Jia. Greats: Online selection of\nhigh-quality data for llm training in every iteration. Advances in Neural Information Processing Systems ,\n37:131197‚Äì131223, 2024.\n[44] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly\ngenerating increasingly complex and diverse learning environments and their solutions. arXiv preprint\narXiv:1901.01753 , 2019.\n[45] Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. IEEE transactions on pattern\nanalysis and machine intelligence , 44(9):4555‚Äì4576, 2021.\n[46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024.\n12\n--- Page 13 ---\n[47] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-\nthought reasoning in llms. arXiv preprint arXiv:2502.03373 , 2025.\n[48] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv\npreprint arXiv:2503.14476 , 2025.\n[49] Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining\nwith data influence models. Advances in Neural Information Processing Systems , 37:108735‚Äì108759,\n2024.\n[50] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo:\nInvestigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint\narXiv:2503.18892 , 2025.\n[51] Shangtong Zhang and Richard S Sutton. A deeper look at experience replay. arXiv preprint\narXiv:1712.01275 , 2017.\n13\n--- Page 14 ---\nA Reproducibility\nOur code repository is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl/ .\nB Discussion\nB.1 Limitations and Future Work\nOur adaptive difficulty prediction framework currently relies on randomly sampling a reference set of K\nquestions at each selection step. While effective, the quality of the reference set can influence prediction\nperformance. In principle, one could improve prediction performance by selecting a more diverse reference set\nthat better covers the dataset. Building on this idea, a natural extension is to fix a shared set of Kreference\nquestions (with sufficient coverage) across training, re-evaluating their adaptive difficulty at each selection step.\nMoreover, while we demonstrate the effectiveness of experience replay in the GRPO setting, our current strategy\nis relatively straightforward: we randomly replay rollouts associated with questions whose average reward across\nall rollouts is neither 0 nor 1. A promising direction for further improving efficiency is to incorporate more\nprincipled strategies, such as drawing inspiration from prioritized experience replay [37, 51].\nB.2 Extended Related Work\nRL fine-tuning of LLMs (with verifiable rewards) has recently attracted significant attention, driven in part by\nthe success of DeepSeek-R1 [ 9]. Compared to the original GRPO algorithm [ 39], recent work has proposed\nseveral algorithmic improvements: DAPO [ 48] introduces techniques such as clip-higher, dynamic sampling,\ntoken-level policy gradient loss, and overlong reward shaping; Dr. GRPO [ 25] removes the length and standard\ndeviation normalization terms to improve stability. Beyond these algorithmic enhancements, V ojnovic and Yun\n[42] and Mroueh [30] provide theoretical insights into GRPO, while Zeng et al. [50] and Yeo et al. [47] conduct\nlarge-scale empirical studies across models, sharing key design choices that enable RL fine-tuning.\nIn contrast, relatively little attention has been paid to data-centric approaches. LIMR [ 22] explores a static data\nselection strategy for RL fine-tuning by prioritizing samples based on their alignment with the policy‚Äôs learning\ntrajectory. However, it requires a full training run over the entire dataset beforehand, limiting its practicality. Our\nonline data selection method DOTS is more efficient and applicable in realistic settings. In addition, prior work\nhas not explored the use of rollout replay in GRPO, which we show can further reduce training costs.\nC Proofs\nProof of Theorem 1. We restate Theorem 1 and provide a complete proof below.\nTheorem 1 (Maximal Gradient Signal at 50% Success Rate) .Consider a single question q, where Gresponses\n{oi}G\ni=1are sampled independently from the current policy œÄŒ∏(¬∑ |q). Each response receives a binary reward\nri‚àà {0,1}, sampled i.i.d. from a Bernoulli (p)distribution, where prepresents the reward success rate. Define\nthe group-relative advantage ÀÜAias in Eq. 1. We consider the unclipped policy gradient estimator for this\nquestion without KL penalty:\ng=GX\ni=1ÀÜAi‚àáŒ∏logœÄŒ∏(oi|q).\nAssume that the likelihood gradients ‚àáŒ∏logœÄŒ∏(oi|q)are independent of the reward distribution parameter p\nand have bounded variance. Then, the expected squared norm of the gradient satisfies:\nE[‚à•g‚à•2]‚àùp(1‚àíp)¬∑(1‚àí1/G),\nand is maximized when p= 0.5.\nProof. Letri‚àà {0,1}be the binary reward for response oi, sampled i.i.d. from a Bernoulli (p)distribution.\nDefine the group-relative advantage as:\nÀÜAi=ri‚àí1\nGGX\nj=1rj.\nWe aim to analyze the expected squared norm of the gradient estimator\ng=GX\ni=1ÀÜAi‚àáŒ∏logœÄŒ∏(oi|q).\n14\n--- Page 15 ---\nAssume that the gradients ‚àáŒ∏logœÄŒ∏(oi|q)are independent of the rewards {ri}and are independent across i,\nwith bounded second moment:\nE[‚à•‚àáŒ∏logœÄŒ∏(oi|q)‚à•2]‚â§C <‚àû.\nBecause ÀÜAiandÀÜAjare correlated, we compute the full second moment, where the expectation is taken with\nrespect to œÄŒ∏:\nE[‚à•g‚à•2] =GX\ni,j=1E[ÀÜAiÀÜAj]¬∑E[‚àáŒ∏logœÄŒ∏(oi|q)‚ä§‚àáŒ∏logœÄŒ∏(oj|q)].\nBy assumption, the log-likelihood gradients are zero-mean, independent, and identically distributed across i:\nE[‚àáŒ∏logœÄŒ∏(oi|q)‚ä§‚àáŒ∏logœÄŒ∏(oj|q)] =(\nV, i =j,\n0, iÃ∏=j.\nSo,\nE[‚à•g‚à•2] =V¬∑GX\ni=1E[ÀÜA2\ni].\nWe now compute E[ÀÜA2\ni]. Let ¬Ør:=1\nGPG\nj=1rj, then:\nE[ÀÜA2\ni] =E[(ri‚àí¬Ør)2] = Var( ri‚àí¬Ør) = Var( ri) + Var(¬Ø r)‚àí2 Cov( ri,¬Ør).\nSince ri‚àºBernoulli (p)andrjare i.i.d.,\nVar(ri) =p(1‚àíp),Var(¬Ør) =p(1‚àíp)\nG,Cov(ri,¬Ør) =p(1‚àíp)\nG.\nSubstitute in:\nE[ÀÜA2\ni] =p(1‚àíp) +p(1‚àíp)\nG‚àí2¬∑p(1‚àíp)\nG=p(1‚àíp)\u0012\n1‚àí1\nG\u0013\n.\nTherefore,\nE[‚à•g‚à•2] =V¬∑G¬∑p(1‚àíp)\u0012\n1‚àí1\nG\u0013\n,\nwhich is maximized when p= 0.5.\nD Details of Adaptive Difficulty Prediction Framework\nD.1 Design and Implementation Details\nThe core of our adaptive difficulty prediction framework lies in obtaining proper embeddings to enable attention-\nbased weighted prediction, as described in Section 4.1. To achieve this efficiently, we freeze the Qwen2.5-Math-\n1.5B-Instruct model as the backbone and augment it with a lightweight adapter and a calibration head.\nThe adapter is a GELU-activated MLP with three hidden layers, each containing 896 units and a dropout rate of\n0.1. A LayerNorm is applied to the projection output to stabilize training. The calibration head is a two-layer\nMLP that takes the mean and standard deviation of reference set difficulties as input. The first output passes\nthrough a Softplus activation to yield the scale parameter w(t), while the second is transformed by a Tanh\nactivation to produce a bounded bias term b(t), as defined in Section 4.1.\nWe collect training data from a set of LLMs that are disjoint from our policy models. These include Qwen2.5-\nInstruct and Qwen2.5-Math-Instruct series [ 46], Eurus-2-7B-PRIME [ 5], Mathstral-7B-v0.14, DeepSeek-R1-\nDistill-Qwen-1.5B [ 9], DeepScaleR-1.5B-Preview [ 27], and Qwen2.5-7B-SimpleRL-Zoo [ 50]. For each model,\nwe sample query questions and reference questions from math datasets and compute their adaptive difficulty\nas supervision labels. Each training instance consists of a query question q, a reference set {(qi, di)}K\ni=1with\nknown difficulty scores, and a ground-truth difficulty label dq. Repeating this procedure across models yields\nthe training dataset Dpred-train .\nWe train the adapter and calibration head using the standard binary cross-entropy loss:\nLBCE=‚àí1\n|Dpred-train|X\n(q,{(qi,di)},dq)‚ààD pred-trainh\ndqlogÀÜdq,cal+ (1‚àídq) log(1 ‚àíÀÜdq,cal)i\n,\nwhere ÀÜdq,calis the calibrated predicted difficulty for the query question.\n4https://huggingface.co/mistralai/Mathstral-7B-v0.1\n15\n--- Page 16 ---\nTable 3: Qualitative example illustrating similarity-based attention mechanism in adaptive\ndifficulty prediction. The table shows one unlabeled question and its top- and bottom-ranked\nreference questions by attention score. High-attention references (red) typically share similar concepts\nand difficulty with the target question ( e.g., rhombus and incircle geometry), while low-attention\nreferences (blue) diverge in topic and are significantly easier.\nData Source: DeepScaleR\nUnlabeled Question [adaptive diff. = 1.000, predicted score = 0.907]\nIn the rhombus ABCD , point Qdivides side BC in the ratio 1 : 3 starting from vertex B, and point Eis the\nmidpoint of side AB. It is known that the median CFof triangle CEQ is equal to 2‚àö\n2, andEQ=‚àö\n2. Find\nthe radius of the circle inscribed in rhombus ABCD .\n# Attention Score Adaptive Diff. Reference Question\n1 0.487 1.000 Rhombus ABCD has‚à†BAD < 90‚ó¶.There is a point Pon the\nincircle of the rhombus such that the distances from Pto the\nlinesDA, AB, andBC are9,5,and16,respectively. Find the\nperimeter of ABCD.\n2 0.093 1.000 Circle œâ1with radius 3 is inscribed in a strip Shaving border\nlinesaandb. Circle œâ2within Swith radius 2 is tangent\nexternally to circle œâ1and is also tangent to line a. Circle œâ3\nwithin Sis tangent externally to both circles œâ1andœâ2, and is\nalso tangent to line b. Compute the radius of circle œâ3.\n. . .\n255 0.000 0.125 A package of milk with a volume of 1 liter cost 60 rubles.\nRecently, for the purpose of economy, the manufacturer reduced\nthe package volume to 0.9 liters and increased its price to 81\nrubles. By what percentage did the manufacturer‚Äôs revenue\nincrease?\n256 0.000 0.125 Given tan\u0000\nŒ±‚àíœÄ\n4\u0001\n= 2, find the value of sin\u0000\n2Œ±‚àíœÄ\n4\u0001\n.\nD.2 Qualitative Examples\nTab. 3 presents a qualitative example from the Qwen2.5-3B model, showing one unlabeled question alongside\nreference questions with the highest and lowest attention scores. The example demonstrates that our difficulty\nprediction framework assigns higher attention to reference questions that share key mathematical topics and\nstructures ( e.g., rhombus, incircle), while down-weighting unrelated questions.\nE Implementation Details\nE.1 Training Datasets and Models\nOur experiments involve three model sizes: Qwen2.5-Math-1.5B, Qwen2.5-3B, and Qwen2.5-Math-7B [ 46].\nWe adopt four open-source datasets of mathematical reasoning for RL fine-tuning:\n‚Ä¢MATH [13]:This dataset contains 12,500 competition-level problems from sources such as AMC and\nAIME, spanning seven mathematical subjects and five difficulty levels. Following [ 22,50], we merge\nthe train and test splits and retain only Level 3‚Äì5 questions. These are guaranteed to have no overlap\nwith the MATH500 benchmark to prevent data contamination.\n‚Ä¢DeepScaleR-40K [27]: A collection of approximately 40,000 curated mathematical problems from\nAMC (pre-2023), AIME (1984‚Äì2023), Omni-MATH [ 8], and Still [ 29]. Deduplication is performed\nusing embedding-based retrieval, and ungradable problems are filtered to ensure high-quality reward\nsignals. We randomly sample 10,240 problems for training.\n‚Ä¢Open-Reasoner-Zero-57K (ORZ) [15]: This dataset includes 57,000 high-quality reasoning problems\nsourced from AIME (up to 2023), AMC, MATH, Numina-MATH [ 21], and Tulu3 MATH [ 19].\nExtensive cleaning via rule-based and LLM-based filters ensures evaluability and difficulty balance.\nWe sample 8,192 problems for training.\n16\n--- Page 17 ---\n‚Ä¢DeepMath-103K [12]: A large-scale dataset focused on high-difficulty mathematical problems,\nconstructed with rigorous data decontamination procedures to support reliable benchmark evaluation.\nWe also sample 8,192 problems for training.\nE.2 RL Fine-tuning Details\nTab. 4 summarizes the hyperparameters used in our GRPO training. We adopt the same configuration across all\nexperiments. Following [ 48,15], we remove the KL regularization terms. For reward computation, we use a\nsimple rule-based function based solely on answer correctness, without incorporating any format-related signals.\nSpecifically, a reward of 1 is assigned for exact matches with the reference answer, and 0 otherwise. Answer\nmatching is implemented using the Math-Verify library5. We adopt a standard chain-of-thought (CoT) prompt\ntemplate, provided in Tab. 5.\nTable 4: Detailed RL fine-tuning recipes.\nOptimizer AdamW\nTotal Batch Size 512\nLearning Rate 1e-6\nLR Schedule Constant\nWeight Decay 0\nWarm-up Ratio 0\nNumber of Steps 60\nMax Prompt Length 1024\nMax Rollout Length 3072/4096\nNumber of Rollouts Per Prompt 8\nRollout Sampling Temperature 0.6\nRollout Sampling Top-p 0.95\nGPU Hardware 8x NVIDIA L40S/8x NVIDIA A100\nTable 5: Prompt template used for RL fine-tuning and evaluation. The placeholder <question> is\nreplaced with the actual mathematical question during fine-tuning and evaluation. Special tokens\n\"<|im_start|>\" and \"<|im_end|>\" are omitted for clarity.\nsystem\nLet‚Äôs think step by step and output the final answer within \\boxed{}.\nuser\n<question>\nassistant\nE.3 Implementation Details of DOTS andRR\nWe present the detailed hyperparameter settings of Algorithm 1 in Tab. 6. For DOTS , data selection is performed\nevery two steps during RL fine-tuning.\nTable 6: Hyperparameters of DOTS andRR.\nTarget Difficulty Œ± 0.5\nReference Set Size K 256\nData Sampling Temperature œÑ 1e-3\nFresh Rollout Fraction Œ¥ 0.5\nBuffer Capacity C 256/512\nE.4 Evaluation Details\nConsistent with RL fine-tuning, we use a sampling temperature of 0.6, top-p of 0.95, and the same prompt\ntemplate. We evaluate model performance on four commonly-used mathematical reasoning benchmarks and\nreport the average accuracy to mitigate benchmark-specific variance. We exclude benchmarks with very few\nquestions, such as AIME 24 (30 questions) and AMC 23 (40 questions), as their limited size leads to high\nevaluation variance and unreliable performance comparisons for smaller models [14].\n5https://github.com/huggingface/Math-Verify\n17\n--- Page 18 ---\n0 20 40 60\nTraining Steps4446485052Avg Accuracy (%)Qwen2.5-Math-1.5B+DeepScaleR\n0 20 40 60\nTraining Steps4445464748495051Avg Accuracy (%)Qwen2.5-3B+DeepScaleR\nDOTS (Ref Size=256) DOTS (Ref Size=128) Original GRPOFigure 6: Average accuracy curves of DOTS (Ref Size = 256), DOTS (Ref Size = 128), and Original\nGRPO on Qwen2.5-Math-1.5B and Qwen2.5-3B. The curves show average performance aggregated\nover four benchmarks with exponential smoothing for visualization. Note that the x-axis is the number\nofsteps (rather than time). The results show that a reference set size of 128 achieves performance\ncomparable to that of 256, indicating the robustness of our method to smaller reference sets.\n‚Ä¢GSM8K [3]: A test set of 1,319 grade school math word problems from the GSM8K dataset, requiring\nmulti-step arithmetic reasoning.\n‚Ä¢MATH500 [23]: A widely used subset of the MATH test split [ 13]. These problems are excluded from our\nMATH training data.\n‚Ä¢Minerva Math [20]: A set of 272 undergraduate-level science and math questions from MIT OpenCourseWare.\n‚Ä¢OlympiadBench [11]: A benchmark of 675 problems from international math olympiads and physics contests.\nF Additional Experimental Results\nF.1 Ablation Study on the Adaptive Difficulty Prediction Framework\nOff-the-shelf embeddings fail to capture difficulty structure. We evaluate a baseline that directly\nuses frozen embeddings from the Qwen2.5-Math-1.5B-Instruct model without any training or calibration. In\ncontrast, our framework incorporates trained adapter layers and a calibration head. As shown in Tab. 7, our\nframework consistently achieves significantly higher Pearson correlation with ground-truth adaptive difficulty\nacross all settings. The poor performance of the off-the-shelf baseline highlights the necessity of further adapter\nlayers and calibration for accurately predicting question difficulty.\nTable 7: Ablation study on training with adapter and calibration. Comparison of average Pearson\ncorrelation ( œÅ) between predicted scores and ground-truth adaptive difficulties, reported as mean\n¬±standard deviation over 60 training steps. Results show that training with adapter layers and\ncalibration significantly improves prediction performance.\nModel DatasetOff-the-shelf\nEmbeddingOur Framework\n(With Adapter Layers + Calibration)\nQwen2.5-Math-1.5BMATH 0.2682 ¬±0.0207 0.7843 ¬±0.0243\nDeepScaleR 0.2064 ¬±0.0518 0.7244 ¬±0.0318\nORZ 0.1598 ¬±0.0266 0.7153 ¬±0.0257\nQwen2.5-3BDeepScaleR 0.2688 ¬±0.0369 0.7789 ¬±0.0191\nDeepMath 0.0671 ¬±0.0168 0.7029 ¬±0.0082\nQwen2.5-Math-7B DeepScaleR 0.1983 ¬±0.0254 0.7076 ¬±0.0195\nDOTS is robust to the size of reference set. We further investigate the impact of the reference set size K\nin RL fine-tuning. Fig. 6 compares the performance of the original GRPO and the DOTS method under reference\nset sizes of 128 and 256, trained with Qwen2.5-Math-1.5B and Qwen2.5-3B on the DeepScaleR dataset. The\nresults show that a reference set size of 128 yields RL performance comparable to that of 256. This indicates that\nour approach is robust to smaller reference sets, enabling more efficient rollout collection without sacrificing RL\nfine-tuning quality.\n18\n--- Page 19 ---\n0 20 40 60\nTraining Steps4446485052Avg Accuracy (%)Qwen2.5-Math-1.5B+DeepScaleR\n0 20 40 60\nTraining Steps4445464748495051Avg Accuracy (%)Qwen2.5-3B+DeepScaleR\nDOTS External Difficulty-based Curriculum Baseline Original GRPOFigure 7: Comparison between DOTS (ours) and an external difficulty-based curriculum baseline.\nThe curves show average performance aggregated over four benchmarks with exponential smoothing\nfor visualization. Note that the x-axis is the number of steps (rather than time). Our method\nconsistently outperforms the baseline.\nF.2 Case Study: Online Data Selection Via External Difficulty-based Curriculum\nWe additionally implement an online data selection baseline that relies on external difficulty annotations.\nSpecifically, we use the DeepScaleR dataset and label each question with GPT-4o-mini, following the difficulty\nannotation prompt introduced in Luo et al. [27]. Each question is annotated 32 times, and the average score is\nused as its final difficulty.\nWe then follow a staged curriculum: in the first third of training steps, batches are sampled from the easiest third\nof the dataset; in the middle third, from the medium-difficulty third; and in the final third, from the hardest third.\nTo ensure a fair comparison of online data selection strategies, we compare this baseline with DOTS (without RR).\nAs shown in Fig. 7, our DOTS method consistently outperforms this baseline on both Qwen2.5-Math-1.5B and\nQwen2.5-3B. We further discuss in the main text the limitations of such approaches, including the high cost of\nannotation and limited adaptability due to their reliance on fixed, hand-crafted curricula.\n19",
  "text_length": 60623
}