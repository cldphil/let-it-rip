{
  "id": "http://arxiv.org/abs/2506.00976v1",
  "title": "Quantization-based Bounds on the Wasserstein Metric",
  "summary": "The Wasserstein metric has become increasingly important in many machine\nlearning applications such as generative modeling, image retrieval and domain\nadaptation. Despite its appeal, it is often too costly to compute. This has\nmotivated approximation methods like entropy-regularized optimal transport,\ndownsampling, and subsampling, which trade accuracy for computational\nefficiency. In this paper, we consider the challenge of computing efficient\napproximations to the Wasserstein metric that also serve as strict upper or\nlower bounds. Focusing on discrete measures on regular grids, our approach\ninvolves formulating and exactly solving a Kantorovich problem on a coarse grid\nusing a quantized measure and specially designed cost matrix, followed by an\nupscaling and correction stage. This is done either in the primal or dual space\nto obtain valid upper and lower bounds on the Wasserstein metric of the\nfull-resolution inputs. We evaluate our methods on the DOTmark optimal\ntransport images benchmark, demonstrating a 10x-100x speedup compared to\nentropy-regularized OT while keeping the approximation error below 2%.",
  "authors": [
    "Jonathan Bobrutsky",
    "Amit Moscovich"
  ],
  "published": "2025-06-01T12:06:31Z",
  "updated": "2025-06-01T12:06:31Z",
  "categories": [
    "cs.LG",
    "stat.CO",
    "stat.ML"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00976v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00976v1  [cs.LG]  1 Jun 2025Quantization-based Bounds on the Wasserstein Metric\nJonathan Bobrutsky\nDepartment of Statistics and Operations Research\nTel Aviv University\njbobrutsky@gmail.com\nAmit Moscovich\nDepartment of Statistics and Operations Research\nTel Aviv University\nmosco@tauex.tau.ac.il\nAbstract\nThe Wasserstein metric has become increasingly important in many machine\nlearning applications such as generative modeling, image retrieval and domain\nadaptation. Despite its appeal, it is often too costly to compute. This has motivated\napproximation methods like entropy-regularized optimal transport, downsampling,\nand subsampling, which trade accuracy for computational efficiency. In this paper,\nwe consider the challenge of computing efficient approximations to the Wasserstein\nmetric that also serve as strict upper or lower bounds. Focusing on discrete\nmeasures on regular grids, our approach involves formulating and exactly solving\na Kantorovich problem on a coarse grid using a quantized measure and specially\ndesigned cost matrix, followed by an upscaling and correction stage. This is done\neither in the primal or dual space to obtain valid upper and lower bounds on the\nWasserstein metric of the full-resolution inputs. We evaluate our methods on\nthe DOTmark optimal transport images benchmark, demonstrating a 10 ×–100 ×\nspeedup compared to entropy-regularized OT while keeping the approximation\nerror below 2%.\n1 Introduction\nThe Wasserstein metric is a basic tool in machine learning with widespread adoption in various do-\nmains, including computer vision, natural language processing, and computational biology [Arjovsky\net al., 2017, Kusner et al., 2015, Schiebinger et al., 2019]. While it is fast to compute in particular\ncases, such as one-dimensional distributions, in general settings computing the Wasserstein metric\ncan be very expensive. For example, consider the calculation of the Wasserstein metric between\nN×Nimages that we treat as discrete measures on a regular grid. Each image has N2pixels, so\ncomputing the Wasserstein metric involves solving a linear program with N2×N2variables and\nΘ(N2)constraints. This is typically done using a network simplex algorithm whose worst-case\nruntime in this case is O(N6logN)[Peyré and Cuturi, 2019]. For 3D signals, the computational cost\nis even worse at O(N9logN). This severely limits the adoption of the Wasserstein metric across\nmany domains and in particular for 2D or 3D discrete signals. As a result of this, many authors\nhave developed fast approximations of the Wasserstein metric [Cuturi, 2013, Deshpande et al., 2018,\nGerber and Maggioni, 2017, Shirdhonkar and Jacobs, 2008].\nWe consider the challenge of designing approximations that also serve as strict bounds on the p-\nWasserstein metric between discrete measures on a regular grid. In this paper we introduce several\nefficient algorithms that are based on quantization (or downscaling) of the inputs onto a coarse grid\nand sum-pooling the measures. We then construct a particular cost matrix for the coarse grid that\nPreprint. Under review.\n--- Page 2 ---\ntakes the original measures into account. This is followed by a correction stage that upscales the\nsolution on the coarse grid to a solution on the original grid and corrects the marginals using iterative\nproportional-fitting The upscaling and correction stage is done separately in the primal and dual\nspaces, to obtain both upper and lower bounds (respectively). Finally, to guarantee the correctness of\nthe bounds without relying on the convergence of the proportional fitting procedure, we introduce an\nadditional total variation correction term.\nWe developed an efficient GPU implementation of the algorithms described in this paper using the\nJAX package and tested it on the DOTmark optimal transport benchmark [Schrieber et al., 2017]. We\ndemonstrate that our algorithms achieve significant computational speedups compared to a baseline\nderived from the entropy-regularized OT. Up to 10 ×for upper bounds and up to 100 ×speed up\nfor the lower bounds, while maintaining a low approximation error (<2%) relative to the exact\nWasserstein distance.\n1.1 Related work\nIn recent years, the efficient computation of the Wasserstein distance has been a focal point of research,\nleading to several innovative approaches. The entropy-regularized Sinkhorn algorithm by Cuturi\n[2013] remains a cornerstone, offering significant computational speedups despite introducing bias.\nAltschuler et al. [2018] advanced this with near-linear time approximation algorithms. Multiscale\nmethods, as discussed by Gerber and Maggioni [2017] and Mérigot [2011], employ hierarchical\nstrategies to enhance computational efficiency. The benefits of the multiscale approach demonstrated\nby Feydy et al. [2021] for analyzing brain tractograms by adapting the Sinkhorn algorithm. Other\nquantization-based methods, notably by Beugnot et al. [2021], improve approximation performance,\nfor sampled continuous measures. For grid data, Solomon et al. [2015] and Chen et al. [2022] exploit\nstructural advantages for speed. In machine learning, Courty et al. [2017] and Alvarez-Melis and Fusi\n[2020] demonstrate optimal transport’s versatility in domain adaptation. Montesuma et al. [2025]\noffer a comprehensive survey of recent advances, underscoring the method’s growing impact.\n2 Background\nNotation We denote the non-negative real numbers by R+and the set of integers {1, . . . , n }by[n].\nThe tensor product is denoted by ⊗whereas pointwise multiplication and division are denoted by\n⊙and⊘, respectively. The Lpnorm of a vector is ∥ · ∥p. The all-ones column vector is 1n∈Rn.\nThe standard vector inner product is denoted by ⟨·,·⟩and we use the same notation for the inner\nproducts of matrices. The support of a matrix A∈Rn×mis the set of indices of nonzero elements\nsupp A={(i, j)∈[n]×[m]|Ai,j̸= 0}. The cardinality of a set Sis denoted by #S.δxis the\nDirac delta function at point x. Complete list of notations used in the paper is provided in Table 7.\n2.1 Optimal Transport\nIn the following, we give a quick review of basic concepts from optimal transport and refer the reader\nto Peyré and Cuturi [2019] for a more thorough introduction. Consider two discrete probability\nmeasures µ,νwith point masses at X={x1, . . . , x n}andY={y1, . . . , y m}respectively. We can\nexpress the measures as a sum of Dirac delta functions,\nµ=nX\ni=1µiδxi, ν =mX\nj=1νjδyj. (1)\nWe identify the measures with their non-negative coefficient vectors µ∈Rn\n+,ν∈Rm\n+. Since µand\nνare probability measures, we must have µ∈Σnandν∈Σmwhere\nΣN:=\b\n(p1, . . . , p N)∈RN\n+:p1+···+pN= 1\t\n(2)\nis the probability simplex. The set of coupling matrices between µandνis\nΠ(µ,ν) :=\b\nπ∈Rn×m\n+\f\fπ1m=µ, π⊤1n=ν\t\n. (3)\nEach coupling can be viewed as a transport plan between µandνwhere πijis the amount of mass\ntransported from xitoyj. The marginal constraints π1m=µmean that the entire source measure\n2\n--- Page 3 ---\nµis transported, whereas the constraints π⊤1n=νmean that this transport results in the target\nmeasure ν. In particular, the set Π(µ,ν)always contains the trivial coupling π⊗:=µ⊗νthat\ndistributes every point mass in Xto all point masses in Yproportionately to ν. Let C∈Rn×m\n+\nbe a ground-cost matrix, where Cijrepresents the cost of transporting a unit mass from xi∈ X\ntoyj∈ Y. The Kantorovich problem is the minimization problem that seeks a cost-minimizing\ncoupling between µandν,\nLC(µ, ν) := min\nπ∈Π(µ,ν)⟨π, C⟩. (4)\nThis is a linear optimization problem with linear constraints. It admits a dual program,\nLC(µ, ν) = max\n(f,g)∈R(C)⟨f,µ⟩+⟨g,ν⟩, (5)\nwhere\nR(C) :={(f,g)∈Rn×Rm|∀i, j:fi+gj≤Cij} (6)\nis the set of admissible dual potentials, also known as Kantorovich potentials. Given a distance metric\nρ:X × X → R+, for any p≥1theWasserstein -pmetric is a metric over the space of discrete\nprobability measures with point masses at X={x1, . . . , x n}, defined as Wp(µ, ν) :=LC(µ, ν)1\np\nwhere Cij=ρ(xi, yj)p.\n2.2 Measure coarsening\nConsider a d-dimensional regular grid X= [N]d. Suppose that Nis a multiple of some scale factor\nκ∈N. In that case we may subdivide the grid along the axes into a set X={X1, . . . , X nd}of\nnon-overlapping hypercubes of cardinality κdthat cover the entire grid X. We define the coarse\ngrid ˜X:={˜x1, . . . , ˜xnd}as the set of all hypercube centers, with ˜xk=mean(Xk). Given a discrete\nmeasure µ∈ΣNdover the grid X, we define a coarsened discrete measure by placing the mass\nassociated with each hypercube at its center point, ˜µ=Pnd\nk=1µ(Xk)δ˜xk.The coarsening of the\nregular grid corresponds to the SumPool andAvgPool operations on the measure and the coordinates,\nrespectively, with size and stride κ.\n3 Methods\nIn this section, we describe several algorithms for computing bounds of the Wasserstein distance\nWp(µ, ν)on a regular grid.\n3.1 Bounds based on Entropy regularized OT\nEntropy regularized optimal transport, also known as Sinkhorn distance [Cuturi, 2013], adds an\nentropy term Hto the primal:\nLε\nC(µ, ν) := min\nπ∈Π(µ,ν)⟨π, C⟩ −εH(π). (7)\nThis makes the problem strongly convex and solvable using Sinkhorn iterations [Knopp and Sinkhorn,\n1967]. In this section we will show how computing entropy regularized OT can be used to construct\nupper and lower bounds on the exact Wasserstein distance.\nLower bound The dual form of the Sinkhorn distance is\nLε\nC(µ, ν) := max\nf∈Rn,g∈Rm⟨f,µ⟩+⟨g,ν⟩ −ε⟨ef/ε, Keg/ε⟩ (8)\nwhere Kij:=e−Cij/εis the Gibbs kernel. The algorithmic solution, defined by the use of a finite\nnumber of iterations tthat achieves some stopping criteria, is known to satisfy a lower bound.\nSummarized in the following proposition.\nProposition 3.1. Letˆf(t)\nε,ˆg(t)\nεbe the iterations of the Sinkhorn distance algorithm in step t∈N.\n⟨ˆf(t)\nε,µ⟩+⟨ˆg(t)\nε,ν⟩ ≤LC(µ, ν) (9)\nas soon as t≥1.\nThis follows directly from Peyré and Cuturi [2019, Propositions 4.5,4.8], so we can define\nWp,ε(µ, ν) := (⟨ˆf(t)\nε,µ⟩+⟨ˆg(t)\nε,ν⟩)1\np≤ W p(µ, ν)for all p≥1.\n3\n--- Page 4 ---\nUpper bound Consider d-dimensional regular grids with side length N∈N,X=Y= [N]d, with\ndiscrete measures µ,ν∈ΣNd. Although the converged regularized optimal coupling\nπ∗\nε= arg min\nπ∈Π(µ,ν)⟨π, C⟩ −εH(π) (10)\ndefines an upper bound on the optimal transport ⟨π∗\nε, C⟩ ≥LC(µ, ν), the algorithmic solution ˆπ(t)\nε\ndoes not. Since the marginals ˆµ(t)\nε= ˆπ(t)\nε1N,ˆν(t)\nε= (ˆπ(t)\nε)⊤1Ndo not identify with the couplings\nµ,ν. We bound the effect of this difference using the weighted total variation. For some x0∈ X,\nusing distance weights w={ρ(x0, x)p}x∈Xthe Wasserstein- pdistance is controlled by weighted\ntotal variation [Villani, 2009],\nT Vw\np(µ, ν) := 21−1\np⟨w,|µ−ν|⟩1\np≥ W p(µ, ν). (11)\nHere| · |is the element-wise absolute value.\nDefining the total variation corrected regularization-based upper bound\nWp,ε(µ, ν) :=⟨ˆπ(t)\nε, C⟩1\np+ ∆ˆµ(t)\nε+ ∆ˆν(t)\nε, (12)\nwhere ∆ˆµ(t)\nε=T Vw\np(ˆµ(t)\nε, µ),∆ˆν(t)\nε=T Vw\np(ν,ˆν(t)\nε)are the marginal corrections with weights\nw={ρ(¯x, xi)}iare taken from the center of the measure ¯x=mean(X). Using the triangle\ninequality for the Wasserstein metric, we can write\nLemma 3.2. Letµ,ˆµ, ν,ˆν∈ΣNddiscrete measures on X= [N]d, and ˆπ∈Π(ˆµ,ˆν)is a coupling\nbetween ˆµandˆν. For p≥1,\nWp(µ, ν)≤ ⟨ˆπ, C⟩1\np+ ∆ ˆµ+ ∆ ˆν (13)\nCombining this with (12) shows that Wp(µ, ν)≤Wp,ε(µ, ν).\nProposition 3.3. Letµ,ˆµ, ν,ˆν∈Σnbe discrete measures in Xandξ >0, satisfying the convergence\ncriteria ∥ˆµ−µ∥1+∥ν−ˆν∥1< ξ. The sum of the marginal corrections is bounded,\n∆ˆµ+ ∆ ˆν<22−2\npξ1\npr (14)\nwhere r:= max x∈X{ρ(¯x, x)}is radius of X.\nProofs for Lemma 3.2 and Proposition 3.3 are provided in Appendix C.1.\n3.2 Weighted-cost upper bound\nIn this subsection we consider an approach to bound Wasserstein distance by down-scaling the grid\nXto˜X=˜Yand the measures to ˜µ,˜ν∈Σndusing regular hypercubes as described in Section 2.2.\nWe define the marginally weighted coarse cost\n¯Ckℓ:=1\nµ(Xk)ν(Yℓ)X\nx∈Xk,y∈Yℓρ(x, y)pµ(x)ν(y). (15)\nIt follows that ¯C=SumPool (C⊙π⊗, κ)⊘SumPool (π⊗, κ), which can be used efficiently for small\nenough grids. Then we compute the optimal coupling for the marginally weighted coarse cost using\nnetwork simplex solver [Bonneel et al., 2011], defining an upper bound\nW⊗\np:=L¯C(˜µ,˜ν)1\np. (16)\nTheorem 3.4. The optimal transport loss under the marginally weighted coarse cost L¯C(˜µ,˜ν)is an\nupper bound to the optimal transport loss LC(µ, ν), and similarly for the Wasserstein distance,\nL¯C(˜µ,˜ν)1\np≥ W p(µ, ν). (17)\nThe proof uses an auxiliary coupling transferring the mass between each pair of sub-regions (Xi, Yj)\nusing the trivial coupling, weighted by a coarse coupling, choosing the optimal coarse coupling. A\ndetailed proof is provided in Appendix C.2.\n4\n--- Page 5 ---\nˆν\nˆµ\nˆπ∈Π(ˆµ,ˆν)/angbracketleftw,|ν−ˆν|/angbracketrightν\nˆν/angbracketleftw,|µ−ˆµ|/angbracketrightµ\nˆµµ\nˆµ\nˆν\nνWp(µ,ˆµ)≤T Vw\np(µ,ˆµ)\nWp(µ,ν)Wp(ˆµ,ˆν)\n≤⟨ˆπ,C⟩1\np\nWp(ˆν,ν)\n≤T Vw\np(ˆν,ν)\nFigure 1: Optimal transport bounds visualization. Left: Illustration of optimal transport between\ndiscrete probability measures. Right: Diagram showing the relationship between measures and their\nWasserstein distances.\n3.3 Min-cost lower bound\nFor the same coarsening, we define the locally minimal cost Cmin\nkℓ:= min x∈Xk,y∈Yℓρ(x, y)p,and\ncompute the optimal coupling for this coarse cost using a network simplex solver, yielding a lower\nbound Wmin\np:=LCmin(˜µ,˜ν)1\np.The following theorem is proved in Appendix C.3.\nTheorem 3.5. The coarse optimal transport cost set by the locally minimal cost, is a lower bound of\nthe optimal transport. LCmin(˜µ,˜ν)≤LC(µ, ν).\n3.4 Primal upscaling upper bound\nIn this approach we upscale an optimal coupling for the coarse cost ˜ckℓ=ρ(˜xk,˜yℓ)pcomputed using\na network simplex solver ˜π∗= arg min˜π∈Π(˜µ,˜ν)⟨˜π,˜C⟩back to the original problem size.\nUp-scaled coupling A coupling matrix πof dimensions nd×ndcan equivalently be represented\nas a2d-tensor of the shape n×n× ··· × n. We formally define operations for reshaping matrices\ninto tensors and back. Let reshape be a cardinality-preserving transformation from A to Bsuch\nthataij=bu1,...,u d, v1,...,v dwhere (u1, . . . , u d)is a multi-index that corresponds to a matrix row by\ni= 1 +Pd\nk=1uknk−1. Similarly the multi-index (v1, . . . , v d)corresponds to the column index j.\nReshaping the coarse optimal coupling ˜π∗into a 2d-tensor ˜P∗, we up-scale the optimal coupling\nusing a normalized positive-valued kernel K, a2d-tensor representing a hypercube of width κ.\nˆP:=˜P∗⊗K. (18)\nBy inversely reshaping the up-scaled tensor ˆPinto an up-scaled matrix ˆπ, we obtain the approximate\nup-scaled coupling. Using a uniform kernel Kis equivalent to nearest-neighbor interpolation.\nLemma 3.6. Using a normalized positive-valued kernel Kensures ˆπrepresents a coupling. Satisfying\nˆπ∈RNd×Nd\n+ andPNd\ni=1PNd\nj=1ˆπij= 1, such that ˆπ∈Π(ˆπ1Nd,ˆπ⊤1Nd).\nIterative proportional fitting The approximate up-scaled coupling is ξ-fitted into ˆπξ=\ndiag(a)ˆπdiag(b)by iterative proportional fitting using Sinkhorn’s theorem , until the marginals\nˆµξ=a⊙(ˆπb),ˆνξ=b⊙(ˆπ⊤a)are converged to ∥ˆµξ−µ∥1+∥ν−ˆνξ∥1< ξ, where a,b∈RNd\n+\nare the vector scale factors. yielding the approximation cWp(µ, ν) :=⟨ˆπξ, C⟩1\np.\n5\n--- Page 6 ---\nUpper bound Using weighted total variation (Equation (11)) we define an upscaling upper bound\nWp(µ, ν) :=cWp(µ, ν) + ∆ ˆµξ+ ∆ ˆνξ, (19)\nTheorem 3.7 (Upscaling Upper Bound) .Wp(µ, ν)is upper bound of the Wasserstein distance,\nWp(µ, ν)≤Wp(µ, ν). (20)\nProof of Theorem 3.7. The up-scaled matrix ˆπ∈Π(ˆπ1Nd,ˆπ⊤1Nd)is normalized as a coupling, by\nLemma 3.6, and ˆπξretains this normalization, by Sinkhorn’s theorem. Wp(µ, ν)is than an upper\nbound of the Wasserstein distance, by Lemma 3.2.\nRemark 3.8.Considering X= [N]dwithL2norm, the radius becomes r=1\n2d1\n2N(see Proposi-\ntion 3.3) such that the weighted total variation correction ∆ˆµ+∆ˆνis at most 21−2\npd1\n2Nξ1\np. Negligible\nforξ≪N−p, and can be ignored for many practical use cases.\n3.5 Dual upscaling lower bound\nWe construct a lower bound for the Wasserstein distance Wp(µ, ν)by solving a down-scaled optimal\ntransport problem using coarsened measures. The coarse optimal Kantorovich potentials are than\nup-scaled using a multi-linear interpolation and improved using a c-transform. Considering the same\nsetting as in Section 3.4, we solve for the optimal potentials of the down-scaled discrete measures\n(˜f∗,˜g∗) = arg max\n(˜f,˜g)∈R(˜C)⟨˜f,˜µ⟩+⟨˜g,˜ν⟩ (21)\nand evaluate the dual transport cost at the original scale by up-scaling the optimal potentials. Up-\nscaling can be performed by any multivariate interpolation method such as nearest-neighbor, spline,\nmulti-linear and polynomial methods. Using an interpolation function R:X ∪˜X →R, the up-scaled\npotential ˆfis defined by\nˆf:={R˜f,˜X(xi)}i∈[Nd]. (22)\nAn important property of the dual formulation is that for every potential f∈Rnwe can easily\nfind a tight potential fc∈Rmsuch that (f,fc)∈ R(C), byfc\nj:= min iCij−fi.This is known\nas a c-transform . It can be shown that repeating this process once more achieves a tight pair\n(fc,fcc)∈ R(C), where fcc\ni:= min jCij−fc\nj. Thus, a lower bound is guaranteed by using the\nc-transform to generate the potential pair from the up-scaled potential f=ˆfccandg=ˆfc, which\nyields the upscaling lower bound\nWp(µ, ν) := (⟨f,µ⟩+⟨g,ν⟩)1\np. (23)\nSince by the admissibility of a c-transformed pair (f,g)∈ R(C), we can write\nProposition 3.9. Considering an approximate potential ˆf∈RNd. Forf=ˆfccandg=ˆfc\n⟨f,µ⟩+⟨g,ν⟩ ≤LC(µ, ν). (24)\n4 Computational complexity analysis\nThe quantization-based bounds involve the following steps: computing the Wasserstein metric on\nthe quantized measures, upscaling the dual potentials or couplings to the original scale, and the\ncalculation of weighted total variation correction terms. The latter is calculated in linear time and\nspace, thus negligible w.r.t. the other steps. In the following, we detail the computational gains\nprovided by the proposed methods.\nDownscaled optimal transport The solution to the Kantorovich problem of the scaled measures\ncan be solved by dedicated linear programming methods, such as the network simplex used in\nBonneel et al. [2011], with O\u0000\nn3dlogn\u0001\ntime complexity [Ahuja et al., 1993]. By solving only for\nthe optimal transport of the coarse measures, we produce a computational speedup of Θ(κ3d)(up to\nlog factors). The space complexity can also be significantly reduced, since one can avoid storing the\nfull cost matrix of size Nd×Nd, by using coarse cost matrices, e.g. ¯C, Cmin, and ˜Cof size nd×nd,\nrealizing a memory gain of Θ(κ2d).\n6\n--- Page 7 ---\nUpscaled optimal coupling The optimal coarse coupling ˜π∗is a sparse matrix with at most 2nd−1\npositive entries [Peyré and Cuturi, 2019, Proposition 3.4]. Thus, the up-scaled approximate coupling\nfrom Equation (18) conserves this sparsity with # supp ˆ π≤κ2d(2nd−1), allowing to calculate the\napproximate optimal transport\n⟨ˆπξ, C⟩=X\n(i,j)∈supp ˆπ(ˆπξ)ijz}|{\naiˆπijbjρ(xi, yj)p. (25)\nwithout impacting the total time and space complexity of the coarse optimal transport solution.\n#C\n# supp ˆ π=N2d\nκ2d(2nd−1)= Θ\u0000\n(N/κ)d\u0001\n. (26)\nLazy c-transform To reduce the memory requirements of Equation (23), we evaluate the c-\ntransform on-demand (i.e. \"lazy\") without storing the entire cost matrix C.\ngj←min\niρ(xi, yj)p−ˆfi (27)\nfi←min\njρ(xi, yj)p−gj (28)\nMethod Time Complexity Space Complexity\nEntropic Regularization-Based Bounds [Lin et al., 2022] ˜O\u0000\n(nκ)2d/ε2\u0001\nO\u0000\n(nκ)2d\u0001\nQuantization-Based Bounds ˜O\u0000\nn3d\u0001\nO\u0000\nn2d\u0001\nTable 1: Complexity of different bounds in terms of the fine-scale cardinality #X=Nd= (nκ)d.\nThe first row corresponds to the methods in Section 3.1 and the second to Sections 3.2 to 3.5.\n5 Experiments\nThe algorithms were implemented in Python and optimized for GPU acceleration using JAX numerical\ncomputation library [Bradbury et al., 2018]. For entropy regularized optimal transport we used OTT-\nJAX [Cuturi et al., 2022] and POT [Flamary et al., 2021] for exact optimal transport. The methods\nwere benchmarked on a machine using an NVIDIA L40 GPU and AMD EPYC 9654 CPU.\nDOTmark The methods were evaluated on 2D images from the discrete optimal transport bench-\nmark [Schrieber et al., 2017], using ρ=L2the euclidean metric, at p={1,2}. To examine the\neffect of the scaling factor, the quantization-based methods were evaluated using κ={2,4}. To\nexamine the effect of the entropic-regularization parameter, the regularization-based methods were\nevaluated using ε={0.001Np,0.004Np}explicitly dependent on Npterm to avoid large ∥C∥∞/ε\ncausing numerical instability [Altschuler et al., 2018], since ∥C∥∞∝Npin our setting. For upper\nbounds, while at p= 1the entropic-regularization upper bound at ε= 0.001Npdelivers the best\napproximation, summarized in Table 2, it does so with significant impact on the computation time, as\nseen in Figure 2. Otherwise, both for upper and lower bound the quantization methods produce the\nbest approximations at κ= 2, while computing with relative time second only to the quantization\nmethods scaled at κ= 4.\nEMDB In the field of structural biology, approximations of the Wasserstein metric are increas-\ningly being used on 2D projection images and 3D volumetric reconstructions of proteins and other\nmacromolecules. Specific applications include molecular alignment, clustering and dimensionality\nreduction, with most methods substituting the Wasserstein metric with a crude approximation that\nis fast to compute [Rao et al., 2020, Riahi et al., 2023, Singer and Yang, 2024, Kileel et al., 2021].\nWe evaluate our algorithms in a challenging 3D alignment setting, where we wish to compute the\nWasserstein- pmetric p∈ {1,2}between rotated 3D density maps of the same molecule. The volu-\nmetric density maps are downloaded from the Electron Microscopy Data Bank (EMDB) [wwPDB\nConsortium, 2024] using the ASPIRE package [Wright et al., 2025]. Figure 4 shows the computed\n7\n--- Page 8 ---\nUpper Bounds Lower Bounds\nWeighted-\nCostPrimal\nUpscalingEntropic\nRegularizationDual\nUpscalingMin-\nCostEntropic\nRegularization\nκ2 κ4 κ2 κ4 ε1 ε4 κ2 κ4 κ2 κ4 ε1 ε4\nClass p\nClassic 1 3.1% 11.0% 9.6% 23.0% 0.9% 5.2% 0.3% 0.7% 10.0% 27.0% 24.0% 88.0%\nImages ±2.0%±7.1%±4.1%±9.9%±0.5%±2.1%±0.2%±0.4%±6.4%±16.0% ±8.3%±15.0%\n21.6% 7.9% 2.2% 8.8% 14.0% 44.0% 0.7% 2.4% 13.0% 33.0% 98.0% 100.0%\n±1.2%±5.2%±1.4%±5.5%±8.8%±25.0% ±0.5%±1.6%±3.7%±8.8% ±8.4% ±0.0%\nMicro- 1 0.9% 3.4% 2.4% 6.5% 0.4% 2.0% 0.4% 0.9% 6.2% 17.0% 9.6% 38.0%\nscopy ±1.7%±5.9%±3.2%±8.4%±0.3%±2.0%±0.5%±0.9%±4.2%±10.0% ±7.1%±22.0%\n20.5% 2.2% 0.7% 2.7% 3.8% 11.0% 0.2% 0.7% 5.5% 16.0% 30.0% 90.0%\n±0.7%±3.5%±1.0%±3.9%±5.9%±18.0% ±0.4%±1.2%±3.4%±8.4%±32.0% ±19.0%\nShapes 1 1.1% 3.6% 3.2% 7.8% 0.7% 2.6% 0.5% 1.0% 7.3% 20.0% 13.0% 51.0%\n±2.2%±4.8%±2.6%±6.3%±2.5%±2.2%±2.8%±3.0%±5.2%±11.0% ±8.1%±22.0%\n21.2% 3.2% 1.4% 3.6% 5.1% 15.0% 0.9% 1.7% 7.7% 20.0% 52.0% 99.0%\n±4.5%±5.0%±4.5%±5.0%±6.2%±17.0% ±5.1%±5.6%±6.1%±9.2%±34.0% ±9.6%\nTable 2: Accuracy comparison of different methods showing the mean ±standard deviation of\nthe relative error computed across all the pairwise distances in the DOTmark class at 128×128\nresolution. Each method is evaluated at different fidelity level κ2= 2 andκ4= 4 and different\nvalues of ε1= 1·10−3Npandε4= 4·10−3Np.\nbounds for rotations between 0◦and180◦of the Plasmodium falciparum 80S ribosome 3D density\nmap [Wong et al., 2014].\nPlots for other molecules are shown in the supplementary material. Dual upscaling (lower bound)\nand weighted-cost (upper bound) methods at κ= 2provide the best approximations. A summary of\nthe computational speed-up is provided in Table 3.\nUpper Bounds Lower Bounds\nWeighted-\nCostPrimal\nUpscalingEntropic\nRegularizationDual\nUpscalingMin-\nCostEntropic\nRegularization\npκ2 κ4 κ2 κ4 ε1 ε4 κ2 κ4 κ2 κ4 ε1 ε4\n1 0.22% 0.12% 1.09% 27.23% 130.95% 144.97% 0.55% 0.33% 0.20% 0.11% 190.88% 197.35%\n±0.06% ±0.26% ±0.34% ±15.99% ±29.46% ±39.00% ±0.21% ±0.21% ±0.06% ±0.29% ±58.51% ±52.12%\n2 0.22% 0.06% 0.30% 0.16% 164.64% 171.42% 0.19% 0.04% 0.21% 0.04% 195.49% 197.76%\n±0.04% ±0.02% ±0.07% ±0.08% ±42.18% ±41.89% ±0.05% ±0.02% ±0.05% ±0.02% ±42.43% ±44.34%\nTable 3: Computation time relative to the exact Wasserstein distance computation for 3D Cryo-EM\ndata at 32×32×32resolution. Results show mean ±standard deviation across rotations.\n6 Conclusion and discussion\nIn this paper, we proposed several methods for computing fast approximations that lower or upper-\nbound the Wasserstein metric between discrete distributions on a regular grid. Our methods are based\non the solution of lower-resolution OT problems that are then upsampled and corrected to yield upper\nand lower bounds to the original problem. Our experiments on 2D images and 3D volumetric data\ndemonstrate significant improvements in computational efficiency and accuracy compared to bounds\nbased on entropic OT. Despite the considerable computational speedups resulting from our methods\ncompared to the only apparent alternative for exact Wasserstein bounds, the methods are still much\nslower compared to many almost linear time state-of-the-art approximation methods practitioners use\nin in practice for such datasets. Looking forward, our approach could be refined and extended by\nexploring different interpolation methods for the upscaling stage and multi-scale approaches. Finally,\ndespite the paper’s focus, the methods could also be extended to domains beyond regular grids such\nas point clouds in Rnand graphs.\n8\n--- Page 9 ---\n10−310−210−1100101\nRelative Time10−410−310−210−1100Relative Errorp = 1\n10−310−210−1100\nRelative Timep = 2\nUpper Bounds\nPrimal Upsacling ( κ= 2)\nWeighted-Cost ( κ= 2)\nPrimal Upsacling ( κ= 4)\nWeighted-Cost ( κ= 4)\nPrimal Entropic Reg. ( ε= 0.001Np)\nPrimal Entropic Reg. ( ε= 0.004Np)Figure 2: Efficiency of Wasserstein Upper Bounds\n10−310−210−1100\nRelative Time10−310−210−1100Relative Errorp = 1\n10−310−210−1100\nRelative Timep = 2\nLower Bounds\nDual Upsacling ( κ= 2)\nMin-Cost (κ= 2)\nDual Upsacling ( κ= 4)\nMin-Cost (κ= 4)\nDual Entropic Reg. ( ε= 0.001Np)\nDual Entropic Reg. ( ε= 0.004Np)\nFigure 3: Efficiency of Wasserstein Lower Bounds\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)102103Wasserstein Distance Boundsp = 1\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)101102p = 2\nMethods\nWeighted-Cost ( κ2)\nWeighted-Cost ( κ4)\nPrimal Upsacling ( κ2)\nPrimal Upsacling ( κ4)\nPrimal Entropic Reg. ( ε1)\nPrimal Entropic Reg. ( ε4)\nDual Upsacling ( κ2)\nDual Upsacling ( κ4)\nMin-Cost (κ2)\nMin-Cost (κ4)\nDual Entropic Reg. ( ε1)\nDual Entropic Reg. ( ε4)\nFigure 4: Wasserstein distance bounds between rotated 3D density maps of the 80S ribosome. The\nleft panel shows an isosurface plot of the 3D density map that we rotated around the z-axis. The\nother two panels compare the different algorithms for producing upper and lower bounds on the\nWasserstein- pmetric. (center) p= 1; (right) p= 2.\n9\n--- Page 10 ---\nReferences\nR. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network Flows: Theory, Algorithms, and Applications .\nPrentice-Hall, Inc., USA, Feb. 1993. ISBN 978-0-13-617549-0.\nJ. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal\ntransport via Sinkhorn iteration, Feb. 2018.\nD. Alvarez-Melis and N. Fusi. Geometric Dataset Distances via Optimal Transport. In Ad-\nvances in Neural Information Processing Systems , volume 33, pages 21428–21439. Cur-\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nf52a7b2610fb4d3f74b4106fb80b233d-Abstract.html .\nM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein Generative Adversarial Networks. In Proceed-\nings of the 34th International Conference on Machine Learning , pages 214–223. PMLR, July 2017.\nURL https://proceedings.mlr.press/v70/arjovsky17a.html .\nA. Bartesaghi, A. Merk, M. J. Borgnia, J. L. S. Milne, and S. Subramaniam. Prefusion structure of\ntrimeric HIV-1 envelope glycoprotein determined by cryo-electron microscopy. Nature Structural\n& Molecular Biology , 20(12):1352–1357, Dec. 2013. ISSN 1545-9985. doi:10.1038/nsmb.2711.\nG. Beugnot, A. Genevay, K. Greenewald, and J. Solomon. Improving approximate optimal transport\ndistances using quantization. In Proceedings of the Thirty-Seventh Conference on Uncertainty in\nArtificial Intelligence , pages 290–300. PMLR, Dec. 2021. URL https://proceedings.mlr.\npress/v161/beugnot21a.html .\nN. Bonneel, M. van de Panne, S. Paris, and W. Heidrich. Displacement interpolation using Lagrangian\nmass transport. ACM Transactions on Graphics , 30(6):1–12, Dec. 2011. ISSN 0730-0301.\ndoi:10.1145/2070781.2024192.\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: Composable transformations of\nPython+NumPy programs, 2018. URL http://github.com/jax-ml/jax .\nY . Chen, C. Li, and Z. Lu. Computing Wasserstein-$p$ Distance Between Images with Lin-\near Cost. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , pages 509–518, New Orleans, LA, USA, June 2022. IEEE. ISBN 978-1-66546-946-3.\ndoi:10.1109/CVPR52688.2022.00060.\nN. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal Transport for Domain Adaptation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 39(9):1853–1865, Sept. 2017.\nISSN 1939-3539. doi:10.1109/TPAMI.2016.2615921.\nM. Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In Advances in\nNeural Information Processing Systems , volume 26. Curran Associates, Inc., 2013.\nM. Cuturi, L. Meng-Papaxanthos, Y . Tian, C. Bunne, G. Davis, and O. Teboul. Optimal Transport\nTools (OTT): A JAX Toolbox for all things Wasserstein, Jan. 2022.\nI. Deshpande, Z. Zhang, and A. G. Schwing. Generative Modeling Using the Sliced Wasser-\nstein Distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition , pages 3483–3491, 2018. URL https://openaccess.thecvf.com/content_cvpr_\n2018/html/Deshpande_Generative_Modeling_Using_CVPR_2018_paper.html .\nJ. Feydy, P. Roussillon, A. Trouvé, and P. Gori. Fast and Scalable Optimal Transport for Brain\nTractograms, July 2021.\nR. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos,\nK. Fatras, N. Fournier, L. Gautheron, N. T. H. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko,\nA. Rolet, A. Schutz, V . Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. POT: Python\nOptimal Transport. Journal of Machine Learning Research , 22(78):1–8, 2021. ISSN 1533-7928.\nURL http://jmlr.org/papers/v22/20-451.html .\n10\n--- Page 11 ---\nS. Gerber and M. Maggioni. Multiscale strategies for computing optimal transport. The Journal of\nMachine Learning Research , 18(1):2440–2471, Jan. 2017. ISSN 1532-4435.\nJ. Kileel, A. Moscovich, N. Zelesko, and A. Singer. Manifold Learning with Arbitrary Norms.\nJournal of Fourier Analysis and Applications , 27(5), Oct. 2021. ISSN 1069-5869, 1531-5851.\ndoi:10.1007/s00041-021-09879-2.\nP. Knopp and R. Sinkhorn. Concerning nonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics , 21(2):343–348, Jan. 1967. ISSN 0030-8730. URL https:\n//projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/\nissue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/\npjm/1102992505.full .\nM. Kusner, Y . Sun, N. Kolkin, and K. Weinberger. From Word Embeddings To Document Distances.\nInProceedings of the 32nd International Conference on Machine Learning , pages 957–966. PMLR,\nJune 2015. URL https://proceedings.mlr.press/v37/kusnerb15.html .\nT. Lin, N. Ho, and M. I. Jordan. On the Efficiency of Entropic Regularized Algorithms for Optimal\nTransport. Journal of Machine Learning Research , 23(137):1–42, 2022. URL http://jmlr.\norg/papers/v23/20-277.html .\nE. C. Meng, T. D. Goddard, E. F. Pettersen, G. S. Couch, Z. J. Pearson, J. H. Morris, and T. E. Ferrin.\nUCSF ChimeraX: Tools for structure building and analysis. Protein Science , 32(11):e4792, 2023.\nISSN 1469-896X. doi:10.1002/pro.4792.\nQ. Mérigot. A Multiscale Approach to Optimal Transport. Computer Graphics Forum , 30(5):\n1583–1592, Aug. 2011. ISSN 01677055. doi:10.1111/j.1467-8659.2011.02032.x.\nE. F. Montesuma, F. M. N. Mboula, and A. Souloumiac. Recent Advances in Optimal Transport\nfor Machine Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 47(02):\n1161–1180, Feb. 2025. ISSN 0162-8828. doi:10.1109/TPAMI.2024.3489030.\nT. H. D. Nguyen, W. P. Galej, X.-c. Bai, C. Oubridge, A. J. Newman, S. H. W. Scheres, and K. Nagai.\nCryo-EM structure of the yeast U4/U6.U5 tri-snRNP at 3.7 Å resolution. Nature , 530(7590):\n298–302, Feb. 2016. ISSN 1476-4687. doi:10.1038/nature16940.\nG. Peyré and M. Cuturi. Computational Optimal Transport: With Applications to Data Science.\nFoundations and Trends ®in Machine Learning , 11(5-6):355–607, 2019. ISSN 1935-8237, 1935-\n8245. doi:10.1561/2200000073.\nR. Rao, A. Moscovich, and A. Singer. Wasserstein K-Means for Clustering Tomographic Pro-\njections. In Machine Learning for Structural Biology Workshop, Neural Information Process-\ning Systems (NeurIPS) , pages 1–12, 2020. URL https://www.mlsb.io/papers/MLSB2020_\nWasserstein_K-Means_for_Clustering.pdf .\nA. T. Riahi, G. Woollard, F. Poitevin, A. Condon, and K. D. Duc. AlignOT: An Optimal Transport\nBased Algorithm for Fast 3D Alignment With Applications to Cryogenic Electron Microscopy\nDensity Maps. IEEE/ACM Transactions on Computational Biology and Bioinformatics , 20(6):\n3842–3850, Nov. 2023. ISSN 1557-9964. doi:10.1109/TCBB.2023.3327633.\nG. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V . Subramanian, A. Solomon, J. Gould, S. Liu,\nS. Lin, P. Berube, L. Lee, J. Chen, J. Brumbaugh, P. Rigollet, K. Hochedlinger, R. Jaenisch,\nA. Regev, and E. S. Lander. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies\nDevelopmental Trajectories in Reprogramming. Cell, 176(4):928–943.e22, Feb. 2019. ISSN\n1097-4172. doi:10.1016/j.cell.2019.01.006.\nJ. Schrieber, D. Schuhmacher, and C. Gottschlich. DOTmark – A Benchmark for Discrete Optimal\nTransport. IEEE Access , 5:271–282, 2017. ISSN 2169-3536. doi:10.1109/ACCESS.2016.2639065.\nS. Shirdhonkar and D. W. Jacobs. Approximate earth mover’s distance in linear time. In\n2008 IEEE Conference on Computer Vision and Pattern Recognition , pages 1–8, June 2008.\ndoi:10.1109/CVPR.2008.4587662.\n11\n--- Page 12 ---\nA. Singer and R. Yang. Alignment of density maps in Wasserstein distance. Biological Imaging , 4:\ne5, Jan. 2024. ISSN 2633-903X. doi:10.1017/S2633903X24000059.\nJ. Solomon, F. de Goes, G. Peyré, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. Guibas.\nConvolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains.\nACM Transactions on Graphics , 34(4):66:1–66:11, 2015. doi:10.1145/2766963.\nS. Stagnoli, F. Peccati, S. R. Connell, A. Martinez-Castillo, D. Charro, O. Millet, C. Bruzzone,\nA. Palazon, A. Ardá, J. Jiménez-Barbero, J. Ereño-Orbea, N. G. A. Abrescia, and G. Jiménez-\nOsés. Assessing the Mobility of Severe Acute Respiratory Syndrome Coronavirus-2 Spike Protein\nGlycans by Structural and Computational Methods. Frontiers in Microbiology , 13, Apr. 2022.\nISSN 1664-302X. doi:10.3389/fmicb.2022.870938.\nC. Villani. Optimal Transport, Old and New , volume 338 of Grundlehren Der Mathematischen\nWissenschaften . Springer, Berlin, Heidelberg, 2009. ISBN 978-3-540-71050-9. doi:10.1007/978-\n3-540-71050-9.\nW. Wong, X.-c. Bai, A. Brown, I. S. Fernandez, E. Hanssen, M. Condron, Y . H. Tan, J. Baum, and\nS. H. Scheres. Cryo-EM structure of the Plasmodium falciparum 80S ribosome bound to the anti-\nprotozoan drug emetine. eLife , 3:e03080, June 2014. ISSN 2050-084X. doi:10.7554/eLife.03080.\nG. Wright, J. Andén, V . Bansal, J. Xia, C. Langfield, J. Carmichael, K. Sowattanangkul, R. Brook,\nY . Shi, A. Heimowitz, G. Pragier, I. Sason, A. Moscovich, Y . Shkolnisky, and A. Singer.\nComputationalCryoEM/ASPIRE-Python: V0.13.2. Zenodo, Mar. 2025.\nwwPDB Consortium. EMDB—the Electron Microscopy Data Bank. Nucleic Acids Research , 52\n(D1):D456–D465, Jan. 2024. ISSN 0305-1048. doi:10.1093/nar/gkad1019.\n12\n--- Page 13 ---\nA Algorithms\nLower-bound based on entropic regularization described in Section 3.1. This method simply runs the\niterative Sinkhorn algorithm an then returns the unregularized cost term of the dual solution to the\nentropic regularization problem.\nAlgorithm 1 Regularization-Based Lower Bound\nRequire: µ,ν∈ΣNdonX= [N]d,p≥1, ε > 0, ξ > 0and metric ρ.\nC← {ρ(xi, yj)p}ij\nK←exp\u0000\n−C\nε\u0001\nInitialize f←0n,g←0m\nInitialize b←exp\u0000g\nε\u0001\nrepeat\na←µ⊘(Kb)\nf←εloga\nb←ν⊘(K⊤a)\ng←εlogb\nuntil∥a⊙Kb−µ∥1+∥ν−b⊙K⊤a∥1< ξ\nreturn (⟨f,µ⟩+⟨g,ν⟩)1\np\nAlgorithm for entropic regularization-based upper bound described in Section 3.1, using the un-\nregularized term of the primal solution to the entropic regularization problem, with weighted total\nvariation marginal corrections.\nAlgorithm 2 Regularization-Based Upper Bound\nRequire: µ,ν∈ΣNdonX= [N]d,p≥1, ε > 0, ξ > 0and metric ρ.\nC← {ρ(xi, yj)p}ij\nK←exp\u0000\n−C\nε\u0001\nInitialize b←1Nd\nrepeat\na←µ⊘Kb\nb←ν⊘K⊤a\nˆµ←a⊙(Kb)\nˆν←b⊙(K⊤a)\nuntil∥ˆµ−µ∥1+∥ν−ˆν∥1< ξ\nˆπε←diag (a)Kdiag (b)\n¯x←mean(X)\nw← {ρ(¯x, xi)p}i\n∆ˆµ←21−1\np⟨w,|ˆµ−µ|⟩1\np\n∆ˆν←21−1\np⟨w,|ν−ˆν|⟩1\np\nreturn ⟨ˆπε, C⟩1\np+ ∆ ˆµ+ ∆ ˆν\nAlgorithm for quantization-based upper bound described in Section 3.2, using coarse cost weighted\nby the trivial coupling.\nAlgorithm 3 Weighted-Cost Upper Bound\nRequire: µ,ν∈ΣNdonX= [N]d,p≥1, κ∈Nand metric ρ.\n˜µ←SumPool (µ;κ)\n˜ν←SumPool (ν;κ)\n¯Ckℓ←\u001a\n1\n˜µk˜νℓP\nx∈Xk\ny∈Yℓρ(x, y)pµ(x)ν(y)\u001b\nkℓ\nSolve L¯C←min ˜π∈Π(˜µ,˜ν)⟨˜π,¯C⟩\nreturn L¯C1\np\n13\n--- Page 14 ---\nAlgorithm for bi-level quantization-based upper bound described in Section 3.4, using nearest-\nneighbor upscaling of the optimal coarse coupling, iterative proportional fitting of the marginals (i.e.\nSinkhorn iterations), with weighted total variation marginal corrections.\nAlgorithm 4 Upscaling Upper Bound\nRequire: µ,ν∈ΣNdonX= [N]d,p≥1, κ∈N, ξ > 0and metric ρ.\n˜X ← AvgPool (X;κ)\n˜µ←SumPool (µ;κ)\n˜ν←SumPool (ν;κ)\n˜C← {ρ(˜xk,˜xℓ)p}kℓ\nSolve ˜π∗←arg min˜π∈Π(˜µ,˜ν)⟨˜π,˜C⟩\n▷Up-scaled coupling\n˜P∗←reshape (˜π∗;n) ▷Reshape as tensor\nK← {κ−2d}t∈[κ]2d\nˆP←˜P∗⊗K ▷Upscaling\nˆπ←reshape−1(ˆP;N)\n▷Iterative proportional fitting\nInitialize b←1Nd\nrepeat\na←µ⊘ˆπb\nb←ν⊘ˆπ⊤a\nˆµ←a⊙(ˆπb)\nˆν←b⊙(ˆπ⊤a)\nuntil∥ˆµ−µ∥1+∥ν−ˆν∥1< ξ\n▷Upper bound\ncWp←\u0010P\n(i,j)∈supp(ˆ π)aiˆπijbjρ(xi, xj)p\u00111\np\n¯x←mean(X)\nw← {ρ(¯x, xi)p}i\n∆ˆµ←21−1\np⟨w,|ˆµ−µ|⟩1\np\n∆ˆν←21−1\np⟨w,|ν−ˆν|⟩1\np\nreturn cWp+ ∆ ˆµ+ ∆ ˆν\nAlgorithm for bi-level quantization-based lower bound described in Section 3.5, using interpolation\nfor upscaling the optimal coarse dual potentials, and c-transform to achieve optimized admissible\ndual potentials pair.\nAlgorithm 5 Upscaling Lower Bound\nRequire: µ,ν∈ΣNdonX= [N]d,p≥1, κ∈Nand metric ρ.\n˜X ← AvgPool (X;κ)\n˜µ←SumPool (µ;κ)\n˜ν←SumPool (ν;κ)\n˜C← {ρ(˜xk,˜xℓ)p}kℓ\nSolve (˜f∗,˜g∗)←arg max\n(˜f,˜g)∈R(˜C)⟨˜f,˜µ⟩+⟨˜g,˜ν⟩\nˆf← {R˜f,˜X(xi)}i∈[Nd]\ng←n\nminiρ(xi, xj)p−ˆfio\nj\nf← {minjρ(xi, xj)p−gj}i\nreturn\u0000\n⟨f,µ⟩+⟨g,ν⟩\u00011\np\n14\n--- Page 15 ---\nUpper Bounds Lower Bounds\nWeighted-\nCostPrimal\nUpscalingEntropic\nRegularizationDual\nUpscalingMin-\nCostEntropic\nRegularization\nκ2 κ4 κ2 κ4 ε1 ε4 κ2 κ4 κ2 κ4 ε1 ε4\nClass p\nClassic 1 4.4% 0.3% 5.0% 0.3% 19.8% 16.0% 4.9% 0.2% 4.7% 0.3% 17.6% 18.7%\nImages ±2.1%±0.2%±2.6%±0.4%±10.1% ±11.2% ±2.6%±0.1%±2.3%±0.1%±7.8%±13.3%\n2 6.1% 0.4% 6.1% 0.3% 7.9% 1.3% 6.1% 0.3% 6.1% 0.3% 12.3% 1.3%\n±3.1%±0.1%±3.0%±0.1% ±6.6% ±0.5%±3.0%±0.1%±2.9%±0.1%±10.4% ±0.6%\nMicro- 1 16.7% 2.3% 20.7% 6.6% 128.0% 90.7% 14.4% 2.2% 15.7% 1.9% 120.8% 86.9%\nscopy ±5.6%±1.4%±9.6%±10.6% ±68.3% ±79.2% ±5.0%±1.2%±4.8%±1.0%±57.2% ±81.3%\n2 15.4% 1.9% 17.4% 3.0% 75.6% 7.7% 15.6% 2.0% 14.9% 1.7% 99.9% 8.1%\n±5.2%±1.2%±5.9%±3.4%±51.3% ±4.9%±5.1%±1.3%±5.0%±1.0%±72.8% ±5.5%\nShapes 1 10.2% 1.9% 15.1% 10.8% 172.9% 68.8% 7.8% 1.5% 8.5% 1.4% 163.6% 77.0%\n±3.6%±1.7%±8.9%±11.9% ±99.0% ±72.7% ±2.0%±0.7%±2.4%±1.2%±88.8% ±77.6%\n2 7.9% 1.5% 9.6% 4.2% 39.5% 6.0% 7.4% 1.2% 7.3% 1.0% 56.2% 5.7%\n±3.8%±1.8%±5.6%±6.4%±45.0% ±11.6% ±2.6%±0.6%±3.0%±1.4%±69.1% ±11.8%\nTable 4: Computational time comparison of different methods showing the mean ±standard deviation\nof the relative computation time compared to exact OT solver. Each method is evaluated at different\nfidelity level κ2= 2andκ4= 4and different values of ε1= 1·10−3Npandε4= 4·10−3Np.\n5 10 15 20 25 30 35\nWp10−410−310−210−1100Relative Errorp = 1\n5 10 15 20 25 30 35 40\nWpp = 2\nUpper Bounds\nPrimal Upsacling ( κ= 2)\nPrimal Upsacling ( κ= 4)\nPrimal Entropic Reg. ( ε= 0.001Np)\nPrimal Entropic Reg. ( ε= 0.004Np)\nWeighted-Cost ( κ= 2)\nWeighted-Cost ( κ= 4)\nFigure 5: Accuracy of Wasserstein Upper Bounds\n5 10 15 20 25 30 35\nWp10−310−210−1100Relative Errorp = 1\n5 10 15 20 25 30 35 40\nWpp = 2\nLower Bounds\nDual Upsacling ( κ= 2)\nDual Upsacling ( κ= 4)\nDual Entropic Reg. ( ε= 0.001Np)\nDual Entropic Reg. ( ε= 0.004Np)\nMin-Cost (κ= 2)\nMin-Cost (κ= 4)\nFigure 6: Accuracy of Wasserstein Lower Bounds. Negative-valued bounds are clipped to 0, evaluat-\ning as 100% relative error.\n15\n--- Page 16 ---\nB Experiments\nB.1 DOTmark\nIn this section we present additional figures and results evaluating the proposed Wasserstein bounds\non the discrete optimal transport benchmark (DOTMark) [Schrieber et al., 2017] presented in the\nmain text. The computational speed up of the proposed methods compared to the exact OT solver are\nsummarized in Table 4. The results show that the quantization methods achieve the most significant\nspeed ups. Notably, the dual upscaling method at κ= 4are calculated in 0.2-2.2% of the time, while\nmaking almost no sacrifice in accuracy. Maintaining no more than 2.4% average error.\nThe relative accuracy of the proposed methods exponentially improves for large values of the exact\nWasserstein distance as evident in Figures 5 and 6. Negative-valued lower bounds are trivially clipped\nto 0, when evaluate in the benchmark.\nB.2 EMDB\nThe Electron Microscopy Data Bank (EMDB) [wwPDB Consortium, 2024] is a repository of\nvolumetric density maps that contains many interesting molecules that were reconstructed from\ncryogenic electron microscopy (cryo-EM) experiments. These reconstructions are estimates of the\n3D electric potential at every point in the molecule. For our 3D experiments, we downloaded and\nprocessed four maps of famous molecules, detailed in Table 5 using the ASPIRE package [Wright\net al., 2025]. In Figure 4 you can see 3D renderings of these molecules that we generated using UCSF\nChimeraX [Meng et al., 2023]. The volumetric maps were downloaded from EMDB, masked inside\na spherical region of radius 128pixels, rotated around the Z axis in increments of 20 degrees and\ndownscaled to 16×16×16voxels. The computational speed up is summarized in Table 6, showing\nthat even at κ= 2 the quantization methods provide substantial speedups. Figure 8 shows the\nWasserstein metrics and bounds between the 3D density map of the molecule in its base orientation\nand its rotations around the Z axis. The exact Wasserstein metric is shown as the thick black line with\nupper and lower bounds next to it using the various markers.\nThe quantization-based methods dominate in accuracy for the lower bounds of both p∈ {1,2},\nwhereas for the upper bounds of the Wasserstein-1 metric, the upper bound based on entropic\nregularization with ε= 0.001Npachieves the best accuracy, although at a significant computational\ncost. The triangular symmetry of EMDB-14621 (SARS-CoV-2 spike protein) and EMDB-2484\n(Trimeric HIV-1 envelope glycoprotein) seen in Figure 8 are easily detectable as the dips at 120◦\nrotation angle.\nTable 5: Selected cryo-EM structures from the Electron Microscopy Data Bank (EMDB).\nName EMDB ID Description\nRibosome EMD-2660 Ribosome of the Plasmodium falciparum parasite which causes\nmalaria in humans [Wong et al., 2014]\nSARS-CoV-2 EMD-14621 SARS-CoV-2 spike protein [Stagnoli et al., 2022]\nYeast EMD-8012 Yeast spliceosome [Nguyen et al., 2016]\nHIV EMD-2484 HIV-1 trimeric spike pre-fusion [Bartesaghi et al., 2013]\n16\n--- Page 17 ---\nXY plane XZ plane YZ planeRibosome\n SARS-CoV-2\n Yeast\n HIV\nFigure 7: Isosurfaces of 3D molecular densities from the Electron Microscopy Data Bank (EMDB).\nIn our experiments, the molecules on the left are rotated around the Z axis, which corresponds to the\ndepth direction here. The middle and right columns show the same molecules rotated by 90 degrees\naround the X and Y axis (respectively).\n17\n--- Page 18 ---\nRibosome\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)101Wasserstein Distance Bounds\np = 1\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)101\np = 2\nMethods\nWeighted-Cost ( κ2)\nPrimal Upsacling ( κ2)\nPrimal Entropic Reg. ( ε1)\nPrimal Entropic Reg. ( ε4)\nDual Upsacling ( κ2)\nMin-Cost (κ2)\nDual Entropic Reg. ( ε1)\nDual Entropic Reg. ( ε4)\nExact Wasserstein SARS-CoV-2\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)10−1100101Wasserstein Distance Bounds\np = 1\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)100101\np = 2\nMethods\nWeighted-Cost ( κ2)\nPrimal Upsacling ( κ2)\nPrimal Entropic Reg. ( ε1)\nPrimal Entropic Reg. ( ε4)\nDual Upsacling ( κ2)\nMin-Cost (κ2)\nDual Entropic Reg. ( ε1)\nDual Entropic Reg. ( ε4)\nExact Wasserstein Yeast\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)100101Wasserstein Distance Bounds\np = 1\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)101\np = 2\nMethods\nWeighted-Cost ( κ2)\nPrimal Upsacling ( κ2)\nPrimal Entropic Reg. ( ε1)\nPrimal Entropic Reg. ( ε4)\nDual Upsacling ( κ2)\nMin-Cost (κ2)\nDual Entropic Reg. ( ε1)\nDual Entropic Reg. ( ε4)\nExact Wasserstein HIV\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)10−210−1100101Wasserstein Distance Bounds\np = 1\n0 20 40 60 80 100 120 140 160 180\nRotation Angle (deg)10−1100101\np = 2\nMethods\nWeighted-Cost ( κ2)\nPrimal Upsacling ( κ2)\nPrimal Entropic Reg. ( ε1)\nPrimal Entropic Reg. ( ε4)\nDual Upsacling ( κ2)\nMin-Cost (κ2)\nDual Entropic Reg. ( ε1)\nDual Entropic Reg. ( ε4)\nExact Wasserstein\nFigure 8: Wasserstein- pmetric and bounds between rotated 3D density maps of the molecules\ndescribed in Table 5. From top to bottom: Ribosome ,SARS-CoV-2 ,Yeast ,HIV . The thick black\nline is the exact Wasserstein metric between the molecule and its rotated self, as a function of the\nrotation angle. The various upper and lower bounds are shown as different color markers. Note the\ndrop around 120 degrees for the SARS-CoV-2 and HIV-1 spikes due to their 3-fold symmetry.\n18\n--- Page 19 ---\nTable 6: Relative computation time compared to exact OT solver of 16×16×16downscaled EMDB\ndensity maps. Results show mean and standard deviation across rotation angles. Lower is better.\nUpper Bounds Lower Bounds\nWeighted-\nCostPrimal\nUpscalingEntropic\nRegularizationDual\nUpscalingMin-\nCostEntropic\nRegularization\nMolecule p κ 2 κ2 ε1 ε4 κ2 κ2 ε1 ε4\nRibosome 1 3.7% 175.6% 156.2% 155.8% 74.5% 3.5% 154.1% 154.1%\n±3.2% ±56.2% ±111.3% ±111.1% ±21.8% ±3.0% ±109.8% ±109.7%\n2 12.4% 178.1% 569.8% 570.2% 12.2% 12.5% 563.2% 563.6%\n±3.7% ±226.5% ±252.8% ±252.3% ±3.6% ±5.0% ±249.8% ±249.6%\nSARS-CoV-2 1 11.4% 164.6% 69.1% 62.7% 69.9% 2.2% 61.4% 61.2%\n±26.7% ±96.4% ±4.7% ±15.1% ±35.7% ±0.3% ±14.0% ±14.8%\n2 19.7% 350.5% 850.9% 658.8% 77.9% 19.0% 642.5% 641.6%\n±3.5% ±421.2% ±895.4% ±303.0% ±184.2% ±5.7% ±295.5% ±294.0%\nYeast 1 4.8% 295.4% 20.8% 3.1% 127.5% 3.3% 5.6% 0.9%\n±4.9% ±379.0% ±31.2% ±3.5% ±153.4% ±3.3% ±4.6% ±0.9%\n2 12.7% 172.2% 919.4% 828.9% 13.3% 11.6% 820.4% 819.7%\n±0.7% ±69.3% ±497.3% ±215.6% ±1.0% ±0.7% ±213.8% ±212.5%\nHIV 1 10.4% 243.1% 163.1% 163.0% 99.6% 2.9% 161.1% 161.1%\n±23.6% ±215.3% ±127.3% ±127.1% ±89.8% ±2.7% ±125.8% ±125.8%\n2 40.7% 198.2% 603.9% 603.9% 45.6% 40.4% 597.0% 597.0%\n±97.7% ±255.3% ±252.2% ±252.0% ±114.3% ±99.0% ±249.3% ±249.3%\nC Proofs\nC.1 Weighted total variation correction terms\nProof of Lemma 3.2. Using the triangle inequality for the Wasserstein metric, we can write\nWp(µ, ν)≤ W p(µ,ˆµ) +Wp(ˆµ,ˆν) +Wp(ˆν, ν) (29)\ncontrolling for each element separately, we have\nWp(µ,ˆµ)≤ T Vw\np(ˆµ, µ) and Wp(ˆν, ν)≤ T Vw\np(ν,ˆν) (30)\nby the property of weighted total variation, and\nWp(ˆµ,ˆν) =\u0012\nmin\nπ∈Π(ˆµ,ˆν)⟨π, C⟩\u00131\np\n≤ ⟨ˆπ, C⟩1\np (31)\nby evaluating the transport cost using a coupling in the problem’s original space.\nProof of Proposition 3.3. Consider the definition of weighted total variation,\nT Vp(ˆµ, µ;w) +T Vp(ν,ˆν;w) (32)\n= 21−1\np⟨w,|ˆµ−µ|⟩1\np+ 21−1\np⟨w,|ν−ˆν|⟩1\np\n= 21−1\np\u0012\u0010X\nρ(¯x, x)p|ˆµx−µx|\u00111\np+\u0010X\nρ(¯x, x)p|νx−ˆνx|\u00111\np\u0013\n≤21−1\np\u0010\nr∥ˆµ−µ∥1\np\n1+r∥ν−ˆν∥1\np\n1\u0011\nbounding radius\n≤22−2\np(∥ˆµ−µ∥1+∥ν−ˆν∥1)1\npr Jensen’s inequality\n<22−2\npξ1\npr convergence criteria\n19\n--- Page 20 ---\nC.2 Proof of Theorem 3.4\nFirst, let us consider the following lemma discussing a coupling constructed ad hoc using coarsened\nmeasures.\nLemma C.1. Letµ, νmeasures with set of admissible couplings Π(µ,ν), the trivial coupling π⊗,\nand˜µand˜νthe respective coarsened measures. For any coupling ˜π∈Π(˜µ,˜ν), the measure on the\nproduct space X × Y\nπ˜π(x, y) :=˜πkX(x)ℓY(y)\nµ(XkX(x))ν(YℓY(y))π⊗(x, y) (33)\nis an admissible coupling π˜π∈Π(µ,ν), where the coarsening inverse index functions kX(x), ℓY(y)\nare defined as kX(x) :={k:x∈Xk}, ℓY(y) :={ℓ:y∈Yℓ}.\nProof of Lemma C.1. Following Definition 1.1 (Coupling) from [Villani, 2009], one can show\nπ˜π(x, y)Equation (33) is admissible. For φ, ψ be any integrable measurable functions on X,Y\nrespectively, than π˜π(x, y)admits\nZ\nX×Y\u0000\nφ(x) +ψ(y)\u0001\ndπ˜π(x, y) (34)\n=X\nk,ℓZ\nXk×Yℓ\u0000\nφ(x) +ψ(y)\u0001\ndπ˜π(x, y)\n=X\nk,ℓZ\nXk×Yℓ\u0000\nφ(x) +ψ(y)\u0001Πkℓ\nµ(Xk)ν(Yℓ)dµ(x)dν(y) plug-in coupling’s definition\n=X\nk,ℓ\u0010˜πkℓ\nµ(Xk)ν(Yℓ)Z\nXk×Yℓφ(x)dµ(x)dν(y) +˜πkℓ\nµ(Xk)ν(Yℓ)Z\nXk×Yℓψ(y)dµ(x)dν(y)\u0011\n=X\nk,ℓ˜πkℓ\nµ(Xk)Z\nXkφ(x)dµ(x) +X\nk,ℓ˜πkℓ\nν(Yℓ)Z\nYℓψ(y)dν(y) sum over marginals\n=X\nkZ\nXkφ(x)dµ(x) +X\nℓZ\nYℓψ(y)dν(y)\n=Z\nXφ(x)dµ(x) +Z\nYψ(y)dν(y)\nNext, we consider the transport cost of such a coupling.\nLemma C.2. The transport loss assigned by the cost c(x, y)and a coupling π˜πidentifies with the\ncoarse transport loss assigned by marginally weighted cost ¯CEquation (15) and the coarse coupling\n˜π,\n⟨π˜π, C⟩=⟨˜π,¯C⟩ (35)\n20\n--- Page 21 ---\nProof.\n⟨π˜π, C⟩=X\ni,jc(xi, yj)π˜π(xi, yj) =X\nk,ℓX\nx∈Xk\ny∈Yℓc(x, y)π˜π(x, y) (36)\n=X\nk,ℓX\nx∈Xk\ny∈Yℓc(x, y)˜πkℓ\nµ(Xk)ν(Yℓ)π⊗(x, y)\n=X\nk,ℓ1\nµ(Xk)ν(Yℓ)X\nx∈Xk\ny∈Yℓc(x, y)µ(x)ν(y) ˜πkℓ\n=X\nk,ℓ¯Ckℓ˜πkℓ=⟨˜π,¯C⟩\nFinally, we can write\nProof of Theorem 3.4. Based on admissibility of π˜πshown in Lemma C.1 the transport cost\n⟨π˜π, C⟩ ≥LC(µ, ν),∀˜π∈Π(˜µ,˜ν). (37)\nIn particular, for ˜π∗= arg min˜π∈Π(˜µ,˜ν)⟨˜π,¯C⟩,\n⟨˜π∗,¯C⟩=⟨π˜π∗, C⟩ ≥LC(µ, ν) (38)\nby the identity shown in Lemma C.2.\nC.3 Additional proofs\nProof of Theorem 3.5. Consider\nπ∗= arg min\nπ∈Π(µ,ν)⟨π, C⟩ (39)\nand coarsening of the optimal coupling\nˆπ∗\nkℓ:=X\nx∈Xk\ny∈Yℓπ∗(x, y) (40)\nsuch that,\nLC(µ,ν) =⟨π∗, C⟩ (41)\n=X\nx∈X\ny∈Yρ(x, y)pπ∗(x, y)\n=X\nk,ℓX\nx∈Xk\ny∈Yℓρ(x, y)pπ∗(x, y)\n≥X\nk,ℓCmin\nkℓX\nx∈Xk\ny∈Yℓπ∗(x, y)\n=⟨ˆπ∗, Cmin⟩\n≥min\n˜π∈Π(˜µ,˜ν)⟨˜π, Cmin⟩\n=LCmin(˜µ,˜ν).\n21\n--- Page 22 ---\nProof of Lemma 3.6. Let˜π∗∈Π(˜µ,˜ν)be the coarse optimal coupling and Kbe the positive valued\nnormalized kernel tensor satisfyingP\nt∈[κ]2dKt= 1. Recall that ˆP=˜P∗⊗Kandˆπis obtained by\nreshaping ˆP.\nFirst, we show that the sum of all elements equals 1:\nNdX\ni=1NdX\nj=1ˆπij=X\nt∈[κ]2dndX\nk=1ndX\nℓ=1˜π∗\nkℓKt (42)\n=ndX\nk=1ndX\nℓ=1˜π∗\nkℓX\nt∈[κ]2dKt\n=ndX\nk=1ndX\nℓ=1˜π∗\nkℓ·1 = 1\nwhere the last equality follows from ˜π∗being a coupling.\nSecond, we show that ˆπis non-negative. Since ˜π∗is a coupling, it is non-negative, and Kis a\npositive-valued kernel, their tensor product ˆPand its reshaped form ˆπare non-negative.\nThus, ˆπsatisfies all the properties of a coupling measure.\n22\n--- Page 23 ---\nD Table of notations\nTable 7: Table of Notations\nNotation Category Description\nSets and Spaces\nR+ Set Non-negative real numbers\n[n] Set Set of integers {1, . . . , n }\nΣN Space Probability simplex {(p1, . . . , p N)∈RN\n+:P\nipi= 1}\nX,Y Set Point sets where measures are defined\nX Set Set of non-overlapping hypercubes covering the grid\n˜X,˜Y Set Coarse grids (set of hypercube centers)\nXk, Yℓ Set Individual hypercubes in the partition\nMeasures and Vectors\n0n,1n Vector All-zeros and all-ones vectors in Rn, respectively\nµ, ν Measure Discrete probability measures\nµ,ν Vector Vector representations of measures µ, ν\n˜µ,˜ν Measure Coarsened measures\n˜µ,˜ν Vector Vector representations of coarsened measures\nf,g Vector Kantorovich potentials\na,b Vector Sinkhorn scaling vectors\nMatrices and Tensors\nC Matrix Ground-cost matrix\n˜C Matrix Coarse cost matrix (center-based)\n¯C Matrix Coarse cost matrix (average-based)\nπ Matrix Transport plan (coupling matrix)\nπ∗Matrix Optimal transport coupling\n˜π Matrix Coarse coupling\nK Tensor Normalized kernel tensor\nFunctions and Operations\nρ Function Distance metric\nWp Function Wasserstein- pmetric\nLC Function Optimal transport cost for ground-cost C\nT Vw\np Function Weighted total variation\n⊗ Operation Tensor product\n⊙,⊘ Operation Pointwise multiplication, division\n⟨·,·⟩ Operation Standard vector/matrix inner product\nParameters and Constants\np Parameter Order of Wasserstein metric ( p≥1)\nκ Parameter Scale factor for coarsening\nξ Parameter Convergence threshold for fitting\nN Constant Side length of regular grid\nd Constant Dimension of the space\nn Constant Side length of coarse grid ( n=N/κ )\n¯x Constant Center point of space X\nr Constant Radius of space X\n∆ˆµ,∆ˆν Constant Marginal weighed total variation corrections\nCode\nAvgPool ,SumPool Function Average/sum pooling layer with identical kernel size and stride\nmean Function Mean of a set of points\nreshape Function Cardinality preserving tensor shape transformation\n23",
  "text_length": 53298
}