{
  "id": "http://arxiv.org/abs/2506.03195v1",
  "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with\n  Multimodal LLMs",
  "summary": "Despite Multimodal Large Language Models (MLLMs) showing promising results on\ngeneral zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to\ndistinguish between visually similar subcategories--details that MLLMs may\neasily overlook without explicit guidance. To address this, we introduce\nAutoSEP, an iterative self-supervised prompt learning framework designed to\nenhance MLLM fine-grained classification capabilities in a fully unsupervised\nmanner. Our core idea is to leverage unlabeled data to learn a description\nprompt that guides MLLMs in identifying crucial discriminative features within\nan image, and boosts classification accuracy. We developed an automatic\nself-enhancing prompt learning framework called AutoSEP to iteratively improve\nthe description prompt using unlabeled data, based on instance-level\nclassification scoring function. AutoSEP only requires black-box access to\nMLLMs, eliminating the need for any training or fine-tuning. We evaluate our\napproach on multiple fine-grained classification datasets. It consistently\noutperforms other unsupervised baselines, demonstrating the effectiveness of\nour self-supervised optimization framework. Notably, AutoSEP on average\nimproves 13 percent over standard zero-shot classification and 5 percent over\nthe best-performing baselines. Code is available at:\nhttps://github.com/yq-hong/AutoSEP",
  "authors": [
    "Yunqi Hong",
    "Sohyun An",
    "Andrew Bai",
    "Neil Y. C. Lin",
    "Cho-Jui Hsieh"
  ],
  "published": "2025-06-01T09:04:07Z",
  "updated": "2025-06-01T09:04:07Z",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.03195v1",
  "full_text": "--- Page 1 ---\narXiv:2506.03195v1  [cs.CV]  1 Jun 2025Unlabeled Data Improves Fine-Grained Image\nZero-shot Classification with Multimodal LLMs\nYunqi Hong\nDepartment of Computer Science\nUniversity of California, Los Angeles\nyunqihong@ucla.eduSohyun An\nDepartment of Computer Science\nUniversity of California, Los Angeles\nsohyun0423@cs.ucla.edu\nAndrew Bai\nDepartment of Computer Science\nUniversity of California, Los Angeles\nandrewbai@cs.ucla.eduNeil Y.C. Lin\nDepartment of Bioengineering\nUniversity of California, Los Angeles\nneillin@g.ucla.edu\nCho-Jui Hsieh\nDepartment of Computer Science\nUniversity of California, Los Angeles\nchohsieh@cs.ucla.edu\nAbstract\nDespite Multimodal Large Language Models (MLLMs) showing promising results\non general zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to dis-\ntinguish between visually similar subcategories—details that MLLMs may easily\noverlook without explicit guidance. To address this, we introduce AutoSEP, an\niterative self-supervised prompt learning framework designed to enhance MLLM\nfine-grained classification capabilities in a fully unsupervised manner. Our core idea\nis to leverage unlabeled data to learn a description prompt that guides MLLMs in\nidentifying crucial discriminative features within an image, and boosts classification\naccuracy. We developed an automatic self-enhancing prompt learning framework\ncalled AutoSEP to iteratively improve the description prompt using unlabeled data,\nbased on instance-level classification scoring function. AutoSEP only requires\nblack-box access to MLLMs, eliminating the need for any training or fine-tuning.\nWe evaluate our approach on multiple fine-grained classification datasets. It consis-\ntently outperforms other unsupervised baselines, demonstrating the effectiveness\nof our self-supervised optimization framework. Notably, AutoSEP in average im-\nproves 13% over standard zero-shot classification and 5% over the best-performing\nbaselines. Code is available at https://github.com/yq-hong/AutoSEP .\n1 Introduction\nWhile Multimodal Large Language Models (MLLMs) exhibit impressive zero-shot image classifica-\ntion capabilities on general image datasets [ 1,2,7,18,4,14,37], their performance often declines on\nfine-grained classification tasks. These tasks, which involve distinguishing between visually similar\nsubcategories like specific bird, animal, or plant species [ 11,9,19], pose a challenge for MLLMs.\nThe difficulty stems from the need to distinguish subtle visual differences; without explicit guidance,\nMLLMs tend to overlook crucial details or misinterpret key characteristics [32, 16, 20].\nPreprint. Under review.\n--- Page 2 ---\nPrevious efforts to enhance MLLMs’ fine-grained classification abilities have largely relied on training\nthe model with large-scale labeled datasets [ 11,35], which are highly resource-intensive, demanding\nboth significant computational resources and vast amounts of high-quality labeled data. This is\nespecially challenging for fine-grained classification tasks, where collecting and annotating data is\nexpensive and time-consuming due to the subtle visual distinctions between classes. In contrast,\nwhile unlabeled images are readily available at test time, there has been limited exploration into how\nsuch data could be leveraged to enhance classification performance. This motivates us to explore\nwhether MLLMs can self-improve their fine-grained classification ability using only unlabeled data\nand with only black-box access to the MLLM .\nBut how can unlabeled data be leveraged to improve MLLMs’ zero-shot prediction capabilities? While\nself-supervised learning literature [ 3,12,28] has demonstrated that unlabeled data can be used to\ntrain encoders to extract better discriminative features, it is nontrivial to achieve this in a zero-shot and\nblack-box setting. To address this challenge, we develop Automatic Self-Enhancing Prompt Learning\n(AutoSEP), a novel framework for improving fine-grained visual classification MLLM with unlabeled\ndata (see Figure 1). AutoSEP introduces an intermediate image description step, in which the MLLM\nis prompted to generate a textual description of an image’s visual features. The MLLM then makes\nthe final classification based on the original image and the additional textual description. This image\ndescription step can be viewed as mapping the raw image to text that highlights discriminative and\nrelevant features for the task, analogous to the role of encoders in contrastive learning. To evaluate\nthe quality of descriptions using unlabeled data, we design an instance-level classification scoring\nfunction that measures the MLLM’s ability to retrieve the correct description for each image relative\nto those of other images. A prompt optimization algorithm is designed to iteratively improve the\ndescription-generation prompt based on this scoring function, without requiring white-box access to\nthe MLLMs. By focusing on subtle yet critical visual differences, the MLLM iteratively improves its\nfine-grained classification performance without requiring any labeled training data.\nImageMLLM\nClassificationPrediction Zero-shot\nAutoSEP Image  MLLM Description\nGenerationDescriptionMLLM\nClassificationPrediction\nOptimized Description Generation Prompt\nAnalyze the bird depicted in the image, focusing on the following details:\n* **Beak:** Describe its color , length, shape (e.g., curved, pointed, blunt), and any distinctive markings.\n* **Head:** Describe its color , any distinctive markings (e.g., eye stripe, eyebrow , throat patch), and the presence or absence of a crest.\n* **Wings:** Note the color , pattern (e.g., banded, barred, spotted), and any white patches or bands.\n* **Tail:** Describe its length, shape (e.g., graduated, rounded, square), and any distinctive patterns (e.g., banding, barring).\n* **Body:** Describe the overall body shape (e.g., slender , stout), the color of the chest and belly , and any distinctive markings.\n* **Overall Size and Posture:**  Mention if the bird appears large or small, and describe its posture (e.g., upright, hunched, relaxed).\n**Exclude any background details or other information from your description.**Optimizing with unlabeled data\nFigure 1: Top: Typical zero-shot pipeline for MLLM classification. Middle: Illustration of AutoSEP pipeline.\nBottom: An example of an AutoSEP-optimized, description-generation prompt.\nTo summarize, our key contributions are as follows:\n•We introduce a new self-supervised, black-box framework, AutoSEP, based on instance-level\ndescription retrieval to improve MLLMs fine-grained image classification capabilities.\n•We show that AutoSEP significantly enhances MLLMs’ fine-grained image classification\ncapabilities (13% average improvement over zero-shot and 5% over the best-performing\nbaselines) on 8 fine-grained tasks.\n•To the best of our knowledge, this is the first work studying how to use unlabeled data to\nimprove zero-shot image classification performance in the black-box setting for MLLMs.\n2\n--- Page 3 ---\n2 Related Work\nFine-grained image classification with MLLMs. MLLMs have demonstrated remarkable perfor-\nmance across a broad spectrum of visual tasks, primarily due to pretraining on large-scale datasets\n[1,18,17,37,24,13,27]. However, since these models are predominantly trained on web-scraped\ncorpora, they tend to underperform on fine-grained classification tasks [ 11,9,19]. To mitigate\nthis limitation, prior works [ 11,35] have employed extensive training using large-scale datasets.\nWhile effective, these approaches are computationally expensive and heavily reliant on high-quality\nlabeled data, rendering them impractical in scenarios where fine-grained distinctions are critical. This\nmotivates us to explore an alternative approach, which leverages readily available unlabeled images to\nimprove fine-grained classification in MLLMs through a lightweight framework that requires neither\nlabeled data nor white-box access to the model.\nPrompt optimization. Automatic prompt optimization techniques have seen substantial progress in\nrecent years, demonstrating their effectiveness in black-box, computationally constrained settings with\na wide range of strategies for searching for effective prompts [ 23,6,34,36,21]. Specifically, several\napproaches [ 36,21,29] utilize LLMs to identify and summarize erroneous cases, which are then\niteratively used to refine the prompts. Others [ 10,8] adopt evolutionary strategies, generating diverse\ncandidates through mutation and selection. Yang et al. [31] treats LLMs as optimizers, leveraging\nhistorical prompt trajectories and associated performance signals to guide prompt generation, while\nYang et al. [33], Juneja et al. [15] decompose prompts into semantic segments for finer-grained\ncontrol. Although these methods have shown strong performance, they largely depend on labeled data\nto supervise the optimization process. To mitigate this dependency, recent work such as SPO [ 30]\nformulates prompt optimization in a self-supervised manner by generating candidate prompts with\nLLMs and evaluating them through LLM-based pairwise preference comparisons of the resulting\nmodel outputs. In contrast, we proposed a higher-level framework for fine-grained visual classification\nthat involves leveraging unlabeled images and explicit prompting for textual descriptions, whereas\nSPO is a lower-level prompt optimization technique. Furthermore, we find that SPO does not perform\nwell even when incorporated into our framework due to its choice of scoring function (see Section 4).\n3 Method\nThe key insight of AutoSEP is leveraging instance-level classification signal, which does not require\nlabel information, to inform the optimization of the description generation prompt for improving fine-\ngrained classification. In Section 3.2, we introduce the technique of generating textual descriptions\nand combining it with image inputs to improve visual classification. In Section 3.3, we detail\nthe procedures for learning the description-generation prompt in a self-supervised manner from\ninstance-level classification scoring.\n3.1 Problem Setting\nLetMdenote the MLLM and qdenote the classification prompt. The vanilla zero-shot prediction is\nˆy=M(x, q), (1)\nwhere xis the given image. While MLLMs often achieve strong performance on commonly seen\nbasic-level classes, they tend to underperform on fine-grained classification tasks. We assume there\nexists a small unlabeled dataset X={x1, x2, ..., x n}(more detailed experiments on different dataset\nsizes can be found in Section 4.3.) Our goal is to utilize Xto improve the zero-shot prediction\naccuracy on fine-grained categories. Also, we consider only black-box access to the MLLM, which\nmeans fine-tuning is prohibited.\n3.2 Image Description Generation for Fine-grained Classification\nTraditional self-supervised learning (SSL) leverages unlabeled data to contrastively learn better\nrepresentations [ 3,12,28]. Inspired by SSL, we introduce an additional description generation step,\nwhere a “description generation prompt” pis used to generate a text description tfor an image x.\nBoth the description and the raw image are then jointly passed to the MLLM for predicting the label\n(see Figure 1):\nˆy=M(x,[q, t]),where t=M(x, p).\n3\n--- Page 4 ---\nIntuitively, M(·, p)functions as an encoder, translating the input xinto a text description t. This\ndescription then helps the MLLM conduct the final prediction.\nHowever, a naive description generation prompt yields non-informative descriptions, and even worse,\nmany descriptions might be misleading. Therefore, our method utilize unlabeled data Xto iteratively\nrefine how MLLMs analyze and describe images, guiding them away from generic or incorrect visual\nfeatures towards distinguishing and precise attributes.\n3.3 Automatic Self-enhancing Prompt Learning\nTo learn a better description-generation prompt, we formalize an optimization objective based on a\nscoring function Ψ(X, p)that evaluates the effectiveness of each candidate prompt p:\np∗= arg max\np∈LΨ(X, p), (2)\nwhere Lrepresents the space of coherent natural language prompts.\nNegative \nSampleImage k Description kDescription k\nDescription lPos.\nNeg.Description\nPromptInstance-Level\nClassificationWrongImprove\nImage lInstance-Level Classification\nText 1: {description_1}\nText 2: {description_2 }\nWhich description correctly describes the image? The first text or the\nsecond text?Error\nReflection\nDescription\nPrompt\nFigure 2: An illustration of automatic self-enhancing prompt learning with instance-level classification.\n3.3.1 Instance-level Classification for Unsupervised Scoring\nInspired by traditional self-supervised learning methods [ 3,12,28], which learn the encoders to\npull positive pairs closer and push negative pairs further apart contrastively, we propose to optimize\nthe prompt so that the MLLM generates highly distinctive descriptions for each individual unla-\nbeled image. By doing so, our method maximizes the inter-image distinctiveness of the generated\ndescriptions.\nWe frame the optimization as an instance-level classification task, where the MLLM learns to correctly\nassociate each image with its own generated description while distinguishing it from the descriptions\nof other images (See Figure 2). To quantify how well the MLLM can distinguish between different\nimage descriptions, we define a sample matching correctness indicator V(xi, xj). Given an image xi\nand two candidate descriptions tiandtj, which are generated from images xiandxj, respectively,\nwe present the MLLM with a binary-choice prompt G(ta, tb). The prompt asks \" Text 1: {ta}. Text 2:\n{tb}. Which description correctly describes the image? The first or the second? \", where (ta, tb)is\neither (ti, tj)or(tj, ti), randomly shuffled with equal probability. The correctness indicator function\nis then formulated as:\nV(xi, xj) :=\u001a1[M(xi, G(ti, tj)) = “first”], Z = 0\n1[M(xi, G(tj, ti)) = “second ”], Z = 1,(3)\nwhere Z∼Bernoulli (0.5)determines the random ordering of descriptions to prevent positional bias.\nThe function V(xi, xj)returns 1 if the MLLM correctly selects the description corresponding to xi,\nand 0 otherwise.\nWe compute the unsupervised scoring function that measures the model’s ability to correctly match\ndescriptions to their respective images. Formally, the scoring function is defined as:\nΨ(X, p) :=X\nxi∈XX\nxj∈X\\{xi}V(xi, xj). (4)\nTo avoid iterating over all descriptions, Ψ(X, p)can be approximated with random negative samples:\nˆΨ(X, p, k ) :=1\nk· |X|X\nxi∈XX\nxj∈OiV(xi, xj), (5)\n4\n--- Page 5 ---\nwhere Oi∼RandomSample (X\\{xi}, k)represents a set of krandomly sampled images from\nX, excluding xi, acting as negative samples. k· |X|normalizes the score by the total number of\nevaluations conducted.\n3.3.2 Description-generation Prompt Optimization\nTo begin the optimization process, we start with a simple yet effective initial generation prompt that\ninstructs the MLLM to describe the object. The prompt can be found in Appendix D.\nFeedback and modification. To iteratively refine the prompt, we adopt a feedback-driven approach\nthat identifies and addresses the weaknesses in the current generation prompt. An illustration\nof AutoSEP workflow and prompts can be found in Appendix B. First, we collect error pairs\nJerror={(xi, xj)|xi, xj∈X, x i̸=xj, V(xi, xj) = 0}, where the generated descriptions failed to\ncorrectly distinguish between images. Then we sample a subset of Jerror and construct a diagnostic\nprompt to analyze potential shortcomings of the current prompt. Specifically, we present the MLLM\nwith the current prompt together with examples of mismatched descriptions, and ask it to explain\nwhy the current prompt may have led to these errors. This process, referred to as the Reflect operator,\nReflect( ·), yields a set of potential reasons explaining the observed errors. Based on MLLM’s\nfeedback, we then prompt it to revise the current prompt by explicitly addressing the identified\nshortcomings. This refinement step is denoted as the Modify operator, Modify( ·), which outputs an\nimproved version of the generation prompt. Each instance of feedback produces a candidate revised\nprompt, resulting in multiple refined versions aimed at enhancing the distinctiveness of the generated\ndescriptions. It has been shown in the literature that this reflect-and-modify approach is effective in\nautomatic prompt engineering [21].\nIterative optimization. Our AutoSEP optimization process follows an iterative framework where\nthe generation prompt is progressively refined to generate highly distinctive descriptions (Algorithm 1).\nStarting with an initial prompt, we evaluate its effectiveness by generating descriptions for a batch\nof images and forming an instance-level classification set by sampling negative images. Errors\nin the instance-level classification are identified and used to expand the prompt by adjusting it to\nbetter capture distinctive attributes. At each iteration, a set of candidate prompts is evaluated using a\nscoring function defined in Section 3.3.1, and only the top-performing prompts are retained for the\nnext round. This iterative adaptation ensures that the MLLM increasingly focuses on fine-grained\ndetails, improving its ability to generate more informative descriptions. The process terminates after a\npredefined number of iterations N, and the best-performing prompt is selected as the final optimized\nprompt.\nBy maximizing ˆΨ(X, p, k ), we iteratively refine the prompt pto improve the distinctiveness of the\ngenerated descriptions, ultimately enhancing the MLLM’s ability to perform fine-grained classifica-\ntion in a self-supervised manner.\n4 Experiments\n4.1 Experiment Settings\nDatasets. We conduct experiments on four fine-grained image classification datasets, including\nCUB-200-2011 [ 26] (bird classification), iNaturalist 2021 [ 25] (various wild species), Stanford\nDogs [ 5], and VegFru [ 22]. For each dataset, we construct fine-grained classification tasks on subsets\nof categories that are particularly challenging for MLLMs. We construct three bird subsets with high\nconfusion rates: CUB_cuckoo (Black-billed, Mangrove, and Yellow-billed cuckoo), CUB_oriole\n(Hooded, Orchard , and Scott’s oriole), and CUB_vireo (Philadelphia, Red-eyed, and Warbling vireo).\nFor iNaturalist, we select two subsets comprising visually similar species: iNat_butterfly (Symbren-\nthia lilaea, Claudina crescent, Elada checkerspot) and iNat_lupine (Arctic lupine, Silvery lupine,\nArizona lupine). For Stanford Dogs, we focus on three closely related terriers: StanfordDogs_terrier\n(Lakeland terrier, Norwich terrier, Cairn terrier). For the VegFru dataset, we also select two subsets\nconsisting of visually similar species: VegFru_greens (Dandelion, Shepherd’s purse, Prickly lettuce)\nandVegFru_allium (Leek, Green Chinese onion, Bunching onion).\n5\n--- Page 6 ---\nAlgorithm 1 AutoSEP: Automatic Self-Enhancing Prompt Learning\nRequire: p0: initial prompt, N: iterations, X: unlabeled dataset, k: negative samples per image, b: top prompts\nretained per iteration, l: number of reflections, ˆΨ(·): scoring function\n1:P0← {p0} ▷Initialize candidate prompt set\n2:Γ← {γ0}, where γ0={M(xi, p0)|xi∈X} ▷Generate descriptions\n3:fort= 1toNdo\n4: Xt∼ U(X) ▷Random minibatch from X\n5: Pc←Pt−1\n6: forp∈Pt−1do\n7: J={(xi, xj)|xi, xj∈Xt, i̸=j}, where for each xi,knegative samples xjare randomly\nselected ▷Construct instance-level classification set\n8: Jerror={(xi, xj)|(xi, xj)∈J, V(xi, xj) = 0} ▷Collect errors\n9: Ji\nerror⊂Jerror is a sampled subset for each i= 1, . . . , l\n10: G={g1, g2, ...gl}=S\ni=1,...lReflect( p, Ji\nerror) ▷Reflect on the errors (Sec. 3.3.2)\n11: H={h1, h2, ...h l}=S\ni=1,...lModify( p, gi, Ji\nerror) ▷Modify prompts (Sec. 3.3.2)\n12: Pc←Pc∪H\n13: Γ←Γ∪ {{M(xi, h)|xi∈X} |h∈H} ▷Generate descriptions for new prompts\n14: end for\n15: Sc={ˆΨ(X, p, k )|p∈Pc} ▷Evaluate prompts\n16: Pt← {p∈Pc|ˆΨ(X, p, k )≥τ}, where τis the bthhighest score in Sc\n17:end for\n18:Return p∗←arg max p∈PNˆΨ(X, p, k )\nBaselines. We compare our method with both optimization-free and optimization-based approaches.\nFor optimization-free methods, we consider: (1) Vanilla zero-shot , where the MLLM is directly\nprompted without additional context; (2) Zero-shot with descriptions , using image descriptions\ngenerated from an initial human-crafted prompt; (3) Zero-shot with majority vote , where we\nprompt the MLLM to generate multiple responses with high temperature and take the most frequent\nprediction as the final answer; (4) Few-shot with random labels , where the MLLM is shown m\nimages with randomly assigned labels and then asked to classify the (m+ 1)thimage; (5) Multiple\nimages display , where the MLLM is shown mimages with the prompt \" The first mimages show\ndistinct types of {specie}. \" before predicting the (m+ 1)thimage; (6) K-means clustering , where we\ncluster image features extracted by a pretrained image encoder, and then within each cluster, use the\nmost frequent zero-shot prediction as the final label for all images in that cluster.\nFor prompt optimization-based methods, we consider three approaches to adapt them to the unsu-\npervised setting: (1) Optimization with random labels , where we randomly assign labels to the\nimages and use the classification accuracy under these random labels as the optimization objective;\n(2)Optimization with majority vote , where we iteratively assign pseudo-labels to images based on\nmajority voting over multiple high-temperature outputs from the MLLM, and use the classification\naccuracy with respect to these pseudo-labels as the optimization objective; (3) Self-Supervised\nPrompt Optimization (SPO) [30], originally designed for text-only tasks, which perform pairwise\ncomparisons with outputs generated by different prompts to infer their relative quality as signals to\nguide the optimization. We adopt SPO to our multimodal setting by asking the MLLM to compare\nthe quality of image descriptions generated by different prompts. Following the original setup, we\nuse GPT-4o [13] to conduct these comparisons.\nModels. We conduct experiments with three state-of-the-art MLLMs: Gemini 1.5 Flash [ 24],\nGPT-4o [ 13], and Qwen2-VL-72B-Instruct [ 27], covering both proprietary and open-source models.\nWe performed the experiments on local servers with 64 CPU cores and 4 Nvidia A6000 GPUs.\n4.2 Experiment Results\nExperiment results in Table 1 demonstrate that our method consistently outperforms all baseline\napproaches across all tested MLLMs. Notably, it achieves an average improvement of 13% over\nstandard zero-shot classification and 5% over the best-performing baselines. These consistent gains\nhighlight the robustness and generalizability of our approach across different model architectures\nand capabilities. Furthermore, our method operates without requiring any labeled data, making it\nparticularly well-suited for low-resource or open-world scenarios.\n6\n--- Page 7 ---\nTable 1: Main results (accuracy %). AutoSEP outperforms baselines consistently across all tasks and MLLMs.\nSource of variability stems from samples in the evaluation dataset.\nModel Gemini GPT-4o Qwen2-VL Gemini GPT-4o Qwen2-VL\nCUB_cuckoo CUB_oriole\nOptimization-free\nVanilla zero-shot 51.68 ±1.79 61.46 ±1.24 58.54 ±4.69 52.13 ±2.90 74.44 ±0.93 43.60 ±2.96\nZero-shot with descriptions 52.20 ±2.49 68.29 ±1.00 51.46 ±2.60 46.74 ±2.31 73.71 ±1.52 50.79 ±3.92\nZero-shot with majority vote 51.22 ±2.04 61.95 ±1.95 59.02 ±1.98 54.38 ±0.55 75.28 ±2.01 46.74 ±4.95\nFew-shot with random labels 38.05 ±6.14 43.90 ±7.48 46.83 ±9.74 40.05 ±12.1 56.85 ±12.3 36.63 ±6.50\nMultiple images display 39.27 ±3.22 45.12 ±8.86 60.73 ±9.20 58.43 ±4.08 59.10 ±10.5 42.92 ±6.98\nK-means clustering 53.17 ±15.2 59.51 ±15.0 56.59 ±2.26 51.46 ±5.83 67.19 ±8.11 44.94 ±7.07\nOptimization-based\nWith random labels 59.15 ±1.83 65.85 ±0.77 52.20 ±3.65 51.69 ±2.36 72.58 ±1.83 51.01 ±1.83\nWith majority vote 44.88 ±4.69 67.32 ±2.10 53.41 ±2.36 57.90 ±1.56 73.03 ±1.74 38.65 ±1.74\nSPO 55.98 ±1.34 66.59 ±2.63 56.10 ±6.19 49.49 ±2.71 76.12 ±2.16 46.52 ±3.37\nAutoSEP (Ours) 61.22 ±2.30 75.12 ±1.83 63.41 ±2.77 58.20 ±0.88 75.96 ±0.90 55.13 ±3.58\nCUB_vireo iNat_butterfly\nOptimization-free\nVanilla zero-shot 49.21 ±3.36 75.73 ±1.83 55.28 ±3.05 61.14 ±2.11 68.00 ±1.46 40.25 ±4.65\nZero-shot with descriptions 51.46 ±4.11 86.07 ±0.90 50.79 ±3.72 56.57 ±3.45 71.14 ±3.88 48.25 ±2.48\nZero-shot with majority vote 49.21 ±1.80 76.69 ±2.01 55.06 ±3.01 62.00 ±2.14 66.07 ±1.86 46.56 ±2.59\nFew-shot with random labels 45.62 ±8.03 57.08 ±18.2 43.82 ±7.78 45.43 ±13.8 64.00 ±10.7 32.13 ±8.47\nMultiple images display 54.61 ±9.81 70.09 ±3.96 54.83 ±8.89 56.00 ±0.62 59.71 ±16.4 46.25 ±7.30\nK-means clustering 40.22 ±9.25 57.08 ±11.2 57.53 ±15.0 58.57 ±5.89 59.14 ±5.39 38.38 ±6.92\nOptimization-based\nWith random labels 41.24 ±6.85 72.81 ±3.13 48.31 ±4.44 53.24 ±1.72 79.86 ±4.43 45.13 ±1.71\nWith majority vote 53.71 ±5.41 87.19 ±1.83 51.24 ±2.72 62.29 ±1.63 61.07 ±1.89 51.50 ±3.36\nSPO 52.81 ±9.59 70.34 ±3.73 51.69 ±1.59 53.14 ±2.24 79.71 ±9.14 45.26 ±2.47\nAutoSEP (Ours) 59.18 ±0.53 87.42 ±1.10 65.45 ±1.24 66.57 ±1.42 82.71 ±2.43 55.38 ±1.00\niNat_lupine StanfordDogs_terrier\nOptimization-free\nVanilla zero-shot 62.00 ±2.72 70.00 ±3.59 44.22 ±3.74 60.80 ±2.13 91.39 ±0.48 73.20 ±2.40\nZero-shot with descriptions 56.67 ±1.92 73.00 ±1.25 44.00 ±2.49 66.22 ±2.41 91.42 ±1.17 76.00 ±2.53\nZero-shot with majority vote 61.78 ±1.24 72.33 ±1.70 47.33 ±2.06 62.67 ±0.73 90.89 ±1.09 76.27 ±2.13\nFew-shot with random labels 47.56 ±7.31 48.33 ±5.48 42.00 ±7.38 60.27 ±10.3 66.89 ±24.7 52.13 ±8.90\nMultiple images display 59.56 ±7.11 59.67 ±3.56 52.22 ±6.63 56.93 ±12.5 70.00 ±7.86 63.73 ±8.23\nK-means clustering 47.11 ±6.50 50.00 ±2.11 30.00 ±1.99 39.60 ±7.50 68.67 ±1.09 64.13 ±5.66\nOptimization-based\nWith random labels 58.44 ±4.80 73.67 ±1.94 46.00 ±3.62 53.33 ±1.94 90.00 ±1.11 63.73 ±1.87\nWith majority vote 59.33 ±1.81 74.17 ±1.83 47.33 ±4.37 62.27 ±1.82 91.11 ±1.41 77.74 ±1.07\nSPO 56.44 ±3.17 64.88 ±6.12 46.56 ±3.44 65.16 ±6.10 81.47 ±2.12 64.27 ±2.00\nAutoSEP (Ours) 66.15 ±1.47 76.60 ±1.40 54.67 ±3.33 69.28 ±0.97 92.18 ±1.09 81.70 ±1.20\nVegFru_greens VegFru_allium\nOptimization-free\nVanilla zero-shot 86.00 ±0.56 78.82 ±0.54 68.89 ±3.06 72.67 ±3.18 60.44 ±1.66 54.22 ±3.40\nZero-shot with descriptions 77.11 ±3.19 77.33 ±1.94 63.61 ±1.82 74.44 ±1.98 64.89 ±1.51 59.17 ±1.98\nZero-shot with majority vote 85.83 ±0.48 78.00 ±0.44 71.39 ±2.53 75.56 ±1.82 62.44 ±2.57 56.89 ±2.37\nFew-shot with random labels 74.44 ±6.30 70.22 ±8.47 33.33 ±0.00 67.33 ±3.18 61.33 ±10.6 33.33 ±0.00\nMultiple images display 69.56 ±4.87 62.67 ±9.44 33.33 ±0.00 76.67 ±2.08 51.78 ±2.69 34.00 ±1.33\nK-means clustering 67.78 ±5.24 67.56 ±4.94 64.00 ±5.19 51.33 ±14.1 50.89 ±9.33 42.67 ±7.72\nOptimization-based\nWith random labels 80.56 ±4.33 79.78 ±0.44 48.52 ±2.77 71.56 ±2.06 64.22 ±1.47 55.19 ±4.48\nWith majority vote 81.19 ±4.27 74.00 ±0.89 55.56 ±0.91 74.22 ±0.44 64.67 ±0.83 56.94 ±1.21\nSPO 84.37 ±5.26 78.40 ±0.87 57.78 ±3.14 73.78 ±1.81 65.56 ±2.90 60.00 ±0.91\nAutoSEP (Ours) 87.78 ±0.70 81.11 ±0.70 72.22 ±0.91 78.89 ±0.99 69.11 ±0.83 62.44 ±2.44\n7\n--- Page 8 ---\nDescription-generation prompt benefits from optimization. We also observe that in some cases,\nthe initial description generation prompt can slightly improve classification performance. However,\nthese improvements are generally marginal. And in some cases, generating descriptions even degrades\nthe performance. This suggests that unguided or naive prompting alone for description generation\nis insufficient. More principled guidance and optimization are necessary for the model to generate\ninformative and task-relevant descriptions that more effectively support classification.\nInstance-level classification is an effective unsupervised scoring signal, outperforming alterna-\ntive unsupervised scoring strategies. A key component of our method is the use of instance-level\nclassification accuracy as the unsupervised scoring signal during optimization. To better understand\nits effectiveness, we compare it against three alternative unsupervised scoring methods for optimiza-\ntion: (1) Classification accuracy with randomly assigned labels; (2) Classification accuracy with\nmajority vote pseudo-labels, where pseudo-labels are obtained by aggregating high-temperature\npredictions from the MLLM; (3) Pairwise output evaluation in SPO [ 30], which relies on pairwise\ncomparisons of model outputs to estimate relative prompt quality.\nWhile these alternatives sometimes yield performance gains over optimization-free baselines, they\nexhibit limited reliability and overall lower performance compared to our instance-level classification\napproach. Majority vote pseudo-labeling tends to perform better than using random labels, but\nboth methods fall short in stability and effectiveness. Although SPO has demonstrated strong\nperformance on several text-only reasoning and question answering tasks, it does not transfer well to\nfine-grained image classification. The results suggest that while LLMs can effectively compare the\nquality of reasoning paths in purely textual contexts, MLLMs struggle to consistently evaluate the\ninformativeness of image descriptions for fine-grained classification. As a result, such evaluations\noffer weak and often noisy feedback signals for optimization. These results highlight the importance\nof instance-level classification as a robust and effective unsupervised signal.\n4.3 Discussion and Analysis\nLearning dynamics. We study how the classification performance evolves throughout the optimiza-\ntion process and how many iterations are usually required for the algorithm to take effect. Full details\nof experimental settings are mentioned in Appendix C.1. The average results and their variances\nreported in Figure 3a and 3b are computed by averaging these classification performances across all\nthree runs. The Max results are computed by taking the best performance among the four prompts at\neach iteration, then averaging across the three independent runs.\nBoth the instance-level (Figure 3a) and class-wise (Figure 3b) classification performance consistently\nimproves with more optimization iterations. More demonstrations on other datasets can be found\nin Appendix C.1. The correlational evidence suggests that instance-level classification serves as an\neffective unsupervised optimization signal to guide the model toward better class-wise discrimination.\nThese results validate our design of leveraging instance-level feedback to drive prompt optimization\nfor fine-grained classification in the absence of labeled data.\n0 1 2 3 4 5 6\nIteration0.660.680.700.720.740.760.780.800.82Accuracy\nCUB_cuckoo - Gemini\nMax\nAverage ± Std\n(a) Instance-level Classification\n0 1 2 3 4 5 6\nIteration0.520.540.560.580.600.62Accuracy\nCUB_cuckoo - Gemini\nMax\nAverage ± Std (b) Class-wise Classification\n0 1 2 3 4 5 6\nIteration0.200.250.300.350.400.450.500.550.60Diversity\nCUB_cuckoo\nGemini\nGPT-4o\nQwen (c) Prompt word diversity\nFigure 3: Evolution of metrics with increasing optimization iterations.\nCorrelation between instance-level classification and class-wise classification. To further support\nthe effectiveness of instance-level classification as an optimization signal, we also compute the statis-\ntical Pearson correlation between instance-level and class-wise classification performance. Table 2\n8\n--- Page 9 ---\nTable 2: Correlation between instance-level classification and class-wise classification.\nDatasets CUB_cuckoo CUB_oriole CUB_vireo iNat_butterfly iNat_lupine\nGemini 1.5 Flash 0.72±0.11 0.47 ±0.24 0.70 ±0.19 0.63 ±0.22 0.53 ±0.02\nQwen2-VL-72B-Instruct 0.63±0.13 0.38 ±0.15 0.64 ±0.10 0.84 ±0.11 0.70 ±0.17\nshows strong positive Pearson correlation across different settings, suggesting that improvements in\ninstance-level accuracy are closely associated with better class-wise classification performance. This\nempirical evidence reinforces our hypothesis that instance-level supervision can serve as a powerful\nproxy to guide the description optimization for fine-grained image classification in the absence of\nlabeled data.\nDiversity of generation prompts. To analyze how much new information is introduced during the\noptimization process, we evaluate the diversity of the generated prompts using a quantitative diversity\nscore. Specifically, for each prompt, we count the number of semantic keywords (lemmas of nouns,\nverbs, and adjectives excluding stop words), as well as the number of unique words, defined as words\nthat appear only in that specific prompt and not in any other prompt throughout the optimization.\nThese counts are normalized to compute the diversity score for each prompt.\nAs shown in Figure 3c, the diversity score steadily increases over iterations, indicating that the\noptimization process encourages the generation of increasingly varied and semantically rich prompts.\nThis trend suggests that the model is progressively exploring a broader and more informative prompt\nspace, which may potentially contribute to improved classification performance.\nComputational complexity. Letbdenote the number of top prompts retained, lis the number of\nprompt reflections generated per retained prompt, kis the number of negative samples used per image\nin the instance-level classification, and nis the number of unlabeled images. Each iteration of our\nalgorithm involves generating b·lreflections, and b·lmodified candidate prompts accordingly. For\neach of these prompts, we generate descriptions for all nimages, resulting in b·l·nquiries to the\nMLLM for description generation. Additionally, instance-level classification requires comparing\neach image’s description against knegative samples under each prompt, leading to b·l·k·nfurther\nMLLM queries. Therefore, the total number of MLLM queries per iteration is (2 +n+kn)·bl.\nIn our implementation, we typically set b= 4, l≤5,andk= 2, resulting in an approximate\nquery complexity of O(60n)per iteration. The actual runtime depends on the inference speed of\nthe MLLMs. For example, one optimization iteration on 60 images with Gemini 1.5 Flash takes 12\nminutes in average, demonstrating that AutoSEP is computationally feasible for practical use.\n0 1 2 3 4 5 6\nIteration0.480.500.520.540.560.580.60Accuracy\nCUB_cuckoo - Gemini\nn=30\nn=59\nn=69\nFigure 4: Classification accuracy of\nGemini with various number of sam-\nples for optimization.More samples, stronger instance-level signals, more stable\noptimization. We investigate how the number of unlabeled\nsamples used during optimization impacts the performance of\nour method. Figure 4 shows a clear trend that the classification\nperformance improves with increased number of samples. More\ndemonstrations on other datasets can be found in Appendix C.2.\nWhen the sample size is too small, instance-level classifica-\ntion is constrained by limited variation and coverage, making\nit difficult to reliably evaluate a prompt’s ability to generate\ndistinctive and meaningful descriptions. This leads to subopti-\nmal performance and introduces more noise and instability into\nthe optimization process. In contrast, with more samples, the\ninstance-level signal becomes more robust, enabling more sta-\nble optimization and resulting in consistent gains in class-wise\nclassification accuracy. Notably, our method begins to perform\nreliably with as few as 60 unlabeled samples , demonstrating\nits practicality and scalability even in limited-data settings.\n9\n--- Page 10 ---\n5 Conclusion\nIn this paper, we present a self-supervised prompt learning framework that improves the fine-grained\nimage classification capabilities of MLLMs using only unlabeled data and black-box MLLM access.\nBy optimizing the prompt for image description generation through instance-level classification\nfeedback, our method enables MLLMs to focus on subtle, discriminative features critical for distin-\nguishing between visually similar categories. Extensive experiments across multiple fine-grained\nclassification tasks and MLLMs demonstrate the robustness of our approach.\nLimitations. Our method relies on the implicit assumption that subtle, class-discriminative visual\nfeatures can be accurately captured and described using natural language. This may not hold in\ncases where visual distinctions are extremely fine-grained or difficult to verbalize. Additionally,\nalthough our approach is fully unsupervised, it still requires a sufficient number of unlabeled samples\nto ensure stable and effective prompt optimization. The absence of labeled data inherently limits the\nperformance compared to fully supervised methods that benefit from explicit class-level guidance.\nReferences\n[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\nURL https://arxiv.org/abs/2204.14198 .\n[2]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\narXiv:2308.12966 , 1(2):3, 2023.\n[3]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning , pages\n1597–1607. PmLR, 2020.\n[4]Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi\nHu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal\nmodels with open-source suites. Science China Information Sciences , 67(12):220101, 2024.\n[5]E Dataset. Novel datasets for fine-grained image categorization. In First workshop on fine grained visual\ncategorization, CVPR. Citeseer. Citeseer. Citeseer , volume 5, page 2. Citeseer, 2011.\n[6]Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P.\nXing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022.\nURL https://arxiv.org/abs/2205.12548 .\n[7]Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang\nZhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image\ncomposition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420 , 2024.\n[8]Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel.\nPromptbreeder: Self-referential self-improvement via prompt evolution, 2023. URL https://arxiv.\norg/abs/2309.16797 .\n[9]Gregor Geigle, Radu Timofte, and Goran Glavaš. African or european swallow? benchmarking large\nvision-language models for fine-grained object classification. arXiv preprint arXiv:2406.14496 , 2024.\n[10] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\nYang. Evoprompt: Connecting llms with evolutionary algorithms yields powerful prompt optimizers, 2025.\nURL https://arxiv.org/abs/2309.08532 .\n[11] Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, and Yuxin Peng. Analyzing and boosting the power of\nfine-grained visual recognition for multi-modal large language models. arXiv preprint arXiv:2501.15140 ,\n2025.\n[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 9729–9738, 2020.\n10\n--- Page 11 ---\n[13] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 ,\n2024.\n[14] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\nWilliam El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825 .\n[15] Gurusha Juneja, Nagarajan Natarajan, Hua Li, Jian Jiao, and Amit Sharma. Task facet learning: A\nstructured approach to prompt optimization, 2024. URL https://arxiv.org/abs/2406.10504 .\n[16] Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, and Sungroh Yoon. Toward robust hyper-detailed\nimage captioning: A multiagent approach and dual evaluation metrics for factuality and coverage. arXiv\npreprint arXiv:2412.15484 , 2024.\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models, 2023. URL https://arxiv.org/abs/2301.\n12597 .\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural\ninformation processing systems , 36:34892–34916, 2023.\n[19] Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu Sebe, and Elisa Ricci. Democratizing\nfine-grained visual recognition with large language models. arXiv preprint arXiv:2401.13837 , 2024.\n[20] Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu,\nHarry Yang, Sernam Lim, and Bo Zhao. Unveiling the ignorance of mllms: Seeing clearly, answering\nincorrectly, 2025. URL https://arxiv.org/abs/2406.10638 .\n[21] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495 , 2023.\n[22] Yushan Feng Saihui Hou and Zilei Wang. Vegfru: A domain-specific dataset for fine-grained visual\ncategorization. In IEEE International Conference on Computer Vision , 2017.\n[23] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts, 2020. URL https:\n//arxiv.org/abs/2010.15980 .\n[24] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nDamien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n[25] Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha.\nBenchmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 12884–12893, 2021.\n[26] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\n[27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any\nresolution. arXiv preprint arXiv:2409.12191 , 2024.\n[28] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment\nand uniformity on the hypersphere. In International conference on machine learning , pages 9929–9939.\nPMLR, 2020.\n[29] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing,\nand Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt\noptimization, 2023. URL https://arxiv.org/abs/2310.16427 .\n[30] Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin\nWu, and Yuyu Luo. Self-supervised prompt optimization. arXiv preprint arXiv:2502.06855 , 2025.\n[31] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V . Le, Denny Zhou, and Xinyun Chen.\nLarge language models as optimizers, 2024. URL https://arxiv.org/abs/2309.03409 .\n11\n--- Page 12 ---\n[32] Dingchen Yang, Bowen Cao, Guang Chen, and Changjun Jiang. Pensieve: Retrospect-then-compare\nmitigates visual hallucination. arXiv preprint arXiv:2403.14401 , 2024.\n[33] Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Junqi Zhang, Yangyang Li, and Fuli\nFeng. Dual-phase accelerated prompt optimization, 2024. URL https://arxiv.org/abs/2406.13443 .\n[34] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E. Gonzalez. Tempera: Test-time\nprompting via reinforcement learning, 2022. URL https://arxiv.org/abs/2211.11890 .\n[35] Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena\nYeung-Levy. Why are visually-grounded language models bad at image classification? arXiv preprint\narXiv:2405.18415 , 2024.\n[36] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. Large language models are human-level prompt engineers, 2023. URL https://arxiv.org/abs/\n2211.01910 .\n[37] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 ,\n2023.\n12\n--- Page 13 ---\nA Examples of Descriptions\nOptimized prompt :\nAnalyze the bird depicted in the image, focusing on\nthe following details:\n* **Beak: ** Describe its color , length, shape (e.g.,\ncurved, pointed, blunt), and any distinctive markings.\n* **Head: ** Describe its color , any distinctive\nmarkings (e.g., eye stripe, eyebrow , throat patch), and\nthe presence or absence of a crest.\n* **Wings: ** Note the color , pattern (e.g., banded,\nbarred, spotted), and any white patches or bands.\n* **Tail:** Describe its length, shape (e.g., graduated,\nrounded, square), and any distinctive patterns (e.g.,\nbanding, barring).\n* **Body: ** Describe the overall body shape (e.g.,\nslender , stout), the color of the chest and belly , and\nany distinctive markings.\n* **Overall Size and Posture: **  Mention if the bird\nappears large or small, and describe its posture (e.g.,\nupright, hunched, relaxed).\n**Exclude any background details or other information\nfrom your description.**Initial prompt :\nDescribe the bird in the given image in detail, focusing\non highly distinctive attributes that are typical to this\nbird. Ignore the background or other information.The bird is a small bird  with a long,  \nslender tail .Partially correct! Black-billed Cuckoo is medium-small, and does have a long,\nslender tail. So tail description is good, but \"small\" is relative.\nIt has a long, pointed  beak and a small  \nhead .Partially correct! The cuckoo does have a long, slightly curved beak, though\n\"pointed\" is not quite accurate. \"Small head\" is a neutral observation.\nIts plumage is a mixture of brown and  \ngray, with  a distinctive white patch on  \nthe back of the neck .Incorrect! Black-billed Cuckoo is plain brown above, light below , no white\nneck patch. The \"white patch\" is not a feature of this species.\nThe bird's eyes are dark brown and its \nlegs are short and slender .Incorrect! Black-billed Cuckoo has a prominent red eye-ring. Red eye is\ndistinctive. Legs being \"short and slender\" is plausible but not a key feature.\nThe bird's head is brown  with a \nreddish-brown eye .Partially correct! Head is brown . Eye is red, but it's\nthe eye-ring that's red. So, close but imprecise.\nThe bird has a long, slender tail with a \ndistinctive white tip .Incorrect! Black-billed Cuckoo has white spots on the underside of the tail,\nnot a distinctive \"white tip.\" This sentence seems to misattribute a tail marking.Generated\nDescriptionsDescription\nAnalysesImage &\nPrompts\nIts wings are short and rounded . Correct! The cuckoo has relatively short, rounded wings.\nThe bird has a distinctive white patch  \non the back of its neck , which is a key  \nidentifying feature of this species.Incorrect! This is not a feature of the Black-billed Cuckoo.\nThere is no white neck patch.\nThe bird has a long, slightly curved,  \ndark beak .Correct! The beak of the Black-billed Cuckoo is long, slightly decurved\n(curved downward), and dark (black). This matches perfectly .\nIts head is brown , with a prominent red  \neye.Mostly correct! The head is brown. The eye is dark with a prominent red\neye-ring. The description says \"red eye,\" which is fine in casual terms, but\ntechnically it’ s a red eye-ring.\nIt has a long, graduated tail  with a \nslightly rounded tip .Correct! The cuckoo’ s tail is long and graduated (steps in feather length\ntoward the edges). The tip appears rounded when folded, which matches the\ndescription.\nThe wings are brown with no  \ndistinctive markings .Correct! The Black-billed Cuckoo's wings are plain brown, no bold patterns\nor bars. This is an accurate observation.\nThe bird's body is slender and brown , \nwith a lighter underside .Correct! The image clearly shows a slender body , brown upperparts,\nand a pale/light underside.\nIt is perched on a branch , appearing  \nrelaxed and upright .Correct! This matches the posture in the image perfectly: relaxed and\nupright on a branch is typical behavior for this species.\n(a) Black-billed Cuckoo\nIt has a long, slender beak with a  \nyellow tip and a black base.Optimized prompt :\nDescribe the bird in the given image in detail, focusing\non its beak color , eye color , and any distinctive\npatterns on its feathers. For example, describe the\npresence or absence of a white throat patch, the color\nof its legs, and the length of its tail. Ignore the\nbackground or other information.\nCorrect! The bird’ s beak is long and slightly curved , with a yellow lower\nmandible  and darker upper mandible , typical of the Yellow-billed Cuckoo.\nIts eyes  are dark brown .Correct! The bird has dark eyes, and this matches the typical dark brown eye\ncolor of the species .\nThe bird has a white throat patch  and \na long, rounded tail .Correct! The bird has a white throat and chest. The long tail is also visible and\nhas the rounded shape typical of Yellow-billed Cuckoos .\nThe feathers  on its body are a brown  \nand white .Correct! The upperparts (back and wings) are uniformly brown, and the\nunderparts are white.The bird is a cuckoo with a long,  \nslender body .Correct! The Yellow-billed Cuckoo has a long, slender body .\nIt has a long, pointed tail  and a short,  \nstout beak .Incorrect! While the tail is indeed long, the beak is not short and stout.\nThe bird's head is small and round with \na distinctive black and white pattern .Incorrect! There is no distinctive black and white facial pattern in Yellow-billed\nCuckoos. The face is mostly plain grayish or brownish, with a subtle white throat and\ncheek.\nThe black stripe that runs through the  \neye is very prominent.Incorrect! The eye appears dark and there is a slight dusky eye-line,\nbut it is not a bold black stripe like in shrikes or some warblers.\nThe bird's plumage is\na mottled  brown and white, with a  \nreddish-brown patch on its back.Incorrect! The plumage is not mottled. It's more cleanly colored with upperparts\nsmooth grayish-brown and underparts clean white.\nThe bird's legs are long and slender , \nwith yellow feet .Incorrect! Yellow-billed Cuckoo legs are fairly short, not long and slender .\nThe legs and feet are grayish or bluish-gray , not yellow .Generated\nDescriptionsFact\nCheckImage &\nPrompts\nInitial prompt :\nDescribe the bird in the given image in detail, focusing\non highly distinctive attributes that are typical to this\nbird. Ignore the background or other information.\n(b) Yellow-billed Cuckoo\nFigure 5: Examples showcasing descriptions generated from two different prompts using Gemini 1.5 Flash.\nAttributes highlighted in green indicate correct information, while those in red denote incorrect attributes.\nFigure 5 shows two exampls of image descriptions generated by Gemini 1.5 Flash [ 24] using prompts before\nand after optimization. The initial descriptions tend to be vague or misleading, often misrepresenting key visual\nattributes. In contrast, the optimized prompts lead to more accurate and reliable descriptions that better reflect\nthe distinctive features of the images.\n13\n--- Page 14 ---\nB Detailed AutoSEP Workflow and Prompts\nInitial\nPrompt\nCandidate\nPrompts\nInstance-Level\nError Collection\nError\nReflection\nGenerateNew\nPrompts\nPredictCandidate  \nDescriptionsSelect Top-k prompts\nInstance-Level\nClassificationError Collecti on\n## Image 1\nCorrect Description: {correct_description}\nWrong Description: {wrong_description}\n## Image 2\nCorrect Description: {correct_description}\nWrong Description: {wrong_description}\n...\nError Reflection  → Reflect(·)\nI'm trying to write a prompt used to generate description for the bird in the image, where the\ngenerated description can enhance the zero-shot classification of the bird in the image.\nThe three types of birds are: A. Black-billed Cuckoo, B. Mangrove Cuckoo, C. Yellow-billed Cuckoo.\nMy current prompt is: {prompt}\nBut this prompt generates image descriptions that are too simple, similar and vague, making it\ndifficult to distinguish which description correctly matches the image and leading to the wrong\ndescription being chosen for the following examples  {error_string}\nGive a reasons why the prompt could have gotten these examples wrong.\nNew Prompts  → Modify(·)\nI'm trying to write a prompt used to generate description for the bird in the image, where the generated description can\nenhance the zero-shot classification of the bird in the image.\nThe three types of birds are: A. Black-billed Cuckoo, B. Mangrove Cuckoo, C. Yellow-billed Cuckoo.\nMy current prompt is: {prompt}\nBut this prompt generates image descriptions that makes it dif ficult to distinguish which description correctly matches the\nimage and leading to the wrong description being chosen for the following examples: {error_string}\nBased on these examples the problem with this prompt is that {feedback_str}\nBased on the above information, I wrote k different improved prompts. Each prompt is wrapped with <ST ART> and <END>.\nThe new prompt is:\nInstance-Level Classification\nText 1: {description_1}\nText 2: {description_2 }\nWhich description correctly describes the image? The first text or the second text?{error_string}\nfeedback_str\nFigure 6: Workflow of AutoSEP with Optimization Prompts. Take CUB_cuckoo as an example.\nFigure 6 shows the detailed workflow of AutoSEP and prompts used in the optimization. The prompt learning\nprocedure is introduced in Section 3.3.2.\nC Additional Ablation Studies\nC.1 Learning Dynamics\nWe study how the classification performance evolves throughout the optimization process and how many\niterations are usually required for the algorithm to take effect. For each experimental setting, we perform three\nindependent runs. In each run, we retain the top four candidate prompts at every iteration based on the scoring\nfunction. We use the average classification performance of these four prompts to represent the performance for\nthat iteration. The average results and their variances reported in Figure 7 are computed by averaging these\nclassification performances across all three runs. The Max results are computed by taking the best performance\namong the four prompts at each iteration, then averaging across the three independent runs.\nC.2 Number of Samples\nMore experiments on different datasets are conducted to study the effect of the number of samples on our\nAutoSEP framework (See Figure 8). The results are quite consistent across different classification tasks. When\nthe sample size is too small, the optimization process is unstable with large variance. In contrast, with more\nsamples, the instance-level signal becomes more robust, resulting in more stable optimization and consistent\ngains in class-wise classification accuracy.\n14\n--- Page 15 ---\n0 1 2 3 4 5 6\nIteration0.660.680.700.720.740.760.780.800.82Accuracy\nCUB_cuckoo - Gemini\nMax\nAverage ± Std(a) Instance-level Classification\n0 1 2 3 4 5 6\nIteration0.600.650.700.750.80Accuracy\nCUB_vireo - Qwen\nMax\nAverage ± Std (b) Instance-level Classification\n0 1 2 3 4 5 6\nIteration0.780.800.820.840.860.880.900.92Accuracy\niNat_butterfly - Gemini\nMax\nAverage ± Std (c) Instance-level Classification\n0 1 2 3 4 5 6\nIteration0.520.540.560.580.600.62Accuracy\nCUB_cuckoo - Gemini\nMax\nAverage ± Std\n(d) Class-wise Classification\n0123456\nIteration0.5000.5250.5500.5750.6000.6250.650Accuracy\nCUB_vireo - Qwen\nMax\nAverage ± Std (e) Class-wise Classification\n0 1 2 3 4 5 6\nIteration0.560.580.600.620.640.660.68Accuracy\niNat_butterfly - Gemini\nMax\nAverage ± Std (f) Class-wise Classification\nFigure 7: Classification accuracy as a function of iteration.\n0123456\nIteration0.4000.4250.4500.4750.5000.5250.5500.575Accuracy\nCUB_oriole - Gemini\nn=30\nn=58\nn=76\n(a) Class-wise Classification\n0 1 2 3 4 5 6\nIteration0.460.480.500.520.540.560.580.60Accuracy\nCUB_vireo - Gemini\nn=30\nn=54\nn=76 (b) Class-wise Classification\n0123456\nIteration0.4500.4750.5000.5250.5500.5750.6000.6250.650Accuracy\niNat_butterfly - Gemini\nn=30\nn=60\nn=90 (c) Class-wise Classification\nFigure 8: Classification accuracy of Gemini with various number of samples for optimization.\nD Prompt Details\nIn this section, we provide the prompts used for baselines in our experiments. Blue boxes show those used\ndirectly for classification; orange box shows the description generation prompt before optimization; red box\nshows the prompt used for SPO [30].\nZero-shot ( CUB_cuckoo )\n# Task\nDetermine what kind of bird this image shows from the following options:\nA. Black-billed Cuckoo\nB. Mangrove Cuckoo\nC. Yellow-billed Cuckoo\nAnswer the letter from A to C as prediction.\nThe answer is:\n15\n--- Page 16 ---\nZero-shot with image description ( CUB_cuckoo )\n# Task\nDetermine what kind of bird this image shows from the following options:\nA. Black-billed Cuckoo\nB. Mangrove Cuckoo\nC. Yellow-billed Cuckoo\nAnswer the letter from A to C as prediction.\n# Prediction\nText: The image shows the following features: {description}\nThe answer is:\nFew-shot with random labels ( CUB_cuckoo )\nYour task is to classify the image to three birds: A. Black-billed Cuckoo, B. Mangrove Cuckoo, C.\n→Yellow-billed Cuckoo.\nThe classification of the 1 image is: C\nThe classification of the 2 image is: B\nThe classification of the 3 image is: C\n...\nThe classification of the last image is: (Answer Letter A or B or C)\nMultiple images display ( CUB_cuckoo )\nYour task is to classify the image to three birds: A. Black-billed Cuckoo, B. Mangrove Cuckoo, C.\n→Yellow-billed Cuckoo.\nThe first k images show distinct types of birds.\nThe classification of the last image is: (Answer Letter A or B or C)\nInitial image description generation ( CUB_cuckoo )\n# Task\nDescribe the bird in the given image in detail, focusing on highly distinctive attributes that\n→are typical to this bird. Ignore the background or other information.\nSPO description quality evaluation ( CUB_cuckoo )\nI’m trying to write a prompt used to generate description for the bird in the image, where the\n→generated description can enhance the zero-shot classification of the bird in the image.\nThe three types of birds are: A. Black-billed Cuckoo, B. Mangrove Cuckoo, C. Yellow-billed Cuckoo.\nThere are two descriptions of the given images generated by two different prompts:\nText 1: {description_1}\nText 2: {description_2}\nWhich description better describes the image? The first text or the second text? Provide your\n→analysis and the choice you believe is better, using XML tags to encapsulate your response.\n<analyse>Some analysis</analyse>\n<choose>First/Second (the better answer in your opinion)</choose>\n16",
  "text_length": 59664
}