{
  "id": "http://arxiv.org/abs/2506.00894v1",
  "title": "CODEMENV: Benchmarking Large Language Models on Code Migration",
  "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious software engineering tasks; however, their effectiveness in code\nmigration, adapting code to run in different environments, remains\ninsufficiently studied. In this work, we introduce CODEMENV: Code Migration\nAcross Environment, a new benchmark specifically designed to assess LLMs'\nabilities in code migration scenarios. CODEMENV consists of 922 examples\nspanning 19 Python and Java packages, and covers three core tasks: (1)\nidentifying functions incompatible with specific versions, (2) detecting\nchanges in function definitions, and (3) adapting code to target environments.\nExperimental evaluation with seven LLMs on CODEMENV yields an average pass@1\nrate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings\ninclude: (i) LLMs tend to be more proficient with newer function versions,\nwhich aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical\ninconsistencies by identifying function changes irrelevant to the intended\nmigration environment. The datasets are available at\nhttps://github.com/xdshen-ai/Benchmark-of-Code-Migration.",
  "authors": [
    "Keyuan Cheng",
    "Xudong Shen",
    "Yihao Yang",
    "Tengyue Wang",
    "Yang Cao",
    "Muhammad Asif Ali",
    "Hanbin Wang",
    "Lijie Hu",
    "Di Wang"
  ],
  "published": "2025-06-01T08:29:59Z",
  "updated": "2025-06-01T08:29:59Z",
  "categories": [
    "cs.SE",
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00894v1",
  "full_text": "--- Page 1 ---\narXiv:2506.00894v1  [cs.SE]  1 Jun 2025CODE ME NV: Benchmarking Large Language Models on Code Migration\nKeyuan Cheng*,1,3,4, Xudong Shen*,1,4, Yihao Yang*,1,4, Tengyue Wang1,4, Yang Cao1,4,\nMuhammad Asif Ali2, Hanbin Wang3, Lijie Hu†,1,2, Di Wang†,1,2\n1Provable Responsible AI and Data Analytics (PRADA) Lab\n2King Abdullah University of Science and Technology\n3Peking University4South China University of Technology\nAbstract\nLarge language models (LLMs) have shown\nremarkable capabilities across various soft-\nware engineering tasks; however, their effec-\ntiveness in code migration—adapting code to\nrun in different environments—remains insuf-\nficiently studied. In this work, we intro-\nduce C ODEME NV:Code Migration Across\nEnvironment, a new benchmark specifically\ndesigned to assess LLMs’ abilities in code\nmigration scenarios. C ODEME NVconsists\nof 922 examples spanning 19 Python and\nJava packages, and covers three core tasks:\n(1) identifying functions incompatible with\nspecific versions, (2) detecting changes in\nfunction definitions, and (3) adapting code\nto target environments. Experimental eval-\nuation with seven LLMs on C ODEME NV\nyield an average pass@1 rate of 26.50%,\nwith GPT-4 Oachieving the highest score at\n43.84%. Key findings include: (i) LLMs\ntend to be more proficient with newer func-\ntion versions, which aids in migrating legacy\ncode, and (ii) LLMs sometimes exhibit log-\nical inconsistencies by identifying function\nchanges irrelevant to the intended migra-\ntion environment. The datasets are available\nathttps://github.com/xdshen-ai/\nBenchmark-of-Code-Migration .\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable abilities in software engi-\nneering tasks, including code generation (Jiang\net al., 2024; Du et al., 2024) and code trans-\nlation (Yuan et al., 2024; Eniser et al., 2024).\nGeneral-purpose LLMs such as GPT-4 (Achiam\net al., 2023), Claude-3 (The), and DeepSeek (Shao\net al., 2024) consistently achieve state-of-the-art\nresults across diverse programming challenges, of-\nten outperforming traditional approaches on es-\n*Equal Contribution.\n†Corresponding Author.\nAfter Numpy 1.26, numpy .compar e_chararrays  is deprecated in\nfavor of numpy .char .compar e_ chararrays.\n(a) code in Numpy 1.26\n(b) code in Numpy 2.0def has_common_char (arr1, arr2):\narr1 = numpy .array (arr1, dtype=str )\n        arr2 = numpy .array (arr2, dtype=str )  \n        if len (arr1) != len (arr2):\nraise  ValueError (\"The lengths of the two arrays must be the same\" )\n        comparison_result  = numpy .compar e_chararrays (arr1, arr2, '==' )\n        return  numpy .any(comparison_result )\ndef has_common_char (arr1, arr2):\narr1 = numpy .array (arr1, dtype=str )\n        arr2 = numpy .array (arr2, dtype=str )  \n        if len (arr1) != len (arr2):\n        raise  ValueError (\"The lengths of the two arrays must be the same\" )\n        comparison_result  = numpy .char .compar e_chararrays (arr1, arr2, '==' )\n        return  numpy .any(comparison_result )\nThe calling method has changed\nFigure 1: The function compare_chararrays un-\nderwent changes after Numpy 1.26, creating compati-\nbility issues between NumPy 1.26 and 2.0.\ntablished benchmarks. Beyond these general\nmodels, a range of specialized CodeLLMs have\nbeen introduced to further advance performance\non code-related tasks. Notable examples include\nCodeT5+ (Wang et al., 2023), CodeLlama (Roz-\nière et al., 2023), and StarCoder2 (Lozhkov\net al., 2024). Thanks to their tailored ar-\nchitectures and training data, these models ex-\nhibit a deep understanding of code structure and\nsyntax, frequently surpassing general LLMs on\nprogramming-specific benchmarks.\nDespite the impressive progress of LLMs in var-\nious code-related tasks, their ability to perform\ncode migration—adapting code to run correctly\nin a different environment—remains largely un-\nexplored. Code migration is a critical challenge\nin practical software development. For example,\nwhen users attempt to run code obtained from\nsources like GitHub, they often encounter compat-\nibility issues that require significant manual effort\nto resolve. If LLMs could automate the migra-\ntion of code to fit a user’s existing environment,\nit would greatly streamline the process of environ-\nment setup and reduce the burden of manual con-\nfiguration.\nThe root cause of these compatibility issues\n--- Page 2 ---\nlies in the ongoing evolution and versioning of\nsoftware libraries. As libraries are updated and\nmaintained, APIs and function calls may change,\nleading to incompatibilities across different en-\nvironments. For instance, as illustrated in Fig-\nure 1, the function call compare_chararrays\nwas moved from the numpy package to the\nnumpy.char submodule. This change results in\nfunctionally equivalent code being implemented\ndifferently in NumPy 1.26 versus NumPy 2.0,\nhighlighting the challenges of code migration\nacross library versions.\nResearch on code migration is still in its in-\nfancy, with the majority of prior work concentrat-\ning on cross-language migration (i.e., code trans-\nlation) rather than migration across different li-\nbrary versions or environments. Only a few recent\nefforts, such as that by Google researchers (Ziftci\net al., 2025), have explored automated LLM-based\napproaches for identifying code changes required\nfor cross-version migration. However, there re-\nmains a significant gap: the absence of compre-\nhensive benchmarks to systematically assess the\ncode migration capabilities of LLMs.\nTo address this gap, we introduce a new bench-\nmark, C ODEME NV (Code Migration Across\nEnvironment). C ODEME NVis built from 922\nreal-world function changes, manually curated\nfrom official sources, spanning 19 Python and Java\npackages. As shown in Figure 2, the benchmark is\norganized into three tasks that collectively evalu-\nate LLMs’ abilities to perform code migration:\nTask 1: Locate version-incompatible functions.\nGiven a code snippet and a specified target en-\nvironment, the model is tasked with identifying\nfunctions or code segments that may not be com-\npatible with the running environment.\nTask 2: Answering function changes. The\nmodel must explain the specific modifications\nthese functions have undergone between versions.\nTask 3: Code migration. The model is then re-\nquired to revise or migrate the provided code to\nensure it runs correctly in the target environment,\naddressing any version-related incompatibilities.\nTo evaluate C ODEME NV, we conduct experi-\nments with nine LLMs. Results show that, for\nthe migration task, the average pass@1 rate across\nseven models is 26.50%, with GPT-4 Oachieving\nthe highest score at 43.84%. Our analysis uncov-\ners several notable insights:\n(i) Familiarity with function versions. LLMstend to be more familiar with newer function\nversions, which enables them to migrate legacy\ncode to modern environments more effectively, but\nmakes adapting newer code to older environments\nmore challenging.\n(ii) Logical inconsistencies. LLMs sometimes\ndisplay logical inconsistencies when identifying\nrelevant function changes for migration. For ex-\nample, when migrating code from version 1.16\nto 2.0, models may mistakenly reference changes\nfrom version 1.0, which are not pertinent to the\nmigration at hand.\n2 Related Work\n2.1 LLMs for Code\nLarge Language Models (LLMs) have achieved\nremarkable progress in code-related tasks such\nas generation, translation, and completion, owing\nto their large parameter counts and training on\nvast code corpora. Proprietary models like GPT-\n4 (Achiam et al., 2023), Claude-3 (The), and Gem-\nini (Reid et al., 2024) consistently deliver strong\nperformance across a broad spectrum of program-\nming challenges. In parallel, open-source mod-\nels such as Qwen2.5 (Team, 2024) have demon-\nstrated competitive or even superior results com-\npared to larger models, leveraging synthetic data\nto enhance their capabilities. Other open-source\nmodels, including Llama-3.1 (Abhimanyu Dubey\net al., 2024), Phi-3 (Abdin et al., 2024), and\nDeepSeek (Shao et al., 2024), also achieve impres-\nsive results, with DeepSeek in particular outper-\nforming many proprietary alternatives.\nThe field has also seen rapid advances in spe-\ncialized CodeLLMs. For example, CodeT5 (Wang\net al., 2021) employs an encoder-decoder architec-\nture and excels at code completion and retrieval,\nwhile CodeT5+ (Wang et al., 2023) introduces\nflexible encoder-only, decoder-only, and encoder-\ndecoder modes, achieving strong results on bench-\nmarks like HumanEval. CodeLlama (Rozière\net al., 2023), developed by Meta, extends Llama\n2 with code-specific training and supports multi-\nple languages, with its 34B variant showing ro-\nbust performance in code generation and comple-\ntion. StarCoder2 (Lozhkov et al., 2024), from Big-\nCode, is designed for multi-language code syn-\nthesis and retrieval, leveraging large-scale permis-\nsively licensed datasets. Qwen-Coder (Hui et al.,\n2024) is notable for its 32B variant, which sur-\npasses GPT-4o on several benchmarks, benefit-\ning from training on 5.5T tokens of diverse data\n--- Page 3 ---\nand strong human preference alignment. These\ndevelopments underscore the rapid evolution of\ndomain-specific LLMs and the narrowing gap be-\ntween open-source and proprietary solutions.\nIn this work, we assess the LLMs’ ability to mi-\ngrate code across different environments.\n2.2 Code Migration\nRecent progress in AI-driven code migration\nhas demonstrated encouraging results across di-\nverse programming scenarios. Amazon Q De-\nveloper (AmazonQ) exemplifies a generative AI\ntool tailored to assist developers in upgrading Java\napplications from versions 8/11 to 17, address-\ning the broader challenges of repository-level code\nmigration. Joe (Khoury, 2024) provides a com-\nprehensive analysis of the current landscape and\npersistent obstacles in large-scale migration ef-\nforts. The dynamics of human-AI collaboration\nin this context are explored by Omidvar Tehrani\net al. (Tehrani et al., 2024), who assess how de-\nvelopers and Amazon Q Developer interact during\nmigration tasks. Google researchers (Ziftci et al.,\n2025) have introduced an automated approach that\nintegrates change location discovery with LLM-\nbased guidance to streamline migration processes.\nIn the domain of legacy system modernization,\nKontogiannis (Kontogiannis et al., 2010) proposes\na semi-automated method for migrating PL/IX to\nC++, emphasizing iterative optimization to ad-\ndress performance issues. More recently, Almeida\net al. (Almeida et al., 2024) demonstrated that\nChatGPT can effectively support API migration,\nwith One-Shot prompting proving particularly ef-\nfective for migrating Python/SQLAlchemy appli-\ncations while preserving functionality and adopt-\ning modern features.\nAdditional discussion of related work is pro-\nvided in Appendix A.\n3 C ODE ME NV\nWe argue that despite significant challenges that\nenvironment-related issues present to program-\nmers, there is currently no systematic benchmark\nfor evaluating model capabilities in code migra-\ntion across different environments. To fill this\ngap, we introduce C ODEME NV(Code Migration\nAcross Environments), a benchmark designed to\nassess models’ understanding of function usage\ndifferences across library versions and their abil-\nity to perform cross-version code migration. The\nremainder of this section details the construction\nand characteristics of C ODEME NV.\nTASK 1 Locating V ersion-Incompatible Function\nThe running environment of this code is Numpy 2.0. Please locate the API that locates\nthe functions that are imcompatible with the environment.\nTASK 2 Answring Function Changes\nWhat changes have been made to this function?\nRigorous Unit T ests (task3) Agent T ests (task 1&2)\ndef has_common_char (arr1, arr2):\narr1 = numpy .array (arr1, dtype=str )\n        arr2 = numpy .array (arr2, dtype=str )  \n        if len (arr1) != len (arr2):\nraise  ValueError (\"The lengths of the two arrays must be the same\" )\n        comparison_result  = numpy .compar e_chararrays (arr1, arr2, '==' )\n        return  numpy .any(comparison_result )\nThis function has risks.\nnumpy .compar e_chararrays  is deprecated after Numpy 1.4\n Wrong ！\nTASK 3 Code Migration\nPlease provide the code that fixes the above error so that it can run normally under\nversion of Numpy 2.0. Figure 2: A data example of C ODEME NV, which in-\ncludes three tasks to evaluate LLMs on environment-\nrelated programming skills.\n3.1 Task Definition\nCODEME NVinclude the following three tasks:\nTask-1: Identify Version-Incompatible Func-\ntions. Given a code snippet and a specified target\nenvironment version, the model is asked to pin-\npoint functions that are incompatible with that en-\nvironment. This task is divided into two levels\nof difficulty: easy, where only one incompatible\nfunction is present, and hard , where multiple in-\ncompatible functions must be identified.\nTask-2: Answering Function Changes. For each\nfunction identified in Task-1 the model must ex-\nplain the nature of the change. This includes spec-\nifying the type of change (e.g. deprecation), the\nversion in which the change occurred, and any re-\nplacement function if applicable.\nTask-3: Code Migration. The model is required\nto revise the provided code so that it is compati-\nble with the target environment. Code migration\nscenarios are further categorized as follows:\n(a) N EW2OLD:The target environment version\nis older than the original, so the model must adapt\nnewer code to run in a legacy environment.\n(b) O LD2NEW:The target environment version\nis newer than the original, requiring the model to\nupgrade legacy code for compatibility with the up-\ndated environment.\n3.2 Dataset Statistics\nCODEME NVencompasses two widely used pro-\ngramming languages in the deep learning commu-\nnity: Python and Java. The Python portion of\nthe dataset spans 11 packages and contains a to-\n--- Page 4 ---\nStep 1  Data Collection\nData\nExampleStep 2  Code Generation\nLLM\nAfter Numpy 1.26, np.compare_chararrays  is deprecated in favor\nof np.char .compare_chararraysFunction Changes:\nFunction description:\nFunction changes\nFunction descriptionPrompt\ndef activation_stats (input_tensor ):\nsigmoid_output  = torch.nn.functional.sigmoid (input_tensor )\nrelu_output  = torch.nn.functional.relu (input_tensor )\n...\nYou are a very experienced\nprogrammer who is familiar\nwith the functions...\nStep 3  Test data Generation\nCodeCode description\nPlease use python code to help me with a function that\nprocesses a 1-dimensional tensor using certain activation\nfunctions from the torch library .Problem description:\nCode:\nPrompt\nLLM\nGenerate\nTest\nCasesCheck\nwhether\nrunnable\nTry three times until all three cases can runFigure 3: The construction process of C ODEME NV.Step 1: We collect function change information and function\ndescriptions from the official website; Step 2: Based on the collected functions, generate code that can run in the\noriginal version and its problem description; Step 3: Generate 3 test cases for each data and repeat three times\nuntil all cases can run correctly.\ntal of 587 samples, which are divided into two\ndifficulty levels: easy andhard . The easy sub-\nset comprises 396 samples, each featuring a single\nline of code that is incompatible with the target\nenvironment. The hard subset includes 191 sam-\nples, each containing kincompatible lines of code,\nwhere k∈ {2,3}. Table 1 summarizes the distri-\nbution of incompatible lines across the datasets.\nFor Java, the dataset covers 8 packages with 335\nsamples. Only the easy difficulty level is included\nfor Java, as the incompatible functions in these\npackages tend to be loosely connected, making it\ndifficult to construct hard instances with multiple\ninterdependent incompatibilities.\nAdditional details and comprehensive statistics\nfor C ODEME NVare provided in Appendix C.1.\n3.3 Function Changes\nWe categorize function changes in C ODEME NV\ninto three main types:\n•Addition (None →fnew): A new function\nfnewis introduced in a later version, meaning\nit is unavailable in earlier environments.\n•Deprecation (fold→None ): The function\nfoldis removed or no longer supported after a\ncertain version, so it cannot be used in newer\nenvironments.\n•Replacement (f→f′): The function fis re-\nplaced or modified to f′, which may involve\nchanges to the function name, parameters, or\nusage patterns.\nTable 6 in Appendix C.1 summarizes the distribu-\ntion of these change types in the Python portion of\nCODEME NV: 98 replacements, 35 deprecations,\nand 79 additions.Datasets 1-incom. 2-incom. 3-incom. Total\nPython (easy) 396 - - 396\nPython (hard) - 103 88 191\nJava 335 - - 335\nTable 1: Statistics of the number of incompatible lines\nof C ODEME NV.\n3.4 Evaluation\nIn this section, we outline the evaluation method-\nology for the three benchmark tasks, utilizing two\nprimary approaches:\nAgent-based Evaluation. To evaluate\nwhether LLMs can accurately identify version-\nincompatible functions and correctly describe\nthe changes those functions have undergone, we\nadopt an agent-based evaluation strategy. The\nagent is provided with ground-truth answers,\nincluding the set of incompatible functions and\ntheir corresponding changes. It then compares the\nLLMs’ predictions to these references according\nto the following criteria:\nFor Task-1, the evaluation requires the model\nto identify all functions that are incompatible with\nthe target environment. The prediction is consid-\nered correct only if the set of identified functions\nexactly matches the ground truth; missing or in-\ncorrectly including any function results in failure.\nThe accuracy for Task-1 is defined as:\nAcc Task-1 = 1[I=T] =(\n1,ifI=T\n0,otherwise,(1)\nwhere Iis the set of ground-truth incompatible\nfunctions, and Tis the set predicted by the LLM.\nFor Task-2, we assess three aspects: (i) whether\nthe LLM correctly identifies the type of change\n(see Section 3.3); (ii) whether the predicted ver-\nsion number of the change is accurate (allowing\n--- Page 5 ---\na margin of 0.5 between predicted and actual ver-\nsion numbers); (iii) for replacement-type changes,\nwhether the LLM correctly specifies the replace-\nment function (this is not required for addition or\ndeprecation cases). The accuracy for Task-2 is\ngiven by:\nAcc Task-2 = 1[ˆt=t|{z}\ntype∧|ˆv−v| ≤0.5|{z}\nversion\n∧(t̸=‘Replace’ ∨ˆf=f)| {z }\nfunction],\n(2)\nwhere ˆtandtare the predicted and ground-truth\nchange types, ˆvandvare the predicted and actual\nversion numbers, and ˆfandfare the predicted\nand ground-truth replacement functions.\nFurther details on the agent-based evaluation\nare provided in the third prompt of Appendix B.\nUnit Test-based Evaluation. For Task-3, we as-\nsess whether the migrated code preserves the orig-\ninal functionality by running a set of test cases on\nboth the original and migrated implementations.\nSpecifically, three test cases are executed for\neach code pair. The outputs of the original code\nserve as the reference, and the migrated code is\nconsidered correct only if it produces identical\noutputs for all test cases. The accuracy for Task-3\nis defined as:\nAcc Task-3 = 1\"3^\ni=1(mi=oi)#\n, (3)\nwhere miis the output of the migrated code and\noiis the output of the original code for the i-th test\ncase.\n3.5 Construction Process\nFigure 3 presents the data curation workflow\nfor C ODEME NV, which comprises three stages:\nStep 1: Data Collection. We begin by gather-\ning a comprehensive set of functions, along with\ntheir associated changes, descriptions, and sup-\nported version ranges.\nTo achieve this, we systematically review ver-\nsion release notes from the official documenta-\ntion of each package, cataloging all modified func-\ntions and detailing their changes across different\nversions. We also extract functional descriptions\nand usage information for each function to support\nsubsequent code generation. Since official docu-\nmentation often omits explicit version compatibil-\nity information, we empirically determine the sup-ported version ranges by executing the functions\nacross multiple package versions.\nIn total, our analysis yields 212 function\nchanges for Python and 114 for Java. The online\nsources referenced for this collection are listed in\nAppendix C.2.\nStep 2: Code Generation. Next, we generate\noriginal code samples based on the collected data.\nThis step leverages the advanced capabilities of\nGPT-4: by providing it with the function changes,\noriginal function definitions, and usage descrip-\ntions, we prompt GPT-4 to generate code that cor-\nrectly utilizes these functions.\nThe code generation scenario depends on the\ntype of function change ( i.e., O LD2NEW or\nNEW2OLD):\n(i) For addition-type changes ( None →fnew),\nwe create N EW2OLDsamples. GPT-4 is\ngiven the newly introduced function fnewand\nasked to generate code compatible with the\nnewer environment where fnewexists. The\nmigration target is the version prior to the\nchange, where fnewis unavailable.\n(ii) For deprecation-type changes ( fold→\nNone ), we create O LD2NEWsamples. GPT-\n4 receives the deprecated function foldand\ngenerates code that runs in the older envi-\nronment where foldis still present. The mi-\ngration target is the version after the change,\nwhere foldhas been removed.\n(iii) For replacement-type changes ( f→f′),\nwe generate both O LD2NEWand N EW2OLD\nsamples. GPT-4 is prompted separately with\neach function to produce the corresponding\ncode samples for both migration directions.\nFurther details on this process are provided in\nthe first and sixth prompts of Appendix B.\nStep 3: Test Case Generation. Finally, we con-\nstruct test cases for each generated code sample\nto ensure that migrated code preserves functional\ncorrectness.\nFor this, GPT-4 is supplied with both the orig-\ninal code and its functional specification and in-\nstructed to generate three test cases. These are ex-\necuted on the original code to obtain ground-truth\noutputs, which are then used in Task-3 to assess\nthe correctness of migrated code.\nOccasionally, generated test cases may exhibit\nissues such as invalid input ranges, incorrect for-\nmats, or runtime errors, which may arise from\n--- Page 6 ---\nBase ModelTask 1 Locating Function Task 2 Answering Change\nPython (easy) Python (hard) Java Avg. Python (easy) Python (hard) Java Avg.\nGPT-T URBO -3.5 85.10 32.98 80.89 66.32 26.01 13.09 63.28 34.13\nGPT-4 O-MINI 77.21 21.99 84.77 61.32 18.73 6.28 68.95 31.32\nGPT-4 O 70.71 25.65 81.19 59.18 22.22 13.61 75.22 37.02\nLLAMA -3.1-8B 70.71 21.99 67.16 53.29 16.16 2.09 53.13 23.79\nLLAMA -3.1-70B 75.51 29.84 81.19 62.18 22.73 8.38 75.22 35.44\nDEEPSEEK-V3 78.48 26.17 82.08 62.24 38.99 16.75 70.44 42.06\nTable 2: Experiment results for Task-1 and Task-2. We bold the best result and underline the second-best result.\nThe first two tasks only test general LLMs, because code-specialized LLMs focus more on generating code.\neither the test cases themselves or defects in the\noriginal code. To address this, we iteratively pro-\nvide GPT-4 with error messages from failed exe-\ncutions, allowing it to refine the test cases. This\nrefinement process is repeated for up to three iter-\nations. If all three test cases execute successfully,\nthe data sample is retained; otherwise, both the test\ncases and the associated code are discarded.\nDetails of this step are provided in the fourth\nprompt of Appendix B.\n4 Experimentation\nIn this section, we present a comprehensive analy-\nsis of our experimental setup and results.\n4.1 Experimental Settings\nLarge Models. We conduct experiments on nine\ndifferent LLMs. These include six general LLMs,\nnamely: GPT-T URBO -3.5 (Ye et al., 2023), GPT-\n4O-MINI(OpenAI et al., 2024a), GPT-4 O(Ope-\nnAI et al., 2024b), L LAMA -3.1-8B-I NSTRUCT\n(Abhimanyu Dubey et al., 2024), L LAMA -\n3.1-70B (Abhimanyu Dubey et al., 2024),\nand D EEPSEEK-V3 (Shao et al., 2024); three\ncode-specialized LLMs: Q WEN 2.5-C ODER -7B-\nINSTRUCT (Hui et al., 2024), S TARCODER 2-15B\n(Lozhkov et al., 2024), and C ODE LLAMA -34B\n(Rozière et al., 2023).\nEvaluation Metrics. We assess model perfor-\nmance on the three tasks using the following met-\nrics: Acc Task_1 , Acc Task_2 , and Acc Task_3 , as de-\ntailed in Section 3.4.\nFor Task-3 (code migration), we further report\nthe Pass@ kmetric (Hendrycks et al., 2021), which\nquantifies the proportion of examples for which\nthe model produces at least one correct migration\nwithin kattempts. Formally,\nPass@ k=I\"k[\ni=1Acc(i)\nTask_3#\n(4)\nwhere Acc(i)\nTask_3is an indicator of whether the\ni-th attempt for Task-3 is successful.Experiment Setup. For all LLMs, we set the\ngeneration temperature to 0.7 and limit the maxi-\nmum output sequence length to 2048 tokens. Pro-\nprietary models (e.g., the GPT series) are eval-\nuated via their official APIs. For smaller open-\nsource models such as Q WEN 2.5-C ODER -7B-\nINSTRUCT , we run inference locally using two\nRTX 4090 GPUs. For large-scale open-source\nmodels, i.e., LLAMA -3.1-70B, we access them\nthrough APIs provided by third-party websites†.\n4.2 Main Experiments\nTable 2 summarizes the experimental results for\nTask-1 and Task-2. Below, we discuss the key\nfindings:\nOverall Performance of Task-1. The average lo-\ncating success rate (Acc Task_1 ) for the six general\nLLMs across both Python and Java is 59.76%.\nPerformance is notably strong on the Python\n(easy) and Java datasets, with average scores of\n74.84% and 79.53%, respectively. However, all\nmodels struggle on the Python (hard) dataset,\nwhich contains multiple incompatible functions\nper example. For instance, Q WEN 2.5-C ODER -\n7B achieves only a 15.71% pass rate in this set-\nting. This drop in performance is primarily due\nto the models’ difficulty in identifying all incom-\npatible functions: when several such functions are\npresent, models often detect only a subset or make\nincorrect identifications.\nOverall, GPT-T URBO -3.5 achieves the high-\nest overall success rate on Task-1, with an aver-\nage of 66.32%. Its strength is particularly appar-\nent on the Python (hard) dataset, where it attains\na 32.98% pass rate—substantially higher than\nother models. This suggests that GPT-T URBO -\n3.5 is more adept at comprehensively locating all\nversion-incompatible functions in complex sce-\nnarios. While other models may overlook or mis-\nclassify some incompatible functions when mul-\ntiple are present, GPT-T URBO -3.5 more consis-\n†https://cloud.siliconflow.cn/models\n--- Page 7 ---\nBase ModelTask 3 Migration (O LD2NEW) Task 3 Migration (N EW2OLD)\nPython (easy) Python (hard) Python (easy) Python (hard)\nPass@1 Pass@5 Pass@1 Pass@5 Pass@1 Pass@5 Pass@1 Pass@5\nGENERAL LARGE LANGUAGE MODEL\nGPT-T URBO -3.5 26.03 34.93 7.32 10.98 24.80 38.40 7.34 9.17\nGPT-4 O-MINI 30.82 49.32 15.85 26.83 29.60 44.00 11.93 16.51\nLLAMA -3.1-8B 23.97 28.08 8.54 10.97 20.80 24.00 7.34 11.93\nLLAMA -3.1-70B 32.88 45.89 19.51 35.37 28.80 40.80 17.43 19.27\nDEEPSEEK-V3 41.20 54.11 20.73 29.27 29.60 39.60 14.68 23.85\nGPT-4 O 43.84 59.59 26.83 47.56 31.60 43.60 22.94 27.52\nCODE-SPECIALIZED LARGE LANGUAGE MODEL\nQWEN 2.5-C ODER -7B 32.19 46.58 14.63 24.39 29.20 38.00 8.26 12.84\nSTARCODER 2-15B 32.19 46.58 12.54 28.73 28.80 38.40 13.50 18.35\nCODE LLAMA -34B 35.62 53.42 21.95 36.49 29.60 40.80 15.76 21.10\nTable 3: Experiment results for Task-3, we report the results in two cases: O LD2NEWand N EW2OLD.\ntently identifies a greater proportion, leading to its\nsuperior performance. Nevertheless, it does not\nachieve the top results on Java, which may reflect\ndifferences in model proficiency across different\nprogramming languages.\nOverall Performance of Task-2. The average\nsuccess rate (Acc Task_2 ) for the six general LLMs\nacross Python and Java is 33.96%, which is no-\ntably lower than their performance on Task-1.\nThis gap highlights a key limitation: while LLMs\ncan often identify version-incompatible functions,\nthey struggle to recall or reason about the specific\ndetails of how those functions have changed across\nversions. In other words, LLMs are less adept at\nproviding precise, contextually accurate descrip-\ntions of function modifications, replacements, or\ndeprecations.\nAmong all models, D EEPSEEK-V3 stands out\nwith the highest average score of 42.06%. Its\nadvantage is particularly evident on the Python\n(easy) dataset, where it achieves 38.99%. A closer\nlook at the scoring criteria for Acc Task_2 (see Sec-\ntion 3.4) reveals that D EEPSEEK-V3’s strength\nlies in its ability to accurately recall the specific\nversion in which a function change occurred—a\ncapability that most other LLMs lack. This sug-\ngests that D EEPSEEK-V3 has either been exposed\nto more up-to-date or detailed training data, or\npossesses better mechanisms for temporal reason-\ning about software evolution.\nConversely, L LAMA -3.1-8B lags behind, with\nan average score of only 23.79%. Its pri-\nmary weakness is in identifying replacement-type\nchanges: it often fails to specify which function\nan incompatible one should be replaced with in\nthe target environment. This indicates that smalleror less specialized models may lack the depth of\ncodebase knowledge or the reasoning ability re-\nquired for nuanced migration scenarios.\nOverall Performance of Task-3. Table 3 presents\nthe results for Task-3 (code migration). In the\nOLD2NEWscenario, the average Pass@1 success\nrate for the nine LLMs is 33.56% on the easy set\nand drops to 16.20% on the hard set. Notably, al-\nlowing more attempts substantially boosts perfor-\nmance: Pass@5 rises to 45.5% (easy) and 26.47%\n(hard), indicating that LLMs can often generate\na correct migration with additional tries, even if\ntheir first attempt fails.\nIn contrast, the N EW2OLDscenario proves\nmuch more challenging. Here, the average\nPass@1 and Pass@5 rates are only 12.77% and\n17.30% (hard set), and additional attempts yield\nlittle improvement. This asymmetry suggests that\nLLMs are more familiar with migrating legacy\ncode to newer environments than the reverse,\nlikely reflecting the distribution of code and docu-\nmentation in their training data.\nAmong all models, GPT-4 Odelivers the\nstrongest performance in the O LD2NEWmigra-\ntion task, achieving a Pass@1 rate of 43.84% and\na Pass@5 rate of 59.59% on the easy set. This\ndemonstrates its superior ability to synthesize and\nadapt code for modern environments, likely due\nto its larger context window, more recent train-\ning data, and advanced reasoning capabilities. In\ncontrast, GPT-T URBO -3.5, which excels at lo-\ncating incompatible functions (Task-1), does not\ntranslate this strength into effective code migra-\ntion: its Pass@1 rate on the hard set is only\n7.32%. This discrepancy highlights that the skills\nrequired for identifying incompatibilities and for\n--- Page 8 ---\n22.7%\n41.7%\n13.6%22.0%Python(Easy)\nCallError RunError WrongAnswer Success64.4%\n24.6%3.1%7.9%Python(Hard)\n37.1%17.2%\n9.6%\n36.1%Python(Easy)\n33.0%23.0%\n19.4%24.6%Python(Hard)\n28.0%24.0%\n12.6%\n35.4%Python(Easy)\n43.5%\n33.0%\n6.3%17.3%Python(Hard)\n(a) Llama-3.1-8B-Instruct (b) GPT-4o (c) Deepseek-V3Figure 4: Error Analysis of Code Migration. CallError represents a function where an incompatible the en-\nvironment is still called. RunError represents that the migrated code enters an infinite loop during execution.\nWrongAnswer represents this code runs normally and gets the result, but it is different from the standard answer.\nWe combine the experimental results of N EW2OLDand O LD2NEWin this pie chart.\ngenerating correct, environment-adapted code are\ndistinct. While GPT-T URBO -3.5 can assist users\nin pinpointing problematic functions, it is less re-\nliable for fully automated migration, especially in\ncomplex scenarios.\nPreference of New Functions. We find that\nLLMs are more familiar with the new functions\ncompared to the old ones. Our experimental\nresults show that LLMs perform better in the\nOLD2NEWtask compared to the N EW2OLDtask.\nFor example, GPT-4 Oachieves a pass@1 rate of\n44.52% in the O LD2NEWtask at easy difficulty,\nwhile for N EW2OLDat the same difficulty, it only\nreaches 28.00%. A possible reason for this is\nthat the demand for writing code for new environ-\nments is more widespread, and during the train-\ning process, the proportion of new functions in\nthe training data is higher than that of old func-\ntions, leading to this function preference. Further-\nmore, this trend varies in magnitude across differ-\nent models. For instance, GPT-4 O-MINIshows a\nsmaller performance gap between N EW2OLDand\nOLD2NEW.\nError Analysis for Code Migration. To better\nunderstand the types of errors that occur during\ncode migration, we conduct an error analysis of\nthe failed cases, as illustrated in Figure 4. We cat-\negorize the failures into several types. The most\nprevalent is CallError , which arises when the\ngenerated code still invokes a function that is in-\ncompatible with the target environment. For ex-\nample, 50.8% of the code produced by L LAMA -\n3.1-8B for the Python ( hard ) migration task fails\ndue to this error. Such failures can occur either\nbecause the model does not successfully identify\nall incompatible functions, or because, even af-\nter correctly locating them, it still generates code\nthat calls an incompatible function. Another com-mon error type is RunError , where the code\ncompiles and runs but enters an infinite loop or\notherwise fails to terminate in a reasonable time.\nFor instance, 33.0% of the code generated by\nDEEPSEEK -CHAT failed due to this issue.\nAdditionally, some migrated code, while calling\nfunctions compatible with the environment and\npassing compilation successfully, produces results\nthat deviate from the expected output, leading to a\nWrongAnswer . For instance, 19.4% of the code\ngenerated by GPT-4 Ofailed due WrongAnswer .\nCase Studies. The goal of this case study (Fig-\nure 5) is to analyze how different LLMs per-\nform on the tasks of locating version-incompatible\nfunctions and accurately describing their changes,\nwith a focus on their reasoning about version con-\nstraints.\nOur findings reveal two main types of er-\nrors. First, both L LAMA -3.1-8B and GPT-\nTURBO -3.5 fail to correctly identify the rele-\nvant incompatible function. Instead, they focus\nonnp.array2string , providing information\nabout changes introduced in NumPy versions 1.17\nand 1.18, even though the target environment is\n1.16. Since these changes do not impact function-\nality in version 1.16, the models’ responses are ir-\nrelevant for the migration task. This suggests a\ncommon failure mode: incorrect reasoning about\nversion ordering, where models conflate changes\nfrom later versions with the requirements of an\nearlier target environment.\nIn contrast, L LAMA -3.1-70B and\nGPT-4 O-MINI correctly identify\nnp.set_printoptions as incompatible with\nNumPy 1.16. However, GPT-4 O-MINIstrug-\ngles to specify the precise version in which the\nfunction change occurred, providing inaccurate\nversion information. This issue—misreporting the\n--- Page 9 ---\ndef custom_array_representation (arr: np.ndarray , precision : int, threshold : int):\n        import numpy  as np\n        np.set_printoptions (precision=precision, threshold=threshold )\n        formatted_array  = np.array2string (arr)\n        return  formatted_array\nLLAMA-3.1-8B LLAMA-3.1-70B\nGPT -TURBO-3.5 GPT -4o-MINI\nnp.set_printoptionsLocated\nFunction:\nFunction\nChange:Original\nversionNumpy 2.0\nTarget\nversionNumpy 1.16\nnp.set_string_function  was\nthe standard way to apply the\nset_string_function function;np.array2stringLocated\nFunction:\nFunction\nChange:The function\nnumpy .array2string  has been\ndeprecated since numpy\nversion 1.17 and will be\nremoved in a future version.\nnp.set_printoptionsLocated\nFunction:\nFunction\nChange:After Numpy 2.0,\nnp.set_string_function  is\ndeprecated in\nfavor np.set_printoptions ;\nnp.array2stringLocated\nFunction:\nFunction\nChange:The function np.array2string\nhas been changed to\nnp.array_str  in numpy version\n1.18.Wrong Locating!\nWrong Locating!\n Wrong Change!Figure 5: Case Study. We plot an example of N EW2OLDfrom Python (Easy) datasets and present the response of\nTask-1 and Task-2 for four LLMs. In this case study, we observe the phenomenon of logical inconsistency, where\nLLAMA -3.1-8B and GPT-T URBO -3.5 provide function changes that are unrelated to the migration process.\nversion associated with a function change—was\nfrequently observed across our evaluation, high-\nlighting a broader challenge for LLMs in tracking\nthe evolution of library APIs with precision.\nOverall, this case study demonstrates that even\nwhen models can identify relevant functions, they\noften fail in reasoning about version boundaries\nand providing accurate change details, which are\ncritical for reliable code migration.\n5 Conclusion\nIn this work, we introduced C ODEME NV, a com-\nprehensive benchmark designed to assess the code\nmigration capabilities of LLMs across different\nenvironments. C ODEME NVencompasses three\ncore tasks: detecting version-incompatible func-\ntions, identifying specific function changes, and\nmigrating code to ensure compatibility.\nOur evaluation of nine LLMs demonstrates\nthat models are generally more proficient with\nnewer function versions, which poses difficulties\nfor migrating code from newer to older environ-\nments (N EW2OLD). Additionally, our error anal-\nysis highlights logical inconsistencies, where the\nchanges proposed by models do not always facil-\nitate successful migration. We hope that C ODE-\nME NVand the insights from our experimentswill inspire further research into improving LLM-\ndriven code migration.\nLimitations\nCODEME NVis relatively small, particularly the\nJava dataset. Additionally, the language features\nof Java make it challenging to establish rigorous\nunit tests. C ODEME NVcurrently involves only\ntwo programming languages, Python and Java. We\nplan to add more programming languages in the\nfuture.\nEthics Statement\nThroughout our work, we have strictly adhered\nto ethical standards. The creation of our dataset\nalso complies with open-source regulations, and\nthe data has undergone manual checks to prevent\nharmful content.\nAcknowledgements\nThis work is supported in part by the fund-\ning BAS/1/1689-01-01, URF/1/4663-01-01,\nREI/1/5232-01-01, REI/1/5332-01-01, and\nURF/1/5508-01-01 from KAUST, and fund-\ning from KAUST - Center of Excellence for\nGenerative AI, under award number 5940.\n--- Page 10 ---\nReferences\nThe claude 3 model family: Opus, sonnet, haiku.\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad\nAwan, Jyoti Aneja, Ahmed Awadallah, Hany Has-\nsan Awadalla, Nguyen Bach, Amit Bahree, Arash\nBakhtiari, Harkirat Singh Behl, Alon Benhaim,\nMisha Bilenko, and Johan Bjorck. 2024. Phi-3 tech-\nnical report: A highly capable language model lo-\ncally on your phone. ArXiv , abs/2404.14219.\nAbhinav Jauhri Abhimanyu Dubey et al. 2024. The\nllama 3 herd of models. ArXiv , abs/2407.21783.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical re-\nport. arXiv preprint arXiv:2303.08774 .\nAylton Almeida, Laerte Xavier, and Marco Túlio Va-\nlente. 2024. Automatic library migration using large\nlanguage models: First results. Proceedings of the\n18th ACM/IEEE International Symposium on Em-\npirical Software Engineering and Measurement .\nAmazonQ. Amazon q developer: Transform code.\n2025.\nKeyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan Zhai,\nLu Yu, Muhammad Asif Ali, Lijie Hu, and Di Wang.\n2024. Multi-hop question answering under temporal\nknowledge editing. ArXiv , abs/2404.00492.\nXueying Du, Mingwei Liu, Kaixin Wang, Han-\nlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng,\nChaofeng Sha, Xin Peng, and Yiling Lou. 2024.\nEvaluating large language models in class-level code\ngeneration. 2024 IEEE/ACM 46th International\nConference on Software Engineering (ICSE) , pages\n982–994.\nHasan Ferit Eniser, Hanliang Zhang, Cristina David,\nMeng Wang, Maria Christakis, Brandon Paulsen,\nJoey Dodds, and Daniel Kroening. 2024. Towards\ntranslating real-world code with llms: A study of\ntranslating to rust. ArXiv , abs/2405.11514.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv ,\nabs/2002.08909.\nDan Hendrycks, Steven Basart, Saurav Kadavath,\nMantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Xiaodong\nSong, and Jacob Steinhardt. 2021. Measuring\ncoding challenge competence with apps. ArXiv ,\nabs/2105.09938.\nCheng-Yu Hsieh, Sibei Chen, Chun-Liang Li, Yasuhisa\nFujii, Alexander J. Ratner, Chen-Yu Lee, Ranjay\nKrishna, and Tomas Pfister. 2023. Tool documen-\ntation enables zero-shot tool-usage with large lan-\nguage models. ArXiv , abs/2308.00675.Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Day-\niheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Kai Dang, An Yang, Rui Men, Fei\nHuang, Shanghaoran Quan, Xingzhang Ren, Xu-\nancheng Ren, Jingren Zhou, and Junyang Lin.\n2024. Qwen2.5-coder technical report. ArXiv ,\nabs/2409.12186.\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and\nSunghun Kim. 2024. A survey on large language\nmodels for code generation. ArXiv , abs/2406.00515.\nJoe El Khoury. 2024. Leveraging large language mod-\nels for automated code migration and repository-\nlevel tasks — part i.\nKostas Kontogiannis, Johannes Martin, Kenny Wong,\nRichard Gregory, Hausi A. Müller, and John My-\nlopoulos. 2010. Code migration through transfor-\nmations: an experience report. In Conference of the\nCentre for Advanced Studies on Collaborative Re-\nsearch .\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. ArXiv , abs/2005.11401.\nXiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie\nYu, Xiaodong Liu, Jing Wang, Bing Ji, and Weimin\nZhang. 2024. Model editing for llms4code: How far\nare we? ArXiv , abs/2411.06638.\nZeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi,\nand Greg Durrett. 2024. Codeupdatearena: Bench-\nmarking knowledge editing on api updates. ArXiv ,\nabs/2407.06249.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-\nerico Cassano, Joel Lamy-Poirier, Nouamane Tazi,\nAo Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,\nTianyang Liu, Max Tian, Denis Kocetkov, Arthur\nZucker, Younes Belkada, Zijian Wang, and Qian\nLiu. 2024. Starcoder 2 and the stack v2: The next\ngeneration. ArXiv , abs/2402.19173.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual as-\nsociations in gpt. Advances in Neural Information\nProcessing Systems , 35:17359–17372.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nediting memory in a transformer. In The Eleventh\nInternational Conference on Learning Representa-\ntions .\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, et al. 2024a. Gpt-4 technical report.\nOpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec\n--- Page 11 ---\nRadford, Aleksander M ˛ adry, Alex Baker-Whitcomb,\nAlex Beutel, et al. 2024b. Gpt-4o system card.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-\nBaptiste Alayrac, Radu Soricut, Angeliki Lazari-\ndou, Orhan Firat, Julian Schrittwieser, Ioannis\nAntonoglou, Rohan Anil, Sebastian Borgeaud, and\nAndrew M. 2024. Gemini 1.5: Unlocking multi-\nmodal understanding across millions of tokens of\ncontext. ArXiv , abs/2403.05530.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, I. Evtimov, Joanna Bitton, Manish P\nBhatt, Cris tian Cantón Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D’efossez, Jade Copet,\nFaisal Azhar, Hugo Touvron, Louis Martin, Nicolas\nUsunier, Thomas Scialom, and Gabriel Synnaeve.\n2023. Code llama: Open foundation models for\ncode. ArXiv , abs/2308.12950.\nZhihong Shao, Damai Dai, Daya Guo, Bo Liu (Ben-\njamin Liu), Zihan Wang, and Huajian Xin. 2024.\nDeepseek-v2: A strong, economical, and effi-\ncient mixture-of-experts language model. ArXiv ,\nabs/2405.04434.\nHongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu,\nBoao Shi, Che Liu, Qian Liu, and Tao Yu. 2024.\nEvor: Evolving retrieval for code generation. In\nConference on Empirical Methods in Natural Lan-\nguage Processing .\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nBehrooz Omidvar Tehrani, Ishaani M, and Anmol\nAnubhai. 2024. Evaluating human-ai partnership\nfor llm-based code migration. Extended Abstracts\nof the CHI Conference on Human Factors in Com-\nputing Systems .\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi\nD. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023.\nCodet5+: Open code large language models for code\nunderstanding and generation. In Conference on\nEmpirical Methods in Natural Language Process-\ning.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code under-\nstanding and generation. ArXiv , abs/2109.00859.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai\nShao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao\nGong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui,\nQi Zhang, and Xuanjing Huang. 2023. A compre-\nhensive capability analysis of gpt-3 and gpt-3.5 se-\nries models.\nZhiqiang Yuan, Weitong Chen, Hanlin Wang, Kai Yu,\nXin Peng, and Yiling Lou. 2024. Transagent: An\nllm-based multi-agent system for code translation.\nArXiv , abs/2409.19894.Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan\nCheng, Lijie Hu, and Di Wang. 2024. Locate-then-\nedit for multi-hop factual recall under knowledge\nediting. ArXiv , abs/2410.06331.\nZexuan Zhong, Zhengxuan Wu, Christopher D Man-\nning, Christopher Potts, and Danqi Chen. 2023.\nMquake: Assessing knowledge editing in language\nmodels via multi-hop questions. arXiv preprint\narXiv:2305.14795 .\nShuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo\nWang, Zhengbao Jiang, and Graham Neubig. 2022.\nDocprompting: Generating code by retrieving the\ndocs. In International Conference on Learning Rep-\nresentations .\nCelal Ziftci, Stoyan Nikolov, Anna Sjovall, Bo Kim,\nDaniele Codecasa, and Max Kim. 2025. Migrat-\ning code at scale with llms at google. ArXiv ,\nabs/2504.09691.\n--- Page 12 ---\nA Additional Related Work\nA.1 Knowledge Editing\nKnowledge editing is an effective way to add the\nlatest knowledge of function changes to LLMs.\nThe research on knowledge editing for LLMs\naims to efficiently modify large model’s param-\neters in order to update its knowledge. Most\nstudies in this field focus on editing natural lan-\nguage knowledge. ROME (Meng et al., 2022a)\nand MEMIT (Meng et al., 2022b) adopt a locate-\nthen-edit paradigm, where the parameter posi-\ntion of the knowledge is first located, and then\nthe parameter is updated to modify the model’s\nknowledge. Some work adopts a plan-and-solve\nparadigm (Zhong et al., 2023; Cheng et al., 2024),\nwhere complex problems are decomposed into the\nknowledge required for each step, which are then\nsolved one by one. (Zhang et al., 2024) pro-\nposes a locate-then-edit paradigm to support effi-\ncient knowledge editing for multi-hop questions.\nThere are only a few research attempts on\nchanges to function: CodeUpdateArena (Liu et al.,\n2024) introduces a benchmark for updating LLMs\nwith new API function knowledge to solve pro-\ngram synthesis tasks. CLMEEval (Li et al., 2024)\npropose a benchmark for evaluating model edit-\ning techniques on LLMs4Code, and proposes A-\nGRACE, an enhanced method for better general-\nization in code knowledge correction. Some of the\nrecent works (Zhou et al., 2022; Su et al., 2024;\nHsieh et al., 2023) use retrieval-augmented ap-\nproaches (Lewis et al., 2020; Guu et al., 2020) to\nprovide models with code change knowledge for\nimproving code generation.\nNote, unlike existing work, C ODEME NVdoes\nnot supply the model with contextual knowl-\nedge of function changes during evaluation. In-\nstead, we prioritize assessing how effectively the\nmodel leverages its inherent knowledge of func-\ntion changes to perform code migration.\nB Prompts for C ODE ME NV\nSee Prompt1 for the generation of original code of\nPython language (step-2 of datasets construction).\nSee Prompt2 for prompt we used in experi-\nment to execute the three tasks of C ODEME NV\n(Python).\nSee Prompt3 for the agent-based evaluation for\nPython language.\nSee Prompt4 for the generation of test cases.See Prompt5 for the improving of test cases.\nSee Prompt6 for the generation of original code\nof Java language (step-2 of datasets construction).\nSee Prompt7 for prompt we used in experiment\nto execute the three tasks of C ODEME NV(Java).\nSee Prompt8 for the agent-based evaluation for\nJava language.\n--- Page 13 ---\nPrompt 1. Original Code Generation: the Second Step for Dataset Construction of Python language\n==== SYSTEM ====\nYou are a very experienced programmer who is familiar with the usage of many functions and is good at applying\nthem. At the same time, you are thoughtful and creative and like to apply some functions to solve algorithmic problems.\nFirst of all, I will give you an existing library function, you will get the function with signatures and functionality, as\nwell as import methods. I hope you can think about the application of this library function according to the description\nof this library function, be bold and creative, and then write a piece of code that calls this library function, we call this\ncode as solution. This solution is a function, and should be able to solve medium and difficult algorithmic problems,\nwhich require \"multiple inferences\", at least three or four steps to solve, rather than simply calling your library\nfunction. There should be no comments in this solution.\nThen, design a problem for the solution you generated, with the requirement that others should be able to derive a\nsolution from this problem. Your problem description should focus on the solution’s functionality, as well as its inputs\nand outputs, rather than guiding the step-by-step generation of the solution within the description\nYou must explicitly specify the data type and dimensionality of each input parameter, as well as the data type and\nparameters of the output. Your problem description should follow this template: \"Please use Python code to implement\na function...\" Indicate which library is being utilized in the description, but refrain from specifying the exact library\nfunction being called. Avoid disclosing any implementation details.\nNote: Do not alias when importing; Here’s a return template,only output the JSON in raw text. Don’t return anything\nelse.\n{\nsolution_function: The function that you generate. Make sure the code you return is runnable.\nsolution_signature: The signature of the function you generated, indicating the input and output. And the name of the\nsolution should derive from its functionality,\nproblem: Generate a literal description of this function. Describe the data type and dimensions of each input parameter\nand the data type and dimension of the output.\n}\n==== USER ====\nThe package name for the new library function is:\n<PACKAGE>\nThe import method is as follows:\n<IMPORT>\nThe signature of the new library function is:\n<SIGNATURE>\nThe feature description of the new library function is:\n[DOC]\n<DOC_STRING>\n[/DOC]\nNote: Do not alias when importing; Only output the JSON in raw text.Don’t return anything else.\n--- Page 14 ---\nPrompt 2. The Prompt used by LLMs to Execute the Three Tasks of C ODE ME NV(Python)\n==== SYSTEM ====\nYou’re a good assistant, and now you need to help me change the code.\nI’ll give you a piece of Python code that contains functions that are incompatible with the target environment. You\nneed to locate the incompatible function in this code. Then give the information about the change of this located\nfunction: (i) It must include the change type deprecation/addition/replacement; (ii) the replaced function(if the change\ntype is replacement); (iii) and the version of this function that changed. Finally return your corrected code, and you\nonly need to fix the incompatible function in the code.\nNote that Only output the JSON in raw text. Don’t return anything else. And here’s an example of what you returned.\n{\nai_api_wrong: There is the wrong function in the code because of the version,\nai_api_change: 1.The specified function(error function) has changed due to version changes, such as being added in\nversion..., being abandoned in version..., or the calling method has changed; 2.The replace method is... 3.The version\nthat the function changed is...\ncode_fixed: Entire code modified\n}\nHere’s an example of an answer.\n{\nai_api_wrong: numpy.compare_chararrays.\nai_api_change: 1.replacement 2.use numpy.char.compare_chararrays instead\n3.The function numpy.compare_chararrays has been removed in numpy version 2.0.\ncode_fixed: def string_array_similarities(strings1, strings2):\nresult = []\nfor s1 in strings1:\ntemp_result = 0\nfor s2 in strings2:\nlength_diff = abs(len(s1) - len(s2))\ncomparison = numpy.char.compare_chararrays(numpy.array(list(s1)), numpy.array(list(s2)), cmp=’==’, as-\nsume_equal=False)\nsimilarity = numpy.sum(comparison) - length_diff\ntemp_result = max(temp_result, similarity)\nresult.append(temp_result)\nreturn result\n}\n==== USER ====\nHere’s the code you need to identify errors.\n[CODE]\n<CODE>\n[/CODE]\nHere’s the Python library you need to modify your code.\n[PACKAGE]\n<PACKAGE>\n[/PACKAGE]\nHere’s the version of above package.\n[VERSION]\n<VERSION>\n[/VERSION]\n--- Page 15 ---\nPrompt 3. Agent-based Evluation: Task-1 and Task-2 of Python Language\n==== SYSTEM ====\nYou are a good helper for a human being. I ask another LLM to locate the function in a piece of code that is\nincompatible with the environment, and its response include the following contents: (i) The located incompatible\nfunction ; (ii) information of the changes of the function; (iii) The migrated code after fixing the error.\n...\nPlease compare the wrong functions returned by the AI and the correct function I give you. If ai_api_wrong contains\napi_wrong, the judge_locate_answer is 1,unless return 0.\nCompare whether the change of the function returned by the AI and the real change I give you. You can loosely com-\npare the two changes. If they are related or only have a little difference, the judge_update_answer is 1. If two changes\nare absolutely are completely irrelevant, return 0. Remember if judge_locate_answer is 0, judge_update_answer must\nbe 0.\nNote that Only output the JSON in raw text. Don’t return anything else. And here’s an example of what you returned.\n{\njudge_reason: The reason why the AI determines whether it is correct or wrong,\njudge_locate_answer: {0/1}\njudge_update_answer: {0/1}\n}\n==== USER ====\nHere’s the code that lets the AI judge that there is an error.\n[CODE]\n<CODE>\n[/CODE]\nHere are the apis given by LLM that are not suitable for the target environment.\n[API_LOCATE_BY_LLM]\n<API_LOCATE_BY_LLM>\n[API_LOCATE_BY_LLM]\nHere’s the information regarding the changes in this API, which was returned by LLM.\n[CHANGE_INFORMATION_BY_LLM]\n<CHANGE_INFORMATION_BY_LLM>\n[CHANGE_INFORMATION_BY_LLM]\nHere are the answers.\n[API_REFERENCE_ANSWER]\n<API_REFERENCE_ANSWER>\n[API_REFERENCE_ANSWER]\n[CHANGE_INFORMATION_REFERENCE_ANSWER]\n<CHANGE_INFORMATION_REFERENCE_ANSWER>\n[CHANGE_INFORMATION_REFERENCE_ANSWER]\nThe version is too high or too low.\n[VERSION_ERROR]\n<VERSION_ERROR>\n[/VERSION_ERROR]\n--- Page 16 ---\nPrompt 4. Test Cases Generation (Step-3 of Datasets Construction)\n==== SYSTEM ====\n# Role\nA very experienced programmer who is good at algorithmic reasoning and can write high-quality code.\n# Responsibilities\nWrite 3 sets of *high-quality* and *comprehensive* input test data based on the problem description and benchmark\ncode.\nThe specific description of these requirements is as follows:\n# Problem:\nThat is, the problem scenario. The type of input data and the range limit of the input data are often given in the\nproblem.\n(Problem is between \"[PROBLEM]\" and \"[/PROBLEM]\")\n# Benchmark code:\nThat is, the given callable code, and its parameters are each set of input data to be passed in (Benchmark code is\nbetween \"[CODE]\" and \"[/CODE]\")\n# Implementation steps\nPlease answer the questions strictly according to the above requirements and the following steps:\n1. Determine the input data\n- First analyze the problem and the given code to determine the type of input data,\n2. Final input data group generation\nBased on step 1, return the string of the input data group\n- Return format: case1:\n====== Task start =====\nBelow is the given problem and function.\n==== USER ====\n[PROBLEM]\n<PROBLEM>\n[/PROBLEM]\n[CODE]\n<CODE>\n[/CODE]\n--- Page 17 ---\nPrompt 5: Improve test cases quality\n==== SYSTEM ====\n# Role\nAn experienced data tester who is good at writing more accurate and higher quality test case based on error information.\n# Responsibilities\nAdjust the test case group according to the provided executable script and running information, and return the adjusted\ntest cases.\n# Executable script:\nThat is, a script that can be compiled and run, and the script code already contains an array of test cases.(BETWEEN\n\"[TARGET_IMPLEMENTATION]\" and \"[/TARGET_IMPLEMENTATION]\")\n# Running information:\nThat is, the running information of each set of test cases when the function is running, mainly focusing on error\ninformation.(BETWEEN \"[MESSAGE]\" and \"[/MESSAGE]\")\n[MESSAGE]\n\"\"\"\n5.0\nerror:function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0 Incompatible shapes:\n[3,2] vs. [3] [Op:Mul]\n10.0\n\"\"\"\n[/MESSAGE]\n- output:\ncase1:[[1.0, 2.0], [3.0, 4.0]], [0.5, 0.5],\ncase2:[[ -1.0, -2.0], [-3.0, -4.0]], [0.5, 0.5],\ncase3:[[10.0]], [1.0]\n# Notes\nHere, you only need to pay attention to the test cases with running errors. For arrays without error information records,\nthere is no need to adjust.\n# Implementation steps\nPlease strictly follow the above requirements and the following steps to answer the questions:\n1. Test cases extraction and identification\n-Extract the parameters passed by the calling function from the executable script as the test cases group\nPrompt 5: Improve test cases quality\n2. Match the test cases group with the corresponding operation information\n-Pair the test cases input groups in sequence according to the operation results\n3. Save the test cases group that runs correctly and replace the test cases group that runs incorrectly\n-Keep the test cases group that runs correctly unchanged\n-For the test cases group that runs incorrectly, analyze the cause according to the error information, avoid similar\nerrors, and replace them with new test case groups.\n4. Finally, just return the modified test cases, do not return unnecessary explanations!\n====== Task start =====\nBelow is the given executable script and running information.\n==== USER ====\n[TARGET_IMPLEMENTATION]\n<TARGET_IMPLEMENTATION>\n[/TARGET_IMPLEMENTATION]\n[MESSAGE]\n<MESSAGE>\n[/MESSAGE]\n--- Page 18 ---\nPrompt 6: Original Code Generation: the Second Step for Dataset Construction of Java language\n==== SYSTEM ====\nYou are a very experienced JA V A programmer who is familiar with various library functions of java and is good\nat applying them. At the same time, you are thoughtful and creative, and like to apply some functions to solve\nalgorithmic problems.\nFirst of all, I will specify that you use an old function to complete a class, this function may have been removed in the\nnew JDK. Assuming that I am running in an old JDK environment, please call the function anyway.\nThen, generate a usage description for your generated code, and I can ask others to be able to generate the code from\nthe problem.\nNote: Do not alias when importing; Here’s a return template,only output the JSON in raw text. Don’t return anything\nelse.\n{\njava_code: The function that you generate. Make sure the code you return is runnable.\nclass_name: The name of the class you generate.\nfunction_description: The usage description of your generated code.\n}\n==== USER ====\nThe signature of the new library function is: <SIGNATURE>\nNote: Do not alias when importing; Only output the JSON in raw text.Don’t return anything else.\nPrompt 7: The Prompt used by LLMs to Execute the Three Tasks of C ODE ME NV(Java)\n==== SYSTEM ====\nYou’re a good assistant, and now you need to help me find the error of the code. I’ll give you a piece of java code that\nhas errors due to a java JDK version mismatch. You need to locate the wrong functions in this code, and explain what\nversion changes have taken place in the function that caused the error you pointed out.\n*Note that your answers must be concise, and you only need to point out the mistake directly.*\nHere’s an example of an answer:\nOutput:\nai_api_wrong: com.sun.javadoc.AnnotatedType\nai_api_change: The declarations in this package have been superseded by those in the package jdk.javadoc.doclet. For\nmore information, see the Migration Guide in the documentation for that package.\n==== USER ====\nHere’s the code you need to identify errors.\n[CODE]\n<CODE>\n[/CODE]\nHere’s the version of the JDK\n[VERSION]\n<VERSION>\n[VERSION]\n--- Page 19 ---\nPrompt 8: Agent-based Evluation: Task-1 and Task-2 of Java Language\n==== SYSTEM ====\nYou are a good helper for a human being. I ask another LLM to locate the function in a piece of code that is\nincorrectly called because of the JDK version mismatch, and its response include the following contents: (i) The\nlocated incompatible function ; (ii) information of the changes of the function; (iii) The migrated code after fixing the\nerror.\nPlease compare the wrong functions returned by the AI and the correct functions I give you. If api_locate_by_llm\ncontains api_reference_answer, the judge_locate_answer is 1,unless return 0.\nCompare whether the change of the function returned by the AI and the real change I give you. You can loosely com-\npare the two changes. If they are related or only have a little difference, the judge_update_answer is 1. If two changes\nare absolutely are completely irrelevant, return 0. Remember if judge_locate_answer is 0, judge_update_answer must\nbe 0.\n{\njudge_reason: The reason why the AI determines whether it is correct or wrong,\njudge_locate_answer: {0/1}\njudge_update_answer: {0/1}\n}\n==== USER ====\nHere’s the code that lets the AI judge that there is an error.\n[CODE]\n<CODE>\n[/CODE]\nHere’s the wrong apis that the AI returned.\n[API_LOCATE_BY_LLM]\n<API_LOCATE_BY_LLM>\n[API_LOCATE_BY_LLM]\nHere’s the change of the wrong apis that the AI returned.\n[CHANGE_INFORMATION_BY_LLM]\n<CHANGE_INFORMATION_BY_LLM>\n[CHANGE_INFORMATION_BY_LLM]\nHere are the answers.\n[API_REFERENCE_ANSWER]\n<API_REFERENCE_ANSWER>\n[API_REFERENCE_ANSWER]\n[CHANGE_INFORMATION_REFERENCE_ANSWER]\n<CHANGE_INFORMATION_REFERENCE_ANSWER>\n[CHANGE_INFORMATION_REFERENCE_ANSWER]\nThe version is too high or too low.\n[VERSION_ERROR]\n<VERSION_ERROR>\n[/VERSION_ERROR]\n--- Page 20 ---\nC C ODE ME NV(Additional Details)\nC.1 Datsets Statistics\nDatasets jdk.nashorn org.xml com.sun java.applet java.beans java.rmi java.util java.security\nJava 188 9 86 9 3 15 7 18\nTable 4: Statistics on the number of changes across different Java packages.\nDatasets numpy python math re os random itertools torch tensorflow pandas csv\nPython (easy) 39 26 51 5 34 3 15 21 154 46 2\nPython (hard) 20 - - - - - - 21 115 35 -\nTable 5: Statistics on the number of changes across different Python packages.\nPackage Replacement Deprecation Addition\nnumpy 2 8 -\npandas - 12 13\ntensorflow 87 2 2\npython 9 7 7\nmath - 1 17\nre - - 2\nos - - 14\nrandom - - 2\ncsv - - 1\nitertools - - 5\ntorch - 5 5\ntotal 98 35 79\nTable 6: Statistics of the number of three types of function changes across different packages of python language.\nC.2 Data Collection Source\n--- Page 21 ---\nURL Description\nhttps://github.com/\npytorch/pytorch/releasesSources for collecting changes related to the Py-\nTorch library.\nhttps://numpy.org/doc/2.\n0/release/2.0.0-notes.\nhtml#changesSources for collecting changes related to the Numpy\nlibrary.\nhttps://docs.oracle.com/\nen/java/javase/11/docs/\napi/deprecated-list.htmlSources for collecting changes related to the Java li-\nbrary.\nhttps:\n//docs.python.org/zh-cn/\n3/library/random.htmlSources for collecting changes related to the random\nlibrary.\nhttps://github.com/\ntensorflow/tensorflow/\nreleases/tag/v2.0.0Sources for collecting changes related to the tensor-\nflow library.\nhttps:\n//docs.python.org/zh-cn/\n3/library/itertools.htmlSources for collecting changes related to the iter-\ntools library.\nTable 7: The URL for collecting data in step 1 of the data construction process.\n--- Page 22 ---\n\n--- Page 23 ---\n\n--- Page 24 ---\n\n--- Page 25 ---",
  "text_length": 64592
}