{
  "id": "http://arxiv.org/abs/2505.24749v1",
  "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating\n  Memory-Efficient LLM Training",
  "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods.",
  "authors": [
    "Yehonathan Refael",
    "Guy Smorodinsky",
    "Tom Tirer",
    "Ofir Lindenbaum"
  ],
  "published": "2025-05-30T16:08:40Z",
  "updated": "2025-05-30T16:08:40Z",
  "categories": [
    "cs.LG",
    "cs.CL",
    "math.OC"
  ],
  "pdf_url": "http://arxiv.org/pdf/2505.24749v1",
  "full_text": "SUMO: Su bspace-Aware M oment-O rthogonalization for Accelerating Memory-Efficient LLM Training Yehonathan Refael Faculty of Engineering Tel Aviv University yehonathan@tau.ac.ilGuy Smorodinsky Department of Computer science Ben Gurion University smorodin@post.bgu.ac.ilTom Tirer Faculty of Engineering Bar-Ilan University tirer.tom@biu.ac.il Ofir Lindenbaum Faculty of Engineering Bar-Ilan University ofir.lindenbaum@biu.ac.il Abstract Low-rank gradient-based optimization methods have significantly improved mem- ory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO ( Subspace-Aware Moment- Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steep- est descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates ap- proximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that ex- act orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO acceler- ates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20% compared to state-of-the-art methods. 1 Introduction Low-rank gradient-based optimization methods have become powerful tools for reducing memory consumption during the pre-training and fine-tuning of large language models (LLMs), often without sacrificing performance, and sometimes even improving it. For instance, while pre-training LLaMA 7B typically requires around 58GB of memory, far exceeding the 24GB available on consumer GPUs like RTX 4090, recent advances, such as those discussed in [ 1–3], have demonstrated that LLaMA 7B can now be trained from scratch on a single 24GB GPU without the need for costly memory offloading. Theoretical analysis in [ 1] attributes this efficiency to the inherent low-rank structure of gradients, which allows for optimization within a significantly reduced latent space. Furthermore, [ 2] found a Preprint. Under review.arXiv:2505.24749v1 [cs.LG] 30 May 2025 consistent decrease in gradient rank throughout training, suggesting that low-rank optimization not only reduces memory usage but also converges toward increasingly compact subspaces. However, despite these advancements, existing methods mainly focus on memory savings and often overlook the potential for accelerating convergence. Current approaches typically rely on standard steepest descent techniques and operate under implicit assumptions of isotropic geometry, which can hinder efficiency in ill-conditioned settings. This observation motivates our primary objective: to develop a subspace-aware optimizer that leverages low-rank structure while adapting to the loss landscape’s geometry. By reevaluating the choice of norm and its influence on gradient descent dynamics, we aim to design an algorithm that improves generalization, accelerates convergence, while preserving the memory advantages of low-rank methods. Classical gradient descent, including SGD [ 4], performs steepest descent under the Euclidean norm, which reflects isotropic curvature. However, deep networks exhibit highly anisotropic loss landscapes, making this assumption suboptimal. Recent work shows that adaptive optimizers like Shampoo [ 5], SOAP, and Muon  can be interpreted as steepest descent under non-Euclidean norms tailored to network architecture and data structure. As shown in [ 8], these methods implicitly adapt to spectral or operator norms, which better capture local curvature and improve convergence. This motivates the design of subspace-aware optimizers that exploit both low-rank structure and appropriate geometry to accelerate training. To formalize the role of geometry in optimization, consider a neural network with a differentiable loss function L:W → Rdefined on a weight space W=Rn. The local behavior around a point wcan be approximated by the Taylor expansion, L(w+ ∆w)≈ L(w) +g⊤∆w+λ 2∥∆w∥2, where g=∇wL(w),λ >0captures the sharpness or curvature of the loss surface and ∥ · ∥ is a chosen norm reflecting the geometry of the optimization landscape. Minimizing this approximation corresponds precisely to performing steepest descent under the given norm constraint. According to, the solution to this minimization explicitly takes the form, ∆w=−∥g∥∗ λargmax t:∥t∥=1g⊤t, where∥·∥∗denotes the dual norm of ∥·∥. Adaptive optimizers differ primarily in their norm choices. Adam utilizes a dynamic Max-of-Max norm constraint. Recent optimizers consider matrix norms while applying steepest descent at the layer level. Muon imposes a fixed Schatten- pnorm constraint for large p, effectively using the spectral norm on weight matrices [ 10,7]. Shampoo [ 5] dynamically learns the optimal approximate Schatten- pnorm for steepest descent, with its variants like SOAP  applying momentum to efficiently navigate the space of possible norms. Muon, by contrast, operates within a relatively fixed but large Schatten- pnorm, striking a balance between the dynamic adaptability of Shampoo and the static spectral norm constraints. Since neural network weights locally act as linear operators on Euclidean spaces, the induced operator (spectral) norm provides a natural constraint aligning with the curvature characteristics of the loss surface. This perspective motivates gradient orthogonalization, which ensures that optimization updates respect the spectral norm, inherently controlling the perturbation magnitude and thus enhancing optimization stability and efficiency. While norm-induced optimization methods offer a principled way to align updates with the geometry of the loss landscape, their practical deployment often incurs substantial computational overhead. For instance, Shampoo requires computing matrix inverses or root operations at every optimization step, which can be computationally expensive for large-scale neural networks. Similarly, Muon’s first- order moments-orthogonalization, though effective, involves an expensive approximation of spectral decompositions, which is computed by applying five iterations of Newton-Schulz [ 12] (referred to as Newton-Schulz5). Therefore, there is an inherent trade-off between the theoretical optimality provided by these norm-induced optimization approaches and their practical computational demands. To bridge the gap between the geometric advantages of norm-induced methods and their computa- tional costs, we first analyze the limitations of existing approximations. We derive an upper bound on the error introduced by the Newton-Schulz orthogonalization, demonstrating that this error increases with the condition number of the moment matrix. This finding explains the increasing instability of the Newton–Schulz5 method in ill-conditioned scenarios, which we subsequently demonstrate to occur in the first-order moment matrices during the training of large language models (LLMs). Building on this analysis, we establish a convergence rate for Muon optimization and compare it to 2 an alternative method that replaces the Newton–Schulz approach with exact Singular Value Decom- position (SVD). Remarkably, we find that the SVD-based approach achieves faster convergence, with improvements directly proportional to the accumulated errors from the moments orthogonalization by the Newton–Schulz5 method. Motivated by the empirical observation that gradients in LLMs often exhibit a low-rank structure, especially during early training, we propose a subspace-aware optimization scheme. This scheme performs exact SVD-based moment orthogonalization within a low-dimensional adaptive subspace. This approach benefits from the relatively low computational cost associated with SVD calculations for low-rank input matrices and enhances the stability of convergence. Also, our approach attains an even greater reduction in memory usage than all previous low-rank training methods by relying solely on first-order moment, as detailed below in Table 1. We support our method with a theoretical convergence guarantee and validate its empirical bene- fits through experiments, demonstrating faster training and better model performance compared to existing methods. Table 1: Comparison of properties between SUMO, GaLore, Adam, Shampoo, and SOAP. Assume W∈Rm×nwithm≥n, a constant projection rank rand a subspace update rate K. SUMO Adam Shampoo SOAP GaLore Computation O(mnr +mn2/K)O(mn)O(m3+n3)O(m3+n3) O(mnr +mn2/K) Optim. states memory nr+mr 2mn m2+n22mn+ 2m2+ 2n22nr+mr Subspace-aware ✓ × × × ✓ Orthogonalization ✓ × × × × 2 Related Work Low-rank gradient optimization. Low-rank gradients naturally emerge during neural network training, as shown in both theoretical and empirical studies [ 13–15]. This structure has been leveraged to reduce memory and computational costs during training [ 16–18]. Recent work [ 2] showed that gradients in reversible layers [ 19] tend to collapse to rank one over time and used this to adaptively adjust gradient rank in Adam. In this paper, we show that the same low-rank trend appears in the first-order moment, which we exploit to apply exact orthogonalization efficiently—avoiding the accumulation of approximation errors, such as Newton-Schultz, during optimization. Memory efficient optimizers. Reducing the memory demands of training large language models (LLMs) has driven extensive algorithmic research. One research direction, initiated by LoRA [ 20], reduces trainable parameters via low-rank adaptation. Yet, such methods often fall short of fully parameterized models, especially during pre-training. Another direction does not restrict the set of trainable parameters but rather optimizes the training methods, with notable examples including AdaRankGrad, GaLore, Fira, Flora, Adam-mini, GaLore-mini, LDAdam, GoLore, LoQT, and Apollo [2,1,21–25,3], integrating low-rank gradient projections in optimization. In this work, we reduce memory usage even further by relying solely on a first-order momentum, as shown in Table 1. Gradient preconditioning. Preconditioning the Gradient method is critical in enhancing optimizers’ efficiency and effectiveness. Several notable approaches for using a preconditioner have emerged, including methods based on signed gradients [ 26–29], gradient clipping [ 30], normalization [ 30,31], and gradient whitening [ 32–34,7,35–37]. Recent studies [ 7,38] explored gradient orthogonalization strategies, speeding up training. Orthogonalizing gradients effectively constrains updates to lie on directions of uniform magnitude (spectral radius = 1), preventing updates from exaggerating certain gradient directions over others. This procedure ensures a form of normalization that mitigates potential instabilities from ill-conditioned gradients. Unlike these methods, which apply preconditioning or approximate orthogonalization in the high-dimensional parameter space, our approach performs exact SVD-based orthogonalization within an adaptively selected low-rank subspace, offering improved stability and lower computational overhead. Orthogonal Stochastic Gradient Descent with Momentum (OSGDM). OSGDM [ 38] is a re- cently introduced first-order optimization method designed to speed up neural network training by orthogonalizing gradients before the optimization step. Specifically, for a data batch ξ(t), OSGDM applies SVD to the gradient matrix Gl=∇WlL\u0000 Φ\u0000 ξ(t);θ\u0001\u0001 of each neural network layer lto 3 generate an orthonormal gradient approximation Ol. This ensures diversity among learned represen- tations and reduces redundancy. The update rule for OSGDM with momentum term γand learning rateηis defined as, O(t) l= orth ( Gl),M(t+1) l←γM(t) l+ηO(t) l,W(t+1) l←W(t) l−M(t+1) l, where orth(G) =\u0000 GG⊤\u0001−1/2Gis the ortogonalization operator, and Mlis the first order moment of layer l. Despite additional computational overhead from SVD, OSGDM empirically achieves faster convergence and improved accuracy compared to common methods such as Adam. Muon optimizer. At iteration t, given weight W(t), momentum µ, learning rate ηt, and objective Lt, Muon, introduced by, constructs the update rule, M(t)=µM(t−1)+G(t) l,O(t) l=Newton-Schulz5 (M(t)),W(t+1)=W(t)−η(t)O(t) l. Here, M(t)is the momentum at iteration t, initialized as a zero matrix when t= 0. The Newton- Schulz5 method [ 35] approximates (M(t)M(t)⊤)−1/2M(t), orthogonalizing M(t)and thus ensuring uniform update directions, avoiding dominance by few directions. Muon explicitly controls the norm of gradient updates—particularly the spectral norm (or Schatten- pnorm with large p), which limits updates to smaller, well-conditioned steps in parameter space. By constraining the spectral norm, moment orthogonalization implicitly prevents overly large or ill-conditioned parameter updates. Such updates often lead to poor generalization due to instability or overfitting. Shortly after the introduction of Muon, the study in [ 39] proposed a framework to scale Muon for larger LLMs, mainly adding weight decay, and carefully adjusting the per-parameter update scale. 3 Method and Main Results 3.1 Theoretical Motivation: Exact moments orthogonalization leads to significantly faster convergence Previous work on pre-training and fine-tuning large language models (LLMs) has primarily focused on reducing memory usage for constrained hardware or lowering computational cost (for example in [1,40,3]). In this paper, we take a step toward accelerating LLM optimization by showing that applying exact orthogonalization (e.g., via SVD) to the first-order moment offers a practical advantage, even over the most accurate approximations, such as the commonly used Newton-Schulz5 method. Specifically, we find that SVD yields faster convergence and lower computational overhead. To support this, we first present a new observation: the moment matrix in LLM training tends to decrease in rank over time. Building on this, we then derive an upper bound on the approximation error of Newton-Schulz5, showing that it depends on both the number of iterations and the matrix condition number, highlighting its limitations in ill-conditioned or low-rank settings (which is exactly the case in LLM optimization moments). This motivates the need for more accurate orthogonalization of moment matrices during LLM training. Of course, applying SVD directly to full-sized layers is generally impractical. The surprising result, however, is that when integrated into a low-rank optimization scheme, the use of SVD becomes not only feasible but preferable. We conclude with a convergence analysis of Muon optimization, which, under these conditions, converges significantly more slowly than the SVD-based alternative. To the best of our knowledge, our convergence analysis of Muon optimization is the first to avoid neglecting the error in the Newton-Schultz approximation. The proofs of all lemmas and theorems of this section are relegated to the Appendix A. Lemma 3.1 (Moment Becomes Low-Rank During Training).LetM(t)∈Rn×mdenote the first mo- ment of a reversible layer1in a moment-based optimization algorithm, updated according to M(t)= β1M(t−1)+G(t),where G(t)is the gradient matrix at iteration t. LetM(t)=U(t)Σ(t)V(t)⊤be the singular value decomposition (SVD) of M(t), and define the rank- rorthogonal projection matrix asP(t)(r) =U(t)[:,1:r]U(t)[:,1:r]⊤. Then the relative error of the best rank-one approximation, κM(t)≜∥M(t)−P(t)(1)M(t)∥2 F ∥M(t)∥2 F, (1) satisfies κM(t)≤O(C−t)for some constant C >1. 1Reversible networks are formally defined in Appendix B.1 4 The above result, in (1), implies that M(t)approaches its rank-one approximation P(t)(1)M(t), as the iteration number increases, namely, M(t)becomes rank-one. The following Lemma 3.2 characterizes the impact of the moments’ low-rank structure on the approximation error of the Newton-Schulz5 orthogonalization. Lemma 3.2 (Orthogonalization error Ei).For a matrix A∈Rm×n, letσ1be the largest singular value of AA⊤andσmbe the smallest (without the loss of generality, assume m≤n). Let r≤mbe the largest index where σr> σr+1=···=σm≥0. Letκ=σ1 σmby the condition number of AA⊤. Denote Eithe error of Newton-Schultz after iiterations. Then we have ∥Ei∥F≤√r·\u0012 1−1 κ\u00132i. (2) According to the lemma, the approximation error grows exponentially with the condition number. Given the low-rank structure of the first-order moments, low-dimensional optimization offers a means to mitigate this error. Specifically, projecting the moment estimates ˆM(t)onto their dominant (small) r-dimensional subspace ensures that the squared moment ˆM(t)ˆM(t)⊤is constructed using only the top rsquared eigenvalues. These dominant components are significantly larger and exclude near-zero values, resulting in a substantially lower condition number compared to that of the full-rank squared moment matrix. This observation motivates the use of the Muon optimizer within a low-rank optimization framework for LLMs, including 2D reversible layers. Such an approach not only preserves the inherent memory efficiency of low-rank methods but also reduces the approximation error in the optimization step, potentially leading to faster convergence and improved performance over full-dimensional training. However, we also empirically observe that the eigenvalues of the moment matrix decay gradually. As shown in Figure 1, even when projecting onto the dominant subspace, the resulting matrix ˆM(t)ˆM(t)⊤, composed of the top r= 16 squared eigenvalues, can still exhibit a large condition number, thereby introducing non-negligible approximation error. (a) Condition number of the first-order moment vs. training step. The red line marks value 10. (b) Illustration of the moment’s singular value decay, taken arbitrarily at step 100. Figure 1: Evidence of anisotropy and ill-conditioning in the first-order moment matrix as a function of the Galore steps of the Roberta-base model [ 42] on the GLUE dataset RTE task [ 43]: (a) condition number growth, (b) spectral decay of moment. To comprehend the cumulative error of Newton-Schulz5 orthogonalization at each optimization step, we proceed to derive the convergence rate of the Moun optimization. To that end, we now provide some notations. Consider a neural network denoted as Φ(·;θ), which consists of Llayers and is parameterized by θ≜\u0014 Wd1×d0 1,...,WdL−1×dL−2 L−1,WdL×dL−1 0 L\u0015. Here, Wirepresents the weights tensor parameters associated with the i-th layer, for i∈[L]. We denote the differential loss L, where with a slight abuse of notation, we write the training problem by minWL(W) =Eξ[L(Φ(W, ξ))], if the context refers to the weights of a certain layer. We use the Frobenius norm, denoted ∥ · ∥F, which is induced by the inner product ⟨X,Y⟩= tr(X⊤Y). Assume that the stochastic gradient ∇L(W, ξ)is an unbiased estimator of the full gradient ∇L(W), with variance bounded by σ2, i.e., E[∥∇L(W, ξ)− ∇L (W)∥2 F]≤σ2.LetE(t) i=orth(M(t))−Newton-Schulz (M(t))denote the 5 approximation error of the Newton-Schulz (with i≥1iteration) at time t, where M(t)denotes the moment at iteration t. Lemma 3.3 (Exact convergence rate of Muon).Consider the Muon optimizer update w.r.t layer W∈Rm×ndefined by M(t)←βM(t−1)+ (1−β)Gt O(t)←U(t)V(t)⊤+E(t) i,(iiterations Newton-Schulz approximation ) W(t+1)←W(t)−ηtO(t), where M(t)=U(t)S(t)V(t)⊤denotes the singular value decomposition of M(t), andE(t) irepresents the Newton-Schulz5 approximation error after iiterations. Suppose the following: •The gradient ∇L(W)isL-Lipschitz continuous. •There exists δ >0such that ∥E(t) i∥ ≤δ∥UtV⊤ t∥=δ√m, for all t. If we take β= 1−αwithα= min(√ RL σ√ T,1),ηt=η=√ 4R √ (10/(1−β)+2m+4mδ+2mδ2)TL, and B= 1 (batch free convergence) than1 TPT t=1E\u0002 ∥∇L(W(t))∥\u0003 is bounded by O \"p RLm (2 + 4 δ+ 2δ2)√ T+σ2 √ RLT+σ(RL)1/4+√σ(RL)1/4 T1/4# 1 1−4√mδ!, where R=L(W(0))− L∗. If we take βas an arbitrary constant, and B=T, we have, 1 TTX t=1E∥∇L(W(t))∥ ≤ O \"p RLm (2 + 4 δ+ 2δ2)√ T+√ RL√ T+σ T3/2+σ√ T# 1 1−4√mδ!. Remark 3.4 (Comparison: slower convergence vs exact orthogonalization).When δ= 0, indicating an absence of error, the convergence rate is aligned with the one derived in [ 41], Theorem 2.1, that is 1 TTX t=1Eh ∥∇L(W(t))∥i ≤ O\u0012√ mRL√ T+σ T3/2+σ√ T\u0013. This result overlooks the error associated with the Newton-Schulz5 approximation because it is based on a theoretically exact method of orthogonalization. Remark 3.5 (The impact of δon the convergence rate).A reduction in δis associated with an improvement in the convergence rate. Furthermore, it should be noted that δinfluences the step size η; a larger δresults in a smaller step size, providing an additional explanation for the convergence rate. Remark 3.6 (The size of δ).We acknowledge that the findings of our analysis are applicable only under the conditions specified in 1−4√mδ > 0⇒δ <1 4√m. In scenarios where δ >1 4√mapplies, the algorithm may fail to converge. To ensure that δremains sufficiently small, the Newton-Schulz5 method necessitates a substantial number of iterations, consequently slowing down the convergence. Remark 3.7 (Speed-up by SVD vs Newton-Schulz5 approximation).According to Lemma 3.2, these low-rank moments, which inherently possess exceptionally high κ, result in an error expressed by (1−ε)2iconcerning a remarkably small ε. This situation necessitates numerous iterations for the Newton-Schultz method to converge. For example, if (1−ε) = 0.99is considered and Newton- Schultz5 is utilized with 5 iterations, the error would be ≈0.9932= 0.725,relative to the norm of the moment, namely M. Recall that in the low-rank setting, accurately computing the pseudoinverse using singular value decomposition (SVD) is numerically advantageous and reasonably computationally affordable com- pared to iterative methods such as Newton–Schulz. For a general matrix A∈Rm×n, where without the loss of generality m≥n, the SVD provides a decomposition A=UΣV⊤, with U∈Rm×m,Σ∈Rm×n, andV∈Rn×n. The Moore–Penrose pseudoinverse is then calculated asA†=VΣ†U⊤, requiring approximately 4nm2+ 8n3floating-point operations (FLOPs) for the initial decomposition, and an additional mn2+m2nFLOPs for subsequent multiplications, totaling roughly 4mn2+ 8n3+nm2+n2mFLOPs. 6 Alternatively, approximating the inverse of A⊤A∈Rm×musing Newton–Schulz iterations involves nm2FLOPs to form the matrix A⊤A, approximately 20m3+ 10m2FLOPs for five iterations, and an additional m2nFLOPs to multiply by A⊤, resulting in a total of about nm2+m2n+ 20m3+ 10m2FLOPs. For example, when the rank is m= 8 andn= 1024, the SVD approach requires approximately twice the number of operations compared to Newton–Schulz5. Nonetheless, given the superior numerical stability and inherent optimality of the SVD-based method, this moderate increase in computational effort remains acceptable, especially when accuracy and stability are prioritized. 3.2 Method We are now ready to present our main algorithm designed to accelerate the low-rank optimization scheme outlined in Algorithm 1. A detailed mathematical formulation of the weight update rule proposed in this paper can be found in Appendix C. The algorithm consists of four primary blocks, all contained within an outer loop that continues until convergence is achieved or a predefined number of epochs is reached. Each block serves a specific purpose, which will be explained in detail below. Algorithm 1 SUMO: Subspace-Aware Moment-Orthogonalization Optimization Input: A weight matrix W∈Rm×nwithm≥n. Step size η, scale factor α, decay rates { β1, β2}, weight decay λ, rank r, subspace update frequency K, small number k∈N,step clipping ratio γ. Initialize:t←0 repeat # Block 1: Calculate low rank gradient projection. Sample mini-batch B=\b ξ1, ξ2,..., ξ |B| Compute G(t)←P|B| i=1∂ ∂WL(Φ(xi,θ), yi) iftmod K= 0 then Qt←Truncated _Randomized _SVD( Gt)# Alternatively Truncated _SVD( Gt) # Block 1.1: Moment subspaces transformation Rr×r←Q(t)⊤Q(t−1)ift≥1,else0r×r M(t)r×n←RM(t−1),ift≥1,else0r×n{1st-order moment} end if # Alternatively criteria ∥ˆG(t)∥ ≤ς ˆG(t)←−Q(t)⊤G(t) # Block 2: Low-rank steepest-decent step (moment ortogonalization) M(t)←µM(t−1)+ˆG(t) O(t)←Orthogonalization_SVD (M(t)) # Block 3 (Optional): if∥O(t)∥ ∥O(t)∥> γ thenO(t)←O(t) ∥O(t)∥·γ O(t−1) # Block 4: Update weight in original space. W(t)←−W(t−1)−αηQ(t)O(t)−η·λW(t−1) t←t+ 1 until convergence criteria met (e.g. epoch number, gradient norm ∥G(t)∥ ≤ξ) return W(T) •Block 1: We select the subspace along the directions of the rlargest eigenvectors, but since computing full SVD for large matrices is computationally intensive and memory-demanding, we leverage the Randomized-SVD by [ 44], which is an efficient technique for producing a “good\" proxy for the optimal low-rank approximation. It solves the optimization problem arg min Q∈Rn×r G−QQ⊤G F,and approximates the matrix GasGapp,r≈QQ⊤G,that requires O(mnr +mr2)operations, instead of O(min( mn2, m2n))applied by SVD. •Block 1.1: We transform the first-order moments evaluated during the low-rank optimization steps, which occur in Block 2, between the preceding and the newly updated subspace. This transformation is required because, as will be demonstrated later, within Block 2, the first moments of the gradients are aligned with the previously projected subspace. Consequently, a transformation is necessary to translate them from the former subspace to the current one. 7 •Block 2: Here we calculate the (steepest) optimization step. SVD operation is adopted to solve exactly (M(t)M(t)⊤)−1/2M(t). LetUΣV⊤=ˆM(t)be the singular value decomposition (SVD) ofˆM(t), we will have (M(t)M(t)⊤)−1/2M(t)=UVT, which orthogonalizes ˆM(t). Formally, the Orthogonalization_SVD (A) = arg min O{∥O−A∥F:either OTO=IorOOT=I}. •Block 3: Rather than using standard gradient clipping, we adopt the Norm-growth Limiter (NL) introduced by [ 21], which has been shown to slightly outperform traditional clipping techniques by better constraining the progression of gradient magnitudes. Specifically, the gradient update is modified as follows, if∥O(t)∥ ∥O(t−1)∥> γ thenO(t)←O(t) ∥O(t)∥·γ O(t−1),where the scalar γserves as a growth threshold to regulate abrupt increases in gradient norm from one iteration to the next. We use γ= 1.1, which empirically leads to the highest results. •Block 4: The pre-trained model parameters are updated using the low-dimensional back-projection matrix along with weight decay. To ensure stable training across parameter matrices of different shapes, we interpret the root mean square (RMS) magnitude of updates as implicit layer-wise learning rate adaptation, following the approach in [ 39]. By scaling updates withp max( m, n), our method compensates for shape-induced differences in magnitude, achieving consistent effective learning rates across layers, similar to adaptive optimizers like AdamW. For clarity, we can assume, without loss of generality, that m≥n. In the opposite scenario, the projection matrix would multiply the gradient from the right-hand side. Theorem 3.8 (Convergence of SUMO).For a loss function L,and given architecture Φ, suppose that the compositions of f≡ L(Φ(·))isβ-smooth non-convex function that is bounded by some M∈R+. LetG(t) jdenote the gradient matrix w.r.t. the j-th reversible layer W(t) j,at time t∈N, for allj∈[L], and Tℓ, ℓ∈Ntimes are set by a convergence criterion (that is, ∥ˆGTℓ∥ ≤ςℓ). Then, there exist C∈R+andNsuch that for all TN>C ε2, and1 TNPN−1 i=0PTi+1−1 t=Ti G(t) j 2 F≤ε. Namely, Algorithm 1 achieves an ε-critical point,2i.e., G(t) j 2 F≤ε, for some t∈N, and any j∈[L]. The proof of Theorem 1 can be found in Appendix A. We emphasize that the convergence proof for Galore in [ 1] addresses only optimization within fixed subspaces, ignoring dynamic updates. AdaRankGrad’s proof [ 2] first established guarantees for the complete dynamic-subspace updates, yet both prior works simplified the inner steps as standard SGD. In contrast, SUMO’s convergence proof explicitly considers the exact optimization steps without simplifications. To reduce memory consumption, Algorithm 1 applies per-layer weight updates during backpropaga- tion, following recent works such as [ 45]. This contrasts with conventional optimizers that store full gradients and update all weights afterward, leading to potential inefficiencies. Details for post-hoc adapter extraction are discussed in Appendix B. 4 Experiments Fine-tuning on GLUE benchmark. Our model was evaluated using the GLUE benchmark [ 43] through the fine-tuning of the pre-trained Roberta-base model [ 42] across eight tasks. The comparative analysis includes full fine-tuning, LoRA, and GaLore methodologies, with the results enumerated in Table 2. The reported metrics are the overall accuracy (matched and mismatched) for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, F1-score for MRPC, and accuracy for the remaining tasks. Evidently, our approach enhances the fine-tuning accuracy while requiring less training memory, utilizing only a single moment compared to GaLore. The experiments were carried out using the NVIDIA A100 GPU. Pre-training LLAMA on C4 Dataset. To demonstrate the benefits of our method for pre-training LLAMA by following the evaluation from [ 1]. Specifically, we compare the performance of SUMO 2Also known as ε-stationary, see, e.g.,. 8 Figure 2: SUMO with SVD demonstrates superior convergence speed ( ∼1.6×faster), attaining comparable or higher accuracy than GaLore and SUMO with Newton-Schultz5 with significantly fewer optimization steps on QNLI. Table 2: Comparison of SUMO against state-of-the-art memory-efficient fine-tuning methods on the GLUE benchmark using the pre-trained RoBERTa-Base model. For comparison, we provide detailed results for SUMO using both SVD and Newton-Schulz5 orthogonalizations (ablation study). Model Memory CoLA STS-B MRPC RTE SST2 MNLI QNLI QQP Full Fine-Tuning 747M 62.24 90.92 91.30 79.42 94.57 87.18 92.33 92.28 LoRA (rank=4) 257M 61.38 90.57 91.07 78.70 92.89 86.82 92.18 91.29 GaLore (rank=4) 253M 60.35 90.73 92.25 79.42 94.0 87.0 92.24 91.06 SUMO (Newton-Schulz5, rank=4) 197M 61.8 90.82 92.43 79.36 94.17 86.92 92.26 91.27 SUMO (SVD, rank=4) 197M 62.3 91.04 93.5 81.07 94.93 87.34 93.26 91.68 LoRA (rank=8) 264M 61.83 90.80 91.90 79.06 93.46 86.94 92.25 91.22 GaLore (rank=8) 257M 60.06 90.82 92.0 79.78 94.38 87.17 92.2 91.11 SUMO (Newton-Schulz5, rank=4) 198M 61.74 90.79 91.94 79.69 94.17 87.21 92.24 91.38 SUMO (rank=8) 198M 61.7 91.1 93.7 81.37 94.82 87.58 93.67 91.72 to state-of-the-art methods in terms of perplexity and memory efficiency. For this evaluation, we trained large LLaMA-based models on the C4 dataset, a curated and extensive version of the Common Crawl web corpus [ 46]. This dataset is widely used for pre-training language models and developing word representations. To better reflect real-world pre-training scenarios, we conducted training on a non-repeating, large-scale dataset and scaled model sizes up to 350 million parameters. The results of these experiments are shown in Table 3. Experiments were conducted using an NVIDIA H200 GPU. Table 3: Comparison of state-of-the-art low-rank algorithms for pre-training LLaMA models of varying sizes on the C4 dataset. The results are reported in terms of validation perplexity. As can be seen, SUMO leads to improved performance with a substantial memory reduction compared to leading parameter-efficient fine-tuning schemes. Method 60M 130M 350M 1B Full-Rank 34.06(0.36G)25.08(0.76G)18.80(2.06G)15.56(7.80G) GaLore 34.88(0.24G)25.36(0.52G)18.95(1.22G)15.64(4.38G) Low-Rank 78.18(0.26G)45.51(0.54G)37.41(1.08G)142.53(3.57G) LoRA 34.99(0.36G)33.92(0.80G)25.58(1.76G)19.21(6.17G) ReLoRA 37.04(0.36G)29.37(0.80G)29.08(1.76G)18.33(6.17G) SUMO 34.26 (0.23G)24.87 (0.51G)18.69 (1.16G)14.68 (1.16G) Training Tokens 1.1B 2.2B 6.4B 13.1B r/d model 128/256 256 /768 256 /1024 512 /2048 9 Few/Zero-shot reasoning and long-context generalization. To evaluate our method’s performance on a complex reasoning task, we use the GSM8K dataset [ 47], testing systematic generalization. For these experiments, we used a batch size of 32 and 10 epochs for fine-tuning. We present the performance result in Table 4 training Phi-2 (2.7B) model [ 48], and in Table 5 training LLaMA (3B) model [ 49]. The results demonstrate that the proposed method significantly improves generalization to out-of-distribution data. The experiments were conducted on an NVIDIA H200 GPU. Table 4: Zero-shot evaluation on GSM8K dataset (Phi-2, 2.7B). Phi-2 (2.7B) Rank Accuracy (0-shot) Base Model 64 15.16% Galore 64 52.24% LoRA 64 42.8% SUMO 64 54.13 %Table 5: 8-shot evaluation on GSM8K dataset (LLaMA, 3B). LLaMA (3B) Rank Accuracy (8-shot) Base Model 64 17.93% Galore 64 74.9% LoRA 64 68.3% SUMO 64 76.7% Additional experiments and ablation studies are presented in the Appendix D. 5 Discussion Our results highlight that exact moment orthogonalization within a low-dimensional adaptive subspace significantly improves both convergence and stability in memory-efficient LLM training. By avoiding the approximation errors of Newton–Schulz5, the proposed SUMO leverages the low-rank structure of gradients to enable accurate, spectral-norm-aligned updates with minimal overhead. Empirically, SUMO outperforms prior low-rank methods in both fine-tuning and pretraining tasks, achieving better performance in memory consumption reduction compared to the memory-efficient benchmarks such as Galore. Our theoretical analysis further confirms its superior convergence properties under practical conditions. These findings position SUMO as a simple yet effective alternative to approximate geometric optimizers. Future work may investigate parallel computations for orthogonalization, integrate quantization techniques, and assess the effectiveness of the method in knowledge editing  or domain generalization [51, 52]. Acknowledgment TT was supported by the Israel Science Foundation (No. 1940/23) and MOST (No. 0007091) grants. References Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024. Yehonathan Refael, Jonathan Svirsky, Boris Shustin, Wasim Huleihel, and Ofir Lindenbaum. Adarankgrad: Adaptive gradient rank and moments for memory-efficient LLMs training and fine-tuning. In The Thirteenth International Conference on Learning Representations, 2025. Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z Pan, Zhangyang Wang, and Jinwon Lee. Apollo: Sgd-like memory, adamw-level performance. arXiv preprint arXiv:2412.05270, 2024. Barak Battash, Lior Wolf, and Ofir Lindenbaum. Revisiting the noise model of stochastic gradient descent. In International Conference on Artificial Intelligence and Statistics, pages 4780–4788. PMLR, 2024. Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization, 2018. Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade. Soap: Improving and stabilizing shampoo using adam, 2025. 10 Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cecista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. Louis Cesista Franz. The Case for Muon, October 2024. Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325, 2024. Dmitry Kovalev. Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization, 2025. Hao Sun, Li Shen, Qihuang Zhong, Liang Ding, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, and Dacheng Tao. Adasam: Boosting sharpness-aware minimization with adaptive learning rate and momentum for training deep neural networks, 2023. Nicholas J Higham. Newton’s method for the matrix square root. Mathematics of computation, 46(174):537–549, 1986. Jingzhao Zhao, Frederik T. Schaefer, and Anima Anandkumar. Zero initialization: Initializing neural networks with only zeros and ones. Transactions on Machine Learning Research, 2022. Romain Cosson, Ali Jadbabaie, Anuran Makur, Armin Reisizadeh, and Devavrat Shah. Low- Rank Gradient Descent. IEEE Open Journal of Control Systems, 2023. Greg Yang, Jacob B. Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. arXiv:2310.17813. M. Gooneratne, K. C. Sim, P. Zadrazil, A. Kabel, F. Beaufays, and G. Motta. Low-rank gradient approximation for memory-efficient on-device training of deep neural network. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020. IEEE, 2020. Shuang Huang, Brian D. Hoskins, Michael W. Daniels, Matthew D. Stiles, and George C. Adam. Low-Rank Gradient Descent for Memory-Efficient Training of Deep In-Memory Arrays. ACM Journal on Emerging Technologies in Computing Systems, 2023. Ionut-Vlad Modoranu, Alexander Kalinov, Ermin Kurtic, Erwin Frantar, and Dan Alistarh. Error Feedback Can Accurately Compress Preconditioners. ArXiv preprint arXiv:2306.06098, 2023. arXiv:2306.06098. Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning with dual deep networks, 2021. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. Xi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xiangyu Yue, Ye Yuan, and Guoren Wang. Fira: Can we achieve full-rank training of llms under low-rank constraint? arXiv preprint arXiv:2410.01623, 2024. Yongchang Hao, Yanshuai Cao, and Lili Mou. Flora: Low-rank adapters are secretly gradient compressors. ArXiv, abs/2402.03293, 2024. Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024. Thomas Robert, Mher Safaryan, Ionut-Vlad Modoranu, and Dan Alistarh. Ldadam: Adaptive optimization from low-dimensional gradient statistics, 2025. Sebastian Loeschcke, Mads Toftrup, Michael J. Kastoryano, Serge Belongie, and Vésteinn Snæbjarnarson. Loqt: Low-rank adapters for quantized pretraining, 2024. 11  Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pages 560–569. PMLR, 2018. Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. Advances in neural information processing systems, 35:9955–9968, 2022. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms. In NeurIPS, 2023. Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be.arXiv preprint arXiv:2304.13960, 2023. Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020. Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. Zhirong Yang and Jorma Laaksonen. Principal whitened gradient for information geometry. Neural Networks, 21(2-3):232–240, 2008. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Dongseong Hwang. Fadam: Adam is a natural gradient optimizer using diagonal empirical fisher information. arXiv preprint arXiv:2405.12807, 2024. Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology, 2024. Jeremy Bernstein and Laker Newhouse. Modular duality in deep learning, 2024. David E Carlson, Edo Collins, Ya-Ping Hsieh, Lawrence Carin, and V olkan Cevher. Precondi- tioned spectral descent for deep learning. Advances in neural information processing systems, 28, 2015. Mark Tuddenham, Adam Prügel-Bennett, and Jonathan Hare. Orthogonalising gradients to speed up neural network optimisation. arXiv preprint arXiv:2202.07052, 2022. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025. Yehonathan Refael, Adam Hakim, Lev Greenberg, Tal Aviv, Satya Lokam, Ben Fishman, and Shachar Seidman. Slip: Securing llms ip using weights decomposition, 2024. Jiaxiang Li and Mingyi Hong. A note on the convergence of muon and further. arXiv preprint arXiv:2502.02900, 2025. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32, 2019. 12  Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, and Xipeng Qiu. Adalomo: Low-memory optimiza- tion with adaptive learning rate, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. Arxiv, 2021. Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 1(3):3, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Amit Rozner, Barak Battash, Lior Wolf, and Ofir Lindenbaum. Knowledge editing in lan- guage models via adapted direct preference optimization. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4761–4774, 2024. Yehonathan Refael, Iftach Arbel, Ofir Lindenbaum, and Tom Tirer. Lorenza: Enhancing generalization in low-rank gradient llm training via efficient zeroth-order adaptive sam, 2025. Amit Rozner, Barak Battash, Lior Wolf, and Ofir Lindenbaum. Domain-generalizable multiple- domain clustering. Transactions on Machine Learning Research. Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd, 2020. Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152–1157, 2015. 13 A Proofs of Section 3 In this section, we prove all the theorems and results of Section 3. Lemma A.1 (Moment Becomes Low-Rank During Training).LetM(t)∈Rn×mdenote the first moment of a reversible layer in a moment-based optimization algorithm, updated according to M(t)=βM(t−1)+G(t), where G(t)is the gradient matrix at iteration t. LetM(t)=U(t)Σ(t)V(t)⊤be the singular value decomposition (SVD) of M(t), and define the rank- rorthogonal projection matrix as P(t)(r) = U(t)[:,1:r]U(t)[:,1:r]⊤. Then the relative error of the best rank-one approximation, κM(t)≜∥M(t)−P(t)(1)M(t)∥2 F ∥M(t)∥2 F, (3) satisfies κM(t)≤O(C−t)for some constant C >1. Proof of Lemma A.1. We aim to show that if the gradient G(t)becomes approximately rank-one ex- ponentially fast, then the exponentially weighted moving average of the gradients (i.e., the momentum M(t)) also exhibits exponential decay of higher-rank components. Consider the singular value decomposition of the gradient G(t)=U(t)Σ(t)V(t)⊤,at iteration t. For all natural numbers r < m,, we define H(t)m×r(r) =U[:,1:r]. To enhance notation clarity, denote P(t)(r) =H(t)(r)H(t)⊤(r),, where P(t)(r)represents an orthogonal projection matrix, sat- isfying the conditions P(t)⊤(r)P(t)(r) =P(t)(r),andP(t)(r) =P(t)⊤(r). Without compromising generality, it is assumed that at t= 0, the rank of G(0)is characterized by rank\u0000 G(0)\u0001 > r. For reversible networks, it has been established in [ 1][Theorem 3.2] that the gradients assume the form G(t)=1 NPN i=1\u0000 Ai−BiW(t)Ci\u0001, characterized by constant matrices {Ai}iand positive semi- definite (PSD) matrices {Bi,Ci}i, fort≥t0, where t0∈Nholds. It is pertinent to recall that the vanilla weight update can be represented as W(t)=W(t−1)+ηG(t−1). LetS≜1 NPN i=1Ci⊗Bi andλ1< λ 2denote its two smallest distinct eigenvalues. To substantiate our findings, we utilize several results and arguments presented in the proof of Lemma 3.3 in [ 1]. Specifically, consider G(t0) as the projection of G(t0)onto the minimal eigenspace V1ofScorresponding to λ1.. According to our assumption, the rank of G(t0)isL, and its singular value decomposition (SVD) is given by G(t0)=PL l=1clzly⊤ l, where {zl}L l=1and{yl}L l=1are orthonormal unit vectors, and {cl}L l=1are the corresponding singular values. Therefore, as per Lemma 3.3 in [ 1], the gradient can be decomposed into, G(t) 2 F≤(1−ηλ2)2t g⊥ 0 2 2+ (1−ηλ1)2t g∥ 0 2 2, where g∥ 0is the projection of G(0)onto the minimal eigenspace V1ofS=1 NPN i=1Ci⊗Bi, and g⊥ 0is orthogonal to V1. Here, λ1< λ 2are the smallest distinct eigenvalues of S. We now unroll the momentum update, M(t)=Pt s=1βt−sG(s).Substitute the decomposition of G(s), M(t) 2 F≤tX s=1βt−sh (1−ηλ1)sg∥ 0+ (1−ηS)sg⊥ 0i =tX s=1βt−s(1−ηλ1)sg∥ 0+tX s=1βt−s(1−ηS)sg⊥ 0. Let us define at≜Pt s=1βt−s(1−ηλ1)s, b t≜Pt s=1βt−s(1−ηS)sg⊥ 0,so that M(t) 2 F= atg∥ 0+bt. Now, compute the squared Frobenius norm: ∥M(t)∥2 F=∥atg∥ 0+bt∥2 F=a2 t∥g∥ 0∥2 F+ 2at⟨g∥ 0, bt⟩+∥bt∥2 F. Since g∥ 0⊥g⊥ 0andbtlies in the span of g⊥ 0, we have ⟨g∥ 0, bt⟩= 0, thus, 14 ∥M(t)∥2 F=a2 t∥g∥ 0∥2 F+∥bt∥2 F. Likewise, the spectral norm ∥M(t)∥2 2≥a2 t∥g∥ 0∥2 2. Hence, the ratio κm(t) =∥M(t)−P(t)(1)M(t)∥2 F ∥M(t)∥2 F≤∥M(t)∥2 F− ∥M(t)∥2 2 ∥M(t)∥2 F≤a2 t∥g∥ 0∥2 F+∥bt∥2 F−a2 t∥g∥ 0∥2 2 a2 t∥g∥ 0∥2 F+∥bt∥2 F. Using that ∥g∥ 0∥2 2=σ2 1, and the decay bound ∥bt∥2 F=O((max{β,1−ηλ2})2t), while a2 t= Ω((max {β,1−ηλ1})2t), we conclude: κm(t)≤O \u0012max{β,1−ηλ2} max{β,1−ηλ1}\u00132t! =O(C−t), for some constant C >1. □ Before proving Lemma??, we shortly present the following two preliminary lemmas. To that end, we present the following notations, •M(t)- The moment in iteration t. Its dimensions are n×m, where n < m. •∥ · ∥ - The Frobenius norm: ∥A∥=∥A∥F=√ AA⊤ •L∗- A stationary point to which the loss Lconverges. •B- Batch size. • For A∈Rm×mandB∈Rn×nwe denote AIm×nB⊤byAB⊤for convenience. Additionally, we note that our proof is based on an equivalent but slightly modified formulation of moment’s update. Specifically, instead of using the standard formulation of the moment’s update M(t+1)=βM(t)+G(t), we consider the convex combination, M(t+1)=βM(t)+ (1−β)G(t). This alternative formulation simplifies the analysis, but equivalent. To show that, we point out that we can choose an modefied learning step η∗=η 1−β>0we get the same weight’s updating step. η∗Orth\u0010 βM(t)+ (1−β)G(t)\u0011 =Orth\u0010 η∗βM(t)+η∗(1−β)G(t)\u0011 =Orth\u0012 ηβ 1−βM(t)+ηG(t)\u0013 =ηOrth\u0012β 1−βM(t)+G(t)\u0013, where Orth is the SVD orthogonalization step, formaly solving arg min O{∥O−A∥F:either OTO=IorOOT=I}. Obviously, β >0could be chosen in a way thatβ 1−βM(t)would result in any required positive real number. We assume the following 4 assumptions throughout our proofs: (A1) The gradient ∇L(W)isL-Lipschitz continuous. 15 (A2)∇L(W, ξ)is an unbiased estimator of ∇L(W)where L(W, ξ)is the gradient of L(W) when taking a single training sample ξ. (A3)E∥∇L(W, ξ)− ∇L (W)∥ ≤σ2. (A4) There exists δ >0such that ∥E(t) 5∥ ≤δ∥U(t)V(t)⊤∥=δ√mfor all t. Lemma A.2 (Descent Lemma with Newton-Schulz Approximation Error).Consider the Muon optimizer update defined by M(t)←βM(t−1)+ (1−β)G(t), O(t)←U(t)V(t)⊤+E(t) 5,(Newton-Schulz 5 iteration approximation), W(t+1)←W(t)−ηtO(t), where M(t)=U(t)S(t)V(t)⊤is the singular value decomposition of M(t), andE(t) 5represents the Newton-Schulz (5 iterations) approximation error. Additionally, assume (A1) - (A4). Then the following holds: L(W(t+1))≤ L(W(t))−\u0010ηt 4−ηt√mδ\u0011 ∥∇L(W(t))∥+ηt5 2∥∇L(W(t))−M(t)∥+η2 tmL 2+η2 tmLδ +η2 tLmδ2 2 Proof. SinceLis L-lipschitz function, the descent lemma holds. Thus we have L(W(t+1))≤ L(W(t)) +⟨∇L(W(t)),W(t+1)−W(t)⟩+L 2∥W(t+1)−W(t)∥2 =L(W(t))−ηt⟨∇L(W(t)),U(t)V(t)⊤+E(t) 5⟩+Lη2 t 2∥U(t)V(t)⊤+E(t) 5∥2 =L(W(t))−ηt⟨∇L(W(t)),U(t)V(t)⊤⟩ −ηt⟨∇L(W(t)),E(t) 5⟩ +Lη2 t 2\u0010 n+ 2⟨U(t)V(t)⊤,E(t) 5⟩+∥E(t) 5∥2\u0011 ≤ (∗)L(W(t))−ηt 4∥∇L(W(t))∥+ηt5 2∥∇L(W(t))−M(t)∥+η2 tmL 2 +ηt∥∇L(W(t))∥∥E(t) 5∥+Lη2 t√m∥E(t) 5∥+Lη2 t 2∥E(t) 5∥2 ≤ L(W(t))−ηt 4∥∇L(W(t))∥+ηt5 2∥∇L(W(t))−M(t)∥+η2 tmL 2 +ηtδ√m∥∇L(W(t))∥+Lη2 t√mδ√m+Lη2 t 2δ2n =L(W(t))−\u0010ηt 4−ηt√mδ\u0011 ∥∇L(W(t))∥+ηt5 2∥∇L(W(t))−M(t)∥+η2 tmL 2+ +η2 tmLδ +η2 tLmδ2 2 Where in (∗)we used, equation 2.8. □ Lemma A.3. For constant ηt=η >0, the following holds η−4η√mδ 4TX t=1∥∇L(W(t))∥ ≤ L(W(1))− L∗+ηt5 2TX t=1∥∇L(W(t))−M(t)∥+η2mLT 2+Tη2mLδ +Tη2Lmδ2 2. Proof. Using Lemma A.2, isolating ∥∇L(W(t))∥and summing over all steps η−4η√mδ 4TX t=1∥∇L(W(t))∥ ≤ 16 L(W(1))− L∗+ηt5 2TX t=1∥∇L(W(t))−M(t)∥+η2mLT 2+Tη2mLδ +Tη2Lmδ2 2 □ Lemma 3.3 (Exact convergence rate of Muon). Consider the Muon optimizer update defined by M(t)←βM(t−1)+ (1−β)Gt O(t)←U(t)V(t)⊤+E(t) i,(iiterations Newton-Schulz approximation ) W(t+1)←W(t)−ηtO(t), where M(t)=U(t)S(t)V(t)⊤denotes the singular value decomposition of M(t), andE(t) irepresents the Newton-Schulz5 approximation error after iiterations. Assuming A(1) - A(4), If we take β= 1−αwithα= min(√ Rl σ√ T,1),ηt=η=√ 4R √ (10/(1−β)+2m+4mδ+2mδ2)TL, and B= 1 (batch free convergence) than1 TPT t=1E\u0002 ∥∇L(W(t))∥\u0003 is bounded by O \"p RLm (2 + 4 δ+ 2δ2)√ T+σ2 √ RLT+σ(RL)1/4+√σ(RL)1/4 T1/4# 1 1−4√mδ!, where R=L(W(0))− L∗. If we take βas an arbitrary constant, we have to take B=T, and we have, 1 TTX t=1E∥∇L(W(t))∥ ≤ O \"p RLn(2 + 4 δ+ 2δ2)√ T+√ RL√ T+σ T3/2+σ√ T# 1 1−4√mδ!. Proof of Lemma Lemma??.The proof follows [ 53]. Using the same notations as [ 53], we denote ˆγ(t)=M(t)− ∇L (W(t)),γ(t)=G(t)− ∇L (W(t))andS(X,Y) =∇L(X)− ∇L (Y). Note that we have the following •E[γ(t)] = 0 from A(2). •E[∥γ(t)∥2]≤σ2 mfrom A(3). •E[⟨γ(i), γ(j)⟩] = 0,∀i̸=jsince γ(i)andγ(j)are independent. •∥S(X,Y)∥ ≤L∥X−Y∥from A(1). Now following the update in (2), we get ˆγ(t+1)=βˆγ(t)+ (1−β)γ(t)+S(X(t),X(t+1)) =βtˆγ(1)+ (1−β)t−1X τ=0βτγ(t−τ)+t−1X τ=0βτS(X(t−τ),X(t+1−τ)), therefore ∥ˆγ(t+1)∥ ≤βt∥ˆγ(1)∥+ (1−β) t−1X τ=0βτγ(t−τ) +ηLt−1X τ=0βτ. Taking expectation we get (using the fact that ˆδ1=δ1): E∥ˆγ(t+1)∥ ≤βtσ m+ (1−β)vuutt−1X τ=0β2τσ2 B+ηLt−1X τ=0βτ ≤σ mβt+σ m1−βp 1−β2+ηL1 1−β 17 ≤σ mβt+σ mp 1−β+ηL1 1−β. All in all, we get TX t=1E[∥ˆγ(t+1)∥]≤σ (1−β)B+Tp 1−βσ m+TηL 1−β. Using Lemma A.3, we get ηt−4Lηt√mδ 4TX t=1∥∇L(W(t))∥ ≤ L(W(1))− L∗+ηt5 2TX t=1∥∇L(W(t))−M(t)∥+η2mLT 2+Tη2mLδ +Tη2Lmδ2 2. Dividing both sides byη−4ηL√mδ 4we get TX t=1∥∇L(W(t))∥ ≤ \" 4(L(W(1))− L∗) η+ 10TX t=1∥∇L(W(t))−M(t)∥+ 2ηmLT + 4TηmLδ + 2TηLmδ2# ·1 1−4√mδ ≤\u00144R η+ 10σ (1−β)m+ 10Tp 1−βσ m+ 10TηL 1−β+ 2ηmLT + 4TηmLδ + 2TηLmδ2\u0015 ·1 1−4√mδ By taking η=q 4R (10/(1−β)+2m+4mδ+2mδ2)TLwe get TX t=1∥∇L(W(t))∥ \u0014 ≤4p RTL (10/(1−β) + 2m+ 4mδ+ 2mδ2) +10σ (1−β)m+ 10Tp 1−βσ (1−β)m\u00151 1−4√mδ. Now we have two types of parameter choice. If we take B= 1 (batch size free), we need to take 1−β= min(1,√ RL σ√ T)so that we have TX t=1∥∇L(W(t))∥ ≤\" 2p RTL (2m+ 4mδ+ 2mδ2) + 2√ 10·σ·(RL)1/4T3/4+ 10σ2r T RL+ 10√σ(RL)1/4T3/4# 1 1−4√mδ, thus 1 TTX t=1Eh ∥∇L(W(t))∥i ≤ O \"p RLn(2 + 4 δ+ 2δ2)√ T+σ·(RL)1/4 T1/4+σ2 √ RLT+√σ(RL)1/4 T1/4# 1 1−4√mδ!. If we take βas an arbitrary constant in (0,1), then we will need to take B=T, so that 1 TTX t=1∥∇L(W(t))∥ ≤ O \"p RLn(2 + 4 δ+ 2δ2)√ T+√ RL√ T+σ T3/2+σ√ T# 1 1−4√mδ!. □ Lemma 3.2 (Orthogonalization error Ei)For a matrix A∈Rm×n, letσ1be the largest singular value of AA⊤andσmbe the smallest (without the loss of generality, assume m≤n). Let r≤mbe 18 the largest index where σr> σr+1=... σ m≥0. Letκ=σ1 σmby the condition number of AA⊤. Denote Eithe error of Newton-Schultz after iiterations. Then we have ∥Ei∥F≤√r·\u0012 1−1 κ\u00132i. (4) Proof of Lemma 3.2. We denote B=AA⊤,Xkthe result after kNewton-Schultz iterations, X0= B ∥B∥2andO=UV⊤where B=UΣV⊤is the SVD decomposition with σ1≥σ2≥ ··· ≥ σmthe singular values. It is known that Newton-Schultz converges quadratically, so we have ∥Ek∥2=∥Xk−O∥2≤ ∥X0−O∥2k 2 We now bound ∥X0−O∥2. We know that X0=UΣV⊤ ∥B∥2=UΣV⊤ σ1 ∥X0−O∥2= UΣV⊤ σ1−UV⊤ 2= U(Σ σ1−I)V⊤ 2= Σ σ1−I 2 Where last equality is due to the fact that ∥ · ∥ 2is unitary invariant. The matrixΣ σ1−Iis diagonal with valuesσi σ1−1on the diagonal. From that observation we get that Σ σ1−I 2= max i σi σ1−1 = max i\u0012 1−σi σ1\u0013 = 1−σm σ1= 1−1 κ For the Frobenius norm, we get a similar analysis. ∥X0−O∥F= Σ σ1−I Fsince∥ · ∥Fis unitary invariant, so we just need to calculate Σ σ1−I F. It is known that Σ σ1−I F≤√r Σ σ1−I 2so all in all we have Σ σ1−I F≤√r Σ σ1−I 2=√r\u0012 1−1 κ\u0013. □ Theorem 3.8 (Convergence of SUMO) For a loss function L,and given architecture Φ, suppose that the compositions of f≡ L(Φ(·))isβ-smooth non-convex function that is bounded by some M∈R+. LetG(t) jdenote the gradient matrix w.r.t. the j-th reversible layer W(t) j,at time t∈N, for allj∈[L]andt∈N, and Tℓ, ℓ∈Ntimes are set by a convergence criterion (that is, ∥ˆG(Tℓ) j∥ ≤ςℓ). Then, there exist C∈R+andNsuch that for all TN>C ε2, and1 TNPN−1 i=0PTi+1−1 t=Ti G(t) j 2 F≤ε. Namely, Algorithm 1 achieves an ε-critical point,3i.e., G(t) j 2 F≤ε, for some t∈N, and any j∈[L]. Proof of Theorem 3.8. for any layer j∈[L]; in the following, for simplicity of notation, we ignore the index jand use G(t)instead. By Lemma A.4, the low-rank optimization block 1 in Algorithm 1 is guaranteed to converge; we denote by Tℓ∈Nthe time index tat which we enter block 1 for the ℓth time (i.e., ∥ˆG(Tℓ) j∥ ≤ς2), for ℓ∈N. Furthermore, we recall that G(t) j≜∇Wjf\u0010 θ(t)\u0011; when clear from the context, we omit jfromWj, and use instead ∇Wjf\u0010 θ(t)\u0011 =∇f\u0000 W(t)\u0001. Consider the SVD decomposition of the gradient ∇Wjf(θTi) =U(Ti)Σ(Ti)V(Ti)⊤. Fort∈[Ti,Ti+1−1], we define the projected gradient as ˆG(t)≜P(Ti)(r)G(t),where P(Ti)(r) =U(Ti)[:,:r]⊤, using the exact truncated-SVD calculation (in Block 1). For simplicity, we refer to Q(Ti)(r)asQ(Ti),and we denote P(Ti)=Q(Ti)⊤Q(Ti). Next, let ht≜f(W(t))−f(W(Ti+1)), and ηtdenote the learning rate. Then, ht+1=f(W(t+1))−f(W(Ti+1)) 3Also known as ε-stationary, see, e.g.,. 19 =f\u0010 W(t)−ηt\u0010 W(t+1)−W(t)\u0011\u0011 −f\u0010 W(Ti+1)\u0011 ≤f(W(t))−f\u0010 W(Ti+1)\u0011 −ηt⟨G(t),O(t)⟩+βη2 t 2∥O(t)∥2 F ≤ (1)f(W(t))−f\u0010 W(Ti+1)\u0011 −ηt 4∥ˆG(t)∥2 F+5 2·ηtσ√m+βη2 tn 2 ≤ (2)ht−ηt 4∥ˆG(t)∥2 F+5 2·ηtσ√m+βη2 tn 2. where (1)follows Eq 9, and (2)follows Lemma A.4. Thus, summing over t∈[Ti,Ti+1), we get Ti+1−1X t=Ti(ht+1−ht)≤Ti+1−1X t=Ti\u0012 −ηt 4∥ˆG(t)∥2 F+5 2·ηtσ√m+βη2 tn 2\u0013 hTi+1≤hTi−Ti+1−1X t=Ti\u0010ηt 4∥ˆG(t)∥2 F\u0011 +Ti+1−1X t=Ti\u00125 2·ηtσ√m+βη2 tn 2\u0013. (5) Assume a constant learning rate ηt=η, and define Ti:=Ti+1−Ti. For each interval [Ti,Ti+1−1], we have, Ti+1−1X t=Tiη 4∥ˆG(t)∥2 F≤hTi−hTi+1+Ti\u00125 2·ησ√m+βη2n 2\u0013. Summing over all i= 1,..., N, NX i=1Ti+1−1X t=Tiη 4∥ˆG(t)∥2 F≤NX i=1\u0000 hTi−hTi+1\u0001 +NX i=1Ti\u00125 2·ησ√m+βη2n 2\u0013 =hT0−hTN+1+T\u00125 2·ησ√m+βη2n 2\u0013, where T=PN i=1Tiis the total number of iterations. Using hT0−hTN+1≤M, we get, NX i=1Ti+1−1X t=Ti∥ˆG(t)∥2 F≤4M η+ 2T\u00125σ√m+βηn\u0013. Letη=q 2M βn·1√ T. Then, the average squared norm of the projected gradients satisfies, 1 TTX t=1∥ˆG(t)∥2 F≤4M ηT+ 2\u00125σ√m+βηn\u0013 =4M\u0010q 2M βn·1√ T\u0011 T+ 2 5σ√m+βn·s 2M βn·1√ T! =4M√βn√ 2M·1 T3/2+ 2·5σ√m+ 2βn·s 2M βn·1√ T =4√2Mβn T3/2+10σ√m+ 2p 2Mβn·1√ T. Thus, the bound becomes, 1 TTX t=1∥ˆG(t)∥2 F≤4√2Mβn T3/2+10σ√m+2√2Mβn√ T. 20 Recall that for P(Ti),it holds that ∥ˆG(Ti)−Q(Ti)ˆG(Ti)∥2 F≤α∥ˆG(Ti)∥2 F, for some α∈(0,1].Then, ∥Q(Ti)⊥ˆG(Ti)∥2 F≤α 1−α∥Q(Ti)ˆG(Ti)∥2 F. From Lemma B.3 in, under η≤2 λmax, we get, E∥ˆG(t)∥2 F≤E∥ˆG(Ti)∥2 F,∀t∈[Ti,Ti+1). Hence, 1 TNN−1X i=0Ti+1−1X t=Ti∥G(t)∥2 F≤1 TNN−1X i=0(Ti+1−Ti)∥G(Ti)∥2 F ≤1 (1−α)TNN−1X i=0(Ti+1−Ti)∥Q(Ti)ˆG(Ti)∥2 F ≤1 (1−α)TN 4√2Mβn T3/2 N+10σ√m+2√2Mβn√TN! ≤4√2Mβn (1−α)T5/2 N+10σ (1−α)TN√m+2√2Mβn (1−α)T3/2 N(6) Accordingly, for any ε >0, ifTNsatisfies 4√2Mβn (1−α)T5/2 N+10σ (1−α)TN√m+2√2Mβn (1−α)T3/2 N≤ε, a sufficient condition for this to hold is, TN≥max(\u001212√2Mβn (1−α)ε\u00132/5,30σ (1−α)ε√m,\u00126√2Mβn (1−α)ε\u00132/3), it follows that min 0≤t≤TN∥G(t)∥2 F≤1 TNN−1X i=0Ti+1−1X t=Ti∥G(t)∥2 F≤ε. Thus, there exists an iteration index t∈[0,TN]such that ∥G(t)∥2 F≤ε, which, by definition, implies that the algorithm reaches an ε-critical point. This concludes that Algorithm 1 achieves an ε-critical point. □ In the following, we provide the auxiliary lemma that is used in the proof of Theorem 3.8. Lemma A.4 (Convergence of the Inner Fixed Low-Rank Optimization).Consider the same setting and assumptions as in Theorem 3.8. Then, the second time t=Tℓ∈Nin which Algorithm 1 enters Block 1 (where it updates the projection matrix) happens for a finite ℓ∈N. Proof. By the β-smoothness of f, we have f(W(t+1))≤f(W(t)) +⟨G(t),W(t+1)−W(t)⟩+β 2∥W(t+1)−W(t)∥2 F. 21 Substituting the update rule W(t+1)=W(t)−ηtO(t), we get f(W(t+1))≤f(W(t))−ηt⟨G(t),O(t)⟩+βη2 t 2∥O(t)∥2 F. (7) Since ˆG(t)=P(t)(r)⊤G(t)andO(t)∈range (P(t)(r)⊤), it holds that ⟨G(t),O(t)⟩=⟨ˆG(t),O(t)⟩. By equation (2.8) in, we have −⟨ˆG(t),O(t)⟩ ≤ −1 4∥ˆG(t)∥2 F+5 2∥ˆG(t)−M(t)∥2 F. Now we bound ∥ˆG(t)−M(t)∥F: ∥ˆG(t)−M(t)∥F=∥ˆG(t)− ∇f(W(t)) +∇f(W(t))−M(t)∥F ≤ ∥ˆG(t)− ∇f(W(t))∥F+∥∇f(W(t))−M(t)∥F ≤σ√m,(by assumptions A(1)-(2)). Substituting this into the previous inequality gives −⟨ˆG(t),O(t)⟩ ≤ −1 4∥ˆG(t)∥2 F+5 2·σ√m. (8) Substituting into equation (7), we obtain f(W(t+1))≤f(W(t))−ηt 4∥ˆG(t)∥2 F+5 2·ηtσ√m+βη2 t 2∥O(t)∥2 F. Since by definition ∥O(t)∥2 F≤n, we get f(W(t+1))≤f(W(t))−ηt 4∥ˆG(t)∥2 F+5 2·ηtσ√m+βη2 tn 2. (9) For constant step size ηt=η, summing over t= 1toT, we get f(W(T+1))≤f(W(1))−η 4TX t=1∥ˆG(t)∥2 F+5 2·ησT√m+βη2nT 2. Rearranging and using f(W(1))−f∗≤M, we conclude 1 TTX t=1∥ˆG(t)∥2 F≤4M ηT+ 10·σ√m+ 2ηβn. □ B Additional Information Definition B.1. (Reversibility [ 19]) A neural network ϕthat maps the input xto output y=ϕ(x;θ) is reversible, if there exists L(x;θ)so that y=L(x;θ)x, and the backpropagated gradient gx satisfies gx=L⊤(x;θ)gy, where gyis the backpropagated gradient at the output y.L(x;θ) depends on the input xand weight θin the network ϕ. 22 Several critical observations regarding Algorithm 1 warrant attention. Initially, in order to minimize memory consumption, Algorithm 1 implements a per-layer weight update during the process of backpropagation, as advocated by contemporary studies, see, e.g., [ 45]. This approach contrasts with conventional optimizers, which typically update all weights after backpropagation by retaining the complete gradients in memory, a method potentially marked by significant inefficiency. Should there be a desire to generate an adapter (i.e., a parallel low-dimensional LoRA-type model) subsequent to fine-tuning, this can be achieved with efficiency through the following steps. Firstly, the training weights gap ∆≜WFine-Tuned −WPretrained is computed, where WFine-Tuned denotes the model weight upon process completion, and WPretrained refers to the original model weight. Subsequently, rAdaptor≜rank(∆)is determined utilizing a matrix ranking algorithm, followed by the resolution ofminA∈Rn×rAdaptor,B∈RrAdaptor×m∥∆−AB∥2 Fthrough any optimization algorithm (e.g., gradient descent). It is noteworthy that any solution to this matrix factorization optimization problem is well-known as a global optimum. C Update Step Rule Formulation Definition C.1. [Subspace-Aware Moment-Orthogonalization (SUMO)] SUMO formulates the subsequent gradient update rules. Refer to SUMO  ˆG(t)=Q(t)⊤∇Wf(Wt;ξt)R(t) M(t+1)=βM(t)+ (1−β)ˆG(t) O(t+1)=Orthogonalization_SVD\u0010 M(t+1)\u0011 W(T)=W(0)+ηT−1X t=0Q(t)O(t+1)R(t)⊤, withQt∈Rm×randRt∈Rr×ndenoting projection matrices, T∈Nrepresenting the subspace update interval, ηindicating the learning rate, ξtconstituting a stochastic batch, and Orthogonalization_SVD( A)as the operator that resolves the following through Singular Value De- composition (SVD), as described in arg min O{∥O−A∥F:either OTO=IorOOT=I}. D Additional Experiments In Table 6, we evaluated SUMO and state-of-the-art memory-efficient fine-tuning methods on the MAWPS[ 55] dataset using the LLaMA2-7B model. We report results across two rank settings (32 and 128), comparing training time, memory usage, and task accuracy. SUMO consistently achieves superior accuracy while maintaining competitive efficiency in both memory and time (comparing to Galore). Table 6: Fine-tuning LLaMA2-7B on MAWPS Methods Rank Time(h) ↓Memory (GB) ↓Accuracy (%) ↑ LoRA 32 0.40 14.36 45.80 DoRA 32 0.69 15.01 44.96 GaLore 32 2.59 15.15 58.40 SUMO (Newton-Shultz5) 32 1.83 13.86 58.47 SUMO (SVD) 32 1.56 13.86 61.23 LoRA 128 0.45 15.64 65.97 DoRA 128 0.72 16.17 66.81 GaLore 128 2.61 15.79 64.29 SUMO (Newton-Shultz5) 128 1.78 14.12 64.41 SUMO (SVD) 128 1.62 14.12 68.03 23 D.1 Details of Fine-Tuning on GLUE We fine-tune the pre-trained RoBERTa-Base model on the GLUE benchmark using the model provided by the Hugging Face. In Table and, we detail the hyper parameters used in fine-tuning. MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Batch Size 16 16 16 32 16 16 16 16 # Epochs 30 30 30 30 30 30 30 30 Learning Rate 1E-05 1E-05 3E-05 3E-05 1E-05 1E-05 1E-05 1E-05 Rank Config. r= 4 Projection back scale 4 Max Seq. Len. 512 Table 7: Hyperparameters of fine-tuning RoBERTa base for the comparison in Table 2 with respect only to rank=4. MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Batch Size 16 16 16 32 16 16 16 16 # Epochs 30 30 30 30 30 30 30 30 Learning Rate 1E-05 2E-05 2E-05 1E-05 1E-05 2E-05 2E-05 3E-05 Rank Config. r= 8 Projection back scale 2 Max Seq. Len. 512 Table 8: Hyperparameters of fine-tuning RoBERTa base for the comparison in Table 2 with respect only to rank=8. 24",
  "text_length": 64981
}