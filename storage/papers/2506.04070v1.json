{
  "id": "http://arxiv.org/abs/2506.04070v1",
  "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward",
  "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.",
  "authors": [
    "Yi Zhao",
    "Siqi Wang",
    "Jing Li"
  ],
  "published": "2025-06-04T15:34:33Z",
  "updated": "2025-06-04T15:34:33Z",
  "categories": [
    "cs.CL",
    "cs.MM"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04070v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04070v1  [cs.CL]  4 Jun 2025LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\nImpaired via GRPO with LLM-as-Follower Reward\nYi Zhao, Siqi Wang and Jing Li\nDepartment of Computing, The Hong Kong Polytechnic University\n{yi-yi-yi.zhao, siqi23.wang}@connect.polyu.edu.hk, jing-amelia.li@polyu.edu.hk\nAbstract\nNavigation instruction generation for visually\nimpaired (VI) individuals (NIG-VI) is criti-\ncal yet relatively underexplored. This study,\nhence, focuses on producing precise, in-situ,\nstep-by-step navigation instructions that are\npractically usable by VI users. Concretely,\nwe propose LaF-GRPO (LLM-as-Follower\nGRPO), where an LLM simulates VI user re-\nsponses to generate rewards guiding the Vision-\nLanguage Model (VLM) post-training. This\nenhances instruction usability while reducing\ncostly real-world data needs. To facilitate\ntraining and testing, we introduce NIG4VI,\na 27k-sample open-sourced benchmark. It\nprovides diverse navigation scenarios with\naccurate spatial coordinates, supporting de-\ntailed, open-ended in-situ instruction genera-\ntion. Experiments on NIG4VI show the effec-\ntiveness of LaF-GRPO by quantitative metrics\n(e.g., Zero-(LaF-GRPO) boosts BLEU +14%;\nSFT+(LaF-GRPO) METEOR 0.542 vs. GPT-\n4o’s 0.323) and yields more intuitive, safer in-\nstructions. Code and benchmark are available\nat https://github.com/YiyiyiZhao/NIG4VI.\n1 Introduction\nThe Visually Impaired (VI) community, comprising\napproximately 2.2 billion individuals globally with\npartial or complete blindness, underscores the sig-\nnificant need for effective assistive technologies.1\nEnhancing their quality of life through Visually\nImpaired Assistance (VIA) has motivated exten-\nsive research, e.g., VIALM (Zhao et al., 2024) and\nWalkVLM (Yuan et al., 2025). This paper specifi-\ncally focuses on Navigation Instruction Generation\nfor VI users ( NIG-VI ), a VIA sub-area. Navigation\nInstruction Generation (NIG) was initially concep-\ntualized for general embodied agents, producing\nhigh-level trajectory plans. Yet, NIG-VI, being\npeople-centered , operates under fundamentally dif-\nferent constraints. As shown in Figure 1, effective\n1https://www.who.int/publications/i/item/9789241516570\nTask Description with Location Information:\n- Current location:   (-97.79, 135.41)\n- Current rotation yaw: -146.17°\n- Next step target location: (-98.2, 135.25)(b)\nLaF-GRPO (Ours) ：<think>  Move straight\nahead at 12 o’clock for 0.4 meters. ...bushes\non the left to avoid the pedestrian; traffic\nsounds from the right road....  </think>\n<answer>  Proceed straight  at 12 o’clock for\n0.4 meters.  Bushes on your left, road with\ntraffic to the right; pause if you hear voices\nor engines.  </answer>ASSITER ：walk straight 0.5 meters, road and\nbeach on your left and street on your right.\nBird-View\nFront-View(a)\nBenchmark GT ：Continue straight toward 12\no’clock for 0.4 meters.  Hedge and water to\nyour left; palm tree and pedestrian at 3\no’clock . Guide cane left to avoid the trunk;\npause if engines or voices approach the road\nedge.Figure 1: A NIG4VI sample. (a) The bird’s-eye view\nmap with waypoints and corresponding front-view im-\nages. (b) The system takes front-view images, task\ndescriptions, and position data (in the blue box) as input\nto generate navigation instructions with the benchmark\noutput (in the yellow box) and our model output (in the\ngreen box). The grey box indicates a non-VI navigation.\nNIG-VI systems must generate in-situ step-level\ninstructions that (1) integrate non-visual sensory\ncues (auditory/tactile landmarks, surface textures)\n(2) provide accurate directional and distance guid-\nance to compensate for lack of visual referencing,\nand (3) adapt to urban obstacles - all within map-\ncoordinate systems while ensuring walking safety.\nEarly attempts such as ASSISTER (Huang et al.,\n2022) laid the initial groundwork in this field, yet\nsuffered from architectural limitations of BERT-\nbased systems (Devlin et al., 2019). The advent of\nVision-Language Models (VLMs) introduces new\nopportunities through their multimodal understand-\ning and generation capabilities. RL-based post-\ntraining methods like GRPO (DeepSeek-AI, 2025)\nfurther enhance reasoning abilities, enabling VLMs\nto align with NIG-VI that demands people-centered\nguidance. However, most existing approaches rely\non large-scale parallel data for fine-tuning, which\ncan be costly and fail to incorporate interactive user\n1\n--- Page 2 ---\nfeedback essential for people-centered guidance.\nTo bridge this gap, we propose LLM-as-\nFollower GRPO (LaF-GRPO) —the first GRPO-\nbased framework for the NIG-VI task, featuring\ntwo novel components: (1) an LLM that simu-\nlates VI user responses to navigation instructions\nby interpreting their likely actions, and (2) a VLM\npost-training procedure for instruction generation,\nguided by LLM-as-Follower reward. LaF-GRPO\nmitigates the need for costly VI user trials while\nensuring instruction usability with human-in-the-\nloop navigation simulation. Also, viewing the\nscarcity of VI navigation benchmarks, we construct\nNIG4VI - a comprehensive VI navigation instruc-\ntion benchmark featuring 27k samples for simula-\ntion experiments. Fully open-sourced with granular\nspatial metadata, NIG4VI enables the generation\nof detailed, open-ended in-situ instructions.\nWe then experiment with LaF-GRPO on\nNIG4VI and find: (1) Qwen2.5-VL models trained\nwith Zero-(LaF-GRPO) outperforms the Zero-Shot\nbaseline, achieving superior scores across diverse\nmetrics. (2) Qwen2.5-VL-7B models trained\nwith SFT+(LaF-GRPO) show leading performance,\nachieving a METEOR score of 0.542, substantially\nhigher than GPT-4o. (3) Beyond quantitative gains,\nLaF-GRPO can potentially help generate people-\ncentered instructions with enhanced linguistic vari-\nety, more intuitive directional cues, richer environ-\nmental details, and crucial safety considerations.\nIn summary, our main contributions are:\n•We propose the LaF-GRPO framework, the\nfirst method to employ GRPO for NIG-VI with a\nnovel LLM-simulated follower feedback;\n•We contribute the NIG4VI benchmark, the\nfirst open-source comprehensive dataset featuring\nprecise multi-modal navigation contexts to facili-\ntate robust model evaluation for VI navigation;\n•We present extensive empirical studies\nacross VLMs under various paradigms (Zero-shot,\nZero-(LaF-GRPO), SFT, and SFT+(LaF-GRPO)),\ndemonstrating the effectiveness of our approach.\n2 Related Work\n2.1 VLMs and VIA\nVLMs (Liu et al., 2023; Dai et al., 2023; OpenAI,\n2024a; Anthropic, 2024; Team, 2024) have drawn\nattention for combining visual perception with lan-\nguage generation. Refining VLMs with Reinforce-\nment Learning (Ouyang et al., 2022) improves\nalignment with human preferences and enhancesreasoning abilities. Recent success in Group Rel-\native Policy Optimization (GRPO) (DeepSeek-AI,\n2025) has led to RL fine-tuned VLMs like VLM-R1\n(Shen et al., 2025), AlphaDrive (Jiang et al., 2025),\nand MedVLM-R1 (Pan et al., 2025), broadening\ntheir application range. VIA with VLMs is closely\nrelated to visual captioning and Visual Question\nAnswering (VQA). VIALM (Zhao et al., 2024)\nframes VIA as a VQA task, generating step-by-\nstep guidance from environment images and user\nrequests. While VIALM emphasizes environment-\ngrounded guidance with tactile information, it is\nnot specifically designed for navigation. WalkVLM\n(Yuan et al., 2025) extends this to dynamic walking\nassistance and introduces the Walking Awareness\nDataset (WAD). Though WalkVLM tackles naviga-\ntion, its focus remains on video captioning rather\nthan precise orientation and mobility guidance.\n2.2 Navigation Instruction Generation (NIG)\nThere are two main branches for NIG studies: NIG\nfor embodied agents and NIG for the visually im-\npaired. Prior research on NIG for embodied agents\nhas predominantly focused on advanced visual\nprocessing techniques while generating trajectory-\nlevel instructions. More details can be found in\nAppendix A. For the NIG-VI branch, ASSISTER\n(Huang et al., 2022) introduced the UrbanWalk\nbenchmark for in-situ instructional guidance and\ndeveloped a navigation assistance model. Our work\nimproves upon ASSISTER in two key ways: (1)\nwe introduce a more detailed evaluation benchmark\ncovering orientation, mobility, scene description,\nand safety warnings; and (2) we leverages advanced\nVLMs within a GRPO framework with an LLM-\nas-Follower reward mechanism, leading to more\neffective navigation instructions.\n3 NIG-VI Task and LaF-GRPO Method\n3.1 NIG-VI Task Formulation\nWe start this section by describing the NIG-VI task\nof generating in-situ step-by-step natural language\ninstructions to guide VI users along a pre-planned\nroute P= [p1, . . . , p K]using a VLM-based as-\nsistant system. The route Pconsists of positional\nwaypoints pileading to a destination and is gen-\nerated using the A* algorithm. At each discrete\nstepiof the navigation, the VLM receives two pri-\nmary inputs: a front-view camera image x(i)\nimageand\na task description which includes the user’s cur-\nrent pose x(i)\npose= (x(i)\nloc, x(i)\nrot)represented by their\n2\n--- Page 3 ---\nO2: <think>The bus is to the right, and a motorcycle is\napproaching from the left. There are streetlights on both sides,\nwith a street ahead. Step forward half a meter; tap the cane\nnear the left to check for light poles, and listen for the cyclist.\n</think>\\n <answer>Move forward half a meter, using your cane to\ncheck both sides for obstacles. Pause if you hear engines or\nnearby voices, and step clear if necessary.</answer>\nQuestion:\nGenerate navigation instructions for visually\nimpaired users based on the provided front-view\nimage and step information:\n- Current location:  {'location': {'x': 98.39,\n'y': 212.54}, 'rotation_yaw': 133.35}\n- Target goal (next step):  {'location': {'x':\n98.39, 'y': 212.55}, 'rotation_yaw': 130.54}\n... describe the surroundings .... If there is a\nhigh likelihood of collision with any object,\ninclude a clear warning for the user....\nAnswer:\nTurn slightly left toward 10 o’clock and hold. Bus idling on your right; motorcycle ahead in fog.\nAvoid proximity to the bus, pause until engine sounds fade—use your cane and listen closely.\nAction interpretation: \n{\"move \": {\"direction\": \"10 o'clock\",\"distance\": \"0 meters\"},\" detailed_hazard_alert \": true}Front-ViewA Training Sample\nText\nSFTNavigation Instruction:\nTurn slightly to your right, about 1 o'clock direction, and proceed forward\nfor approximately 0.5 meters...Trees and vegetation ....Be cautious of\npotential traffic, as there are roads and a parked blue car nearby. ...The\nground is paved, likely a sidewalk or road edge...changes in surface texture.\nAction Interpretation:\n\"{'move': {'direction': \\\"1 o'clock\\\", 'distance': '0.5 meters'},\n'detailed_hazard_alert': True}\"Input: \n  A Navigation Instruction\nOutput: \n  Action Interpretation\nρ φLLM Training Data ConstructionTraining A LLM-based Action InterpreterVLM Two-Stage Post-Training\nSFT Input: ( Image , Question )\nOutput: Answerπ θ\nO1: <think> Move forward slightly left\ntoward 11 o’clock. Stay near the sidewalk\nedge on the left to avoid the bus by your\nright; listen for motorcycle engines\nahead. </think>\\n <answer> Proceed at 11\no’clock, adjusting slightly left. Stay left\nof the bus and pause if engines approach.\n</answer>  \nπ θ\nReward\nFunctions\nπ( r ef)Input: ( Image , Question )\nReward (r1)   1.0\nReward (r 2)   1.0Reward  (r1)   0.19\nReward  (r2)   0.19Reward (r1)   0.7\nReward (r 2)   0.0πρ φLLM-as-Follower reward Format reward Text Generation reward\nGroup ComputationGRPOStage 1 Supervised Fine-tuning W arm-Up\nStage 2  Reinforcement Learning ExplorationFigure 2: Method Overview .Top left : Training sample with input, target output, and generated navigation\ninstruction’s action interpretation. Bottom left : Action interpreter training using LLaMA-3-8B-Instruct to simulate\nVI users’ navigation responses. Right : Post-training procedures for VLMs processing with LaF-GRPO using\nmultiple reward functions (format, text generation, and LLM-as-Follower reward).\nlocation x(i)\nloc∈R3and rotation x(i)\nrot∈R3within\na global map coordinate system, as well as the\nnext target waypoint pi+1∈P. Based on these\ninputs, the VLM πgenerates a sequence of tokens\ny=y(i)= (y(i)\n1, y(i)\n2, . . . , y(i)\nt)of token length\nt. The generated instruction ymight also include\ndetails about the current surroundings captured in\nx(i)\nimageand any necessary safety alerts:\nyj∼πθ(y(i)\nj|x(i)\nimage, x(i)\nloc, x(i)\nrot, pi+1, y(i)\n<j)(1)\nwhere θdenotes the adjustable model parameters.\n3.2 The LaF-GRPO Framework\nWe then discuss our LaF-GRPO framework to\ntackle the NIG-VI task. It aims to address the chal-\nlenges of ensuring that navigation instructions are\npeople-centered, practically usable by the VI users ,\nwhile mitigating the need for costly real-world data\ncollection with VI participants . The overview of\nLaF-GRPO is illustrated in Figure 2, where the\nframework comprises two key components: (1)\nan LLM (without a visual encoder to “see”) that\nsimulates VI users’ responses to navigation instruc-\ntions by interpreting how these users would act\nupon hearing the instructions, and (2) a VLM post-\ntraining procedure that generates these instructions\nwith (1)’s feedback. LaF-GRPO first employs an\naction interpreter to produce structured interpreta-\ntions of potential user actions, which are then used\nto compute the LLM-as-Follower reward. This re-\nward signal subsequently guides the VLM training\nprocess of navigation instruction generation thatis more likely to be effectively followed by VI users\nin real-world navigation scenarios.\nAction Interpreter. The action interpreter mod-\nels VI user responses to navigation instructions. We\nfine-tune an LLM ρwith parameters φto predict\npotential user actions from verbal guidance. Given\nVLM-generated instruction tokens y, it produces\na structured action interpretation A. Formally, we\ndefine Aas a structured dictionary containing: (1)\na‘move’ action with associated ‘direction’ (indi-\ncated using clock positions) and ‘distance’ parame-\nters, and (2) a ‘detailed_hazard_alert’ boolean flag\nthat indicates whether the user perceives warnings\nabout nearby obstacles, as illustrated in Figure 2\nLeft. To train such an action interpreter, we utilize\ntraining samples generated based on the prompt\ntemplate detailed in Table 8 in Appendix E.\nNavigation Instruction Generator. For VI guid-\nance, we use a pre-trained VLM πwith parameters\nθfor in-situ navigation instruction generation. The\ntraining of this generator involves two stages: Su-\npervised Fine-tuning (SFT) and Group Relative Pol-\nicy Optimization (GRPO). For SFT training details,\nplease refer to Appendix B. For the GRPO, specif-\nically, we propose LaF-GRPO, which is based on\nthe standard GRPO (see below) reward function\nyet incorporates a novel LLM-as-Follower reward.\nGRPO. The training process of GRPO aims to\noptimize the policy πθby maximizing the objective\nfunction JGRPO(θ). For a given query q, GRPO\nfirst samples a batch of Goutputs {o1, o2, . . . , o G}\nusing an older version of the policy, πθold. The\ntraining process of GRPO aims to optimize the\n3\n--- Page 4 ---\npolicy πθby maximizing the objective function:\nJGRPO (θ) =Eq,{oi}∼πθold\"\n1\nGGX\ni=1Li−βDKL(πθ||πref)#\n(2)\nHere, the term Lirepresents the clipped surrogate\nobjective used in PPO (Schulman et al., 2017):\nLi= min( wiAi,clip(wi,1−ϵ,1 +ϵ)Ai)(3)\nwhere wi=πθ(oi|q)\nπθold(oi|q)is the importance sam-\npling ratio, Aiis the estimated advantage for the\noutput oi, based on relative rewards of the out-\nputs inside each group only, calculated as Ai=\nri−mean ({r1,r2,...,rG})\nstd({r1,r2,...,rG}), and ϵis a clipping hyperpa-\nrameter. The second term, −βDKL(πθ||πref), regu-\nlarizes the policy by penalizing divergence from a\nreference policy πrefwith coefficient β. This regu-\nlarization stabilizes training by keeping the model\nclose to the original effective policy, preventing it\nfrom losing previously learned capabilities.\n3.3 LaF-GRPO Reward Functions\nGRPO leverages verifiable rewards to simplify the\nreward modeling process. To integrate LLM’s feed-\nback to enable smooth, people-centered language\nguidance in NIG-VI, LaF-GRPO utilizes three re-\nward functions as follows. The reward calculation\nalgorithm is detailed in Algorithm 1 (Appendix C).\nFormat Reward. To encourage controllable\ngeneration for easy training, we adopt this\nbinary reward ( rformat∈ {0,1}) that evaluates\nstructural compliance with the expected response\nformat. Here, the reward would equal 1 if\nthe output follows the required format pattern\n‘<think>.*?</think>\\n<answer>.*?</answer>’\nin sequence, and 0 otherwise.\nText Generation Reward. We adopt METEOR\nto verify text generation rewards to align the gener-\nation to the ground-truth style. Here, METEOR is\nselected based on its evaluation of semantic overlap,\nincorporating synonymy and stemming to provide\na nuanced, human-correlated assessment.\nLLM-as-Follower Reward. To incorporate\nLLM’s feedback for navigation, we propose the\nreward rLaF to assess the navigational quality\nof generated instructions by comparing their\ninterpreted actions (move direction, move distance,\nand alert flag) against those of a reference. Our\nintuition is that spatial factors, such as directional\naccuracy ( adir) and movement distance precision\nGenerate navigation instructions for\nvisually impaired...\n- Current location: \n{'location': {'x': 115.43, 'y': -28.51},\n'rotation_yaw': -91.57}\n- Target goal (next step): \n{'location': {'x': 115.37, 'y': -29.01},\n'rotation_yaw': -93.03} ...Output:\nTurn slightly right toward 1 o’clock\nand walk half a meter. Buildings on\nboth sides, brush along the right\nedge—use your cane to check for\npoles and stay right to avoid the left\nroad; pause if footsteps approach.\nTask Description:  Generate\nconcise, real-time\nnavigation guidance for\nvisually impaired users by\ncombining outputs from 2 or\n3 AI models.\nRequirements:\n1. Content Structure ...\n2. Style ...\n3. Format ...Input:\nMove forward approximately 0.5 meters in the direction of 0 o'clock,\nensuring you stay on the sidewalk. There is a building on your left and\nvegetation on your right. The path ahead is clear, but be mindful of\npotential dynamic objects like pedestrians. Keep close to the right side to\navoid the road, which is to your far left. There may be obstacles such as\npoles or signs near the walkway; proceed with caution.\nProceed straight ahead for approximately 0.5 meters. You are on a wide\nsidewalk with tall office buildings to your left. The road curves to your left.\nOn your right, there's a low stone wall bordering a grassy area with trees.\nAhead, you'll encounter some street vendors with colorful umbrellas. Be\ncautious of any uneven surfaces on the sidewalk and stay alert for other\npedestrians or obstacles. The path is clear, but remain aware of your\nsurroundings as you move forward.\nSlightly turn right and move forward for 0.5 meters. There are\nbuildings on your left and right. Be cautious of dynamic objects and\npoles on the sidewalk.GPT-4o\nClaude-3.5\nGemini-2-\nthinkingDeepseek-R1Figure 3: DeepSeek-R1 refines initial predictions from\nGPT-4o, Claude-3.5, and Gemini-2 into coherent in-\nstructions, using Vision-R1’s modality bridging method\n(Huang et al., 2025). These are then reviewed and mod-\nified by human annotators for quality and accuracy.\n(adist), play a direct and critical role in determining\nnavigation success. In contrast, safety alert flags\n(aalert) serve as supplementary support for VI nav-\nigation by indicating potential hazards, though they\nare not primary determinants of success (Giudice\nand Legge, 2008; Younis et al., 2019). Considering\nthese, the reward is computed as:\nrLaF=wdirδ(adir, aref\ndir) +wdistδ(adist, aref\ndist)\n+walertδ(aalert, aref\nalert)\n(4)\nδ(·)denotes exact match comparison. To prioritize\nspatial factors, weighting coefficients are set such\nthatwdir+wdist> w alert. This, in the end, yields\nanrLaFscore ranging from 0 to 1.\n4 Benchmark: NIG4VI\nWe introduce the NIG4VI benchmark to address\nthe scarcity of benchmark resources in this field.\nInspired by the UrbanWalk, NIG4VI utilizes the\nopen-sourced CARLA Simulator (Dosovitskiy\net al., 2017) to collect samples from a diverse range\nof scenarios. These scenarios span remote rural-\nlike settings (e.g., Town01) and complex metropoli-\ntan areas (e.g., Town10), and encompass vari-\nous weather conditions, such as foggy and sunny\nweather. Pedestrian trajectories are generated using\nA* route planning algorithm, with precise geospa-\ntial coordinates, orientation, frontal-view images,\nand semantic segmentation images being recorded\nat each step. NIG4VI offers two main advantages:\n(1) its use of a realistic coordinate system facili-\ntates easier transfer to real-world GPS applications,\nand (2) it enables the cost-effective generation of\naccurate and extensive data. Table 1 demonstrates\nNIG4VI’s advantages compared to other datasets.\n4\n--- Page 5 ---\nBenchmark Level # Samples VIA NIG Spatial Acc. Open-ended Open-sourced\nR2R (Anderson et al., 2018) High 21k ✗! ✗ ! !\nREVERIE (Qi et al., 2020) High 10k / 6k ✗! ✗ ! !\nUrbanWalk (Huang et al., 2022) Detailed 2.6k ! ! ! ✗ ✗\nMerchant et al. (2024) Detailed 48 ! ! ✗ ! ✗\nVIALM (Zhao et al., 2024) Detailed 200 ! ✗ ✗ ! !\nWAD (Yuan et al., 2025) Detailed 12k / 120k ! ! ✗ ! !\nNIG4VI (Ours) Detailed 3k / 24k ! ! ! ! !\n- w/o pre-calculation Detailed 1.5k / 12k ! ! ! ! !\n- with pre-calculation Detailed 1.5k / 12k ! ! ! ! !\nTable 1: Comparison of the NIG4VI dataset with existing benchmarks. Unlike WAD, NIG4VI employs geospatial\ncoordinates for high spatial accuracy evaluation, and surpasses UrbanWalk through open-ended instruction genera-\ntion that benchmarks VLMs’ capacity to produce natural navigation guidance.\n4.1 Dataset Construction\nEach question’s input includes the user’s current lo-\ncation/rotation, the next step’s location/orientation,\nand visual scene data. The complete prompt tem-\nplate used to structure these inputs is detailed in\nTable 6 of Appendix E. The synthesis of the output\nis a multi-stage process involving both advanced\nreasoning models and human annotation, as illus-\ntrated in Figure 3. Initially, several leading VLMs,\nspecifically GPT-4o, Claude-3.5, and Gemini-2,\ngenerate predictions. Following a modality bridg-\ning approach, similar to that employed in Vision-\nR1 (Huang et al., 2025), these outputs are pro-\ncessed through DeepSeek-R1 to enhance blindness-\noriented spatial guidance and navigability. The\nspecific prompt guiding DeepSeek-R1 in this re-\nfinement task is detailed in Table 7.\nCrucially, all instructions undergo rigorous hu-\nman verification. This task is carried out by two\nannotators, both proficient in English and hold-\ning at least an undergraduate-level education, fol-\nlowing a similar practice in (Zhao et al., 2024).\nThe verification involves a two-stage process: first,\none annotator performs initial content adjustments,\nadhering to task requirements. Subsequently, the\nsecond annotator reviews and verifies this work.\nThroughout this entire process, both annotators fo-\ncus on ensuring: (1) elimination of visual refer-\nences (e.g., color-based descriptors), (2) validation\nof non-visual landmarks, and (3) confirmation of\nmetric precision for mobility-critical parameters.\n4.2 Dataset Statistics\nTable 2 details the statistics of the dataset, which\ncomprises routes collected from six distinct towns\nwithin the CARLA simulator. Further details on\nroute sampling in the CARLA simulator are inTown Routes Avg dist. Avg steps # Samples\nTown01 25 111.41 401 1,500 / 613\nTown02 26 99.38 327 2,579\nTown03 25 128.23 409 2,260\nTown04 26 131.49 337 2,316\nTown05 25 107.81 288 1,935\nTown10 30 102.74 361 2,133\nAvg. 26.2 113.51 353.8 2,222.7\nTable 2: Statistics for sample routes. Dataset: 1,500\nTown01 samples for training; the remaining 613\n(Town01) and all other town samples for testing. ‘Avg\ndist. ’ : average Euclidean distance (route start to end).\n‘Avg steps’ : average steps per route. ‘# Samples’ : dedu-\nplicated step-level (image, question) samples per town.\nAppendix D. On average, each town contributes\napproximately 26.2 navigation routes. The average\nEuclidean distance between the start and end points\nof these routes is 113.51 units, with an average of\n353.8 steps required for completion. After dedu-\nplication, the dataset yielded an average of 2,222.7\nstep-level (image, question) samples per town. It is\npartitioned into a training set of 1,500 samples from\nTown01 and a test set. The test set comprises the\nremaining 613 intra-town samples from Town01,\nalong with all inter-town samples from Town02\n(2,579), Town03 (2,260), Town04 (2,316), Town05\n(1,935), and Town10 (2,133). Each data sample\nis available in two versions: ‘with pre-calculation’\nand‘without pre-calculation’ . The ‘without pre-\ncalculation’ version requires the VLM to indepen-\ndently calculate navigational parameters (e.g., dis-\ntance, direction), presenting a greater challenge in\nguidance generation. Conversely, the ‘with pre-\ncalculation’ version provides the VLM with basic\nmathematical movement information. The VLM\nmust then validate this data and assess the surround-\nings to generate the final navigation instruction.\n5\n--- Page 6 ---\nPre-Cal. Paradigm ModelIntra-town ( N= 613) Inter-town ( N=11,223)\nBLEU ↑ROUGE ↑METEOR ↑SPICE ↑BLEU ↑ROUGE ↑METEOR ↑SPICE ↑\nNoZero-ShotDeepSeek-VL-7B 2.179 0.152 0.182 0.116 2.223 0.157 0.196 0.112\nMiniCPM-o-8B 2.009 0.145 0.234 0.131 1.969 0.142 0.233 0.129\nIntern-VL-8B 1.448 0.150 0.215 0.126 1.517 0.149 0.216 0.120\nQwen-VL-7B 3.204 0.202 0.211 0.166 3.128 0.194 0.210 0.157\nGPT-4o 1.748 0.169 0.249 0.149 1.617 0.165 0.249 0.142\nClaude-3.5 2.803 0.216 0.304 0.211 2.749 0.211 0.301 0.202\nGemin-2 4.105 0.236 0.232 0.232 4.422 0.252 0.238 0.236\nZero-\n(LaF-GRPO)Qwen-VL-3B 3.292 0.230 0.248 0.230 3.972 0.255 0.259 0.244\nQwen-VL-7B 3.272 0.234 0.256 0.222 3.566 0.252 0.260 0.227\nSFTQwen-VL-3B 9.099 0.282 0.496 0.274 8.949 0.284 0.500 0.276\nQwen-VL-7B 9.937 0.291 0.518 0.275 9.709 0.294 0.526 0.281\nSFT+\n(LaF-GRPO)Qwen-VL-3B 10.921 0.323 0.528 0.274 10.157 0.309 0.527 0.276\nQwen-VL-7B 10.037 0.284 0.545 0.283 9.002 0.276 0.535 0.278\nYesZero-ShotDeepSeek-VL-7B 2.517 0.170 0.224 0.161 2.600 0.173 0.237 0.161\nMiniCPM-o-8B 2.349 0.166 0.210 0.136 2.517 0.177 0.220 0.144\nIntern-VL-8B 1.496 0.132 0.233 0.133 1.517 0.134 0.238 0.132\nQwen-VL-7B 2.903 0.188 0.231 0.178 3.080 0.194 0.243 0.180\nGPT-4o 2.766 0.204 0.302 0.198 2.967 0.213 0.323 0.211\nClaude-3.5 4.124 0.236 0.349 0.257 3.400 0.214 0.326 0.224\nGemin-2 5.132 0.252 0.266 0.269 6.144 0.276 0.283 0.284\nZero-\n(LaF-GRPO)Qwen-VL-3B 3.798 0.249 0.280 0.261 4.584 0.271 0.288 0.274\nQwen-VL-7B 3.678 0.241 0.281 0.229 4.284 0.262 0.286 0.230\nSFTQwen-VL-3B 9.923 0.308 0.512 0.280 10.724 0.318 0.519 0.280\nQwen-VL-7B 9.639 0.270 0.521 0.283 9.710 0.272 0.524 0.287\nSFT+\n(LaF-GRPO)Qwen-VL-3B 11.727 0.342 0.541 0.286 10.813 0.333 0.535 0.279\nQwen-VL-7B 10.499 0.285 0.556 0.292 9.232 0.275 0.542 0.288\nTable 3: Evaluation results on the NIG4VI dataset across Intra-town and Inter-town subsets. Gray cells indicate\nresults pertaining to Qwen2.5-VL models. Blue values highlight the best performing Qwen2.5-VL model within\nthe Zero-Shot and Zero-(LaF-GRPO) categories. Bold values represent the highest score for each metric under a\nspecific setting (with / without pre-calculation), while underlined values indicate the second-highest score.\n5 Experimental Settings\nDataset. Experiments utilized the NIG4VI\ndataset, comprising Intra-town ( N= 613 ) and\nInter-town ( N= 11 ,223) test subsets, under\n’with/without pre-calculation’ conditions.\nModels. Diverse VLMs were evaluated, falling\ninto two main groups. The first group includes\nremote models: GPT-4o (OpenAI, 2024b), Claude-\n3-5-sonnet-20240620 (Anthropic, 2024), and Ge\nmini-2.0-flash-thinking-exp-01-21 (Google\nDeepMind, 2024). The second group comprises\nsmaller, locally runnable VLMs: DeepSeek-VL-7B\n(Lu et al., 2024), MiniCPM-o-2.6-8B (Yao et al.,\n2024), Intern-VL-2.5-8B (Chen et al., 2024), and\nQwen2.5-VL-3B/7B (Bai et al., 2025).\nEvaluation Metrics. Following previous studies\nin NIG (Huang et al., 2022; Fan et al., 2024; Kong\net al., 2024), model performance was evaluated us-\ning a suite of widely adopted metrics: BLEU (Pap-\nineni et al., 2002), ROUGE (Lin, 2004), METEOR(Banerjee and Lavie, 2005), and SPICE (Ander-\nson et al., 2016). For each of these metrics, higher\nscores denote superior performance.\nBaselines. We compare LaF-GRPO against two\nprimary baseline methods: (1) Zero-shot : This in-\nvolves models directly on NIG4VI without prior\ntask-specific fine-tuning. (2) Supervised Fine-\ntuning (SFT) : Models are fine-tuned to generate\ninstructions from the input. Furthermore, we im-\nplement two variants of LaF-GRPO to understand\nits different operational modes: (a) Zero-(LaF-\nGRPO) : LaF-GRPO is applied directly to the base\nmodel without SFT. (b) SFT+(LaF-GRPO) : LaF-\nGRPO is applied to models that have first under-\ngone SFT. Both LaF-GRPO variants utilize the pro-\nposed LLM-as-Follower reward mechanism.\nImplementation Deatails. LaF-GRPO training\nutilized a single NVIDIA H20 GPU (96 GB of\nmemory). This hardware supports loading an 8B-\nparam LLM ( LLaMA-3-8B ) and a 3B/7B-param\nQwen2.5-VL model for LoRA (Hu et al., 2022)\n6\n--- Page 7 ---\nPre-Cal.Reward Types Intra-town ( N= 613) Inter-town ( N= 11,223)\nFormat Meteor LLM BLEU ↑ROUGE ↑METEOR ↑SPICE ↑BLEU ↑ROUGE ↑METEOR ↑SPICE ↑\nNo✓ 10.251 0.318 0.524 0.278 9.401 0.304 0.523 0.279\n✓ ✓ 10.912 0.317 0.525 0.279 10.076 0.306 0.521 0.279\n✓ ✓ ✓ 10.921 0.323 0.528 0.274 10.157 0.309 0.527 0.276\nYes✓ 11.269 0.337 0.538 0.292 10.217 0.328 0.530 0.282\n✓ ✓ 11.602 0.339 0.539 0.284 10.753 0.331 0.531 0.280\n✓ ✓ ✓ 11.727 0.342 0.541 0.286 10.813 0.333 0.535 0.280\nTable 4: Ablation study results for the Qwen2.5-VL-3B model on the NIG4VI dataset with different reward\nfunctions . Bold values represent the highest score for each metric under its specific pre-calculation condition.\nPre-Cal. ModelIntra-town ( N= 613) Inter-town ( N= 11,223)\nBLEU ↑ROUGE ↑METEOR ↑SPICE ↑BLEU ↑Rouge ↑METEOR ↑SPICE ↑\nNo7B-format+meteor+LLM ( 1k) 9.401 0.283 0.529 0.274 8.963 0.281 0.530 0.275\n7B-format+meteor+LLM ( 2k) 9.657 0.280 0.539 0.276 9.001 0.276 0.535 0.274\n7B-format+meteor+LLM ( 3k)10.037 0.284 0.545 0.283 9.002 0.276 0.535 0.278\nYes7B-format+meteor+LLM ( 1k) 10.265 0.279 0.543 0.286 9.463 0.271 0.540 0.285\n7B-format+meteor+LLM ( 2k) 10.136 0.284 0.550 0.292 9.245 0.276 0.541 0.284\n7B-format+meteor+LLM ( 3k)10.499 0.285 0.556 0.292 9.232 0.275 0.542 0.288\nTable 5: Ablation study results for the Qwen2.5-VL-7B model on the NIG4VI dataset with varying training sample\nsizes . Bold values represent the highest score for each metric under its specific pre-calculation condition.\nfine-tuning. The reward weights were configured\nas(wdir, wdist, walert) = (0 .4,0.4,0.2)based on\nanalysis of navigation failure factors, prioritizing\nspatial parameters over contextual alerts. Training\non3ksamples took approximately 15hours, with\nthe key hyperparameter group size Gset to 8.\n6 Results and Discussions\n6.1 Main Results\nWe present Table 3, which summarizes model\nperformance on NIG4VI, categorized by pre-\ncalculation and training paradigms, and evaluated\non intra-town and inter-town subsets. Compar-\ning LaF-GRPO with the baselines reveals: (1)\nZero-Shot vs. Zero-(LaF-GRPO): Zero-(LaF-\nGRPO) significantly enhances the Zero-Shot per-\nformance of VLMs, validating the effectiveness\nof LaF-GRPO. While the Zero-(LaF-GRPO) re-\nsults suggest that increased model size (from 3B\nto 7B) does not necessarily guarantee improved\nperformance across all metrics, it is noteworthy\nthat for METEOR evaluations, specifically in intra-\ntown scenarios, the 7B model achieved the high-\nest scores (i.e., 0.256and0.281). This outcome\nmay be attributable to the use of METEOR as a\ntext generation reward during training and to the\npotentially more refined tuning applied to the 7B\nmodels. (2) SFT & SFT+(LaF-GRPO): SFT and\nSFT+(LaF-GRPO) yield significantly superior per-\nformance compared to Zero-Shot and Zero-(LaF-GRPO) models across all metrics and subsets, af-\nfirming the efficacy of fine-tuning. The SFT+(LaF-\nGRPO) approach further enhances performance be-\nyond SFT. Moreover, under the SFT+(SFT-GRPO)\nparadigm, Qwen-VL-3B consistently achieves the\nhighest BLEU and ROUGE scores, while Qwen-\nVL-7B excels in METEOR and SPICE. This perfor-\nmance pattern is observed for both intra-town and\ninter-town subsets and holds true regardless of pre-\ncalculation. This may be attributable to 7B mod-\nels demonstrating enhanced linguistic diversity in\ntheir outputs relative to 3B models. (3) Additional\nObservations: Scores are generally higher with\npre-calculation than without, likely because it re-\nduces the mathematical computation difficulty for\nthe models. While both intra-town and inter-town\nresults demonstrate consistent trends, intra-town\nevaluations typically yield higher scores. This is\nunderstandable as the intra-town test set comprises\nsamples from the same town environments as the\ntraining set, leading to a closer data distribution.\n6.2 Ablation Study\nReward Types Table 4 presents an ablation study\ninvestigating the impact of different reward types\nduring SFT+(LaF-GRPO) training with the Qwen-\nVL-3B model. LaF-GRPO, incorporating the LLM-\nas-Follower reward, consistently achieves the high-\nest BLEU, ROUGE, and METEOR scores. This\ntrend holds true across both intra-town and inter-\ntown evaluations, with or without pre-calculation.\n7\n--- Page 8 ---\n<think> ...</think> <answer> Turn\nright toward 130 degrees and\nwalk two small steps. Stay near\nthe brick wall on your right; road\nedge left—use your cane to\ncheck for poles. Pause if engines\napproach. </answer><think> ...</think> <answer> Turn\nslightly right toward 1 o’clock\nand walk two small steps . Keep\nyour cane near the pole on the\nleft and the fence with bushes\nto the right; stay alert for\nnearby movement. </answer><think> ...</think> <answer> Turn\nslightly right toward 167\ndegrees and  walk two small\nsteps.  Buildings left, trees right\n—use your cane near railings;\nlisten for traffic. </answer><think>.. .</think> <answer> Step\nforward 0.5 meters. Buildings\nleft, bushes right; use your cane\nnear the left railing and listen\nfor traffic. Pause if you hear\nengines. </answer>Turn slightly right toward 132\ndegrees and walk half a meter.\nKeep your cane near the road\nedge on the left; buildings close\non both sides. Pause if you hear\nengines or bells—use your cane\nto check for poles ahead.Turn slightly right toward 150\ndegrees. Keep your cane near\nthe building wall on the left;\nvegetation and a fence line the\nright. Pause if you hear\nfootsteps ahead—use your cane\nto check for uneven pavement.Turn slightly right toward 167\ndegrees. Buildings on left, trees\nalong right; use your cane to\ncheck for uneven pavement.\nPause if you hear engines or\nbells.Turn slightly right toward 175\ndegrees. Buildings on left,\nbushes and benches along right\nsidewalk; road sounds from\nright. Use your cane to check\nfor benches ahead—pause if\nsurfaces feel uneven.Front-\nView\nSFT\nSFT+\nLaF-GRPOFigure 4: A comparative case study of navigational guidance provided by SFT and SFT+(LaF-GRPO) methods\nacross successive steps. Findings indicate that SFT+(LaF-GRPO) (Ours) generates instructions with greater\nlinguistic variety and more effectively incorporates o’clock directions and specific travel distances.\nThis underscores the significant benefit of the LLM-\nas-Follower reward signal for the NIG4VI task.\nLaF-GRPO vs. Standard GRPO We conducted\nan additional experiment on the inter-town sub-\nset. It reveals that LaF-GRPO-trained models\ndemonstrate superior navigational accuracy ( 68.1%\nvs.67.3%) and their instructions were more fre-\nquently selected by GPT-4o for helpfulness and\nclarity ( 58.3%vs.41.7%of cases). We identify\ntwo primary advantages of LaF-GRPO over stan-\ndard GRPO, which utilizes only format and text\ngeneration rewards: (1) Navigational Accuracy:\nLaF-GRPO provides more precise movement and\norientation accuracy. (2) Instruction Clarity: The\ninclusion of an action interpreter requirement en-\ncourages VLMs to produce instructions compre-\nhensible to the follower and thus clearer and more\nstructured. Details are provided in Appendix F.\nTraining Sample Sizes Table 5 presents an abla-\ntion study on the Qwen2.5-VL-7B model trained\nwith SFT+(LaF-GRPO), illustrating the effect of\nvarying training sample sizes ( 1k,2k, and 3k). For\nthe two metrics METEOR and SPICE, which are\noften considered more comprehensive in text gen-\neration, increasing the volume of training data gen-\nerally leads to enhanced performance. Across the\nmajority of evaluated conditions (with/without pre-\ncalculation), scaling up to 3ksamples typically\nyields the optimal or near-optimal scores. Never-\ntheless, for the 7B models in the inter-town setting,\ntraining with 2ksamples also achieves comparable\nMETEOR scores (i.e., 0.535and0.541), indicating\ntraining data efficiency at this sample size.6.3 Case Study\nFigure 4 provides a qualitative comparison of our\nSFT+(LaF-GRPO) method against the SFT base-\nline. Notably, SFT+(LaF-GRPO) generates instruc-\ntions with greater linguistic variety and more in-\ntuitive directional cues. For instance, in Step 2,\nSFT+(LaF-GRPO) employs an o’clock direction\n(“Turn slightly right toward 1 o’clock\" ) and a relat-\nable distance ( \"two small steps\" ), contrasting with\nSFT’s numerical bearing ( \"150 degrees\" ). This ap-\nproach can yield guidance that is more naturally\nunderstood by VI users. Furthermore, SFT+(LaF-\nGRPO), leveraging its internal reasoning process\n(i.e. the <think>...</think> blocks), frequently\nincorporates more environmental details and safety\nconsiderations. For example, its instruction for\nStep 4 ( \"Step forward 0.5 meters; ...use your cane\nnear the left railing and listen for traffic\" ) also\nemphasizes immediate safety interactions. Full\ninstruction texts for Zero-(LaF-GRPO), SFT, and\nSFT+(LaF-GRPO) are available in Appendix F.\n7 Conclusion\nThis study addresses navigation instruction gen-\neration for the visually impaired individuals. We\nconstructed the NIG4VI benchmark. Following\nthis, we developed LaF-GRPO, a training paradigm\nfor VLMs that incorporates an LLM-as-Follower\nreward. Experimental evaluations established LaF-\nGRPO’s superiority over baselines and standard\nGRPO, with future qualitative analysis confirming\nthe generated instructions’ real-world practicality.\n8\n--- Page 9 ---\nLimitations\nThe main limitations of this work include two pri-\nmary aspects: the data source for the benchmark\nNIG4VI and the computational demands of our pro-\nposed training methodology, LaF-GRPO. (1) The\nfirst limitation pertains to the benchmark NIG4VI,\nas its data is collected within a simulated environ-\nment. A domain gap inevitably exists between such\nsimulated conditions and the multifaceted complex-\nities of the real world, as simulators may not fully\ncapture the entire spectrum of real-world environ-\nmental dynamics. Nevertheless, simulators facil-\nitate large-scale data acquisition, enable precise\nand automated annotation, and provide a crucial\nfoundation for initial model development and sys-\ntematic evaluation. Future work will focus on incor-\nporating more diverse, real-world data and explor-\ning sim-to-real transfer techniques to mitigate this\ngap. (2) Second, training the LaF-GRPO model\nis resource-intensive, requiring substantial compu-\ntational power and considerable time. However,\nthis high cost is primarily a one-time investment\nand does not significantly affect deployment. Once\ntrained, the LaF-GRPO model operates efficiently\nduring inference, ensuring practical, responsive\nreal-time navigation instructions for end-users.\nEthical Considerations\nOur work has been conducted with a strong com-\nmitment to ethical practices and transparency\nthroughout the development process: (1) Licens-\ning: Data collection was performed using the\nCARLA Simulator, which is distributed under an\nMIT license. Our use of CARLA aligns with its\noriginal intended purpose and adheres to its licens-\ning terms. Our research incorporates publicly avail-\nable open-source VLMs, specifically the Qwen2.5-\nVL-3B and Qwen2.5-VL-7B models. These mod-\nels are licensed under Apache 2.0. This permissive\nlicensing allows code modification, fostering in-\nnovation and broader application. (2) Instruction\nBiases and Safety : Our method generates naviga-\ntion instructions. A core design principle of our\nNIG-VI system is safety awareness. This focus on\nsafety and bias mitigation supports the responsible\napplication of AI in navigation technologies.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson,\nand Stephen Gould. 2016. Spice: Semanticpropositional image caption evaluation. Preprint ,\narXiv:1607.08822.\nPeter Anderson, Qi Wu, and et al. 2018. Vision-and-\nlanguage navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In\nCVPR , pages 3674–3683. Computer Vision Foun-\ndation / IEEE Computer Society.\nAnthropic. 2024. The claude 3 model family: Opus,\nsonnet, haiku.\nAnthropic. 2024. Claude 3.5 sonnet. https://www.\nanthropic.com/news/claude-3-5-sonnet . Ac-\ncessed: 2024-11-26.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-\nbin Ge, Sibo Song, Kai Dang, Peng Wang, Shi-\njie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu,\nMingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei\nWang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others.\n2025. Qwen2.5-vl technical report. arXiv preprint\narXiv:2502.13923 .\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nZhe Chen, Jiannan Wu, and et al. 2024. Internvl: Scal-\ning up vision foundation models and aligning for\ngeneric visual-linguistic tasks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 24185–24198.\nWenliang Dai, Junnan Li, and et al. 2023. Instruct-\nblip: Towards general-purpose vision-language mod-\nels with instruction tuning. In NeurIPS .\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning.\nPreprint , arXiv:2501.12948.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, An-\ntonio Lopez, and Vladlen Koltun. 2017. CARLA:\nAn open urban driving simulator. In Proceedings of\nthe 1st Annual Conference on Robot Learning , pages\n1–16.\nSheng Fan, Rui Liu, Wenguan Wang, and Yi Yang. 2024.\nNavigation instruction generation with BEV percep-\ntion and large language models. In ECCV , volume\n9\n--- Page 10 ---\n15080 of Lecture Notes in Computer Science , pages\n368–387. Springer.\nNicholas A Giudice and Gordon E Legge. 2008. Blind\nnavigation and the role of technology. The engineer-\ning handbook of smart technology for aging, disabil-\nity, and independence , pages 479–500.\nGoogle DeepMind. 2024. Introducing gemini 2.0:\nOur new ai model for the agentic era. https:\n//blog.google/technology/google-deepmind/\ngoogle-gemini-ai-update-december-2024 .\nAccessed: 2025-01-21. Published: December 2024.\nMuraleekrishna Gopinathan, Martin Masek, Jumana\nAbu-Khalaf, and David Suter. 2024. Spatially-aware\nspeaker for vision-and-language navigation instruc-\ntion generation. In ACL, pages 13601–13614. Asso-\nciation for Computational Linguistics.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations .\nWenxuan Huang, Bohan Jia, and et al. 2025. Vision-\nr1: Incentivizing reasoning capability in multimodal\nlarge language models. Preprint , arXiv:2503.06749.\nZanming Huang, Zhongkai Shangguan, Jimuyang\nZhang, Gilad Bar, Matthew Boyd, and Eshed Ohn-\nBar. 2022. ASSISTER: assistive navigation via con-\nditional instruction generation. In ECCV , volume\n13696 of Lecture Notes in Computer Science , pages\n271–289. Springer.\nBo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu,\nand Xinggang Wang. 2025. Alphadrive: Unleash-\ning the power of vlms in autonomous driving via\nreinforcement learning and reasoning. Preprint ,\narXiv:2503.07608.\nXianghao Kong, Jinyu Chen, Wenguan Wang, Hang\nSu, Xiaolin Hu, Yi Yang, and Si Liu. 2024. Con-\ntrollable navigation instruction generation with chain\nof thought prompting. In ECCV , volume 15087 of\nLecture Notes in Computer Science , pages 37–54.\nSpringer.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. In NeurIPS .\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-\noshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu,\nZhenda Xie, and Chong Ruan. 2024. Deepseek-vl:\nTowards real-world vision-language understanding.\nPreprint , arXiv:2403.05525.Zain Merchant, Abrar Anwar, Emily Wang, Souti\nChattopadhyay, and Jesse Thomason. 2024. Gen-\nerating contextually-relevant navigation instruc-\ntions for blind and low vision people. Preprint ,\narXiv:2407.08219.\nOpenAI. 2024a. Gpt-4 technical report. Preprint ,\narXiv:2303.08774.\nOpenAI. 2024b. Gpt-4o system card. Preprint ,\narXiv:2410.21276.\nLong Ouyang, Jeffrey Wu, and et al. 2022. Training\nlanguage models to follow instructions with human\nfeedback. In NeurIPS .\nJiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan\nZhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang,\nand Daniel Rueckert. 2025. Medvlm-r1: Incentiviz-\ning medical reasoning capability of vision-language\nmodels (vlms) via reinforcement learning. Preprint ,\narXiv:2502.19634.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA , pages 311–318. ACL.\nYuankai Qi, Qi Wu, and et al. 2020. REVERIE: re-\nmote embodied visual referring expression in real\nindoor environments. In CVPR , pages 9979–9988.\nComputer Vision Foundation / IEEE.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Prox-\nimal policy optimization algorithms. Preprint ,\narXiv:1707.06347.\nHaozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang,\nYibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang,\nKangjia Zhao, Qianqian Zhang, Ruochen Xu, and\nTiancheng Zhao. 2025. Vlm-r1: A stable and general-\nizable r1-style large vision-language model. Preprint ,\narXiv:2504.07615.\nGemini Team. 2024. Gemini 1.5: Unlocking multi-\nmodal understanding across millions of tokens of\ncontext. Preprint , arXiv:2403.05530.\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo\nCui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin\nZhao, Zhihui He, and 1 others. 2024. Minicpm-v:\nA gpt-4v level mllm on your phone. arXiv preprint\narXiv:2408.01800 .\nOla Younis, Waleed Al-Nuaimy, Fiona Rowe, and Mo-\nhammad H Alomari. 2019. A smart context-aware\nhazard attention system to help people with periph-\neral vision loss. Sensors , 19(7):1630.\nZhiqiang Yuan, Ting Zhang, Ying Deng, Jiapei Zhang,\nYeshuang Zhu, Zexi Jia, Jie Zhou, and Jinchao\nZhang. 2025. Walkvlm:aid visually impaired peo-\nple walking by vision language model. Preprint ,\narXiv:2412.20903.\n10\n--- Page 11 ---\nYi Zhao, Yilin Zhang, Rong Xiang, Jing Li, and Hillm-\ning Li. 2024. Vialm: A survey and benchmark\nof visually impaired assistance with large models.\nPreprint , arXiv:2402.01735.\nAppendix\nA Related Work Details\nGeneral NIG for Embodied Agents Prior re-\nsearch on navigation instruction generation (NIG)\nfor embodied agents has predominantly focused\non advanced visual processing techniques while\ngenerating trajectory-level instructions. BEVIn-\nstructor (Fan et al., 2024) employs a Bird’s-Eye\nView encoder and iterative refinement for clearer\ninstructions. SAS (Gopinathan et al., 2024) uses\nstructural and semantic knowledge with adversar-\nial reward learning to improve instruction qual-\nity. C-Instructor (Kong et al., 2024) focuses on\nstyle-controlled instruction generation and adopts a\nchain-of-thought with landmarks mechanism. Our\napproach differs in two significant ways: (1) while\nexisting methods emphasize visual representation\ntechniques, our LaF-GRPO approach prioritizes\nnavigation feedback for VLM fine-tuning; and (2)\ntraditional NIG systems generate trajectory-level\ninstructions for complete routes, whereas NIG-VI\nscenario provides step-level in-situ instructions.\nB SFT Details\nSFT The objective of SFT is to maximize the\nlikelihood of the generated instruction ygiven\nthe input image and question. The output y=\n{y1, y2, ..., y t}is a sequence of navigation instruc-\ntion tokens. The input question xquestion includes\npositional information xloc,xrot, andpi+1. The loss\nfunction is defined as:\nLSFT=−TX\nt=1logPθ(yt|y<t, ximage, xquestion ),\n(5)\nwhere where ytrepresents the t-th token of the\nnavigation instruction, y<tdenotes all preceding\ntokens, and Tis the total instruction length.\nC Algorithm Details\nD Route Sampling in CARLA\nFigure 5 illustrates BEV maps of CARLA towns\nwith sampled routes highlighted in blue. Figure 6\nillustrates examples of sampled start and end points\nfor pedestrians in CARLA Town03 and Town10.Algorithm 1 LaF-GRPO Reward Calculation\nRequire: Generated output oi, Reference output oref, Weights\nα, β, γ (default)\nEnsure: Combined reward score ri\n1:function CALCULATE REWARD (oi, oref)\n2: ▷Format Reward :r(1)←1/0\n3: ▷Text Generation Reward :\n4: r(2)←METEOR (oi, oref)\n5: ▷LLM-as-Follower Reward :\n6: ai←ExtractAction (oi)\n7: aref←ExtractAction (oref)\n8: r(3)←action_compare (ai, aref)\n9: ▷Compute weighted reward:\n10: ri←α·r(1)+β·r(2)+γ·r(3)\n11: return ri\n12:end function\nAlong a given route or trajectory, front-facing RGB\nimages and corresponding semantic segmentation\nviews can be collected (Figure 7).\nE Prompt Details\nTable 6 presents a detailed prompt template.\nThis specific version is formulated without pre-\ncalculation. For experimental conditions employ-\ning a \"with pre-calculation\" approach, this base\nprompt is augmented by an additional sentence\nthat explicitly states the calculated distance and\ndirection, such as: \"The movement direction is\n[DIRECTION] direction with a distance of [DIS-\nTANCE].\" Table 7 outlines the prompt used for gen-\nerating step-level instructions with the DeepSeek-\nR1 model. Finally, Table 8 describes the prompt\nfor generating action interpreter samples.\nF Results Details\nAdditional Observations Scores are generally\nhigher with pre-calculation than without. This is\nlikely because pre-calculation reduces the mathe-\nmatical computation difficulty for the models. Fur-\nthermore, while both intra-town and inter-town\nresults demonstrate consistent trends, intra-town\nevaluations typically yield higher scores. This is\nunderstandable as the intra-town test set comprises\nsamples from the same town environments as the\ntraining set, leading to a closer data distribution.\nLaF-GRPO vs. Standard GRPO Table 9 de-\nscribes the prompt used by GPT-4o to calculate\nnavigational accuracy, while Table 10 presents the\nprompt for GPT-4o to select a better navigation\ninstruction. Examples are in Table 11 and Table 12.\n11\n--- Page 12 ---\n(a) Town01\n (b) Town02\n(c) Town03\n (d) Town04\n(e) Town05\n (f) Town10\nFigure 5: BEV maps of CARLA towns with sampled routes highlighted in blue\n(a) Town03\n (b) Town10\nFigure 6: Examples of sampled start and end points for pedestrians in CARLA Town03 and Town10.\n12\n--- Page 13 ---\nFigure 7: Example pairs of front-facing RGB images and their corresponding semantic segmentation views.\nPrompt Template\nGenerate navigation instructions for visually impaired users based on the provided front-view image and\nstep information:\n- Current location:\n{’location’ : {’x’: [START_X], ’y’: [START_Y]},\n’rotation_yaw’ : [START_YAW]}\n- Target goal (next step):\n{’location’ : {’x’: [TARGET_X], ’y’: [TARGET_Y]}, ’rotation_yaw’ : [TARGET_YAW]}\nDetermine the direction and distance to the next step:\n- If the angle is between -15° and 15°, the direction is straight ahead (\"0 o’clock\") .\n- If the angle is between 15° and 45°, the direction is slightly to the right (\"1 o’clock\") .\n- For angles beyond this, map the directions incrementally in 30 °steps (e.g., \"2 o’clock\" through \"11\no’clock\") .\n- For angles near ±180° or wraparound (165° to -165°), the direction is directly behind (\"6 o’clock\") .\n- Calculate the distance using the Euclidean distance formula.\nSemantic view information: [\n‘top_left ’: [TOP_LEFT_SEM_OBJ],\n‘top_middle ’: [TOP_MID_SEM_OBJ],\n‘top_right ’: [TOP_RIGHT_SEM_OBJ],\n‘bottom_left ’: [BOTTOM_LEFT_SEM_OBJ],\n‘bottom_middle ’:[BOTTOM_MID_SEM_OBJ],\n‘bottom_right ’:[BOTTOM_RIGHT_SEM_OBJ] ]\n- Incorporate this information to describe the surroundings in combination with the image. If there is a\nhigh likelihood of collision with any object, include a clear warning for the user .\nProvide only the final navigation instructions for the step, including spatial and environmental details, as\nwell as potential safety warnings. Do not include any additional or unrelated text.\nTable 6: Detailed prompt template for navigation instructions for visually impaired users (without Pre-Calculation).\n13\n--- Page 14 ---\nPrompt for Generating Step-Level Instructions with DeepSeek-R1\nTask Description : Generate concise, real-time navigation guidance for visually impaired users by\ncombining outputs from 2 or 3 AI models.\nRequirements :\n1. Content Structure must include : (1) Action: Provide clock-direction references (e.g., \"11 o’clock\")\nwith metric distances (e.g., \"0.5 meters\") or relatable analogies (e.g., \"two small steps\"). (2) Environment:\nDescribe left, right, and ahead in 5-10 words, focusing on tactile or sound cues (e.g., \"bushes on left\",\n\"traffic sounds from right\"). (3) Hazards: Explicitly identify immediate dangers (e.g., \"benches\", \"road\nedge\") with actionable warnings (e.g., \"pause and listen for bike bells\").\n2. Style : Use natural, conversational language, avoiding robotic terms. Prioritize critical information,\nremoving redundant descriptions, and incorporate sensory guidance when necessary (e.g., \"use your cane\nto check\", \"listen for engines\").And avoid words like \"watch\" or \"see\" since visually impaired individuals\nmay not be able to perceive their environment through sight.\n3.Format : Limit to 2-3 sentences. Output only the polished instruction text.\nOutput the concise navigation guidance text only.\nThis is an output example : Turn slightly left toward 11 o’clock and walk half a meter. Keep your cane\nnear the bushes on the left to avoid benches; listen closely for traffic from the right road. Pause if you\nhear engines or bells.\nThe model 1’s generated text is: [GPT-4o’s INSTRUCTION]\nThe model 2’s generated text is: [Gemini-2’s INSTRUCTION]\nThe model 3’s generated text is: [Claude-3.5’s INSTRUCTION]\nYou must make sure output the polished instruction only, without additional words!\nNow output the polished result:\nTable 7: Prompt for Generating Step-Level Instructions with DeepSeek-R1\nCase Study Figure 8 compares the navigational\nguidance outputs and instructions generated by\nZero-(LaF-GRPO), SFT, and SFT+(LaF-GRPO).\n14\n--- Page 15 ---\n<think> The sidewalk curves\nslightly right ahead; road to your\nleft with traffic sounds. Keep\ncentered, use your cane near the\nbrick wall on the right to avoid\npoles. Pause if you hear engines\nor footsteps approaching.\n</think> <answer> Turn right\ntoward 130 degrees and walk\ntwo small steps. Stay near the\nbrick wall on your right; road\nedge left—use your cane to\ncheck for poles. Pause if engines\napproach. </answer><think> The building on your right\nhas a low fence with bushes; the\nleft has a pole near the sidewalk\nedge. Stay centered between\nthe pole and fence, using your\ncane to check uneven pavement.\nPause if you hear footsteps or\nengines. </think> <answer> Turn\nslightly right toward 1 o’clock\nand walk two small steps. Keep\nyour cane near the pole on the\nleft and the fence with bushes\nto the right; stay alert for\nnearby movement. </answer><think> The path is clear ahead,\nbordered by buildings on your\nleft and trees to the right. Use\nyour cane near the left railing to\navoid uneven steps; listen for\ntraffic from the right road.\nProceed straight two small\nsteps.</think> <answer> Turn\nslightly right toward 167\ndegrees and walk two small\nsteps. Buildings left, trees right\n—use your cane near railings;\nlisten for traffic. </answer><think> The path is clear ahead,\nbordered by buildings on your\nleft and trees to the right. Use\nyour cane near the left railing to\navoid benches; listen for traffic\nfrom the road to your right.\nProceed straight one small step.\n</think> <answer> Step forward\n0.5 meters. Buildings left,\nbushes right; use your cane near\nthe left railing and listen for\ntraffic. Pause if you hear\nengines. </answer><think> The user is currently\nstanding near the corner of a\nbuilding, facing slightly left. The\ntarget goal is just ahead and to\nthe right, indicating a slight turn\nto the right. The surroundings\ninclude a sidewalk, a pole, and\nsome vegetation. There is no\nimmediate obstacle in the path,\nbut the user should be cautious\nof the fence and the wall on the\nright side to avoid collision.\n</think> <answer> Move straight\nahead for about 0.8 meters.\nWatch for the fence and wall on\nyour right to avoid collision.\n</answer><think> The user is currently\nstanding near the entrance of a\nbuilding, facing slightly left. The\nnext step involves moving\nforward towards another\nbuilding entrance. The path\nappears clear, but there is a\ntree trunk on the right side that\ncould potentially obstruct the\npath if not carefully navigated\naround. The surroundings include\nbuildings, sidewalks, and\nvegetation, indicating an urban\nenvironment. </think>\n<answer> Move straight ahead for\nabout 0.2 meters. Watch for the\ntree trunk on your right to avoid\ncollision. </answer><think> The user is currently\nstanding on a sidewalk near a\nbuilding, facing slightly left. The\nnext step involves moving\nforward along the sidewalk\ntowards another part of the\nbuilding. The environment\nincludes trees, poles, and a truck\nvisible in the distance. There are\nno immediate obstacles in the\npath, but the user should be\naware of the railing on the left\nside to avoid potential collisions.\n</think> <answer> Move straight\nahead for about 0.5 meters.\nWatch for the railing on your\nleft to avoid collision. </answer><think> The user is currently\nstanding on a sidewalk near a\nbuilding, facing slightly to the\nright. The next step involves\nmoving forward towards another\npart of the sidewalk. The\nenvironment includes buildings,\npoles, and a fence. There is a\nhigh likelihood of collision with\nthe pole and fence if the user\nmoves directly ahead without\ncaution. It's important to\nnavigate carefully around these\nobstacles. </think> <answer> Move\nstraight ahead for about 0.5\nmeters. Watch out for the pole\nand fence on your right to avoid\na collision. </answer>\nTurn slightly right toward 132\ndegrees and walk half a meter.\nKeep your cane near the road\nedge on the left; buildings close\non both sides. Pause if you hear\nengines or bells—use your cane\nto check for poles ahead.Turn slightly right toward 150\ndegrees. Keep your cane near\nthe building wall on the left;\nvegetation and a fence line the\nright. Pause if you hear\nfootsteps ahead—use your cane\nto check for uneven pavement.Turn slightly right toward 167\ndegrees. Buildings on left, trees\nalong right; use your cane to\ncheck for uneven pavement.\nPause if you hear engines or\nbells.Turn slightly right toward 175\ndegrees. Buildings on left,\nbushes and benches along right\nsidewalk; road sounds from\nright. Use your cane to check\nfor benches ahead—pause if\nsurfaces feel uneven.Front-\nView\nZero-\n(LaF-GRPO)\nSFT\nSFT+\n(LaF-GRPO)Figure 8: Case study of Zero-(LaF-GRPO), SFT and SFT+(LaF-GRPO) navigational guidance over steps.\n15\n--- Page 16 ---\nPrompt for Generating Action Interpreter Samples with DeepSeek-R1\nAssume you are a blind person. Analyze the instruction through these steps:\n1. Determine if movement parameters exist:\n- Extract direction: Convert any directional information (e.g., left, right, east, west) into the \"X o’clock\"\nformat (e.g., \"2 o’clock\" for a slight right turn, \"9 o’clock\" for a left turn).\n- Extract distance: Ensure the distance includes a numerical value and a unit (e.g., meters, steps).\n2. Check for danger alerts:\n- Identify if the instruction includes detailed warnings about hazards (e.g., specific obstacles at specific\ndirections or distances). If hazards are mentioned but lack detail, consider the alert as non-detailed.\n3. If both direction and distance are missing, or if the instruction is unclear or ambiguous, return None\nfor the movement parameters\nOutput Format:\n{\n\"move\": {\n\"direction\": \"X o’clock\", // Replace \"X\" with the appropriate value (e.g., \"2 o’clock\").\n\"distance\": \"Y meters/steps\" // Replace \"Y\" with the numerical value and unit.\n},\n\"detailed_hazard_alert\": true/false // Set to ‘true‘ if detailed hazard warnings are present, otherwise\n‘false‘.\n}\nor, if the instruction is invalid or incomplete:\n{\n\"move\": None,\n\"detailed_hazard_alert\": true/false // Set to ‘true‘ if any hazard warnings are present, even if incom-\nplete.\n}\nExamples:\nExample 1:\nInput: ‘Walk forward approximately 0.5 meters, maintaining your current direction (0 o’clock). The\nsurroundings include a sidewalk on the right with walls and vegetation, buildings to the left, and roads\nahead. ’\nOutput: {\"move\": {\"direction\": \"0 o’clock\", \"distance\": \"0.5 meters\"},\"detailed_hazard_alert\": false}\nExample 2:\nInput: ‘Turn left, and move forward to avoid the building on your right. ’\nOutput: {\"move\": {\"direction\": \"9 o’clock\", \"distance\": \"0 meters\"}, \"detailed_hazard_alert\": true}\nExample 3:\nInput: ‘Caution: wet floor ahead. ’\nOutput: {\"move\": None,\"detailed_hazard_alert\": true}\nNow, solve the following task:\nInput Instruction: [INPUT INSTRUCTION]\nOutput:\nTable 8: Prompt for Generating Action Interpreter Samples with DeepSeek-R1\n16\n--- Page 17 ---\nPrompt Template for Navigational Accuracy Calculation\nI will provide you with one prediction and one reference_ground_truth. Your task is to evaluate the\nprediction against the reference_ground_truth based on two criteria: movement direction and distance.\nYou will then return a score of 1, 0.5, or 0 based on the following rules:\nReturn 1if:\n–The movement direction in the prediction is correct when compared to the reference_ground_truth,\nAND\n– The distance in the prediction is correct when compared to the reference_ground_truth .\nReturn 0.5if:\n–The movement direction is correct BUT the distance is incorrect, OR\n–The movement direction is incorrect BUT the distance is correct.\nReturn 0if:\n–The movement direction in the prediction is incorrect when compared to the reference_ground_truth,\nAND\n–The distance in the prediction is incorrect when compared to the reference_ground_truth .\nPrediction: [PREDICTION]\nReference Ground Truth: [REFERENCE]\nOutput only the score without additional words.\nTable 9: Prompt Template for Navigational Accuracy Calculation.\nPrompt template for Instruction Clarity Evaluation\nI will provide you with two predictions: pred_1 and pred_2. Your task is to determine which of these two\npredictions offers clearer and better-structured instructions.\nEvaluate pred_1 and pred_2 based on the following criteria for instruction quality:\nMovement Guidance: How clear, concise, and actionable is the guidance for movement?\nEnvironment Surrounding: How well are relevant details about the surrounding environment described?\nPotential Safety Alerts: Is necessary information regarding potential safety hazards included appropri-\nately?\nBased on your comparative evaluation of these criteria, decide which prediction is better.\npred_1 : [PREDICTION_1]\npred_2 : [PREDICTION_2]\nOutput: Return “1” if pred_1 is better, or “2” if pred_2 is better.\nOutput only 1 or 2 without additional words.\nTable 10: Prompt Template for Evaluating Instruction Clarity.\n17\n--- Page 18 ---\nTypes Info Format+METEOR Format+METEOR+LLM (Ours)\nNavigational\nAccuracyTown05,\nw/o pre-calculation\nmove_gt:\n(11 o’clock,\n0.5 meters)<think> Move forward half a meter\ntoward 12 o’clock. Buildings line your\nleft; road sounds from the right. Avoid\npoles ahead—use your cane to check\npavement. </think> <answer> Turn\nslightly left toward 11 o’clock and walk\nhalf a meter. Keep your cane near the\nbuildings on the left; pause if you hear\ntraffic approaching from the right.\n</answer><think> Turn slightly left toward 11\no’clock and walk half a meter. Keep\nyour cane near the building edge on the\nright to avoid poles; traffic sounds from\nthe left road. Pause if you hear engines\nor bells. </think>\n<answer> Turn slightly left toward 11\no’clock and walk half a meter. Buildings\non your right, road sounds from the left;\nuse your cane to check for poles and\npause if engines approach. </answer>\nTown05,\nw/o pre-calculation\nmove_gt:\n(10 o’clock,\n0.5 meters)<think> Move forward half a meter\ntoward 12 o’clock. Keep your cane near\nthe bushes on the left; road edge and\nrocks to the right. Pause if you hear\ntraffic and check for poles with your\ncane. </think>\n<answer> Proceed straight at 12\no’clock for half a meter. Use your cane\nto avoid poles and the rocky wall on the\nright; listen closely for engines.\n</answer><think> The road curves left toward 10\no’clock, with bushes on your left and a\nrocky cliff to the right. A pole is ahead\nat 12 o’clock; steer slightly left to avoid\nit and check the sidewalk edge with your\ncane. </think>\n<answer> Turn slightly left toward 11\no’clock and take two small steps. Use\nyour cane to check the left curb and\navoid the pole ahead; listen for traffic\nfrom the right road. </answer>\nTown10,\nwith pre-calculation\nmove_gt:\n(11 o’clock,\n0.2 meters)<think> Move slightly right toward 1\no’clock for 0.2 meters. Keep your cane\nnear the building edge on the left; traffic\nsounds from the right road. Pause at the\npole ahead—check for uneven pavement\nwith your cane. </think>\n<answer> Turn slightly right toward 1\no’clock and walk 0.2 meters. Use your\ncane to check the sidewalk edge and\npause if you hear traffic approaching.\n</answer><think> Turn slightly left toward 11\no’clock and walk 0.2 meters. Keep your\ncane near the building on the right to\navoid the traffic pole; listen for engines\nfrom the road to your left. </think>\n<answer> Turn slightly left toward 11\no’clock and walk 0.2 meters. Building\non your right, road to the left; use your\ncane to check for the traffic pole ahead\nand pause if you hear engines.\n</answer>\nTable 11: Example comparison of different formats for Navigational Accuracy\n18\n--- Page 19 ---\nTypes Info Format+METEOR Format+METEOR+LLM\n(Ours)\nInstruction\nClarityTown05,\nw/o pre-calculation\nmove_gt:\n(12 o’clock, 0.5 meters)<think> Move forward half a\nmeter toward 12 o’clock.\nBuildings line your left; road\nsounds from the right. Avoid\npotted plants ahead—pause if\nengines approach. </think>\n<answer> Proceed straight at\n12 o’clock for half a meter. Use\nyour cane to check for planters\nand listen for traffic.\n</answer><think> Move forward half a\nmeter toward 12 o’clock.\nBuildings line your left; road\nsounds from the right. Avoid\npotted plants ahead—use your\ncane to check. </think>\n<answer> Proceed straight at\n12 o’clock for half a meter.\nBuildings on left, road sounds\nright; use your cane to\nnavigate around potted plants\nahead. Pause if engines\napproach. </answer>\nTown03,\nwith pre-calculation\nmove_gt:\n(12 o’clock, 0.5 meters)<think> Move slightly left\ntoward 11 o’clock for half a\nmeter. Keep your cane near the\nbushes on the left to avoid the\nbench; listen for traffic from\nthe right road. Pause if you\nhear engines or bells.\n</think>\n<answer> Turn left toward 11\no’clock and walk half a meter.\nUse your cane to check the left\nhedge; pause for traffic sounds\nfrom the right road. </answer><think> Move slightly left\ntoward 11 o’clock for half a\nmeter. Keep your cane near the\nbushes on the left to avoid the\nbench; listen for traffic from\nthe right road. </think>\n<answer> Turn left toward 11\no’clock and walk half a meter.\nBushes on your left, road\nsounds from the right; use your\ncane to check the path and\npause if you hear engines.\n</answer>\nTown10,\nwith pre-calculation\nmove_gt:\n(1 o’clock, 0 meters)<think> Turn slightly right\ntoward 1 o’clock and take two\nsmall steps. Keep your cane\nnear the building on the left;\ntraffic sounds from the right\nroad. Pause if you hear\nengines or bells. </think>\n<answer> Turn slightly right\ntoward 1 o’clock and take two\nsmall steps. Use your cane\nnear the building on the left;\npause for traffic sounds from\nthe right. </answer><think> Turn slightly right\ntoward 1 o’clock and take two\nsmall steps. Keep your cane\nnear the building on the left;\ntraffic sounds from the right\nroad. Pause if engines\napproach and check for uneven\npavement. </think>\n<answer> Turn slightly right\ntoward 1 o’clock and take two\nsmall steps. Buildings on your\nleft, road with trucks to the\nright; use your cane to avoid\npoles and pause if engines\napproach. </answer>\nTable 12: Example comparison of different formats for Instruction Clarity\n19",
  "text_length": 69925
}