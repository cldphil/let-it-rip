{
  "id": "http://arxiv.org/abs/2506.03123v1",
  "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation",
  "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.",
  "authors": [
    "Zhengyao Lv",
    "Chenyang Si",
    "Tianlin Pan",
    "Zhaoxi Chen",
    "Kwan-Yee K. Wong",
    "Yu Qiao",
    "Ziwei Liu"
  ],
  "published": "2025-06-03T17:55:04Z",
  "updated": "2025-06-03T17:55:04Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.03123v1"
}