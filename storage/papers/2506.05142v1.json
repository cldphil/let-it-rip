{
  "id": "http://arxiv.org/abs/2506.05142v1",
  "title": "Do Large Language Models Judge Error Severity Like Humans?",
  "summary": "Large Language Models (LLMs) are increasingly used as automated evaluators in\nnatural language generation, yet it remains unclear whether they can accurately\nreplicate human judgments of error severity. In this study, we systematically\ncompare human and LLM assessments of image descriptions containing controlled\nsemantic errors. We extend the experimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multimodal (text + image) settings,\nevaluating four error types: age, gender, clothing type, and clothing colour.\nOur findings reveal that humans assign varying levels of severity to different\nerror types, with visual context significantly amplifying perceived severity\nfor colour and type errors. Notably, most LLMs assign low scores to gender\nerrors but disproportionately high scores to colour errors, unlike humans, who\njudge both as highly severe but for different reasons. This suggests that these\nmodels may have internalised social norms influencing gender judgments but lack\nthe perceptual grounding to emulate human sensitivity to colour, which is\nshaped by distinct neural mechanisms. Only one of the evaluated LLMs, Doubao,\nreplicates the human-like ranking of error severity, but it fails to\ndistinguish between error types as clearly as humans. Surprisingly,\nDeepSeek-V3, a unimodal LLM, achieves the highest alignment with human\njudgments across both unimodal and multimodal conditions, outperforming even\nstate-of-the-art multimodal models.",
  "authors": [
    "Diege Sun",
    "Guanyi Chen",
    "Fan Zhao",
    "Xiaorong Cheng",
    "Tingting He"
  ],
  "published": "2025-06-05T15:24:33Z",
  "updated": "2025-06-05T15:24:33Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.05142v1",
  "full_text": "--- Page 1 ---\narXiv:2506.05142v1  [cs.CL]  5 Jun 2025Do Large Language Models Judge Error Severity Like Humans?\nDiege Sun1, Guanyi Chen2,3,4 *, Fan Zhao1∗, Xiaorong Cheng1, Tingting He2,3,4\n1School of Psychology,\n2Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning,\n3National Language Resources Monitor and Research Center for Network Media,\n4School of Computer Science, Central China Normal University\n{g.chen, zfan}@ccnu.edu.cn\nAbstract\nLarge Language Models (LLMs) are increas-\ningly used as automated evaluators in natural\nlanguage generation, yet it remains unclear\nwhether they can accurately replicate human\njudgments of error severity. In this study, we\nsystematically compare human and LLM as-\nsessments of image descriptions containing\ncontrolled semantic errors. We extend the ex-\nperimental framework of van Miltenburg et al.\n(2020) to both unimodal (text-only) and multi-\nmodal (text + image) settings, evaluating four\nerror types: age, gender, clothing type, and\nclothing colour. Our findings reveal that hu-\nmans assign varying levels of severity to dif-\nferent error types, with visual context signifi-\ncantly amplifying perceived severity for colour\nand type errors. Notably, most LLMs assign\nlow scores to gender errors but disproportion-\nately high scores to colour errors, unlike hu-\nmans, who judge both as highly severe but\nfor different reasons. This suggests that these\nmodels may have internalised social norms in-\nfluencing gender judgments but lack the per-\nceptual grounding to emulate human sensi-\ntivity to colour, which is shaped by distinct\nneural mechanisms. Only one of the evalu-\nated LLMs, Doubao, replicates the human-like\nranking of error severity, but it fails to dis-\ntinguish between error types as clearly as hu-\nmans. Surprisingly, DeepSeek-V3, a unimodal\nLLM, achieves the highest alignment with hu-\nman judgments across both unimodal and mul-\ntimodal conditions, outperforming even state-\nof-the-art multimodal models.\n1 Introduction\nLarge Language Models (LLMs) are increasingly\nbeing explored not only as generators of text (Xuan-\nfan and Piji, 2023), but also as evaluators, offering\na scalable alternative to human judgment in Natural\nLanguage Generation (NLG) evaluation (Gu et al.,\n*Corresponding Authors\nFigure 1: Image 34419 from MSCOCO: A man wearing\na yellow shirt on a tennis court plays tennis.\n2024; Gao et al., 2025). Their use as “LLM-as-a-\njudge” has shown promise in areas such as sum-\nmarisation and dialogue (Liu et al., 2023; Liusie\net al., 2024), where traditional metrics, such as\nBLEU (Papineni et al., 2002) and ROUGE (Lin,\n2004), often fall short of capturing semantic nu-\nances and user preferences.\nNonetheless, key questions remain about\nwhether LLMs can reliably mirror human judg-\nment (Bavaresco et al., 2024; Gao et al., 2025).\nOne underexplored challenge is the assessment of\nerror severity in generated text. van Miltenburg\net al. (2020) observed that, in image captioning,\nhuman evaluators do not treat all errors equally:\nfor example, given Figure 1, the caption “ a man\nwearing a redshirt” is judged more severely incor-\nrect than “ a man wearing a yellow coat”. Whether\n(multi-modal) LLMs’ assessments of error severity\nalign with such human intuitions remains an open\nquestion.\nvan Miltenburg et al. (2020) focused on multi-\nmodal NLG evaluation, where human judges evalu-\nate outputs with access to both text and visual input.\nThey showed that humans assign different levels of\nseverity to different types of errors, suggesting nu-\n--- Page 2 ---\nanced perceptions of correctness. However, many\nreal-world NLG tasks, such as summarisation, di-\nalogue, and data-to-text generation, are unimodal\n(text-only), and most current LLMs are also uni-\nmodal. This raises a critical question: Are the\nseverity distinctions observed in multimodal evalu-\nations dependent on the presence of visual context,\nor do they reflect more general patterns in human\njudgment?\nIn other words, it remains unclear how sever-\nity judgments of human beings are formed in uni-\nmodal settings, and how the presence of visual con-\ntext may influence those judgments. With these\ntwo questions about human severity judgement\nin mind, we are further interested in whether uni-\nmodal LLMs’ severity judgement aligns with that\nof humans and whether visual context has a similar\nimpact on multimodal LLMs.\nWe therefore come up with two core research\nquestions:\n•Do Large Language Models judge error\nseverity like humans? We examine the extent\nto which LLM-based evaluations agree with\nhuman assessments across a range of error\ntypes in NLG.\n•How does the presence of images influence\nhuman judgment of errors in NLG? We\nexplore whether access to visual context al-\nters human perceptions of error severity and\nwhether this impact is mirrored in LLM-based\nassessments.\nTo address these two questions, we begin by up-\ndating the experimental setup of van Miltenburg\net al. (2020), allowing human judges to evaluate\noutputs in Chinese under two conditions: with ac-\ncess to both visual and textual inputs (multimodal),\nor with access to only textual inputs (unimodal).\nThis design enables us to examine how severity\njudgments are formed in unimodal settings and to\nassess the influence of visual context. Next, we in-\nvestigate the extent to which LLMs’ assessments of\nerror severity align with those of humans by repli-\ncating the experiment using a range of unimodal\nand multimodal LLMs. Finally, we conduct a sys-\ntematic comparison of human and LLM behaviour,\nevaluating which models best approximate human\njudgments of error severity across unimodal and\nmultimodal conditions.2 Hypotheses\nFollowing van Miltenburg and Elliott (2017) and\nvan Miltenburg et al. (2020), we are interested\nin four types of errors in Chinese Image Descrip-\ntions: AGE,GENDER ,CLOTHING -COLOUR (hence-\nforth, COLOUR ), and CLOTHING -TYPE (henceforth,\nTYPE ) (See Appendix A for examples of each er-\nror). The current section discusses the hypotheses\nof our experiment.\nvan Miltenburg et al. (2020) asked participants\nto rate the quality of image descriptions, each con-\ntaining a single type of error, with reference to\nthe corresponding image. Lower ratings indicated\nmore severe errors. Their findings revealed the fol-\nlowing ranking of error types in terms of severity:\nCOLOUR ≺GENDER ≺TYPE≺AGE (1)\nwhere A≺Bmeans error type Areceives lower\nscores (i.e., was judged more severe) than B. No-\ntably, the differences in severity between COLOUR\nand AGE,COLOUR and TYPE , as well as GENDER\nand AGE, were found to be statistically significant.\nIn this study, we allow participants to access\neither both visual and textual inputs or only tex-\ntual inputs to explore the impact of visual con-\ntext. Previous studies in cognitive science have\nshown that visual context influences the processing\nof colour perception (Adelson, 1993; Chen et al.,\n2019), age perception (Pilz and Lou, 2022), object\ntype recognition (Fenske et al., 2006), and gender\nrepresentation (Guilbeault et al., 2024) in the hu-\nman brain. Based on these findings, we expect\nthat the presence of visual context will significantly\naffect the ratings given by participants. However,\nwe do not expect it to alter the severity ranking of\nerrors observed in van Miltenburg et al. (2020). For\nthe LLM replications, considering the potential of\nLLMs to approximate human judgments, we hy-\npothesise that these models will replicate human\nbehaviour in our updated experiment. Formally, we\nformulate the following hypotheses:\nH1Humans will reproduce the same severity rank-\ning of error types as reported by van Mil-\ntenburg et al. (2020), under both multimodal\nand unimodal conditions.\nH2The presence of visual context will have a sig-\nnificant effect on human judgments.\nH3Both multimodal and unimodal LLMs will\nreplicate the same severity ranking of error\ntypes as observed in human judgments.\n--- Page 3 ---\nH3Visual context will influence multimodal\nLLMs in a manner similar to its effect on hu-\nman judgments.\n3 Experiments\nWe elaborate on experimental settings and stimuli\nused for our human and LLM experiments.\n3.1 Human Experiment\nMaterial. We selected a total of 9 images, 5 of\nwhich are sourced from MS COCO (Lin et al.,\n2014). To prevent data contamination (Balloccu\net al., 2024) in the subsequent LLM experiment, the\nremaining 4 images were sourced from the inter-\nnet. These images were chosen following the same\ncriteria outlined by van Miltenburg et al. (2020)\nto minimise ambiguity in error types (for further\ndetails, see Appendix B). This ensures that, for ex-\nample, when a description refers to a person in the\nimage as “ young ”, the individual in the image is\nunmistakably young.\nFollowing the approach of van Miltenburg et al.\n(2020), we tasked two native speakers of Mandarin\nChinese with writing a reference description for\neach image, as well as creating minimal pairs be-\ntween erroneous descriptions and the reference de-\nscriptions. Specifically, the annotators were in-\nstructed to generate erroneous descriptions based\non four error types: AGE,GENDER ,COLOUR , and\nTYPE , by altering a single character in the Chinese\ntext. To minimise the influence of vagueness, the\nannotators were asked to ensure that the difference\nbetween the reference and erroneous descriptions\nwas substantial. For instance, they were instructed\nto change “red” to “blue” rather than to a more\nsubtle colour like “orange”. The two annotators\ndiscussed finalising the descriptions. As a result,\neach image is paired with one reference description\nand four erroneous descriptions. An example can\nbe found in Appendix A.\nDesign. The stimuli in our experiment consist\nof two conditions: multimodal and unimodal. In\nthe multimodal condition, participants were shown\nan image alongside a corresponding erroneous de-\nscription. In the unimodal condition, participants\nwere shown a reference description along with a\ncorresponding erroneous description. This results\nin a total of 9×4×2 = 72 stimuli. It is important\nto note that our experimental design in the multi-\nmodal condition differs from that of van Miltenburg\net al. (2020). Specifically, we did not present the\nFigure 2: Screenshots of a trial in (a) multimodal condi-\ntion, and (b) unimodal condition.\nreference description in the multimodal condition\nto avoid potential bias in participants’ decisions\ncaused by prior exposure to the reference text.\nStimuli of one image from MS COCO were used\nas the practice for participants. We thus have 64\nstimuli in the main experiment. In each trial, partic-\nipants rated the quality of the erroneous description\nusing a slider ranging from 0 to 100 (i.e., Magni-\ntude Estimation; Bard et al. (1996)). Screenshots\nof both conditions are shown in Figure 2.\nProcedure. Participants were invited to the lab to\ntake part in the experiment. The session began with\nan instruction informing them that their task was to\nrate the quality of a set of descriptions generated by\nan artificial intelligence system, based on either ref-\nerence images or reference texts. Participants were\ntold that the ratings would be done using a slider\nranging from 0 to 100, where a higher score indi-\ncated better quality. They were not informed that\nthe descriptions they would rate contained errors.\nThe experiment began with a demographic sur-\nvey, where participants provided information about\ntheir age, gender, educational background, and\n--- Page 4 ---\nFigure 3: The results of both our human experiment and LLM experiment. Each error bar indicates the standard\ndeviation of scores.\nwhether they had colour blindness. Following this,\nparticipants moved on to the practice phase, where\nthey rated 8 practice stimuli corresponding to the\nsame image. These 8 practice stimuli were shuffled\nrandomly and were identical for all participants.\nAfter completing the practice phase, participants\nwere asked to press the “START\" button when they\nfully understood the experimental procedure and\nwere ready to proceed to the main experiment. The\nstimuli in the main experiment were divided into\ntwo groups. Each group contained 32 stimuli: 16\nstimuli of 4 images in the multimodal condition\nand 16 stimuli of the other 4 images in the uni-\nmodal condition, with stimuli shuffled randomly\nwithin each group. The main experiment consisted\nof three phases. In the first phase, participants\nrated the stimuli from the first group without in-\nterruption. The second phase was a rest period,\nduring which participants were asked to press the\n“START\" button when they were ready to continue.\nIn the third phase, participants rated the stimuli\nfrom the second group. During the main experi-\nment, we recorded each participant’s ratings for\neach trial, as well as their reaction times.\nParticipants. We used G*Power to estimate the\nrequired sample size for our experiment. Since\nwe planned to use a Linear Mixed-Effects Model\n(LMM) for statistical analysis (see Section 4 forHuman GPT-4o Doubao\nError Type < .001∗∗∗< .001∗∗∗< .001∗∗∗\nVisual Context .002∗∗< .001∗∗∗< .001∗∗∗\nInteraction Effect .014∗.677 .004∗∗\nTable 1: The LMM-based statistical test results for the\nmain effects of error type and visual context, as well as\ntheir interaction effect. Significance Marking:∗∗∗p <\n.001;∗∗p < . 01;∗p < . 05.\nfurther details), we set the effect size measure (Co-\nhen’s f) to 0.23, the significance level ( α) to 0.05,\nand the statistical power ( 1−β) to 0.8. Based on\nthe number of stimuli, the required sample size\nwas estimated to be 18 participants. To mitigate\npotential ceiling or floor effects, we ultimately col-\nlected data from 25 participants. Of these, 14 were\nmale and 11 were female. The participants had\na mean age of 22.92 years (SD = 3.52). All par-\nticipants were university students with higher ed-\nucation backgrounds. Prior to the experiment, all\nparticipants confirmed that they had no history of\ncolour blindness or colour vision deficiencies.\n3.2 LLM Experiment\nWe used several unimodal and multimodal LLMs\nto replicate the above experiment to explore their\nerror severity judgements.\n--- Page 5 ---\nHuman GPT-4o Doubao\nEst. t p-value Est. t p-value Est. t p-value\n(Intercept) 52.795 19.777 < .001∗∗∗45.833 17.831 < .001∗∗∗39.167 13.937 < .001∗∗∗\nColour Error -20.085 -9.679 < .001∗∗∗2.083 0.658 .512 -10.417 -2.957 .003∗∗\nType Error 0.625 0.301 .763 12.917 4.077 < .001∗∗∗-4.583 -1.301 .195\nGender Error -27.570 -13.287 < .001∗∗∗-9.167 -2.893 .004∗∗-13.750 -3.903 < .001∗∗∗\nUnimodal -1.895 -0.913 .361 32.292 10.191 < .001∗∗∗27.083 7.688 < .001∗∗∗\nColour ×Unimodal 7.635 2.602 .009∗∗-0.833 -0.186 .853 6.250 1.255 .211\nType×Unimodal 7.800 2.658 .008∗∗-3.958 -0.883 .378 7.917 .113 .113\nGender ×Unimodal 2.325 0.792 .428 1.250 0.279 .781 -8.333 -1.673 .096\nTable 2: The estimated fixed effects in LMM for Human, GPT-4o and Doubao. It uses the age error in the multimodal\ncondition as the reference group (i.e., intercept). ‘Est.’ is the estimated fixed effect coefficient, and ‘t’ is the t-value.\nMaterial and Design. The same 64 stimuli used\nin the human experiment were also employed in\nthe LLM experiment. We instructed the LLMs to\nassign each stimulus a quality score ranging from 0\nto 100. For unimodal stimuli, we used the prompt\nin Table 5 (in Appendix C) to guide both unimodal\nand multimodal LLMs in rating these stimuli. For\nmultimodal stimuli, we used the prompt in Table 6\nto instruct multimodal LLMs to rate the stimuli.\nIn both prompts, we also requested that the LLMs\nprovide an explanation for the scores they assigned.\nProcedure and Models. We selected two power-\nful multimodal LLMs, GPT-4o (Hurst et al., 2024)\nand Doubao1, and two Unimodal LLMs, DeepSeek-\nV3 (Liu et al., 2024a)2and DeepSeek-R1 (Guo\net al., 2025). We included DeepSeek-R1 to investi-\ngate whether reasoning LLMs are better at distin-\nguishing the severity of different types of errors.\nFor each trial, we run each LLM 3 times to mitigate\nthe influence of randomness.\n4 Results\nFigure 3 reports the scores given by humans and 5\nLLMs.\n4.1 Human Performance\nHypothesis H1predicted that the severity ranking\nof error types would align with the ranking reported\nby van Miltenburg et al. (2020) in both multimodal\nand unimodal conditions. The results of our human\nexperiment show that, while the ranking is consis-\ntent across the two conditions, it differs from the\none reported by van Miltenburg et al. (2020):\nGENDER ≺COLOUR ≺AGE≺TYPE (2)\n1https://seed.bytedance.com/en/special/doubao_\n1_5_pro\n2We used the DeepSeek-V3-0324.Thus, we must reject hypothesis H1. A potential\nexplanation for this discrepancy is that our exper-\nimental settings differ slightly from those of van\nMiltenburg et al. (2020) in two key ways: (1) we\nconducted an in-lab experiment, while they used\na crowdsourcing approach, and (2) in the multi-\nmodal condition, we showed participants only the\nimage and the erroneous description to minimize\nbias, whereas van Miltenburg et al. (2020) also\npresented the reference description.\nEffects of Error Type and Visual Context. The\nresults suggest that participants assigned varying\nquality scores across different error types. Ad-\nditionally, the presence of visual context had a\nslight influence on the scores participants assigned,\nthough it did not alter the severity ranking. To ascer-\ntain the influence of these two factors, we conduct\na statistical analysis using a Linear Mixed-Effects\nModel (LMM).3The outcomes of the analysis us-\ning LMM are reported in Table 1, which shows\nthat both the error type and the presence of visual\ncontext have significant main effects on human\njudgements. Additionally, the interaction effect be-\ntween these two factors is significant, indicating\nthat their impacts are dependent on each other.\nTo further investigate how the presence of visual\ncontext influences human judgments, we report the\nestimated fixed effects in Table 2. Interestingly, the\nfixed effect estimates indicate that, overall, the pres-\nence of visual context does not significantly alter\nthe quality scores assigned by human participants\n(p=.361; see the ‘Unimodal’ line in Table 2).\n3We chose an LMM because, although the default position\nof the slider in the experiment is in the middle, the resulting\nscores are not normally distributed. This was confirmed using\na Shapiro-Wilk test, which indicated that the distribution of\nscores significantly deviates from a normal distribution ( W=\n0.959, p < . 001).\n--- Page 6 ---\nHuman GPT-4o Doubao DeepSeek V3 DeepSeek R1\nType 1 Type 2 Both Uni. Mul. Both Uni. Mul. Both Uni. Mul. Uni. Uni.\nAge Colour16.27\n***12.45\n***20.09\n***-1.67\n×-1.25\n×-2.08\n×7.29\n*4.17\n×10.42\n*-1.04\n×-4.38\n×\nAge Type-4.53\n*-8.43\n***-0.63\n×-10.94\n***-8.96\n*-12.92\n***0.63\n×-3.33\n×4.58\n×-8.33\n***-10.83\n***\nAge Gender26.41\n***25.25\n***27.57\n***8.54\n**7.92\n*9.17\n**17.92\n***22.08\n***13.75\n***15.63\n***13.33\n***\nColour Type-20.79\n***-20.88\n***-20.71\n***-9.27\n***-7.71\n*-10.83\n**-6.67\n*-7.50\n×-5.83\n×-7.29\n**-6.46\n*\nColour Gender10.14\n***12.8\n***7.49\n***10.21\n***9.17\n*11.25\n**10.63\n***17.92\n***3.33\n×16.67\n***17.71\n***\nType Gender30.93\n***33.67\n***28.20\n***19.48\n***16.88\n***22.08\n***17.29\n***25.42\n***9.17\n*23.96\n***24.17\n***\nTable 3: Estimated differences for the pairwise comparisons between different error types. ‘Uni.’ means the\nunimodal condition, and ‘Mul’ means the multimodal condition. Significance Marking:∗∗∗p < . 001;∗∗p < . 01;\n∗p < . 05;×p≤.05. The results highlighted in red represent differences that are in the opposite direction to those\nof humans. The results highlighted in yellow represent differences that align with the direction of human judgments\nbut are not statistically significant. The results highlighted in green represent differences that are in the opposite\ndirection to those of humans, but all of these differences are statistically insignificant.\nHowever, it does have a significant main effect\non human judgments due to interaction effects on\ncolour errors ( p=.009) and type errors ( p=.008;\nsee the last three lines in Table 2). In other words,\nthe answer to the hypothesis H2is that the presence\nof visual context significantly influences human\njudgments of error severity, but only for colour and\ntype errors, making participants perceive these two\nerrors as more severe.\nPairwise Comparisons. We extracted the esti-\nmated marginal means from the Linear Mixed-\nEffects Model (LMM) to perform pairwise compar-\nisons between different error types. The estimated\ndifferences and their corresponding significance\nlevels are reported in Table 3. These pairwise\ncomparisons were conducted both for the main\neffects (i.e., comparing the overall quality scores\nbetween pairs of error types) and for the interaction\neffects (i.e., comparing the quality scores separately\nin the unimodal and multimodal conditions). To\ndetermine the significance of these pairwise com-\nparisons, we applied Tukey’s Honestly Significant\nDifference (HSD) test (Tukey, 1949) for the main\neffects and the Holm–Bonferroni method (Holm,\n1979) for the interaction effects.\nThe results indicate that almost all differences\nbetween scores for different error types are sig-\nnificant. This is a bit different from the findings\nof van Miltenburg et al. (2020), where half of the\ndifferences were found to be statistically indistin-\nguishable. One possible explanation for this dis-\ncrepancy is that the in-lab setting of our experimentlikely enhanced participants’ focus, enabling them\nto identify errors more easily. The only exception\nto this pattern is the difference between age and\ntype errors in the multimodal condition, which was\nnot significant. This may be attributed to the pres-\nence of visual context, which, as mentioned earlier,\nled participants to perceive type errors as more se-\nvere, thereby reducing the difference between type\nand age errors.\nReaction Time Figure 4 shows the distribution\nof reaction times for participants across each error\ntype and condition. An LMM analysis indicates\nthat both error type and modality have a significant\neffect on reaction time ( p < . 001). The presence of\nvisual context reduces participants’ reaction times,\nsuggesting that multimodal inputs enable partici-\npants to judge the quality of the texts more quickly.\nWe also observe that participants judge gender and\ncolour errors, which they consider more severe than\nthe other errors, more quickly.\nThe interaction effect between these two factors\nis insignificant ( p=.601), indicating that although\nhumans judge gender and colour errors faster, this\nis not due to the presence of images.\n4.2 LLM Performance\nHypothesis H3predicted that all multimodal and\nunimodal LLMs are able to replicate the severity\nranking of error types. However, the results in\nFigure 3 show that only Doubao in the unimodal\ncondition fully replicates the order in Equation 2.\nGPT-4o, DeepSeek-V3 and DeepSeek-R1 have\n--- Page 7 ---\nFigure 4: The box plot of the distribution of reaction\ntimes for participants across each error type and condi-\ntion.\na slightly different ranking:\nGENDER ≺AGE≺COLOUR ≺TYPE.\nUnlike humans, these three LLMs assign low\nscores to gender errors but high scores to colour\nerrors. Humans perceive colour and gender errors\nas the most severe for different reasons. The sever-\nity of colour errors is due to the fact that colour\nfeatures are processed by distinct neural mecha-\nnisms in the human brain, compared to other object\nproperties (Connell, 2007; Liu et al., 2024b). In\ncontrast, the severity of gender errors is largely in-\nfluenced by social norms (Ashrafova, 2024). The\nresults suggest that these three LLMs are likely to\nhave learned social norms but have not captured\nthe specialised neural processing mechanisms as-\nsociated with colour perception in humans.\nDoubao exhibits the same error severity ranking\nas humans in the unimodal condition. In the mul-\ntimodal condition, it assigns slightly lower quality\nscores to type errors compared to age errors, but\nthis difference is not statistically significant (see\nfurther details in later statistical analyses). Given\nthat this difference is also insignificant in the hu-\nman experiment, we can conclude that among the 4\nLLMs we tested, only Doubao replicates the human\nerror severity ranking.\nEffects of Error Type and Modality. For the\ntwo multimodal LLMs, GPT-4o and Doubao, the\nresults show that they assign significantly higher\nquality scores to unimodal inputs than to multi-\nmodal inputs. We conducted the same statistical\nanalysis as in the human experiment, using a Lin-\near Mixed-Effects Model (LMM). The outcomes\nin Table 1 suggest that, similar to humans, both theerror type and the presence of visual context have\nsignificant main effects on Doubao, and there is\nalso a significant interaction effect. For GPT-4o,\nalthough the main effects of error type and visual\ncontext are significant, no interaction effect is ob-\nserved. This suggests that showing GPT-4o images\ndecreases the scores it assigns, but the change is\nconsistent across different error types.\nThe estimated fixed effects in Table 2 show sim-\nilar results. Presenting images to the LLMs gener-\nally decreases the scores they assign, with almost\nno interaction effect observed for GPT-4o and very\nsmall, statistically insignificant interaction effects\nfor Doubao. Recall that for humans, the presence\nof visual context significantly reduces their ratings\nfor colour and type errors. Therefore, the expecta-\ntion of hypothesis H4does not hold: the presence\nof visual context does not influence multimodal\nLLMs in the same way it affects human judgments.\nAs for the two unimodal LLMs, DeepSeek-V3\nand DeepSeek-R1, we tested only the effect of error\ntype, which, as expected, is significant ( p < . 001).\nPairwise Comparisons. We conducted pairwise\ncomparisons between error types, as in the human\nexperiment, and report the results in Table 3. For\nGPT-4o, as mentioned earlier, it assigns higher\nquality scores to colour errors compared to humans.\nThis leads to differences between age and colour\nerrors that are in the opposite direction to those ob-\nserved in humans in both unimodal and multimodal\nconditions, although these differences are not statis-\ntically significant. All other differences align with\nthe direction observed in human judgments and are\nstatistically significant.\nFor Doubao, since its severity ranking largely\naligns with humans, the only inconsistency in terms\nof the direction of difference is between age and\ntype errors. Nonetheless, since both Doubao and\nhumans consider this difference to be statistically\ninsignificant, this mismatch can be considered ac-\nceptable. Another issue with Doubao is that, al-\nthough it ranks error severity in the same order\nas humans, it perceives most of the differences\nbetween errors as statistically insignificant. As a\nresult, while Doubao may assign scores that follow\na similar trend to human judgments, using Doubao\nas a judge for NLG evaluation could lead to differ-\nent conclusions, as it regards many types of errors\nas statistically indistinguishable.\nThe unimodal LLMs, DeepSeek-V3 and\nDeepSeek-R1, exhibit a similar pattern to GPT-\n--- Page 8 ---\nHuman Uni. Human Mul.\nPearson Spearman Pearson Spearman\nGPT-4o Uni. .7232 .7789 .5951 .6674\nGPT-4o Mul. .4315 .4259 .5992 .5062\nDoubao Uni. .7391 .8305 .6662 .7451\nDoubao Mul. .3652 .2357 .6801 .5439\nDeepSeek V3 .7762 .8246 .6700 .7663\nDeepSeek R1 .7561 .7492 .6902 .7529\nTable 4: The correlations between the quality scores\nassigned by humans and those assigned by LLMs in\nboth unimodal and multimodal conditions are presented.\n4o: the difference between age and colour errors is\ninsignificant and in the opposite direction to that of\nhumans. As aforesaid, this is likely because these\nmodels have not learned the specialised process-\ning mechanisms for colour that are present in the\nhuman brain, leading them to assign overly high\nquality scores to descriptions with colour errors.\nAll other differences align with the direction ob-\nserved in human judgments and are statistically\nsignificant.\n4.3 Correlation Analysis\nAnalogue to many studies about LLM-as-a-judge,\nwe were also curious about how well the quality\nassigned by LLMs aligns with that of humans in\nour controlled experiments. We, thus, computed\nthe Pearson and Spearman correlation coefficients\nnot only between scores by LLMs and humans in\nthe same condition, but also between scores by\nhumans in the multimodal condition and LLMs in\nthe unimodal condition to investigate whether one\ncan use unimodal LLMs (which is way cheaper\nthan multimodal LLMs) to evaluate multimodal\nNLG. Table 4 charts the correlation coefficients.4\nFor the two multimodal LLMs, Doubao aligns\nmore closely with humans than GPT-4o in both\nunimodal and multimodal conditions. As expected,\neach LLM aligns better with humans in the uni-\nmodal condition when evaluating unimodal inputs\ncompared to evaluating multimodal inputs. How-\never, it is surprising that the LLM in the unimodal\ncondition aligns better with humans in the multi-\nmodal condition than the LLM in the multimodal\ncondition itself. This may suggest that, when evalu-\nating multimodal NLG using the “LLM-as-a-judge”\n4The correlations between the quality scores assigned by\nLLMs in the multimodal condition and those assigned by\nhumans in the unimodal condition are crossed off, as using an\nLLM in the multimodal condition to replicate human rating\nbehaviours for unimodal NLG is practically irrelevant.paradigm, it might be more effective to provide the\nLLM with only the textual inputs.\nWhat is even more surprising is that the uni-\nmodal LLMs, DeepSeek-V3 and DeepSeek-R1,\nwhich have never been trained with multimodal in-\nputs, align better with humans than the multimodal\nLLMs in both unimodal and multimodal condi-\ntions. The results also indicate that DeepSeek-V3\noutperforms DeepSeek-R1, suggesting that reason-\ning ability does not necessarily enhance an LLM’s\ncapacity to evaluate NLG outputs.\n5 Discussion: Which LLM is the “Best”?\nSo far, we have replicated the human experiments\non multiple LLMs and analysed these models from\nvarious perspectives. The final question is which\nLLM is best at handling error severity in NLG. The\nanswer to this question is two-fold: first, which\nLLM most accurately replicates human behaviours,\nand second, which LLM is best suited to serve as\nan evaluator for NLG tasks, particularly for image\ncaptioning.\nRegarding the first question, the winner is\nDoubao. It is the only LLM that fully replicates the\nranking of error severity observed in humans. All\nother models assign high scores to colour errors,\nlikely because they have not learned the specialised\nprocessing mechanisms for colour features.\nHowever, using Doubao as an NLG evaluator\nmay lead to very different conclusions compared to\nhuman evaluation. This is because Doubao tends\nto judge different types of errors as equally se-\nvere. The best choice for an NLG evaluator is\nDeepSeek-V3. As a unimodal model, it achieved\nthe highest correlation with human judgments in\nboth the unimodal and multimodal conditions. It\nis the most cost-effective LLM among those we\nexamined. The only drawback is that it may strug-\ngle when many colour errors are present in NLG\noutputs.\n6 Conclusion\nThis study investigates whether large language\nmodels (LLMs) judge the severity of errors in im-\nage descriptions in a manner consistent with human\nevaluators. Building upon the framework of van\nMiltenburg et al. (2020), we designed a controlled\nexperiment comparing human and LLM assess-\nments across unimodal and multimodal conditions.\nOur findings show that while humans demonstrate\nclear and nuanced severity distinctions across dif-\n--- Page 9 ---\nferent error types, especially influenced by visual\ncontext in the case of colour and type errors, LLMs\nstruggle to fully replicate these patterns.\nInterestingly, several LLMs, including GPT-4o\nand the DeepSeek models, assign lower scores to\ngender errors but surprisingly high scores to colour\nerrors, diverging from human evaluators. This dis-\ncrepancy may stem from the fact that humans per-\nceive colour errors as severe due to specialised\nneural mechanisms for processing colour, whereas\ngender errors are judged more harshly due to so-\ncially embedded norms. The LLMs appear to have\ninternalised aspects of social norms but lack the\nperceptual grounding needed to emulate the human\nsensitivity to colour-related inaccuracies.\nAmong the evaluated models, Doubao was the\nonly LLM to reproduce the human-like severity\nranking, though it did not consistently distinguish\nseverity across error types. In contrast, DeepSeek-\nV3 emerged as the best overall LLM for serving\nas an NLG evaluator. It achieved the highest cor-\nrelation with human judgments across both uni-\nmodal and multimodal inputs, and it did so with\ngreater cost efficiency than the multimodal models.\nDespite overestimating the acceptability of colour\nerrors, DeepSeek-V3 demonstrated the most con-\nsistent alignment with human assessments.\nLimitations\nOne limitation of this study is that it includes\ntwo closed-source LLMs, Doubao and GPT-4o,\nwhose underlying model architectures and eval-\nuation pipelines are not publicly documented. This\nraises the possibility that different internal models\nor processing strategies may be used for unimodal\nand multimodal inputs, potentially affecting the va-\nlidity of some interaction effect tests in this study.\nNevertheless, this limitation does not undermine\nthe methodology we propose. Our experimental\nframework remains broadly applicable and can be\nreadily used to evaluate other multimodal LLMs in\ncontrolled settings, particularly when the models\ncan be deployed locally with full transparency.\nReferences\nEdward H Adelson. 1993. Perceptual organiza-\ntion and the judgment of brightness. Science ,\n262(5142):2042–2044.\nIlaha Ashrafova. 2024. Language and gender: Explor-\ning structures and bias in linguistic norms. Acta\nGlobalis Humanitatis et Linguarum , 1(1):39–50.Simone Balloccu, Patrícia Schmidtová, Mateusz Lango,\nand Ondrej Dusek. 2024. Leak, cheat, repeat: Data\ncontamination and evaluation malpractices in closed-\nsource LLMs. In Proceedings of the 18th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers) , pages 67–93, St. Julian’s, Malta. Association\nfor Computational Linguistics.\nEllen Gurman Bard, Dan Robertson, and Antonella So-\nrace. 1996. Magnitude estimation of linguistic ac-\nceptability. Language , pages 32–68.\nAnna Bavaresco, Raffaella Bernardi, Leonardo Berto-\nlazzi, Desmond Elliott, Raquel Fernández, Albert\nGatt, Esam Ghaleb, Mario Giulianelli, Michael\nHanna, Alexander Koller, and 1 others. 2024. Llms\ninstead of human judges? a large scale empirical\nstudy across 20 nlp evaluation tasks. arXiv preprint\narXiv:2406.18403 .\nGuanyi Chen, Kees van Deemter, and Chenghua Lin.\n2019. Generating quantified descriptions of abstract\nvisual scenes. In Proceedings of the 12th Interna-\ntional Conference on Natural Language Generation ,\npages 529–539, Tokyo, Japan. Association for Com-\nputational Linguistics.\nLouise Connell. 2007. Representing object colour in\nlanguage comprehension. Cognition , 102(3):476–\n485.\nMark J Fenske, Elissa Aminoff, Nurit Gronau, and\nMoshe Bar. 2006. Top-down facilitation of visual\nobject recognition: object-based and context-based\ncontributions. Progress in brain research , 155:3–21.\nMingqi Gao, Xinyu Hu, Xunjian Yin, Jie Ruan, Xiao\nPu, and Xiaojun Wan. 2025. Llm-based nlg evalua-\ntion: Current status and challenges. Computational\nLinguistics , pages 1–28.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan\nShen, Shengjie Ma, Honghao Liu, and 1 others.\n2024. A survey on llm-as-a-judge. arXiv preprint\narXiv:2411.15594 .\nDouglas Guilbeault, Solène Delecourt, Tasker Hull,\nBhargav Srinivasa Desikan, Mark Chu, and Ethan\nNadler. 2024. Online images amplify gender bias.\nNature , 626(8001):1049–1055.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint\narXiv:2501.12948 .\nSture Holm. 1979. A simple sequentially rejective multi-\nple test procedure. Scandinavian journal of statistics ,\npages 65–70.\n--- Page 10 ---\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, and 1\nothers. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276 .\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out , pages 74–81.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer vision–\nECCV 2014: 13th European conference, zurich,\nSwitzerland, September 6-12, 2014, proceedings,\npart v 13 , pages 740–755. Springer.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, and 1 others.\n2024a. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437 .\nDonglin Liu, Lijuan Wang, and Ying Han. 2024b. Men-\ntal simulation of colour properties during language\ncomprehension: influence of context and comprehen-\nsion stages. Cognitive Processing , 25(4):587–600.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using gpt-4 with better human align-\nment. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 2511–2522, Singapore. Association for Com-\nputational Linguistics.\nAdian Liusie, Potsawee Manakul, and Mark Gales. 2024.\nLLM comparative assessment: Zero-shot NLG eval-\nuation through pairwise comparisons using large lan-\nguage models. In Proceedings of the 18th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 139–151, St. Julian’s, Malta. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics , pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nKarin S Pilz and Hao Lou. 2022. Contextual and own-\nage effects in age perception. Experimental Brain\nResearch , 240(9):2471–2480.\nJohn W Tukey. 1949. Comparing individual means in\nthe analysis of variance. Biometrics , pages 99–114.\nEmiel van Miltenburg and Desmond Elliott. 2017.\nRoom for improvement in automatic image de-\nscription: an error analysis. arXiv preprint\narXiv:1704.04198 .Emiel van Miltenburg, Wei-Ting Lu, Emiel Krahmer,\nAlbert Gatt, Guanyi Chen, Lin Li, and Kees van\nDeemter. 2020. Gradations of error severity in auto-\nmatic image descriptions. In Proceedings of the 13th\nInternational Conference on Natural Language Gen-\neration , pages 398–411, Dublin, Ireland. Association\nfor Computational Linguistics.\nNi Xuanfan and Li Piji. 2023. A systematic evaluation\nof large language models for natural language gen-\neration tasks. In Proceedings of the 22nd Chinese\nNational Conference on Computational Linguistics\n(Volume 2: Frontier Forum) , pages 40–56, Harbin,\nChina. Chinese Information Processing Society of\nChina.\nA Examples of Each Error Type\nFor the image in Figure 1, we have the following\nreference description and erroneous descriptions:\n•一位男人穿着黄色上衣在网球场打网球。\n‘A man in a yellow shirt plays tennis on the\ntennis court.’\n•Gender Error: 一位女人穿着黄色上衣在网\n球场打网球。\n•Age Error: 一位男孩穿着黄色上衣在网球\n场打网球。\n•Type Error: 一位男人穿着黄色大衣在网球\n场打网球。\n•Colour Error: 一位男人穿着紫色上衣在网\n球场打网球。\nB Criteria for Image Selection\n1. They should be full-colour images.\n2.There should be a human protagonist, with\ntheir face and at least half their body visible.\n3.The content of the images should be clearly\nrecognisable.\n4.Each clothing item should have a single\ncolour.\n5. Clothing items should have different colours.\nC Prompts for the LLM Experiment\nFor unimodal stimuli, we used the prompt in Ta-\nble 5 to guide both unimodal and multimodal LLMs\nin rating these stimuli. For multimodal stimuli, we\nused the prompt in Table 6 to instruct multimodal\nLLMs to rate the stimuli.\n--- Page 11 ---\n你是一位图像描述系统的质量评估专家。\n现在你将看到两段文字：\n第一段是某张图像的正确描述（作为参考）\n第二段是图像描述系统生成的待评估描述\n请你根据参考描述的内容，对系统生成的描述在整体\n质量上的表现进行主观评分（1-100分）。\n打分时请考虑描述与参考描述之间在语义上的接近\n程度、信息覆盖、表达合理性等因素，但不需细分维\n度。\n请严格按照以下格式作答：\n分数：[0-100]\n理由：[打分的理由]\n输入：\n正确描述：{Reference Description}\n待评估描述：{Description}\nTable 5: Prompt used for Instructing LLMs to rate\nUnimodal Stimuli.\n你是一位图像描述系统的质量评估专家。\n现在你将看到一张图片和系统为这张图片生成的一段\n文字描述。\n请你从专业角度出发，依据整体质量对这段描述进\n行0到100分的打分。\n评分标准仅基于你对描述整体质量的主观判断，而不\n需要从多个维度进行细分分析。\n请严格按照以下格式作答：\n分数：[0-100]\n理由：[打分的理由]\n描述：{Description}\n{Image}\nTable 6: Prompt used for Instructing LLMs to rate\nMultimodal Stimuli.",
  "text_length": 40975
}