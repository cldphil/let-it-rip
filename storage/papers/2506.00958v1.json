{
  "id": "http://arxiv.org/abs/2506.00958v1",
  "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning\n  Nonverbal Cues from Video-Grounded Dialogues",
  "summary": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input.",
  "authors": [
    "Youngmin Kim",
    "Jiwan Chung",
    "Jisoo Kim",
    "Sunghyun Lee",
    "Sangkyu Lee",
    "Junhyeok Kim",
    "Cheoljong Yang",
    "Youngjae Yu"
  ],
  "published": "2025-06-01T11:07:25Z",
  "updated": "2025-06-01T11:07:25Z",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.00958v1",
  "full_text": "--- Page 1 ---\nSpeaking Beyond Language: A Large-Scale Multimodal Dataset for\nLearning Nonverbal Cues from Video-Grounded Dialogues\nYoungmin Kim♠*Jiwan Chung♠∗Jisoo Kim♠Sunghyun Lee♠\nSangkyu Lee♠Junhyeok Kim♠Cheoljong Yang♣Youngjae Yu♠\n♠Yonsei University♣NC Research, NCSOFT Corporation\nwinston1214@yonsei.ac.kr\nAbstract\nNonverbal communication is integral to human\ninteraction, with gestures, facial expressions,\nand body language conveying critical aspects\nof intent and emotion. However, existing large\nlanguage models (LLMs) fail to effectively in-\ncorporate these nonverbal elements, limiting\ntheir capacity to create fully immersive con-\nversational experiences. We introduce MARS ,\na multimodal language model designed to un-\nderstand and generate nonverbal cues alongside\ntext, bridging this gap in conversational AI. Our\nkey innovation is VENUS , a large-scale dataset\ncomprising annotated videos with time-aligned\ntext, facial expressions, and body language.\nLeveraging VENUS , we train MARS with a next-\ntoken prediction objective, combining text with\nvector-quantized nonverbal representations to\nachieve multimodal understanding and genera-\ntion within a unified framework. Based on vari-\nous analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our\nquantitative and qualitative results demonstrate\nthatMARS successfully generates text and non-\nverbal languages, corresponding to conversa-\ntional input. Our dataset and code are available\nat https://github.com/winston1214/nonverbal-\nconversation.\n1 Introduction\nHuman conversations are a complex interplay of\nverbal and nonverbal-cues. Beyond spoken words,\nfacial expressions, gestures, and body language\nplay an integral role in conveying emotions, inten-\ntions, and subtle meanings (Phutela, 2015). For\ninstance, “Do you know what time it is?” with a\nneutral expression seeks information, while a frown\nand crossed arms imply a rebuke. These nonverbal\nelements are essential for creating rich and nuanced\ninteractions.\nRecent advancements in large language mod-\nels (LLMs) have resulted in conversational agents\n*Equal contribution.that closely resemble human interactions in written\nform. However, these models are still predomi-\nnantly limited to text-based communication, over-\nlooking the crucial role of nonverbal expressions.\nAlthough recent works (Ng et al., 2022; Park et al.,\n2024) have made strides in addressing this gap, they\nhave primarily concentrated on facial expressions,\nneglecting the broader spectrum of body language,\nwhich is essential for more realistic and immersive\ncommunication.\nA major challenge in developing multimodal\nconversational agents lies in the lack of large-\nscale training datasets. Existing video conversation\ndatasets are either limited in scale or lack anno-\ntated nonverbal cues, as summarized in table 1.\nTo address this, we introduce VENUS (VidEo with\nNonverbal cues and Utterance Set), a novel corpus\ndesigned for multimodal conversations with nonver-\nbal annotations. VENUS consists of 10-minute clips\nfrom dialogue-rich podcasts featuring two-person\ninteractions, carefully curated to ensure accurate\nspeaker diarization and motion tracking. Transcrip-\ntions were generated using Speech-to-Text (STT)\nmodels, while pseudo-3D motion parameters were\nextracted and annotated separately for facial ex-\npressions and body gestures, providing a detailed\nresource for aligning verbal and nonverbal cues.\nUsing VENUS , we develop MARS ,Multimodal\nlAnguage Model with nonve Rbal-cue S, a multi-\nmodal conversational agent capable of understand-\ning and generating nonverbal cues alongside textual\ncontext in dialogues. Nonverbal cues, such as facial\nexpressions and body movements, are represented\nas discrete latent tokens, compressed using VQ-\nV AE (Van Den Oord et al., 2017). Both textual and\nnonverbal tokens are trained jointly with a unified\nnext-token prediction objective, enabling natural\nmodeling of multimodal dialogues within a single\nframework.\nWe conduct extensive quantitative and qualita-\ntive analyses to evaluate the contributions of VENUSarXiv:2506.00958v1  [cs.AI]  1 Jun 2025\n--- Page 2 ---\nandMARS to multimodal dialogue modeling. First,\nwe examine the distributional diversity of nonver-\nbal elements in VENUS (section 4). Next, we assess\nthe trade-off between compression efficiency and\nreconstruction quality of nonverbal token discretiz-\ners in section 5.2. Finally, we evaluate the multi-\nmodal conversational modeling capabilities of the\nMARS LLM in section 5.3.\nOur key contributions are as follows:\n•Introduction of VENUS , the first large-scale\nmultimodal conversational dataset designed\nfor modeling nonverbal expressions.\n•Development of MARS , a multimodal conversa-\ntional agent leveraging VENUS to enable both\nthe understanding and generation of nonverbal\nexpressions within dialogue contexts.\n•Comprehensive experimental validation,\ndemonstrating the effectiveness of multimodal\ntokens in MARS for producing natural and\ncontextually aligned nonverbal expressions\nalongside text, supported by user studies,\nquantitative evaluations, and qualitative\nanalyses.\n2 Related Works\nMultimodal Large Language Models. Recent\nstudies have introduced models that combine vari-\nous modalities with large language models (LLMs),\nextending their capabilities beyond text to in-\nclude visual, auditory, and multimodal reason-\ning. Specifically, to enhance visual comprehen-\nsion capabilities of LLMs, LLaV A (Liu et al.,\n2024b), Qwen-VL (Bai et al., 2023) and MiniGPT-\n4 (Chen et al., 2023) have successfully integrated\nvision encoders into pre-trained LLMs. Further-\nmore, VideoChat (Li et al., 2023) and Video-\nLLaMA (Zhang et al., 2023a) extend these ca-\npabilities to video understanding, while models\nsuch as Unified-IO-2 (Lu et al., 2024) and GPT-4-\nO (Achiam et al., 2023) expand the scope to include\nauditory modalities, showing robust multimodal\nreasoning across various inputs.\nLearning Dialogue in Video. The importance\nof analyzing conversational sentiment using mul-\ntimodal data ( e.g., text, audio, and visual) from\nvideos has driven the development of numerous\ndatasets (Busso et al., 2008; Zadeh et al., 2018;\nPoria et al., 2019). This has further spurred re-\nsearch into generating and understanding dialoguesfrom videos, leveraging multimodal cues. For in-\nstance, Champagne (Han et al., 2023) introduced\nthe YTD-18M dataset for dialogue generation us-\ning visual signals and LLMs, while MultiDia-\nlog (Park et al., 2024) combined audio and visual\ndata for generating conversations. Beyond text,\nefforts like (Shafique et al., 2023) and Emotion-\nCLIP (Zhang et al., 2023c) focus on recognizing\nnonverbal cues, such as gestures and emotions. Ad-\nditionally, works like FurChat (Cherakara et al.,\n2023) and (Lee et al., 2023) explore applying non-\nverbal signals to enhance robotic facial expres-\nsions and actions. However, existing conversational\ndatasets are often limited in scale or fail to include\ndetailed 3D facial and body language information\nnecessary for modeling nonverbal cues effectively.\nOur VENUS dataset addresses these gaps by being\nboth large-scale and scalable, offering comprehen-\nsive conversational data that integrates not only text\nbut also 3D facial expressions and body languages.\nThis enables a more nuanced understanding of non-\nverbal cues and supports the generation of richer,\ncontext-aware conversations.\nHuman Motion Synthesis in Conversation. Re-\ncent advancements in 3D human reconstruc-\ntion (Lin et al., 2023; Dwivedi et al., 2024; Dan ˇeˇcek\net al., 2022) have significantly improved the qual-\nity of pseudo-ground truth data, providing a scal-\nable and accessible alternative to traditional sensor-\nbased methods (Yi et al., 2023). Leveraging these\ndatasets, recent works (Wu et al., 2024; Lu et al.,\n2023b) have focused on generating human motions\nfrom text. Building on this progress, our work\nutilizes pseudo labels derived from our VENUS,\nwhich addresses the lack of large-scale dataset for\nconversational settings. Unlike previous works\nlike (Ng et al., 2023, 2022), which primarily gener-\nate listener facial motions from text, our approach\nextends to produce text, facial expressions, and\nbody language, aligned with conversational con-\ntext.\n3 Learning Real-World Conversation\nwith Nonverbal-Cues\nPrevious studies have primarily focused on dia-\nlogue models and datasets that consider either text\nalone or text along with facial expressions. How-\never, real conversations rely on both facial expres-\nsions and body gestures, utilizing the whole body\nfor effective communication. To address this gap,\nwe propose a dialogue model, MARS , for realistic\n--- Page 3 ---\nP2) Extract T ranscriptionF3) Ar e ther e 2 people?F1) Is ther e people in a thumbnail image?P1) Segment by 10 mins & remove first minuteF2) Over than 10 mins?00:50:3700:20:37Speaker 0Speaker 1EnFr&EnEnEnEn\n01:00~1 1:0001:00~1 1:0001:00~1 1:001 1:00~21:00......................\n.........................01:04:4600:56:55\n(a) Data Collection and Filtering(b) ASR T ranscripts(c) Identifying Speaker(d) Extracting Nonverbal-CuesPodcast V ideos\n41:00~51:0001:00~1 1:00...\n31:00~41:00F4) Is it English?\nIt just happens. Because ...That’ s what that is. It’ s ...\nSpeaker 0Frame indexFrame indexSpeaker 0\nSpeaker 1Speaker 1\n00:51:00~01:01:00\nThisisabigdealSpeaker 0Speaker 1\nY eah,realvalidation(sec)(sec)ItItjusthappens3.53.5871281301351471519 21051 124.511.233.472.354.255.126 .054.51justThat’ swhatthatis\nP3)  Detect active speaker\nP4)  Crop & Align speakerF 5 ) Ar e spea k ers visible?\nP5)  Extract Body Parameter\nP6)  Extract Face Parameter\n00:0 8 :13\nhappensThat’ s5.126 .05whatisthat......\nFigure 1: Overview of VENUS collection pipeline. (a) and (b) use only audio information, while (c) and (d)\nalso utilize visual information. The blue boxes contain filtering criteria ( F), and the yellow boxes pertain to the\nprocessing steps ( P). The final box shown in (d) represents the facial expression and body language combined and\nrepresented using SMPL-X parameters. For more details, refer to the Section 3.1.\ninteractions. Since no existing dataset simultane-\nously aligns text, facial expressions, and body lan-\nguage, we constructed a large-scale dataset, VENUS ,\nin which text, facial expressions, and body lan-\nguage are aligned in the wild.\n3.1 VENUS : Video with Nonverbal-Cues and\nUtterance Set\nIn this section, we introduce our pipeline to col-\nlectVENUS , which is outlined in Figure 1. Further\ndetails can be found in Appendix A.\nData Collection and Filtering. We collected\nYouTube podcast videos to learn nonverbal ex-\npressions included in conversations. Our goal was\nto efficiently extract and collect extensive conver-\nsation data from YouTube videos with only two\npeople conversing. We followed the filtering pro-\ncess presented in (Han et al., 2023; Zellers et al.,\n2021a). Initially, we screened thumbnails using a\nlightweight detector model (Jocher et al., 2023) to\ncheck for the presence of people, discarding videos\nwithout any people in the thumbnails ( F1). We\nthen removed the first minute to eliminate opening\nmusic or other introductory content ( P1). Subse-\nquently, to maximize the extraction of information\nfrom each video, we segmented each video into\n10-minute segments and discarded any segments\nshorter than 10minutes ( P1 & F2 ). In this step, we\nset the frames per second (FPS) at 25.\nAutomatic Speech Recognition Transcripts. To\ntrain the conversational model, we collected videos\nfeaturing interactions between two speakers. Weonly downloaded audio to collect and filter videos,\nwhich is a cost-effective strategy. Using PyAn-\nnote (Bredin et al., 2020), we performed speech\ndiarization to identify videos with precisely two\nspeakers and discarded videos without exactly two\nspeakers ( F3).\nNext, we utilized the state-of-the-arts speech-to-\ntext model, WhisperX (Bain et al., 2023), to filter\nand retain only English videos ( F4). For these se-\nlected videos, we leveraged WhisperX to generate\ntime-aligned speech transcripts ( P2). By align-\ning the results predicted by the two models, we\nextracted the speaker’s transcript at the word, se-\nquence, and utterance levels.\nIdentifying Speakers in Video. To effectively ex-\ntract verbal and nonverbal features from videos, it\nis crucial to distinguish between the speaker and\nthe listener. To achieve this, we utilized the Light-\nASD (Liao et al., 2023) active speaker detection\nmodel to identify speakers within the video ( P3).\nAdditionally, we integrated a pretrained person de-\ntector model (Jocher et al., 2023) to extract visual\nfeatures associated with each speaker. Here, we can\nextract frames with the speaker and their bounding\nbox coordinates. If the number of predicted speaker\nframes is less than the more number of predicted\nwords from WhisperX, we consider it to lack visual\nvariation and discard it ( F5). Then, we cropped the\nspeaker’s image, f, using the detected speaker’s\nbounding boxes. To handle cases where multiple\nspeakers are speaking simultaneously, we used a\nlightweight model (Sandler et al., 2018) to extract\n--- Page 4 ---\nthe features of each speaker and align the speaker’s\nimages by comparing them with previous frames\nbased on cosine similarity ( P4). The specific steps\nof this process are detailed in the Appendix A.3.\nTo align the text and the speaker’s frames, we\nsegmented the speech into utterances in a video.\nThen, using the time and FPS of the speaker’s\nvideo, we calculate the set of frames for each ut-\nterance, Uj={f1, f2,···, fi}. Through this cal-\nculation, we can construct a set of uutterances,\nU= [U]u\nj=1, for each video.\nExtracting Nonverbal-Cues. We represent nonver-\nbal cues as 3D parameters and, following the previ-\nous approaches (Lin et al., 2024; Liu et al., 2024a),\nextract facial parameters using the FLAME (Li\net al., 2017) and body and hand gesture param-\neters using the SMPL-X (Pavlakos et al., 2019).\nTo achieve this, we used EMOCA-v2 (Lu et al.,\n2023a) for facial expression and OSX (Lin et al.,\n2023) for the whole body, extracting the param-\neters Mf\nj={mf\nl}|Uj|\nl=1where, mf\nl∈R156and\nMb\nj={mb\nl}|Uj|\nl=1where, mb\nl∈R179, respectively\n(P5&P6). Finally, we annotated the video with\nnonverbal expressions, represented as 3D parame-\nters that are aligned with the text for each utterance.\n3.2 Nonverbal-Cues Quantization\nIn this section, we introduce the tokenization pro-\ncess for large-scale collected nonverbal expressions\nfrom VENUS , as illustrated in Figure 2-(a).\nNotation and Problem Setup. We denote the se-\nquence parameters of face and body movement\nat the utterance level as Mf\nj={mf\nl}|Uj|\nl=1and\nMb\nj={mb\nl}|Uj|\nl=1, respectively. We represent the\nfacial components using the expression ( ψ) and jaw\nparameters ( θjaw), resulting in |ψ|+|θjaw|= 53 di-\nmensions per frame (i.e., 50 expression parameters\nand 3 jaw pose parameters). Similarly, for body lan-\nguage, we focus on the upper body ( θubody), and the\nleft and right hands ( θlhand, θrhand) This represen-\ntation results in |θubody|+|θrhand|+|θlhand|= 117\ndimensions per frame (i.e., 27 upper body parame-\nters and 45 left and right hand parameters, respec-\ntively). These are expressed as a sequence of W\nframes, and to ensure smoothness, we apply the\nSavitzky–Golay method (Gorry, 1990) to the se-\nquence. Therefore, the sequence of face and body\nparameters follows:\nˆMf\nj={ˆmf\nl}W\nl=1ˆMb\nj={ˆmb\nl}W\nl=1, (1)\nwhere ˆmf\nl= [ψl, θjaw\nl]∈RW×53andˆmb\nl=[θubody, θrhand, θlhand]∈RW×117.\nArchitecture. To enable the conversational model,\nspecifically the LLM, to understand nonverbal cues,\nwe need to quantize continuous nonverbal features\ninto discrete tokens. To discrete tokenize nonverbal-\ncues, we adopted the architecture based on VQ-\nV AE (Van Den Oord et al., 2017; Razavi et al.,\n2019), which consists of an encoder-quantizer-\ndecoder framework, to achieve this tokenization\nof nonverbal cues. For the purposes of this expla-\nnation, we will denote both input values ˆmf\nland\nˆmb\nlasml∈RW×dwhere dis the length of the\nparameters, which can be either 53or117.\nIn this framework, the encoder, E, and de-\ncoder, D, are convolution networks with down-\nsample ratio q, the quantizer contains a codebook\nZ ∈RK×C, where Kdenotes the codebook size\nandCrepresents codebook dimension. In the en-\ncoder process, when the sequence vector m1:Wis\ninput, it is downsampled to obtain latent vector z,\nwhich follows:\nE(m1:W)→z∈RC×τwhere, τ=W\nq.(2)\nGiven the latent vector zand the quantizer Q(·;Z),\nthe quantized vector ˆzis determined as:\nˆ z=Q(z;Z) = arg min\nek∥z−ek∥2\n2, (3)\nwhere ekdenotes the k-th embedding in the code-\nbookZ. To stabilize training, we employ expo-\nnential moving averages (EMA) based codebook\nupdates following (Zhang et al., 2023b; Guo et al.,\n2024). The quantized vector ˆzis the element se-\nlected from the codebook that minimizes the recon-\nstruction error with respect to z. During decoder\nprocess, the quantized latent vector ˆ zundergoes up-\nsampling process to reconstruct the original input\nsequence vector m1:W.\nD(ˆ z)→ˆm1:W∈Rd. (4)\nBased on this architecture, we developed models\nfor facial and body language, designated as Face\nVQ-V AE and Body VQ-V AE, respectively.\nTraining losses. We train Face VQ-V AE and\nBody VQ-V AE with the following loss functions\nLfaceandLbody, respectively:\nLface=Lvq+λf\nreconLf\nrecon +λf\nvelLf\nvel\nLbody=Lvq+λb\nreconLb\nrecon +λb\nvelLb\nvel(5)\n--- Page 5 ---\nI was just wondering \nbecause …Face \nDecoderMotion \nDecoder\nHello World, How are you?. . .Text Token Motion Token\n. . .\n(b) MARSAlign by time\nFace Token\n. . .\n(a) Nonverbal -Cues QuantizationFace Codebook\nQuantizer\nDownsampling\nBlockConv\nEncoderFace Encoder\nUpsampling\nBlockConv\nDecoderFace Decoder\nDownSampling\nBlockConv\nEncoderMotion Encoder\nUpsampling\nBlockConv\nDecoderMotion DecoderQuantizerMotion Codebook\nText Codebook\n...Huh? Where'd you get that fact?. . .. . .. . .\n. . .. . .. . .\nUser\nMARS\n< SOT >Text\ntokenFace\ntokenMotion\ntokenFace\ntokenMotion\ntoken. . .Text\ntoken< EOT >Text\ntokenText\ntokenMotion\ntoken< SOT >Text\ntokenFace\ntokenMotion\ntokenFace\ntokenMotion\ntoken. . . < EOT >Text\ntokenText\ntokenText\ntokenMotion\ntokenI was just wondering because …\nAssistant\nFigure 2: System overview . Our system consists of two main parts: (a) the VQ-V AE model trained to quantize\nnonverbal cues, and (b) a MARS trained to process quantized nonverbal expressions alongside text. The output\ngenerated by the assistant is visualized by replacing both face and body parameters with SMPL-X.\nFor codebook learning, we use commitment loss,\nLvq, in the proposed (Van Den Oord et al., 2017).\nLvq=β||z−sg(ˆ z)||2\n2, (6)\nwhere sg(·)is a stop gradient operation and βis\ncommitment loss weight.\nFirst, we introduce Lf\nrecon for the training of\nFace VQ-VAE . For training face features recon-\nstruction, the expression components ψland jaw,\nθjaw\nlare separated, and each part is calculated, re-\nspectively. It follows:\nLf\nrecon =λψ\nreconL1(ψl,ˆψl)\n+λjaw\nreconL1(θjaw\nl,ˆθjaw\nl).(7)\nNext, to preserve the temporal continuity and natu-\nral dynamics of facial motion, we design a facial\nmotion velocity loss, Lf\nvel, as follows:\nLf\nvel=L1(v(ψl), v(ˆψl))\n+λθL1(v(θjaw\nl), v(ˆθjaw\nl)).(8)\nHere, the function v(p)computes the temporal ve-\nlocity of a sequence pby taking the frame-wise\ndifference:\nv(pl) =pl+1−pl. (9)\nSimilarly, the training objectives for the Body\nVQ-V AE ,Lb\nrecon , is defined similarly to those usedin the Face VQ-V AE model. For motion recon-\nstruction, each component is calculated separately\nasLb\nrecon = Σi\nbodyL1(θi−ˆθi)where, body∈\n{ubody, rhand, lhand }.\n3.3 MARS : Multimodal Language Model with\nNonverbal-Cues\nUsing the quantized codebooks from Face VQ-\nV AE and Body VQ-V AE, the generation of text\nand nonverbal-cues sequences relies on their re-\nspective decoders and quantized representations.\nPrevious studies typically follow an auto-regressive\napproach; however, this cannot be directly applied\nwhen utilizing two codebooks. Inspired by meth-\nods proposed in studies that involve multiple code-\nbooks (Lu et al., 2023b), we propose MARS , a mul-\ntimodal language model with nonverbal-cues, de-\nsigned to predict hierarchical discrete codes that\ncapture nonverbal cues effectively. This is illus-\ntrated Figure 2 - (b).\nTraining. TheMARS is designed with the Trans-\nformer (Vaswani, 2017) architecture, where the\ninput consists of textual tokens paired with cor-\nresponding nonverbal tokens. The code indices\ncorresponding to the facial expression and body\nlanguage parameter sequences, ˆMf\njand ˆMb\nj, are\ndenoted as Xf= [xf\n1,xf\n2,···xf\nW/q]andXb=\n[xb\n1,xb\n2,···xb\nW/q], respectively. Thus, the input to-\nkens are composed of three elements: the word\ntokens Xw= [xw\n1,xw\n2,···,xw\nl], along with the\n--- Page 6 ---\nDataset # Dialogues # Turns Length (hrs) Text Video Nonverbal cues\nIEMOCAP (Busso et al., 2008) 151 7 ,333 12 ✓ ✓ ✗\nCMU-MOSEI (Zadeh et al., 2018) 3,228 - 65 ✓ ✓ ✗\nMELD (Poria et al., 2019) 1,433 13 ,708 13 .7 ✓ ✓ ✗\nYTD-18M (Han et al., 2023) 18M 54M∗30K∗✓ ✓ ✗\nMultiDialog (Park et al., 2024) 8,733 187 ,859 340 ✓ ✓ ✗\nBEAT (Liu et al., 2022) ✗ ✗ 76 ✓ ✗ ✓\nEMAGE (Liu et al., 2024a) ✗ ✗ 60 ✓ ✗ ✓\nTalkShow (Yi et al., 2023) ✗ ✗ 27 ✗ ✗ ✓\nOurs (VENUS) 89,459 1,114,328 14,910 ✓ ✓ ✓\nTable 1: Comparison of the VENUS dataset with the previous conversational and 3D gesture dataset. The first\nblock represents the conversation dataset, while the second block represents the gesture dataset. “*” represents an\nestimated value. For # Turns , it was calculated by multiplying the average number of utterances per video 3by the\nnumber of videos. The Length (hrs) was considered to be a maximum of 1 minute per video for the calculations.\nNonverbal cues indicate whether 3D data or any other annotations for facial expressions or body language are\nprovided. Best andsecond are highlighted. Our dataset is the largest conversational dataset with annotations of\nnonverbal cues.\nfacial and body code indices, XfandXb.\nGiven that we input and generate nonverbal-cues\ncorresponding to each word, the input sequences,\nT, are organized to align with their respective\ntimestamps.\nT={x|xi∈[\ncXc, c∈ {w, f, b}},(10)\nwhere the sequence is ordered as T=\n[xw\n1,xf\n1,xb\n1,xw\n2,···].\nTherefore, the word, face, and body token code\nindices prediction can be formulated as an auto-\nregressive prediction problem:\np(T) =lY\nj=1pθ(xw\nj|T<j)\nW/qY\nk=1h\npθ(xf\nk|T<k)·pθ(xb\nk|T<k)i\n,\n(11)\nwhere θrepresents the trainable parameters of the\nmodel. In this formulation, the word tokens are\npredicted first, followed by the face and body token\nindices.\n4 VENUS Dataset Analysis\nWe conducted data analysis to demonstrate the qual-\nity of the VENUS dataset. Additional analysis results\ncan be found in the Appendix A.\nStatistic. The summary statistics of our dataset\nand comparison with statistics from other conversa-\ntional and 3D gesture datasets are shown in Table 2Total number of collected channels 869\nTotal number of collected videos 27,128\nTotal number of collected nonverbal expressions 1B\nTotal number of dialogues 89,459\nTotal number of turns 1,114,328\nTotal number of sentences 7,118,654\nTotal of unique words 527,270\nAverage number of turns per dialogue 21\nAverage length of utterances per dialogue in words 170.829\nAverage length of utterances per dialogue in seconds 55.305\nAverage number of nonverbal expressions per utterance in frames 547\nTable 2: Summary of VENUS statistics. The “video”\nrefers to the video before it is segmented into 10-minute\nintervals, while “dialogues” refers to the conversations\nextracted from the videos segmented into 10-minute\nintervals.\nand Table 1, respectively. As shown in Table 2, our\ndataset is large-scale, featuring lengthy utterances\nwith numerous words and rich nonverbal expres-\nsions. Each conversation averages 21 turns, which\nsupports effective training for multi-turn dialogues.\nTable 1 highlights that, compared to existing video-\nbased multi-modal dialogue datasets, our dataset\nis the first to include annotations for nonverbal ex-\npressions. While YTD-18M (Han et al., 2023) has\nmore videos, its conversations are segmented into\nintervals of up to one minute, potentially hindering\ncontext comprehension. In contrast, VENUS despite\nhaving fewer videos, includes longer conversations,\nmaking it better suited for understanding extended\ndialogues. Furthermore, our dataset stands out as\nthe largest-scale 3D annotated dataset when com-\npared to previous 3D gesture datasets.\nDistribution of Nonverbal Cues. To analyze\nthe diversity of nonverbal expressions in our\ndataset, we sampled 10 random frames per video\n--- Page 7 ---\nFace Body\nVMSE ( 10−1)↓LVD ( 10−3)↓w-VL2 ( 10−7)↓Diversity ↑Variation ↑VMSE ↓LVD ( 10−1)↓w-VL2 ( 10−4)↓Diversity ↑Variation ( 10−1)↑\nGT 9.3323 0 .8760 2 .4189 0 .2803\n(Ng et al., 2023) 0.5787 0 .4422 0 .3832 7 .5866 0 .5873 2 .6424 0 .1268 0 .4338 2.0151 0 .1985\n(Guo et al., 2024) 0.5474 0 .4160 0 .3429 7 .7693 0 .6253 2 .0608 0 .0994 0 .2100 1 .9934 0 .1951\nOurs 0.5106 0.4020 0.2339 7.8430 0.6236 1.9946 0.0962 0.2027 1.9998 0.1956\nLreconL1 0.5106 0.4020 0.2339 7.8430 0.6236 1.9946 0.0962 0.2027 1.9998 0.1956\nL2 0.5471 0 .4124 0 .3630 6 .3334 0.6425 2.3384 0 .1139 0 .3078 1 .9732 0 .1879\nsmooth L1 0.4106 0 .4034 0 .3313 6 .3874 0 .6052 2 .3210 0 .1128 0 .2787 2 .0603 0.2025\nDim8 0.5106 0.4020 0.2339 7.8430 0.6236 2.0596 0 .0995 0 .2280 1 .9183 0 .1794\n16 0.5217 0 .4100 0 .2582 7 .6855 0 .6023 1.9946 0.0962 0.2027 1.9998 0.1956\n32 0.5294 0 .4150 0 .2439 7 .6986 0 .6006 2 .1199 0 .1022 0 .2192 1 .9838 0 .1926\n64 0.5152 0 .4071 0 .2360 7 .6203 0 .5890 2 .1577 0 .1037 0 .2312 1 .9947 0 .1942\n128 0.5222 0 .4153 0.2314 7.7554 0 .6098 2 .1427 0 .1037 0 .2244 1 .9633 0 .1876\n256 0.5296 0 .4183 0 .2443 7 .8247 0 .6212 2 .1410 0 .1034 0 .2387 1 .9936 0 .1939\nSize64 0.6628 0 .5181 0 .4472 6 .6604 0 .4566 4 .2495 0 .1993 0 .8084 0 .7093 0 .0306\n128 0.5770 0 .4514 0 .3549 7 .3002 0 .5458 2 .1905 0 .1054 0 .2670 1 .9114 0 .1801\n256 0.5313 0 .4184 0 .2583 7 .6053 0 .5890 2 .074 0 .1003 0 .2119 1 .9663 0 .1889\n512 0.5106 0.4020 0.2339 7.8430 0.6236 1.9946 0.0962 0.2027 1.9998 0.1956\nTable 3: Experimental results on Face VQ-VAE and Body VQ-VAE. “Lrecon ” represents Lf\nrecon andLb\nrecon ,\n“Dim” refers to the codebook embedding dimension, and “size” indicates the codebook size. Our key results are\nhighlighted. The Face VQ-V AE achieved the best performance with L1 loss, an embedding dimension of 8, and a\ncodebook size of 512, while the Body VQ-V AE performed best with L1 loss, an embedding dimension of 16, and\nthe same codebook size.\n(a) Distribution of facial expression (b) Distribution of body language\nFigure 3: Visualization of the distribution of\nnonverbal-cues . (a) Facial expression embeddings are\nwell-clustered despite the absence of emotion class la-\nbels, capturing meaningful emotion patterns. (b) Body\nlanguage embeddings are similarly well-clustered, rep-\nresenting common conversational gestures that enhance\ncommunication or naturally occur during dialogue. Rep-\nresentative examples are provided for each cluster.\nfrom approximately 1,000videos and applied T-\nSNE (Van der Maaten and Hinton, 2008) for di-\nmensionality reduction. In Figure 3, we display\nthe results by creating 7 clusters for facial expres-\nsions and 8 clusters for body languages using DB-\nSCAN (Ester et al., 1996).\nFigure 3-(a) displays the distribution of facial\nexpressions, covering both the ψandθjaw. We\ncan observe a variety of emotions, despite the ab-\nsence of emotion labels. Notably, the blue and\ngreen points appeared the most since podcast con-\nversations target to entertain or inform the view-\ners, leading to a larger portion of neutral and pos-\nitive expressions. In Figure 3-(b) the distribution\nof body language θubody,θlhandandθrhandis dis-\nplayed. The most common body language observed\ninvolves arms in a relaxed, lowered position, whichtypically reflects a conversational attitude. In addi-\ntion, gestures that enhance or clarify the speaker’s\nmessage, such as resting the chin on the hand or ex-\npressive hand movements, were frequently noted.\n5 Experiments\n5.1 Experiment Setup\nWe trained and evaluated our model using a subset\nof the VENUS dataset in our experiments Both\nVQ-V AE and MARS were trained on 3,924videos\nand69,412utterances. For evaluation, VQ-V AE\nused the full test set consisting of 997videos and\n30,390utterances, whereas MARS was evaluated on\na subset of 1,000utterances sampled from the test\nset.\n5.2 Nonverbal-cues Quantization\nEvaluation Metric. We quantitatively evaluate\nhow realistically facial expressions and body lan-\nguages have been quantized, based on evaluation\nmethods proposed in previous studies (Ng et al.,\n2022, 2023; Liu et al., 2024a). To this end, we\nadopt five metrics to assess the realism and diver-\nsity of facial expressions and body language. To\nevaluate realism, we use VMSE ,LVD , and win-\ndow Vertex L2 , while diversity is assessed using\ndiversity andvariance . Detailed explanations of\nthese metrics are provided in the Appendix B.2.\nResults. We conducted an ablation study to evalu-\nate our Face and Body VQ-V AE models, varying\none component at a time (Table 3). Based on the\nresults, we chose L1 loss for the Face VQ-V AE and\n--- Page 8 ---\nText Nonverbal\nPPL↓ BERT ↑METEOR ↑NLL-F ↓NLL-B ↓\nLLaMA 1Bzero-shot 5427.1 0 .811 0 .110 16 .232 17 .039\nMARS 1665 .8 0 .834 0 .130 8 .676 5 .330\nQwen 1.5Bzero-shot 3315.5 0 .823 0.116 15.019 15 .911\nMARS 2990 .0 0 .839 0.115 8.812 6 .144\nLLaMA 3Bzero-shot 5477.0 0 .818 0.136 16.504 17 .574\nMARS 926.9 0 .835 0.133 8.057 5 .325\nQwen 3Bzero-shot 56781 .1 0 .811 0.131 20.850 20 .874\nMARS 800.0 0 .839 0.123 7.295 4 .666\nTable 4: Quantitative results of MARS .↓means a\nlower score is better, ↑means a higher score is bet-\nter. Here, “NLL-F” and “NLL-B” denote the negative\nlog-likelihood (NLL) for face tokens and body tokens,\nrespectively. MARS demonstrates superior precision in\ngenerating nonverbal cues, highlighting its effectiveness\nin producing both text and nonverbal expressions.\nL1 loss for the Body VQ-V AE, with embedding\ndimensions of 8 and 16, respectively. Both used a\ncodebook size of 512. These settings outperformed\nprevious works (Ng et al., 2023; Guo et al., 2024).\n5.3 Semantic Evaluation for MARS\nTraining Settings. We employ LLaMA 3.2 In-\nstruct (Meta, 2024) and Qwen 2.5 Instruct (Yang\net al., 2024) as the large language model. To clarify\nthe model’s role, we incorporated a system prompt\nthat facilitates effective generation of both non-\nverbal and textual tokens. Additionally, since the\nnonverbal token is added as a special token, we per-\nformed supervised fine-tuning to ensure model’s\nunderstanding of them. Further details can be\nfound in the Appendix C.\nEvaluation metrics. To evaluate MARS , we sepa-\nrately assess the quality of its text and nonverbal\ntoken outputs, as ensuring accurate alignment be-\ntween these token types is inherently challenging.\nFirst, we use Perplexity ( PPL) as a general measure\nfor both text and nonverbal tokens. For text tokens,\nwe use BERT-score andMETEOR as evaluation\nmetrics, while for nonverbal tokens, we rely on\nNegative log-likelihood ( NLL ).\nQuantitative Results. We compared the quanti-\ntative performance of the LLM (Meta, 2024) and\nourMARS model. As shown in Table 4, the con-\nventional LLM model showed limitations in un-\nderstanding special tokens containing nonverbal\ninformation, failing to generate them properly. In\ncontrast, MARS , which was trained by interleaving\nnonverbal tokens within the textual input, achieved\nthe lowest perplexity and the highest BERTScore\nacross all model sizes, indicating its superior ability\nto generate semantically coherent dialogues. Fur-\nIt was all last minute as well because when I sent you the text,\nit was just the day after or it was the same day?\nDay after, right.  And that was about 14 days ago,\n and I'm still sort of trying to remember what day it was.\nGenerated \nFacial \nexpressionInput Text\nGenerated  \nText\nGenerated \nBody \nlanguageFrames\nGT Text Day after, right.Figure 4: Qualitative results for MARS .Qualitative re-\nsults showcasing inputs and outputs of our MARS model.\nInputs include the user’s text, face, and body language,\nwhile MARS outputs corresponding text, facial expres-\nsions, and body language. Underlined text indicates\nwhere MARS matches the ground truth (GT). Moreover,\nMARS produces improved text compared to GT and also\nsuccessfully generates corresponding facial and body\nlanguage aligned with the context.\nthermore, the significantly lower NLL scores for\nnonverbal cues demonstrate that MARS successfully\ncaptures and generates nonverbal behaviors. These\nresults not only validate the effectiveness of our\napproach in handling multimodal signals but also\nhighlight the scalability of MARS , as its performance\nimproves with larger model sizes in both textual\nand nonverbal generation tasks.\nQualitative Results. We use qualitative results to\nassess the effectiveness of our model in generating\nthe listener’s text and nonverbal expressions. As\nshown in Figure 4, our MARS not only aligns with\nthe ground-truth (GT) but also produces more con-\ntextually enriched text and corresponding face and\nbody languages. This demonstrates the qualitative\neffectiveness of our model in generating richer and\nmore expressive listener responses.\n6 Conclusion\nIn this work, we introduce VENUS , a video-based\nmultimodal conversation dataset designed to un-\nderstand and generate both text and nonverbal ex-\npressions, and present MARS . This language model\ncan produce both dialogue and corresponding non-\nverbal behaviors. The VENUS dataset is built from\nYouTube videos, including real conversational text\nand the accompanying nonverbal cues (such as fa-\ncial expressions and body language) annotated in\n--- Page 9 ---\n3D parameters. Using VENUS , our MARS model\nlearns to align and generate both textual and non-\nverbal elements, resulting in more engaging and\nnatural interactions. We believe that our VENUS\ndataset and MARS model will support a wide range\nof applications, such as virtual humans and gaming,\nby enabling the production of nonverbal behaviors\nin 3D.\n7 Limitations\nThis study explores the development of a large\nlanguage model (LLM) for generating nonverbal\ncues nameed MARS , supported by a custom dataset\nnamed VENUS designed to capture diverse nonver-\nbal communication patterns. While the proposed\napproach demonstrates promising results, certain\nlimitations remain that warrant further exploration.\nFirst, the VENUS dataset utilized in this research\nis primarily curated from the Podcast channel,\nwhich may limit the diversity of nonverbal expres-\nsion patterns in the data (e.g., crying or angry ex-\npressions). Furthermore, pseudo-labeling was em-\nployed in the dataset, which, while effective, could\nintroduce potential inaccuracies that require fur-\nther refinement. Additionally, not all data within\ntheVENUS dataset was utilized, leaving room for\nbroader exploration in future work. Second, the\nevaluation metrics used in this study, though ef-\nfective for assessing initial performance, may not\nfully capture the nonverbal communication. More\nsophisticated and comprehensive metrics are nec-\nessary to evaluate the system’s performance in real-\nworld scenarios.\nLooking ahead, future work will aim to address\nthese limitations by incorporating a wider range of\nnonverbal modalities, such as vocal expressions, to\nenrich the dataset and enhance the robustness of\nthe model. Moreover, we plan to develop advanced\nevaluation metrics that better reflect the complexity\nof nonverbal communication. These improvements\nwill further generalize and validate the applicability\nof our approach across diverse datasets and scenar-\nios.\n8 Ethical Considerations\nIn this paper, we introduce a large-scale multimodal\nconversational dataset named VENUS derived from\npublicly available YouTube videos. The dataset\nis designed to advance research in real-world con-\nversational understanding by including frames, re-\nconstructed facial expressions and body languageof the interlocutors. While this dataset provides\nvaluable insights for understanding conversational\nbehavior, it may raise privacy concerns as it cap-\ntures the visual and auditory cues of individuals. To\naddress these concerns, we follow ethical practices\nadopted by prior works (Zellers et al., 2021b, 2022;\nHan et al., 2023) and release only the video IDs in-\nstead of the raw video frames. Additionally, the re-\nconstructed face and body motions are represented\nas template meshes, ensuring anonymization and\npreventing direct identification of individuals. To\nfurther protect user privacy, future directions may\ninclude further anonymizing faces and improving\nmethods for deidentifying personal information.\nWe remain committed to respecting user privacy\nand ensuring compliance with ethical standards in\ndataset creation and usage.\nAcknowledgements\nThis work was supported by NCSOFT, the Na-\ntional Research Foundation of Korea (NRF) grant\nfunded by the Korea government (MSIT) (No. RS-\n2024-00354218), and the Institute of Information &\nCommunications Technology Planning & Evalua-\ntion (IITP) grants funded by the Korea government\n(MSIT) (No. RS-2024-00457882, AI Research\nHub Project; No. RS-2025-02263598, Develop-\nment of Self-Evolving Embodied AGI Platform\nTechnology through Real-World Experience).\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A versatile vision-\nlanguage model for understanding, localization, text\nreading, and beyond.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023. Whisperx: Time-accurate speech tran-\nscription of long-form audio. arxiv. arXiv preprint\narXiv:2303.00747 .\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\n--- Page 10 ---\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. Ad-\nvances in neural information processing systems , 13.\nHervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gre-\ngory Gelly, Pavel Korshunov, Marvin Lavechin,\nDiego Fustes, Hadrien Titeux, Wassim Bouaziz, and\nMarie-Philippe Gill. 2020. Pyannote. audio: neural\nbuilding blocks for speaker diarization. In ICASSP\n2020-2020 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n7124–7128. IEEE.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database. Language resources\nand evaluation , 42:335–359.\nJose Camacho-Collados, Kiamehr Rezaee, Talayeh\nRiahi, Asahi Ushio, Daniel Loureiro, Dimosthe-\nnis Antypas, Joanne Boisson, Luis Espinosa-Anke,\nFangyu Liu, Eugenio Martínez-Cámara, et al. 2022.\nTweetnlp: Cutting-edge natural language processing\nfor social media. arXiv preprint arXiv:2206.14774 .\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-\nghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. 2023. Sharegpt4v: Improving large multi-\nmodal models with better captions. arXiv preprint\narXiv:2311.12793 .\nNeeraj Cherakara, Finny Varghese, Sheena Shabana,\nNivan Nelson, Abhiram Karukayil, Rohith Kulothun-\ngan, Mohammed Afil Farhan, Birthe Nesset, Meriam\nMoujahid, Tanvi Dinkar, et al. 2023. Furchat: An em-\nbodied conversational agent using llms, combining\nopen and closed-domain dialogue with facial expres-\nsions. In Proceedings of the 24th Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue , pages 588–592.\nGlen Coppersmith and Erin Kelly. 2014. Dynamic word-\nclouds and vennclouds for exploratory data analysis.\nInProceedings of the Workshop on Interactive Lan-\nguage Learning, Visualization, and Interfaces , pages\n22–29.\nRadek Dan ˇeˇcek, Michael J Black, and Timo Bolkart.\n2022. Emoca: Emotion driven monocular face cap-\nture and animation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition , pages 20311–20322.\nSai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng,\nand Michael J Black. 2024. Tokenhmr: Advancing\nhuman mesh recovery with a tokenized pose represen-\ntation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages\n1323–1333.\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei\nXu, et al. 1996. A density-based algorithm for dis-\ncovering clusters in large spatial databases with noise.\nInkdd, volume 96, pages 226–231.Peter A Gorry. 1990. General least-squares smooth-\ning and differentiation by the convolution (savitzky-\ngolay) method. Analytical Chemistry , 62(6):570–\n573.\nChuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen\nWang, and Li Cheng. 2024. Momask: Generative\nmasked modeling of 3d human motions. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 1900–1910.\nSeungju Han, Jack Hessel, Nouha Dziri, Yejin Choi,\nand Youngjae Yu. 2023. Champagne: Learning real-\nworld conversation from large-scale web videos. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision , pages 15498–15509.\nSeungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang,\nBill Yuchen Lin, Nathan Lambert, Yejin Choi, and\nNouha Dziri. 2024. Wildguard: Open one-stop mod-\neration tools for safety risks, jailbreaks, and refusals\nof llms. arXiv preprint arXiv:2406.18495 .\nGlenn Jocher, Ayush Chaurasia, and Jing Qiu. 2023.\nUltralytics YOLO.\nYoon Kyung Lee, Yoonwon Jung, Gyuyi Kang, and\nSowon Hahn. 2023. Developing social robots with\nempathetic non-verbal cues using large language\nmodels. arXiv preprint arXiv:2308.16529 .\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\nhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. 2023. Videochat: Chat-centric video under-\nstanding. arXiv preprint arXiv:2305.06355 .\nTianye Li, Timo Bolkart, Michael J Black, Hao Li, and\nJavier Romero. 2017. Learning a model of facial\nshape and expression from 4d scans. ACM Trans.\nGraph. , 36(6):194–1.\nJunhua Liao, Haihan Duan, Kanghui Feng, Wanbing\nZhao, Yanbing Yang, and Liangyin Chen. 2023. A\nlight weight model for active speaker detection. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 22932–\n22941.\nJing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai,\nRuimao Zhang, Haoqian Wang, and Lei Zhang. 2024.\nMotion-x: A large-scale 3d expressive whole-body\nhuman motion dataset. Advances in Neural Informa-\ntion Processing Systems , 36.\nJing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and\nYu Li. 2023. One-stage 3d whole-body mesh re-\ncovery with component aware transformer. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 21159–21168.\nHaiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen\nPeng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya\nIwamoto, Bo Zheng, and Michael J Black. 2024a.\nEmage: Towards unified holistic co-speech gesture\ngeneration via expressive masked audio gesture mod-\neling. In Proceedings of the IEEE/CVF Conference\n--- Page 11 ---\non Computer Vision and Pattern Recognition , pages\n1144–1154.\nHaiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen\nPeng, Zhengqing Li, You Zhou, Elif Bozkurt, and\nBo Zheng. 2022. Beat: A large-scale semantic and\nemotional multi-modal dataset for conversational ges-\ntures synthesis. In European conference on computer\nvision , pages 612–630. Springer.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2024b. Visual instruction tuning. Advances in\nneural information processing systems , 36.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen\nZhang, Savya Khosla, Ryan Marten, Derek Hoiem,\nand Aniruddha Kembhavi. 2024. Unified-io 2: Scal-\ning autoregressive multimodal models with vision\nlanguage audio and action. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 26439–26455.\nLiying Lu, Tianke Zhang, Yunfei Liu, Xuangeng Chu,\nand Yu Li. 2023a. Audio-driven 3d facial ani-\nmation from in-the-wild videos. arXiv preprint\narXiv:2306.11541 .\nShunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin,\nRuimao Zhang, Lei Zhang, and Heung-Yeung Shum.\n2023b. Humantomato: Text-aligned whole-body mo-\ntion generation. arXiv preprint arXiv:2310.12978 .\nMeta. 2024. Llama 3 & 2 connect 2024: Vision for\nedge and mobile devices. [Online]. Accessed: 2024-\n12-16.\nEvonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor\nDarrell, Angjoo Kanazawa, and Shiry Ginosar. 2022.\nLearning to listen: Modeling non-deterministic\ndyadic facial motion. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 20395–20405.\nEvonne Ng, Sanjay Subramanian, Dan Klein, Angjoo\nKanazawa, Trevor Darrell, and Shiry Ginosar. 2023.\nCan language models learn to listen? In Proceed-\nings of the IEEE/CVF International Conference on\nComputer Vision , pages 10083–10093.\nSe Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu\nKim, Joanna Hong, Jeong Hun Yeo, and Yong Man\nRo. 2024. Let’s go real talk: Spoken dialogue\nmodel for face-to-face conversation. arXiv preprint\narXiv:2406.07867 .\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas,\nand Michael J Black. 2019. Expressive body capture:\n3d hands, face, and body from a single image. In Pro-\nceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pages 10975–10985.\nDeepika Phutela. 2015. The importance of non-verbal\ncommunication. IUP Journal of Soft Skills , 9(4):43.Soujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the Associ-\nation for Computational Linguistics , pages 527–536.\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals.\n2019. Generating diverse high-fidelity images with\nvq-vae-2. Advances in neural information processing\nsystems , 32.\nMark Sandler, Andrew Howard, Menglong Zhu, An-\ndrey Zhmoginov, and Liang-Chieh Chen. 2018. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks.\nInProceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4510–4520.\nZoya Shafique, Haiyan Wang, and Yingli Tian. 2023.\nNonverbal communication cue recognition: A path-\nway to more accessible communication. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 5666–5674.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in neural\ninformation processing systems , 30.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research , 9(11).\nA Vaswani. 2017. Attention is all you need. Advances\nin Neural Information Processing Systems .\nQi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-\nKeung Tang. 2024. Motionllm: Multimodal motion-\nlanguage learning with large language models. arXiv\npreprint arXiv:2405.17013 .\nQwen An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei\nZhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jun-\nyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin\nZhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang\nSu, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu\nCui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan,\nand Zekun Wang. 2024. Qwen2.5 technical report.\nArXiv , abs/2412.15115.\nHongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao,\nYandong Wen, Timo Bolkart, Dacheng Tao, and\nMichael J Black. 2023. Generating holistic 3d hu-\nman motion from speech. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 469–480.\nAmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria,\nErik Cambria, and Louis-Philippe Morency. 2018.\nMultimodal language analysis in the wild: Cmu-\nmosei dataset and interpretable dynamic fusion graph.\n--- Page 12 ---\nInProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 2236–2246.\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,\nYanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-\npati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.\nMerlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 16375–16387.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\n2021a. Merlot: Multimodal neural script knowledge\nmodels. Advances in neural information processing\nsystems , 34:23634–23651.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\n2021b. Merlot: Multimodal neural script knowledge\nmodels. Advances in neural information processing\nsystems , 34:23634–23651.\nHang Zhang, Xin Li, and Lidong Bing. 2023a. Video-\nllama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint\narXiv:2306.02858 .\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong\nZhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and\nYing Shan. 2023b. Generating human motion from\ntextual descriptions with discrete representations. In\nProceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 14730–\n14740.\nSitao Zhang, Yimu Pan, and James Z Wang. 2023c.\nLearning emotion representations from verbal and\nnonverbal communication. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 18993–19004.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675 .\n--- Page 13 ---\nA Details of VENUS Dataset Collection\nIn this section, we provide more details about\nVENUS that are not included in the main paper.\nA.1 Safety Filtering\nWe utilized WildGuard (Han et al., 2024) to filter\nunsafe contents in video transcriptions. WildGuard\nassesses the risk level(“harmful” or “unharmful”)\nand the parsing error on a single-turn basis for\nboth prompts and responses. To maintain conver-\nsational context while applying safety filtering, we\ntransformed video transcriptions into single-turn\nsegments using a sliding-window approach. Our\nsafety filtering strategies are as follows: 1) An ut-\nterance is flagged as harmful if it is identified as\nsuch when considering both the prompt and the\ncorresponding response. 2) An utterance is also\ndeemed harmful if it is classified as harmful inde-\npendently, whether it appears as a prompt or as a\nresponse, within a single turn. 3) If the cumulative\nduration of harmful utterances within a video ex-\nceeds three minutes, the entire video is discarded to\nensure safety compliance. By implementing these\nmeasures, we ensure robust safety filtering while\npreserving as much video information as possible.\nA.2 Video Collection Strategy\nTo collect videos centered on conversations, we\nfirst used the YouTube API1to collect channel\nIDs that include the word “Podcast” in their chan-\nnel names. After identifying these channels, we\nretrieved up to 300 videos per channel that were\ncreated between January 1, 2015, and December\n31, 2023. Due to the inherent limitations of the\nYouTube API, duplicate videos were occasionally\nretrieved during this process. To ensure the quality\nof the dataset, we removed all duplicates, retaining\nonly unique videos.\nA.3 Re-annotate Speaker\nTo align the text by the speaker with nonverbal ex-\npressions, we segmented the speech into individual\nutterances in a video, U= [Uj]n\nj=1where nis the\nnumber of utterances in a video. Next, we used\nthe time of the utterances, T= [(tstart\nj, tend\nj)]n\nj=1,\nextracted from WhisperX and the FPS to calculate\nthe start and end frames of each utterance. Then,\nwe cropped the speaker’s image to focus on the seg-\nments where the speaker is actively speaking. To\nhandle speaker alignment, we used a lightweight\n1https://developers.google.com/youtubeAlgorithm 1 Cropping and Aligning Speaker\nInput : Frames with the speaker, F= [fi]m\ni=1, speaker’s\nbounding box coordinates, B, and utterance start and end\ntime,T.\nOutput : Utterance frames set without duplicates,\nUj\n1:(sj, ej)← ⌊(tstart\nj, tend\nj)×FPS⌋\n2:Fj← F [sj:ej]\n3:U′\nj←[]\n4:for all finFjdo\n5: u′\nj,k←f[xj\ntop:xj\nbottom, yj\ntop:yj\nbottom]\n6: Append u′\nj,ktoU′\nj\n7:end for\n8:Uj← {}\n9:uprev←None\n10:foreach cropped frame u′\nj,kinU′\njdo\n11: ifk= 2then\n12: ep←MobileNet (uprev)\n13: ej,1←MobileNet (u′\nj,1)\n14: ej,2←MobileNet (u′\nj,2)\n15: sim←arg max( cos(ej,1, ep),cos(ej,2, ep))\n16: uj←u′\nj,sim\n17: else\n18: uj←u′\nj,1\n19: end if\n20: Append ujtoUj\n21: uprev←uj\n22:end for\n23:return Uj\nmodel (Sandler et al., 2018) to extract the features\nof the speaker’s cropped images and re-aligned\nthem by comparing with previous frames based on\ncosine similarity. This is shown in Algorithm 1.\nA.4 Batching for Nonverbal Cue Annotation\nTo efficiently extract 3D information from a large\ncorpus of speaker images, batch processing is es-\nsential. However, since we detect and crop speakers\nfrom video frames using the detection model, the\nresulting images I∈Rh×winherently vary in di-\nmensions due to differences in the bounding boxes,\nwhere handwdenote height and width of each\nimage, respectively.\nTo address the challenge of variable image sizes\nand enable batch inference, we propose a resizing\nand padding strategy that preserves the aspect ratio\nof each speaker image while standardizing their\ndimensions. The main idea is to scale each image\nsuch that its longest side matches a predetermined\nsizeS, followed by padding to create a square im-\nage of dimensions S×S. Firstly, we compute the\nscaling factor sbased on the original dimensions\nof the image:\ns=S\nmax( w, h)(12)\nThis scaling factor ensures that the largest di-\n--- Page 14 ---\nFigure 5: The diversity of topics of videos in VENUS ,\ndisplayed as a word cloud. Larger words indicate more\nvideos from that topic.\nmension of the image is resized to S, maintaining\nthe asepct ratio. The image is then resized to new\ndimensions h′=s×handw′=s×w.\nAfter resizing, we create a zero-initialized square\nimage Ipad∈RS×S, and resized image Iresized∈\nRh′×w′is then placed at the center of Ipadto ensure\nspatial consistency and preserve central features of\nthe speaker. The offsets for centering are calculated\nas :\nδh=\u0016S−h′\n2\u0017\n, δ w=\u0016S−w′\n2\u0017\n(13)\nThe padded image Ipadis then defined as:\nIpad(i, j)\n=\n\nIr(i−δh, j−δw)ifi∈\u0002\nδh, δh+h′\u0001\nj∈\u0002\nδw, δw+w′\u0001\n0 otherwise.\n(14)\nThis approach maintains the aspect ratio of the\noriginal images and ensures that all images have a\nuniform size, facilitating efficient batch processing.\nA.5 Topic analysis\nWe visualized the titles of videos from the entire\ndataset in Figure 5 as a Venn-style word cloud (Cop-\npersmith and Kelly, 2014), with the size propor-\ntional to the number of videos gathered for that\ntopic. The most frequent 3 topics are interview\n(6.64%), life ( 4.51%), and recap ( 4.3%). As these\nproportions indicate, the topics of the VENUS videos\nare almost uniformly distributed, covering a wide\nrange of conversational topics.\nA.6 Text-Based Sentiment Analysis\nFor data analysis, we automatically predicted the\nsentiment (neutral, positive, negative) of the text us-\nPositive NegativeWord Cloud “think” “well”\nFigure 6: Word cloud for text-based sentiment analy-\nsis.It illustrates changes in facial expressions and body\nlanguage when each word carries a positive or negative\ncontext.\ning a Roberta-based sentiment classifier (Camacho-\nCollados et al., 2022). In the sentiment analy-\nsis conducted with VENUS at the sentence level,\nthe results showed that 63.79% of the sentences\nwere classified as neutral, 17.36% as positive, and\n18.85% as negative. Based on the sentiment anal-\nysis results at the sentence level, we conducted a\nfrequency analysis accordingly.\nThese results were visualized using a word cloud,\nas illustrated in Figure 6. First, an analysis of the\nwords reveals positive and negative associations\nwith certain professions and religions, with “sol-\ndier” appearing in both positive and negative con-\ntexts. Interestingly, in real-world conversations,\n“Friday” is often associated with positive sentiment,\nwhile “Monday” is linked to negative sentiment.\nAlso, Figure 6 shows the nonverbal cues associ-\nated with words such as “think” and “well”, com-\nparing their usage in positive versus negative senti-\nment contexts. For words like “think” and “well”,\nsentiments are not prominently reflected in body\nlanguage. However, these words often convey a\nthoughtful or pondering demeanor. Notably, facial\nexpressions tend to include frowning when spoken\nwith negative sentiments. We can infer from these\nresults that nonverbal cues are closely related to\nsentiment, and leveraging these expressions can\nenhance the understanding and interpretation of\nconversations.\nA.7 VENUS Annotation\nIn this section, we describe the annotation structure\nof the VENUS dataset, as illustrated in Figure 9.\nThe primary keys in VENUS include “Channel\nID”, “Video ID”, “Duration”, “FPS”, “Segment\n--- Page 15 ---\nID”, “Conversation”, “Facial expression”, “Body\nlanguage”, “Speaker bbox” and “Harmful utterance\nID”. Among these, “Conversation” key contains the\ncomplete conversation information for a specific\nvideo segment, encompassing all data related to\nutterances. Within “Conversation” key, the “Words”\nkey provides time-aligned word information and\ntheir corresponding timestamps for each utterance,\nensuring temporal alignment of words within the ut-\nterance. “Facial expression” and “Body language”\nkeys represent all nonverbal cue features within\nthe video segment. These nonverbal features are\nprovided alongside utterance IDs and frame infor-\nmation to enable mapping between utterances and\nfeatures. Features of “Facial expression” include\na total of 153features, encompassing information\nabout facial shape, expressions, and jaw. Mean-\nwhile, features of “Body language” comprises 179\nfeatures, which include details about the root of the\nbody, upper and lower body, left and right hands,\njaw, and overall body shape. “Speaker bbox” rep-\nresents the results of active speaker detection, pro-\nviding information about the speaker location in\neach frame. This information is expressed in the\nform of coordinates [xtop, ytop, xbottom, ybottom], ac-\ncurately indicating the detected speaker’s region\nin every frame. Finally, we introduce the “Harm-\nful utterance ID” key to mark utterances identified\nas harmful by our safety strategy. If an utterance\nID is included under this key, it does not appear\nin the “Conversation” key. This approach allows\nus to preserve the maximum amount of video data\nby retaining all safe utterances while filtering out\nthose deemed harmful, thereby maintaining both\nethical standards and dataset integrity.\nA.8 VENUS Visualization\nWe present data visualizations to demonstrate the\nhigh quality of the annotated nonverbal expressions\nin our dataset. For visualization, we converted\nthe FLAME parameters from EMOCA-v2 to the\nSMPL-X parameters. As shown in Figure 8, VENUS\neffectively captures key nonverbal expressions, in-\ncluding facial expressions and body language.\nIn the first video of Figure 8, the phrase “ get\nout” is accompanied by a gesture resembling throw-\ning something away from the speaker. In the sec-\nond video, the word “ quote ” is articulated with a\nhand gesture resembling air quotes, emphasizing\nthe quoted content in the speech. These represent\nthe emphasis and intended meaning that nonver-\nbal expressions add to verbal interactions. VENUS\n×3 ×3Encoder\nDecoderConv 1D Conv 1D\nResNet  1D\nAttention\nResNet  1DResNet  1D\nAttention\nResNet  1D\nResNet  1D\nAttention\nUpsampleResNet  1D\nConv 1D Conv 1DResNet  1DDownsampleAttention\nResNet  1DQuantize\nEMA\nPost Quantize\n×3\n×3\nQuantizerFigure 7: Overview of VQ-VAE architecture. Encoder\n(left) quantizes the speaker’s noverbal-cues, while the\ndecoder (right) projcets the learned discrete codebook\ntokens back into continuous nonverbal-cues sequence\nspace. The downsampling block consists of 1D convolu-\ntional layers with a stride of 2. Both the Face VQ-V AE\nand Body VQ-V AE follow the same architecture.\nannotates these expressions, ensuring a rich rep-\nresentation of the subtle, yet essential, aspects of\nhuman interaction.\nB Details of VQ-VAE\nWe trained a VQ-V AE to quantize facial expres-\nsions and body language patches, which are uti-\nlized as the input and output for the predictor model.\nOur Face VQ-V AE and Body VQ-V AE were con-\nstructed based on the structure proposed by (Guo\net al., 2024), with the internal detailed illustrations\nprovided in Figure 7.\nB.1 Implementation Details\nFor our VQ-V AE, we use a codebook size of 512\nand set the downsampling factor q= 8in the en-\ncoder. When training, we set the sequence length,\nW= 512 , to effectively learn utterance-level se-\nquences, with shorter utterances padded with ze-\nros. The learning rate is initialized at 1e−4, and\nthe model is trained for 100epochs. We set 10%\nwarmup steps and apply a learning rate decay of\n0.1 after 50% steps and 0.01after 75% steps. For\nregularization and optimization, we employ EMA\nwith a decay rate of 0.99, L2 regularization with\nweight decay of 0.1, gradient clipping with a maxi-\nmum norm of 1.0, and gradient accumulation over\n4 steps. We also apply L2 normalization to the\ncodebook vectors. The optimal model checkpoint\nis selected based on the validation reconstruction\nloss.\nWhen codebook learning in Lvq, we set commit-\n--- Page 16 ---\nment loss weight, β= 0.02. For the Face VQ-V AE,\nthe the reconstruction loss weight λf\nrecon is set to\n1, with λψ\nrecon = 1 andλjaw\nrecon = 5, determined\nempirically. And the face velocity loss weight λf\nvel\nis set to 0.5, with λθ= 5is also empirically chosen.\nSimilarly, for the Body VQ-V AE, the reconstruc-\ntion loss weight and velocity loss weight are set to\nλb\nrecon = 1andλb\nvel= 0.5, respectively.\nB.2 Evaluation Metrics\nTo evaluate the performance of the VQ-V AE, we\nutilize several metrics to assess both realism and\ndiversity. These evaluation metrics are inspired by\nprior works (Ng et al., 2023; Zhang et al., 2023b;\nLiu et al., 2024a) We denote ground-truth motion\nfeatures and generated motion features as mgt, and\nmpred. For realism, we calculate the window Ver-\ntex L2 ,VMSE , and LVD while for diversity, we\ncalculate the diversity andvariance .\nVMSE . This metric evaluates the reconstruction\nerror by calculating the mean squared difference\nbetween predicted and ground truth vertices in 3D\nspace, offering an intuitive and precise measure of\ngeometric accuracy. We denote the function that\nmaps to the vertex space as V(·)and the VMSE is\ndefined as follows:\nVMSE =1\nNNX\ni=1||V(mpred,i)−V(mgt,i)||2\n2.\n(15)\nLVD . This is a metric similar to VMSE, measuring\nthe L1 distance in the vertex space, and it is defined\nas follows:\nLVD =1\nNNX\ni=1||V(mpred,i)−V(mgt,i)||1.(16)\nWindow Vertex L2 . This metric evaluates the tem-\nporal consistency of predicted motion by comput-\ning the L2 distance between the averaged ground-\ntruth and predicted vertex positions over sliding\nwindows:\nwV L 2 =1\nWWX\ni=1\r\r\r\r\r\r1\nSSX\nj=1V(i,j)\ngt−1\nSSX\nj=1V(i,j)\npred\r\r\r\r\r\r2\n2(17)\nDiversity . This metric quantifies the variability of\nmotion parameters by assessing the spatial distance\nbetween selected pairs, providing the diversity of\nmotion representations. This follows as:\nDiversity =1\nKKX\nk=1∥mik−mjk∥2\n2, (18)where Krepresents the number of randomly se-\nlected pairs, while mikandmjkdenote the motion\nparameters from the first and second indices, re-\nspectively. Here, we randomly selected 1,000 pairs\n(K= 1,000) and computed the diversity by repeat-\ning this process 10 times.\nVariance . This metric quantifies the average tem-\nporal variability of motion parameters. Given a\nmotion sequence with Tframes and Dparameters,\nwhere md∈RTrepresents the trajectory of the\nd-th parameter over time and ¯mdis its mean, the\nvariance is computed as the mean of per-parameter\ntemporal variances:\nVariance =1\nDDX\nd=11\nTTX\nt=1(md,t−¯md)2(19)\nC Details of MARS\nC.1 Details\nWe trained MARS using the LLaMA 3.2-Instruct and\nQwen 2.5-Instruct formats and incorporated a sys-\ntem prompt to enhance the model’s understanding\nof nonverbal tokens. This is presented in Table 5.\nFor supervised fine-tuning, we set the batch size\nper GPU at 8and the maximum sequence length\nat4,096, and trained over a total of 50 epochs.\nDuring inference, we set the maximum sequence\nlength to 512.\nC.2 Evaluation Metrics\nBERT-score (Zhang et al., 2019) evaluates the sim-\nilarity between generated text and reference text at\na deeper semantic level. It leverages contextual em-\nbeddings derived from pre-trained BERT models to\ncompare candidate and reference tokens. By com-\nputing F1 scores based on the cosine similarity of\nthese embeddings, BERTScore provides a nuanced\nand robust assessment of the semantic alignment\nand quality of the generated outputs.\nNegative Log-Likelihood (NLL) (Bengio et al.,\n2000) is a function that guides the training of prob-\nabilistic models by maximizing the likelihood of\nthe observed data. It measures the discrepancy be-\ntween the probability distribution predicted by the\nmodel and the actual observed data, thereby eval-\nuating how well the model approximates the true\ndata distribution.\nPPL (Bengio et al., 2000), or perplexity, quantifies\nhow effectively a language model predicts the next\nword in a sequence. Lower perplexity values sig-\nnify greater confidence and accuracy in the model’s\n--- Page 17 ---\npredictions, indicating higher quality in generating\ncoherent and contextually appropriate outputs.\nMETEOR (Banerjee and Lavie, 2005), short for\nMetric for Evaluation of Translation with Explicit\nOrdering, evaluates the quality of generated text\nby aligning it with the reference text. It incorpo-\nrates factors like precision, recall, and semantic\nsimilarities, such as synonyms and paraphrasing,\nto provide a more nuanced evaluation.\nSystem Prompt\nYou are a helpful assistant. Text includes\nnonverbal tokens <FACE_*>, <BODY_*> interleaved\nwith language. Help interpret meaning while\nconsidering these cues.\nInput Format\n{\n\"role\" : [\"user\" / \"assistant\"],\n\"name\" : [role_ID],\n\"content\" : \"Text interleaved with special tokens\n<FACE_TOKEN_ID> (facial cues), <BODY_TOKEN_ID>\n(body languages).\"\n}\nExamples\n{\n\"role\" : \"user\",\n\"name\" : \"crXEd-NEsS8_000_9\"\n\"content\" : \"Yeah, <FACE_259><BODY_172> do you\nhave one of those little <FACE_12> <BODY_359>\nthings in your car?\"\n}\n{\n\"role\" : \"assistant\",\n\"name\" : \"crXEd-NEsS8_000_10\"\n\"content\" : \"I have\n<FACE_12><BODY_239><FACE_251><BODY_492> one.\"\n}\nTable 5: Input for training MARS\n--- Page 18 ---\nTextYoutube  ID: sHWtnfnmM6o  (00:52:15 ~ 00:52:16)\nVideo \nframestimeIt’s\n like okay, get out of here.\nBody\nlanguage\nFacial\nexpression\nTextYoutube  ID: s73z0TJGOqs  (00:52:07 ~ 00:52:10)\nVideo \nframestimeThis is quote, healthy person York virus.\nBody\nlanguage\na inNew who died from the\nFacial\nexpression\nTextYoutube  ID: KTuZxA9FDWc  (00:34:48 ~ 00:34:51)\nVideo \nframestimeour\nBody\nlanguageFacial\nexpression\nThis life. is\nTextYoutube  ID: WOpEaklhS8w  (00:11:04 ~ 00:11:09)\nVideo \nframestimeWehadanemployee   that’s ···    ···      ··· that would          miss a deadline.\nBody\nlanguageFacial\nexpression\nFigure 8: Visualization for VENUS dataset. This demonstrates the capability of the VENUS dataset to capture\nmultimodal communication, encompassing speech, body language, and facial expressions. Words are time-aligned\nusing WhisperX, with YouTube IDs providing access to ground truth transcription. “ ···” indicates an omission in\nthe text.\n--- Page 19 ---\n{\n“Channel_id ” : “UCbk_QsfaFZG6PdQeCvaYXJQ” ,\n“Video_id ” : “G51M8YGs_OM” ,\n“Duration” : “01:01:00 ~ 01:11:00” ,\n“FPS” : 25,\n“Segment_id ” : 5\n“Conversation” :[ \n {\n  “Utt_id ” : 0, \n  “Speaker” : 0, \n  “Text” :“after that they come and recruit everyone in …” , \n  “Start time” : 0.109 , \n  “End time” : 66.088 ,\n  “Words” : [\n   { “Word” : “after” , “Start_time ” : 0.109 , “End_time ” : 0.896 },\n                         . . .   \n  ] \n }, . . .\n],\n“Facial expression” : [\n  {“Utt_id ”:0, “Frame” : 2, “Features” : [\n    2.81959653e -01, \n    1.82807636e+00 , …\n   ]\n  },\n],\n“Body language” : [\n  {“Utt_id ”: 0, “Frame” : 2, “Features” : [\n    0, \n    3.14159274e+00 , …\n   ]\n  },\n],\n“Speaker bbox ” : [\n  {“Frame” :2, “Bbox ”: [\n    167.741073 , \n    49.3815689 , \n    783.573852 , \n    474.881866\n    ] \n  }\n]\n“Harmful_utterance_id ” : [ ]  \n}\nFigure 9: VENUS annotation format. This is an example of an annotation for a single segmented video. We provide\ntheVENUS dataset in JSON format.",
  "text_length": 69292
}