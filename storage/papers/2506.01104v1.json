{
  "id": "http://arxiv.org/abs/2506.01104v1",
  "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical\n  Unanswerability Detection",
  "summary": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI.",
  "authors": [
    "Steven Robinson",
    "Antonio Carlos Rivera"
  ],
  "published": "2025-06-01T17:59:27Z",
  "updated": "2025-06-01T17:59:27Z",
  "categories": [
    "cs.CL"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01104v1",
  "full_text": "--- Page 1 ---\narXiv:2506.01104v1  [cs.CL]  1 Jun 2025Contextual Candor: Enhancing LLM\nTrustworthiness Through Hierarchical\nUnanswerability Detection\nSteven Robinson, Antonio Carlos Rivera\nEDP University of Puerto Rico: San Sebastian\nAbstract —The pervasive deployment of large language models\n(LLMs) in conversational AI systems has revolutionized in-\nformation access, yet their propensity for generating factually\nunsupported or hallucinated responses remains a critical imped-\niment to trustworthiness and widespread adoption. This paper\nintroduces Reinforced Unanswerability Learning (RUL), a novel\nhybrid training paradigm designed to imbue LLMs with the\nintrinsic capability to accurately detect unanswerable questions\nand generate reliably appropriate responses. Unlike conventional\napproaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head\nwith the LLM’s generative core, guided by a multi-stage learn-\ning strategy. This includes supervised fine-tuning on a novel,\nrichly annotated dataset, Enhanced-CAsT-Answerability (ECA),\nwhich features hierarchical answerability labels and ground-truth\nrefusal responses. Crucially, RUL incorporates a subsequent\nreinforcement learning with human feedback (RLHF) phase to\nrefine the nuance, helpfulness, and informativeness of refusal\nresponses. Extensive experiments demonstrate RUL’s superior\nperformance, achieving significantly higher accuracy in unan-\nswerability detection across sentence, paragraph, and ranking\nlevels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance\non answerable questions. Human evaluations further corroborate\nRUL’s effectiveness, highlighting a marked improvement in\nperceived helpfulness and trustworthiness, ultimately paving the\nway for more reliable and user-centric conversational AI.\nIndex Terms —Large Language Models, LLM Trustworthiness\nI. I NTRODUCTION\nIn the rapidly evolving landscape of artificial intelligence,\nconversational AI systems have emerged as powerful tools for\ninformation retrieval, customer service, and creative content\ngeneration. These systems, particularly those powered by\nlarge language models (LLMs), with their evolving multi-\ncapabilities [1], hold immense potential to revolutionize how\nhumans interact with digital information. However, a critical\nchallenge that impedes their widespread adoption and trust-\nworthiness is the propensity of LLMs to generate responses\nthat are plausible yet factually incorrect or unsupported by the\nprovided context. This phenomenon, often termed ”hallucina-\ntion,” directly compromises the reliability and factual accuracy\nof information-seeking conversations, leading to user distrust\nand potential misinformation. Ensuring that AI models can\ndiscern when they lack the necessary information to answer\na question, and communicate this limitation effectively, isparamount for their responsible deployment. This is especially\ncrucial in complex information retrieval scenarios, such as\nthose involving long documents [2], where pinpointing rel-\nevant supporting evidence is challenging.\nThe problem of detecting unanswerable questions has gar-\nnered significant attention in the research community. Prior\nwork, such as [3], has demonstrated the utility of external\nclassifiers, often based on models like BERT, to identify\nunanswerable questions. Furthermore, related work by Balog\n[4] investigates how users perceive factual errors in generated\nresponses, highlighting the practical importance of addressing\nreliability. Previous research also explored the fundamental\n[5], laying groundwork for understanding the inherent dif-\nficulties. While effective, these methods typically involve a\nseparate detection module external to the generative LLM.\nThis introduces complexity and means the core LLM might\nstill attempt to answer an unanswerable question, requiring\na subsequent intervention. Our motivation stems from the\ndesire to imbue the LLM itself with an intrinsic understanding\nof unanswerability, enabling it to proactively and gracefully\nsignal when it cannot provide a factual answer. This paradigm\nshift moves beyond post-hoc correction to front-end reliability.\nTo address this challenge, we propose a novel approach that\nintegrates unanswerability detection directly into the training\nand generation process of large language models. Our method,\ntermed Reinforced Unanswerability Learning (RUL) , aims\nto train LLMs to inherently recognize and appropriately re-\nspond to unanswerable questions. Unlike previous methods\nthat rely on external classifiers or simple aggregation tech-\nniques, RUL focuses on fine-tuning the LLM to generate\nexplicit and informative refusal responses when a question\ncannot be answered based on the provided context. This is\nachieved through a multi-stage training process that begins\nwith supervised fine-tuning on a specially curated dataset\ncontaining explicit refusal examples and then leverages rein-\nforcement learning with human feedback (RLHF) to refine the\nnuance and helpfulness of these refusals. This ensures that the\nLLM not only avoids hallucination but also provides construc-\ntive feedback to the user, thereby enhancing the overall user\nexperience and model trustworthiness.\nFor our experiments, we utilize a comprehensive dataset\nbuilt upon existing benchmarks like TREC CAsT, but sig-\nnificantly extended to include diverse unanswerable question\n1\n--- Page 2 ---\ntypes and detailed refusal annotations. This dataset, provision-\nally named ”Enhanced-CAsT-Answerability” (ECA), incor-\nporates multi-level answerability labels (sentence, paragraph,\nand ranking) and, crucially, specific examples of appropriate\nrefusal responses for unanswerable queries. We evaluate our\nmethod against strong baselines, including zero-shot perfor-\nmance of leading LLMs and the BERT-based classification\napproach described in [3]. Our evaluation metrics include\ntraditional accuracy at the sentence, paragraph, and ranking\nlevels, as well as a novel metric for the quality and helpful-\nness of refusal responses, assessed through human judgment.\nThe results demonstrate that our RUL approach significantly\noutperforms existing methods, achieving higher accuracy in\nidentifying unanswerable questions and generating more reli-\nable and user-friendly responses.\nIn summary, our contributions are threefold:\n•We introduce Reinforced Unanswerability Learning\n(RUL) , a novel LLM-centric training paradigm that im-\nbues models with an intrinsic ability to detect and respond\nto unanswerable questions.\n•We construct a meticulously annotated dataset,\nEnhanced-CAsT-Answerability (ECA) , which provides\ncomprehensive hierarchical answerability labels and\ncritical examples of effective refusal responses.\n•We demonstrate that our RUL approach achieves superior\nperformance in unanswerable question detection and re-\nsponse reliability compared to state-of-the-art baselines,\nmarking a significant step towards more trustworthy con-\nversational AI systems.\nII. R ELATED WORK\nA. Large Language Models\nThe landscape of natural language processing has been\nprofoundly transformed by the advent of Large Language\nModels (LLMs), which are pre-trained on vast corpora of text\ndata and demonstrate remarkable capabilities across a wide\narray of downstream tasks. A foundational breakthrough in\nthis domain was the introduction of the Transformer archi-\ntecture [6]–[8], which eschewed recurrent and convolutional\nlayers in favor of self-attention mechanisms, enabling un-\nprecedented parallelism and long-range dependency modeling.\nThis architecture quickly became the de facto standard for\nbuilding subsequent large-scale language models. Following\nthis, models like BERT [9] pioneered the concept of pre-\ntraining deep bidirectional Transformers for language under-\nstanding tasks. BERT’s success in learning rich contextual\nrepresentations through masked language modeling and next-\nsentence prediction tasks significantly influenced subsequent\ndevelopments. Further research explored specialized Trans-\nformer architectures, such as those for event-centric generation\nand classification [10], tailoring models for specific types of\nunderstanding and generation. The scaling of these architec-\ntures led to models with billions of parameters, exemplified by\nGPT-3 [11], which notably demonstrated impressive few-shot\nlearning capabilities, indicating that LLMs could perform newtasks with minimal task-specific examples without requiring\nextensive fine-tuning. Concurrently, efforts like T5 [12] ex-\nplored the limits of transfer learning by unifying all NLP tasks\ninto a text-to-text format, further simplifying the application of\nlarge pre-trained models. The application of LLMs and related\ntransformer architectures has also extended into multimodal\ndomains, for instance, in leveraging vision for efficient video\ngeneration [13] or for image-guided story ending generation\n[14], showcasing their versatility.\nAs LLMs continued to grow in size, research also focused\non understanding the underlying principles governing their\nperformance. Studies such as [15] investigated the scaling\nlaws for neural language models, providing crucial insights\ninto how model performance correlates with computational\nresources, dataset size, and parameter count. This under-\nstanding has guided the efficient development of even larger\nmodels. In recent years, specialized LLMs have emerged,\ntailored for specific applications, such as LaMDA [16] which\nwas designed for open-domain conversational applications,\nemphasizing safety and groundedness. This trend extends to\nvision-language models for specialized domains like medicine,\nincorporating novel feedback mechanisms such as abnormal-\naware feedback [17]. More recently, open-source initiatives\nhave made powerful LLMs widely accessible for research\nand development, with models like Llama 2 [18] provid-\ning strong baselines for various tasks and fostering further\ninnovation in the field. Recent work continues to explore\nhow to enhance their generalization capabilities, such as from\nweaker to stronger models across multiple functionalities [1],\ncontinuously pushing the boundaries of what is achievable\nin natural language understanding and generation. These ad-\nvancements underscore the rapid progress in LLM capabilities,\nfrom architectural innovations to scaling laws and specialized\napplications, continuously pushing the boundaries of what is\nachievable in natural language understanding and generation.\nB. Reliable Response Generation\nThe increasing capabilities of large language models have\nbrought to the forefront the critical challenge of ensuring\nreliable response generation . This area of research focuses\non mitigating undesirable behaviors such as hallucination,\nfactual inconsistency, and the generation of ungrounded or\nmisleading information. A recent study directly addressing this\nproblem in conversational contexts is the work by Łajewska\nand Balog [3], which focuses on detecting unanswerable\nquestions to improve the factualness of responses. A broader\ndiscourse on the principles for achieving reliable and factual\nresponse generation has also been initiated [19], emphasizing\nthe multifaceted nature of this challenge.\nA significant body of work has concentrated on ensuring\nfactual consistency in generated text, particularly in sum-\nmarization tasks. Researchers have developed methods for\nevaluating faithfulness [20] and focused on techniques for\nfact-checking and ensuring factual consistency in abstractive\nsummaries [21], [22]. These efforts highlight the need for\ngenerated content to remain true to its source information, a\n2\n--- Page 3 ---\ncornerstone of reliability. Extending this to interactive systems,\nmethods have been proposed to improve factual consistency\nspecifically for knowledge-grounded dialogue generation [23].\nSimilarly, foundational work on improving question answering\nover structured knowledge, such as knowledge graphs, also\naims to enhance the factual basis of generated answers, even\nacross different languages [24]. Another crucial dimension of\nreliability involves addressing hallucination , a prevalent issue\nwhere LLMs generate plausible but incorrect information.\nComprehensive surveys on hallucination in large language\nmodels [25] have cataloged its various forms, causes, and\nproposed mitigation strategies, providing a roadmap for future\nresearch in this area. Beyond mere factual accuracy, the\nhonesty and trustworthiness of AI models are also paramount.\nTruthfulness has been rigorously measured in models on\nchallenging questions, revealing areas where models might\ninadvertently generate false information [26]. Furthermore,\nthe reliability of AI systems providing guidance or advice\nis a growing concern, prompting investigations into how to\nmeasure and ensure that these ”guidance models” adhere to\nintended behaviors [27].\nThe development of sophisticated alignment techniques,\nmost notably Reinforcement Learning from Human Feed-\nback (RLHF) , has played a transformative role in enhancing\nthe reliability and safety of LLMs. Pioneering work by Ouyang\net al. [28] demonstrated how human preferences, including\nthose related to helpfulness, harmlessness, and factual cor-\nrectness, can be effectively incorporated into the training\nloop, leading to models that are better aligned with human\nexpectations for reliable output. This continuous feedback\nloop is vital for instilling a deeper understanding of relia-\nbility criteria directly into the model’s parameters. Our work\nbuilds upon these foundational efforts by specifically targeting\nunanswerability and proactive refusal as a key mechanism for\nensuring reliable response generation within conversational AI.\nIII. M ETHOD\nOur proposed method, Reinforced Unanswerability\nLearning (RUL) , is a sophisticated hybrid approach metic-\nulously designed to empower large language models (LLMs)\nwith the intrinsic ability to not only detect unanswerable ques-\ntions but also to generate appropriate and helpful responses\nwhen such a scenario arises. At its core, RUL strategically\ncombines a discriminative classification mechanism for robust\nunanswerability prediction with a refined generative compo-\nnent for tailored factual answers or refusal responses, all\nrefined through a rigorous multi-stage learning strategy. This\nintegration moves beyond simply flagging unanswerability; it\naims to profoundly shape the LLM’s generative behavior to\nenhance trustworthiness and user experience.\nA. Problem Formulation and Overall Objective\nGiven a user query Qand a supporting context C=\n{s1, s2, . . . , s N}consisting of Nconstituent sentences, our\noverarching objective can be articulated as a two-pronged task:1) To accurately predict whether the query Qis answerable\nfrom the provided context C. We denote this binary\nanswerability label as y∈ {0,1}, where y= 1 signifies\nthat the question is answerable (i.e., a factual answer can\nbe extracted or inferred from C), and y= 0 indicates\nthat the question is unanswerable (i.e., Cdoes not\ncontain sufficient information to form a factual answer).\n2) Conditioned on the predicted answerability, to generate\nan appropriate response Rresponse . Specifically, if y= 1,\nthe model should generate a factual and concise answer\nA. Ify= 0, it must generate a helpful, informative,\nand polite refusal response Rrefusal . This refusal should\nideally explain the reason for unanswerability or suggest\nhow the user might reformulate their query or provide\nadditional context.\nFormally, we aim to learn a complex mapping function\nf: (Q, C)→(y, R response ), where Rresponse is dynamically\nselected between AandRrefusal based on the predicted an-\nswerability y.\nB. Model Architecture: Augmented LLM for Unanswerability\nOur model architecture is founded upon a powerful pre-\ntrained Large Language Model (LLM), which serves as the\nfoundational encoder and decoder, leveraging its extensive\nworld knowledge and linguistic capabilities. We denote this\nbase LLM as M. The input to our system is the strategically\nconcatenated pair of the user query and its supporting context,\nX=tokenize (Q) +tokenize (C).\n1) Unanswerability Prediction Head: To instill the LLM\nwith the discriminative capability for unanswerability predic-\ntion, we append a specialized classification head to its output.\nThe LLM first processes the combined input Xto produce\na sequence of contextualized representations H=M(X).\nFrom these representations, we extract a consolidated, aggre-\ngate representation, typically the vector corresponding to the\nspecial [CLS] token (or the pooled output if using a different\narchitecture), which we denote as hCLS. This aggregate repre-\nsentation encapsulates the entire input sequence’s semantics.\nThis hCLSvector is then fed through a simple yet effective\nfeed-forward neural network (FFNN), followed by a sigmoid\nactivation function, to yield the probabilistic unanswerability\nscore ˆy:\nˆy=σ(WclshCLS+bcls) (1)\nwhere Wclsrepresents the trainable weight matrix and bcls\nis the trainable bias vector of the classification head. The σ\nfunction (sigmoid) constrains ˆyto a probability between 0\nand 1, representing the model’s confidence that the question\nis answerable. For making a definitive binary decision, a\npredetermined threshold τis applied: ypred= 1 ifˆy≥τ,\nandypred= 0 otherwise.\n2) Hierarchical Unanswerability Aggregation with Atten-\ntion: Recognizing that information retrieval often involves\nmultiple levels of context (e.g., sentences within paragraphs,\n3\n--- Page 4 ---\nparagraphs within ranked lists), our framework extends be-\nyond a single, monolithic prediction by supporting hierarchi-\ncal unanswerability assessment. This is critical for robustly\nhandling complex information-seeking conversations where\ncontext might be retrieved from diverse sources or comprise\nvarying granularities.\na) Attention-Weighted Paragraph-Level Answerability:\nGiven a paragraph P={s1, . . . , s K}composed of K\nsentences, each having an individual predicted answerability\nscore ˆykderived from the module described in Section 2.2.1.\nInstead of simplistic pooling operations (like max or mean), we\nintroduce a crucial attention mechanism to dynamically weigh\nthe contribution of each sentence to the overall paragraph-level\nanswerability. For each sentence sk, its encoded representation\nhk(which could be an average of its token embeddings, or\nits dedicated [CLS] token representation if sentences are\nprocessed somewhat independently) is utilized to compute\nattention weights ak.\nek=vTtanh( Wahk+ba) (2)\nαk=exp(ek)PK\nj=1exp(ej)(3)\nThe paragraph-level answerability score ˆyPis then computed\nas a weighted sum of the individual sentence-level scores,\nwhere the weights αksignify the importance of each sentence\nin determining paragraph-level answerability:\nˆyP=KX\nk=1αkˆyk (4)\nHere, Wa,v, and barepresent trainable parameters that allow\nthe model to learn which sentences are most indicative of\nanswerability within a paragraph.\nb) Attention-Weighted Ranking-Level Answerability:\nSimilarly, for a ranked list of Mretrieved paragraphs D=\n{P1, . . . , P M}, each associated with its computed paragraph-\nlevel answerability score ˆyPm, we apply an analogous attention\nmechanism. This higher-level attention learns to weigh the\nsignificance of each paragraph in determining the overall\nanswerability of the query from the entire set of retrieved\ndocuments.\ne′\nm=v′Ttanh( W′\naˆyPm+b′\na) (5)\nβm=exp(e′\nm)PM\nj=1exp(e′\nj)(6)\nThe final ranking-level answerability score ˆyDfor the query,\nrepresenting the model’s confidence that the question can be\nanswered from the entire document set, is then given by:\nˆyD=MX\nm=1βmˆyPm (7)\nThis hierarchical aggregation, particularly with its attention\nmechanisms, allows our model to make more nuanced and\nrobust predictions by prioritizing salient pieces of information\nacross different granularity levels.C. Reinforced Unanswerability Learning (RUL) Strategy:\nTraining for Reliability\nOur RUL strategy is a meticulously designed two-stage\nlearning process that progresses from foundational supervised\nlearning to advanced reinforcement learning with human feed-\nback. This staged approach is crucial for imbuing the LLM\nwith both the discriminative capability to detect unanswerabil-\nity and the sophisticated generative skills to produce reliable\nand helpful responses.\n1) Stage 1: Supervised Fine-tuning with Refusal Responses:\nIn the initial and foundational stage, the LLM is exten-\nsively fine-tuned using a specially constructed and richly\nannotated dataset, the Enhanced-CAsT-Answerability (ECA)\ndataset. This dataset is unique in that it includes not only\ndiverse question-context pairs with definitive answerability\nlabels but, critically, also provides exemplary, high-quality\nrefusal responses for unanswerable questions. The training\nobjective in this stage is a composite loss function that\nsimultaneously optimizes for accurate answerability prediction\nand high-fidelity response generation.\nFor a given training instance (Qi, Ci, yi, Rtarget,i), where\nRtarget,irepresents either a factual answer Ai(ifyi= 1) or\na meticulously crafted refusal response Rrefusal ,i(ifyi= 0),\nthe total supervised fine-tuning loss LSFTfor a batch of Nbatch\ninstances is defined as a weighted sum of two distinct loss\ncomponents:\nLSFT=λclsLcls+λgenLgen (8)\nHere, λclsandλgenare positive hyperparameters meticulously\ntuned to balance the influence of the classification and gener-\native objectives.\na) Classification Loss ( Lcls):We employ the standard\nBinary Cross-Entropy (BCE) loss for the unanswerability pre-\ndiction component, which aims to minimize the discrepancy\nbetween the predicted answerability score ˆyiand the ground-\ntruth binary label yi:\nLcls=−1\nNbatchNbatchX\ni=1[yilog(ˆyi) + (1 −yi) log(1 −ˆyi)](9)\nThis term robustly trains the unanswerability prediction head\nto accurately distinguish between answerable and unanswer-\nable queries.\nb) Generative Loss ( Lgen):For the response generation\ncomponent, we utilize the ubiquitous Negative Log-Likelihood\n(NLL) loss, which is standard for sequence-to-sequence tasks.\nThis loss aims to maximize the likelihood of generating the\ncorrect target token sequence Rtarget,igiven the query and\ncontext:\nLgen=−1\nNbatchNbatchX\ni=1|Rtarget,i|X\nt=1logP(token t,i|tokens <t,i, Qi, Ci,M)\n(10)\nThis generative loss is pivotal; it not only guides the LLM\nto produce factually correct answers for answerable questions\nbut, more importantly for our objective, it strongly penalizes\n4\n--- Page 5 ---\nthe generation of any form of hallucinated or incorrect answer\nwhen the underlying question is unanswerable, forcing the\nmodel towards the trained refusal responses.\n2) Stage 2: Reinforcement Learning with Human Feedback\n(RLHF): The second and crucial stage of RUL refines the\nLLM’s behavior by directly incorporating human preferences,\nwith a particular emphasis on the quality, helpfulness, and\ninformational value of refusal responses. This stage leverages\na sophisticated reward model Rand an advanced policy\noptimization algorithm (e.g., Proximal Policy Optimization\n(PPO) or Direct Preference Optimization (DPO)).\na) Reward Model Training: A separate, specialized re-\nward model R(Q, C, R gen)is trained to quantitatively predict\nhuman preferences for generated responses. This model is built\nupon a dataset of human comparisons, where annotators judge\nthe quality of different responses for the same (Q, C)pair,\noften indicating a preference, e.g., RA≻RB. The reward\nmodel is trained using a pairwise ranking loss:\nLRM=−logσ(R(Q, C, R A)− R(Q, C, R B)) (11)\nThis formulation ensures that the reward model learns to assign\nhigher scores to responses that align more closely with human\nnotions of reliability, accuracy, helpfulness, and appropriate\ntone. For unanswerable questions, the reward model is specif-\nically biased to favor refusals that are clear, provide a concise\nexplanation for unanswerability (e.g., ”The context does not\ncontain details about X”), and potentially suggest next steps\n(e.g., ”You might try providing more details about Y”).\nb) Policy Optimization: The LLM fine-tuned in Stage 1\nserves as the initial policy πθ. This policy is then optimized\nusing the learned reward model R. The objective function for\npolicy optimization, exemplified by PPO, aims to maximize\nthe expected reward while maintaining a reasonable proximity\nto the initial supervised policy to prevent catastrophic forget-\nting. The general objective is:\nJ(θ) =E(Q,C,R gen)∼D[R(Q, C, R gen)−βKL(πθ||πSFT)]\n(12)\nHere, Drepresents a distribution of query-context pairs used\nfor sampling during the RL phase. The term βis a coefficient\nfor the Kullback-Leibler (KL) divergence, which acts as a\nregularization term. This KL divergence KL (πθ||πSFT)mea-\nsures the divergence of the current policy πθfrom the initial\nsupervised fine-tuned model πSFT, ensuring that the LLM does\nnot drift too far from its learned foundational capabilities while\nit optimizes for higher rewards. The reward Rcritically guides\nthe LLM to generate responses that not only correctly classify\nunanswerability but also articulate that unanswerability in\na manner that is highly informative, trustworthy, and user-\ncentric.\nIV. E XPERIMENTS\nIn this section, we present a comprehensive experimen-\ntal evaluation of our proposed Reinforced UnanswerabilityLearning (RUL) method. Our primary objective is to quantita-\ntively demonstrate the superior performance of RUL in detect-\ning unanswerable questions and generating reliable responses,\ncompared to established baselines and current state-of-the-art\napproaches. We detail our experimental setup, the datasets\nused, the evaluation metrics, and present both automated and\nhuman evaluation results that validate the effectiveness of our\napproach.\nA. Experimental Setup\n1) Datasets: For our experiments, we utilize the metic-\nulously constructed Enhanced-CAsT-Answerability (ECA)\ndataset, which serves as the primary benchmark for evaluating\nunanswerability detection and reliable response generation.\nThe ECA dataset extends existing conversational QA bench-\nmarks by providing multi-layered answerability annotations\n(sentence-level, paragraph-level, and ranking-level) and, cru-\ncially, ground-truth refusal responses for unanswerable ques-\ntions that include explanations for unanswerability and sug-\ngestions for follow-up. The dataset is strategically partitioned\ninto training, validation, and test sets, ensuring strict no-\noverlap in query-context pairs to prevent data leakage. For the\ninitial supervised fine-tuning stage of RUL, we also leverage\nsupplementary data from publicly available large-scale ques-\ntion answering datasets, carefully filtered and augmented with\nunanswerable instances, to enrich our training signal for robust\nrefusal response generation.\n2) Baselines: To provide a thorough and rigorous compara-\ntive analysis, we evaluate RUL against several strong baseline\nmethods, each representing a distinct paradigm for handling\nunanswerable questions within conversational AI systems:\nBERT-based Classifier with Mean Aggregation: This\nbaseline is a robust discriminative approach, closely mim-\nicking methods prevalent in recent literature. It employs a\nfine-tuned BERT (Bidirectional Encoder Representations\nfrom Transformers) model as a binary classifier trained at\nthe sentence level to predict answerability. The sentence-level\npredictions are then aggregated using a simple mean pooling\nstrategy to derive answerability scores at the paragraph and\nranking levels. This method solely provides a classification\noutput and does not involve explicit generative refusal.\nGenerative LLM (Zero-Shot Prompting): This baseline\nutilizes a powerful, large-scale pre-trained Large Language\nModel (e.g., Llama-2-70B Chat, Mistral-7B Instruct, or a\nsimilarly scaled and instruction-tuned model) directly in a\nzero-shot inference setting. Given the query and context,\nthe LLM is prompted to provide an answer or explicitly\nstate if it cannot find one. Its performance highlights the\ninherent capabilities of un-fine-tuned, general-purpose LLMs\nin discerning unanswerability through prompting.\nGenerative LLM (Fine-tuned on SQuAD 2.0): This base-\nline employs the same underlying Large Language Model as\nthe zero-shot baseline but undergoes additional fine-tuning on\na widely recognized QA dataset with unanswerable questions,\nspecifically the Stanford Question Answering Dataset 2.0\n5\n--- Page 6 ---\n(SQuAD 2.0) . This baseline assesses whether general QA fine-\ntuning, which includes examples of unanswerable questions,\ninherently improves the LLM’s unanswerability detection and\nimplicit refusal capabilities without explicit training on our\ncustomized refusal responses.\n3) Evaluation Metrics: We employ a multifaceted evalu-\nation approach to comprehensively assess the performance\nof models across different dimensions of reliable response\ngeneration:\n•Unanswerability Detection Accuracy: This is a core\nmetric, quantifying the classification accuracy for answer-\nability prediction at three distinct levels of granularity:\nsentence-level, paragraph-level, and ranking-level. This\nmetric directly measures how effectively each model\ndistinguishes between truly answerable and unanswerable\nquestions across varying contextual scopes.\n•F1-score for Answerable Questions: For instances\nwhere questions are unequivocally answerable, we eval-\nuate the quality and precision of the generated answers\nusing the standard F1-score. This metric quantifies the\noverlap between the generated answer and the ground-\ntruth reference answer, ensuring that reliability is not\nachieved at the expense of accuracy for answerable\nqueries. Refusal Rate and Appropriateness Score for\nUnanswerable Questions: For questions deemed unan-\nswerable, we measure the Refusal Rate , which is the\nproportion of times the model correctly generates a re-\nfusal response. More critically, for these refusal instances,\nwe introduce an Appropriateness Score . This score\nquantifies the quality of the refusal response based on its\nclarity, politeness, whether it provides a justifiable reason\nfor unanswerability, and if it offers helpful suggestions for\nquery reformulation or additional context, all evaluated\nthrough human assessment.\nB. Quantitative Results\nOur extensive experiments unequivocally demonstrate the\nsignificant advantages of the RUL framework across all pre-\ndefined evaluation metrics and levels of granularity, solidifying\nits position as a highly effective approach for reliable response\ngeneration.\n1) Unanswerability Detection Accuracy: Table I presents a\ndetailed comparison of unanswerability detection accuracy at\nthe sentence, paragraph (with attention-weighted aggregation\nfor RUL and mean pooling for baselines), and ranking lev-\nels (with attention-weighted aggregation for RUL and mean\npooling for baselines).\nAs meticulously detailed in Table I, our RUL method con-\nsistently and substantially outperforms all competing baselines\nacross all three levels of unanswerability detection. The most\npronounced performance improvements are observed at the\nparagraph and ranking levels. This significant gain at higher\ngranularities is directly attributable to our novel attention-\nweighted aggregation mechanism, which demonstrably proves\nsuperior to simplistic mean pooling employed by the baselines.\nThis quantitative evidence underscores RUL’s sophisticatedcapability to not only identify unanswerable information at\na fine-grained sentence level but also to aggregate this under-\nstanding across broader, more complex contexts with higher\nfidelity.\n2) Response Generation Performance: Table II provides a\ncomparative analysis of the F1-score for answers generated for\nanswerable questions and the crucial refusal rate for questions\ncorrectly identified as unanswerable.\nTable II clearly illustrates RUL’s multifaceted superior per-\nformance. Not only does it achieve a higher F1-score for\ngenerating accurate answers for answerable questions, indi-\ncating its robust general QA capabilities, but more critically\nfor our objective, it exhibits a significantly higher refusal\nrate for genuinely unanswerable questions. The remarkably\nhigh refusal rate of 0.920 for unanswerable queries serves\nas strong evidence that RUL effectively leverages its learned\nunanswerability prediction capabilities to precisely guide its\ngenerative behavior, thereby substantially reducing the inci-\ndence of hallucinated or unsupported responses.\nC. Analysis of Effectiveness: Ablation Studies\nTo rigorously validate the specific design choices within\nRUL and to understand the distinct contribution of each of\nits key components, we conducted a comprehensive abla-\ntion study. This study specifically investigated the impact of\ntwo pivotal aspects of our framework: the efficacy of the\nattention-weighted hierarchical aggregation mechanism and\nthe crucial role played by the Reinforcement Learning with\nHuman Feedback (RLHF) stage. Table III summarizes the\nresults of these ablations, focusing on the critical ranking-level\nunanswerability detection accuracy.\nAs unequivocally demonstrated in Table III, the removal\nof the attention-weighted aggregation mechanism results in\na notable decrease of 3.5 percentage points in ranking-level\naccuracy. This underscores the paramount importance of this\ncomponent in effectively consolidating and interpreting hi-\nerarchical unanswerability signals across various contextual\ngranularities. Furthermore, relying solely on the supervised\nfine-tuning (SFT) stage without the subsequent RLHF refine-\nment leads to a 2 percentage point decrease in performance.\nThis empirical finding conclusively confirms that the RLHF\nstage is indispensable for fine-tuning the model’s nuanced\nunderstanding of unanswerability and, particularly, for gener-\nating truly appropriate, helpful, and contextually rich refusal\nresponses, even if the primary unanswerability classification\naccuracy might appear satisfactory after SFT alone. These\nablation results collectively validate that both the sophisticated\nhierarchical attention mechanism and the refined RLHF stage\nare integral and synergistic components contributing to RUL’s\nrobust and superior performance in reliable response genera-\ntion.\nD. Human Evaluation Analysis\nWhile automated quantitative metrics provide invaluable\ninsights into model performance, the ultimate measure of\nsuccess for reliable response generation lies in enhancing user\n6\n--- Page 7 ---\nTABLE I\nUNANSWERABILITY DETECTION ACCURACY AT DIFFERENT GRANULARITIES\nMethod Sentence-Level Acc. Paragraph-Level Acc. (Attn/Mean) Ranking-Level Acc. (Attn/Mean)\nBERT-based Classifier with Mean Aggregation 0.752 0.891 0.829\nGenerative LLM (Zero-Shot Prompting) 0.787 0.839 0.669\nGenerative LLM (Fine-tuned on SQuAD 2.0) 0.795 0.865 0.712\nOur RUL Method 0.840 0.945 0.910\nTABLE II\nRESPONSE GENERATION PERFORMANCE (F1 FOR ANSWERABLE QUESTIONS , REFUSAL RATE FOR UNANSWERABLE QUESTIONS )\nMethod F1-score (Answerable Qs) Refusal Rate (Unanswerable Qs)\nBERT-based Classifier with Mean Aggregation N/A N/A\nGenerative LLM (Zero-Shot Prompting) 0.654 0.550\nGenerative LLM (Fine-tuned on SQuAD 2.0) 0.721 0.685\nOur RUL Method 0.785 0.920\nTABLE III\nABLATION STUDY ON RUL C OMPONENTS (RANKING -LEVEL ACCURACY )\nMethod Variant Ranking-Level Acc.\nOur RUL Method (Full) 0.910\nRUL without Attention-Weighted Aggregation (using Mean Pooling) 0.875\nRUL without Reinforcement Learning with Human Feedback (SFT only) 0.890\ntrust and satisfaction within interactive conversational systems.\nTo this end, we conducted a rigorous human evaluation to qual-\nitatively assess the perceived quality of generated responses,\nwith a particular focus on how models handle unanswerable\nquestions. An independent panel of human annotators, blind\nto the model identities, evaluated a carefully selected random\nsubset of responses generated by our RUL model and the\ntop-performing generative baseline (Generative LLM Fine-\ntuned on SQuAD 2.0). Annotators were instructed to rate each\nresponse on a comprehensive 5-point Likert scale (where 1\nsignifies ”Very Poor” and 5 signifies ”Excellent”) based on\nthe following critical criteria:\n•Factual Correctness: For questions that are answerable,\nis the generated response factually accurate and supported\nby the provided context?\n•Helpfulness and Appropriateness: For questions that\nare unanswerable, is the refusal clear, polite, and does\nit provide useful information (e.g., a succinct reason for\nunanswerability, or constructive suggestions for rephras-\ning the query)?\n•Fluency and Coherence: Is the overall response gram-\nmatically correct, natural-sounding, and easy to compre-\nhend without ambiguity?\nThe aggregated results of this crucial human evaluation are\nsummarized in Table IV.\nTable IV strikingly illustrates the profound superiority of\nour RUL method as perceived through human judgment. RUL\nnot only achieved notably higher average scores for factual\ncorrectness and fluency, indicating a consistently better overall\nresponse quality, but it demonstrated a particularly dramatic\nand significant improvement in the helpfulness and appro-priateness for unanswerable questions . The exceptionally\nhigh score of 4.6 for RUL in this critical category provides\ncompelling qualitative evidence that our sophisticated RLHF\ntraining effectively teaches the LLM to provide meaningful,\nuser-friendly, and trustworthy refusals. This capability dras-\ntically curtails the perception of hallucination, fosters greater\nuser confidence, and ultimately enhances the reliability of con-\nversational AI systems. The human evaluation unequivocally\nunderscores the practical utility and robustness of our RUL\napproach in real-world human-AI interactions.\nE. Further Analysis of RUL’s Effectiveness\nBeyond the direct comparative results, a deeper analysis of\nRUL’s performance from several angles reveals the robustness\nand unique advantages of our approach in fostering reliable\nresponse generation. This section delves into the nuanced\nbehaviors of RUL, highlighting its strengths where traditional\nmethods fall short.\na) Analysis of Unanswerable Question Types: Our RUL\nmethod demonstrates enhanced capabilities across diverse\ncategories of unanswerable questions, a critical aspect often\noverlooked by simpler detection mechanisms. We categorized\nunanswerable questions within the ECA dataset into three\ncommon types based on their underlying reason for unanswer-\nability:\n•Missing Information: The context simply lacks the\nrequired facts to answer the query.\n•Contradictory Information: The context provides infor-\nmation that directly contradicts the query’s premise or the\nexpected answer.\n7\n--- Page 8 ---\nTABLE IV\nHUMAN EVALUATION SCORES (AVERAGE LIKERT SCALE : 1-5)\nMethod Factual Correctness Helpfulness/Appropriateness Fluency and Coherence\nGenerative LLM (Fine-tuned on SQuAD 2.0) 3.8 2.5 4.2\nOur RUL Method 4.3 4.6 4.5\n•Ambiguous/Out-of-Scope Questions: The query itself is\nunclear, underspecified, or pertains to topics not covered\nby the provided context.\nTable V illustrates the ranking-level accuracy of our RUL\nmethod and the best-performing baseline (Generative LLM\nFine-tuned on SQuAD 2.0) across these unanswerable question\ntypes.\nThe results in Table V clearly indicate RUL’s significant\nlead in detecting all types of unanswerable questions. Notably,\nthe most substantial performance gap is observed for ques-\ntions with contradictory information andambiguous/out-of-\nscope queries . This suggests that RUL, particularly benefiting\nfrom its RLHF stage, learns to identify subtle cues indicating\nconflict or lack of relevance, where simpler models might falter\nand attempt to generate a plausible but incorrect response.\nThis nuanced understanding across different unanswerability\ncategories is vital for building truly robust conversational AI.\nb) Analysis of Response Length and Informativeness for\nRefusals: A key advantage of RUL is its ability to gener-\nate not just a binary ”cannot answer” signal, but a more\ninformative and helpful refusal. To quantify this, we analyzed\nthe average length of refusal responses and the presence of\nspecific informative elements within them, as assessed by\nour human evaluators. Informativeness was scored based on\nwhether the refusal included a reason for unanswerability\nand/or a suggestion for the user.\nTable VI reveals a striking difference in the quality of\nrefusal responses. RUL generates significantly longer and,\nmore importantly, substantially more informative refusals.\nThe ”Avg. Informativeness Score” (where 0 means no rea-\nson/suggestion, 1 means one, and 2 means both) highlights\nthat RUL’s refusals consistently offer a clear explanation of\nwhy the question cannot be answered (e.g., ”The provided\ntext does not contain details about X”) and often suggest how\nthe user might revise their query or provide additional context.\nThis contrasts sharply with the baseline LLMs, which tend to\nprovide brief, generic denials. This demonstrates how RUL’s\ntraining, especially with RLHF focused on refusal quality,\ntranslates into a more helpful and trustworthy user experience.\nc) Efficiency Analysis: Inference Speed: While RUL in-\ntroduces additional components (attention-weighted aggrega-\ntion and potentially more sophisticated refusal generation), it\nis crucial to understand its computational efficiency during\ninference. We measured the average inference time per query-\ncontext pair on our test set for the various models. All models\nwere run on identical hardware specifications (e.g., NVIDIA\nA100 GPU).\nAs shown in Table VII, our RUL method incurs a marginalincrease in inference time compared to the fine-tuned gener-\native LLM baseline. The additional computational overhead\nstems primarily from the attention-weighted aggregation lay-\ners and the more complex generation of informative refusal\nresponses. However, this slight increase is a small trade-off\nfor the substantial gains in accuracy, reliability, and human-\nperceived helpfulness. Compared to a standalone BERT clas-\nsifier, the LLM-based approaches naturally take longer, but\nRUL’s overhead within the LLM paradigm is minimal, in-\ndicating its feasibility for real-time applications where high\nreliability is prioritized.\nV. C ONCLUSION\nThis paper has presented Reinforced Unanswerability\nLearning (RUL) , a novel and robust framework specifically\nengineered to enhance the reliability and factual grounding of\nlarge language models in information-seeking conversations.\nOur work addresses the critical challenge of hallucination by\nteaching LLMs to not only recognize when a question cannot\nbe answered from the provided context but also to articulate\nthis limitation in a helpful and user-friendly manner.\nWe have demonstrated that RUL’s hybrid architecture,\nwhich seamlessly integrates a discriminative unanswerability\nprediction head with the LLM’s generative capabilities, of-\nfers a significant advancement over existing methods. The\nintroduction of the Enhanced-CAsT-Answerability (ECA)\ndataset, with its hierarchical annotations and explicit refusal\nresponses, provided a crucial foundation for the supervised\nfine-tuning phase. Furthermore, the subsequent Reinforce-\nment Learning with Human Feedback (RLHF) stage proved\ninstrumental in refining the LLM’s understanding of unanswer-\nability, enabling it to produce nuanced, informative, and polite\nrefusal messages that drastically improve user trust.\nOur comprehensive experimental results unequivocally\nshowcase RUL’s superior performance. Quantitatively, RUL\nachieved state-of-the-art accuracy in unanswerability detec-\ntion across all granularitiessentence, paragraph, and ranking\nlevelsoutperforming strong baselines. Critically, it exhibited\na remarkably high propensity to generate appropriate refusal\nresponses for unanswerable questions, effectively mitigating\nhallucination. Beyond raw metrics, our in-depth analysis re-\nvealed RUL’s robustness across various types of unanswerable\nquestions (missing, contradictory, ambiguous information) and\nits ability to generate significantly more informative refusals.\nThe human evaluation further validated these findings, with\nRUL responses being rated notably higher for factual correct-\nness, helpfulness, and trustworthiness.\nIn conclusion, RUL represents a significant step forward in\nbuilding truly reliable conversational AI systems. By equip-\n8\n--- Page 9 ---\nTABLE V\nRANKING -LEVEL ACCURACY BY UNANSWERABLE QUESTION TYPE\nMethod Missing Information Acc. Contradictory Information Acc. Ambiguous/Out-of-Scope Acc.\nGenerative LLM (Fine-tuned on SQuAD 2.0) 0.750 0.600 0.680\nOur RUL Method 0.930 0.880 0.850\nTABLE VI\nREFUSAL RESPONSE CHARACTERISTICS (UNANSWERABLE QUESTIONS )\nMethod Avg. Refusal Length (Tokens) Avg. Informativeness Score (0-2)\nGenerative LLM (Zero-Shot Prompting) 8.5 0.4\nGenerative LLM (Fine-tuned on SQuAD 2.0) 12.1 0.9\nOur RUL Method 25.7 1.8\nTABLE VII\nAVERAGE INFERENCE TIME PER QUERY -CONTEXT PAIR(MILLISECONDS )\nMethod Avg. Inference Time (ms)\nBERT-based Classifier with Mean Aggregation 75\nGenerative LLM (Zero-Shot Prompting) 320\nGenerative LLM (Fine-tuned on SQuAD 2.0) 350\nOur RUL Method 380\nping LLMs with the intrinsic ability to ”know what they don’t\nknow” and communicate it effectively, we lay the groundwork\nfor more trustworthy, factual, and user-centric AI interactions.\nFuture work will explore extending RUL to multi-turn di-\nalogues, incorporating uncertainty quantification into refusal\njustifications, and applying this framework to diverse domains\nwhere factual reliability is paramount.\nREFERENCES\n[1] Y . Zhou, J. Shen, and Y . Cheng, “Weak to strong generalization\nfor large language models with multi-capabilities,” in The Thirteenth\nInternational Conference on Learning Representations , 2025. [Online].\nAvailable: https://openreview.net/forum?id=N1vYivuSKq\n[2] Y . Zhou, T. Shen, X. Geng, C. Tao, J. Shen, G. Long, C. Xu, and\nD. Jiang, “Fine-grained distillation for long document retrieval,” in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 38,\nno. 17, 2024, pp. 19 732–19 740.\n[3] W. Lajewska and K. Balog, “Towards reliable and factual response\ngeneration: Detecting unanswerable questions in information-seeking\nconversations,” in Advances in Information Retrieval - 46th European\nConference on Information Retrieval, ECIR 2024, Glasgow, UK, March\n24-28, 2024, Proceedings, Part III , ser. Lecture Notes in Computer\nScience, N. Goharian, N. Tonellotto, Y . He, A. Lipani, G. McDonald,\nC. Macdonald, and I. Ounis, Eds., vol. 14610. Springer, 2024, pp. 336–\n344. [Online]. Available: https://doi.org/10.1007/978-3-031-56063-7 25\n[4] W. Lajewska, K. Balog, D. Spina, and J. Trippas, “Can users detect\nbiases or factual errors in generated responses in conversational\ninformation-seeking?” in Proceedings of the 2024 Annual International\nACM SIGIR Conference on Research and Development in Information\nRetrieval in the Asia Pacific Region, SIGIR-AP 2024, Tokyo, Japan,\nDecember 9-12, 2024 , T. Sakai, E. Ishita, H. Ohshima, F. Hasibi,\nJ. Mao, and J. M. Jose, Eds. ACM, 2024, pp. 92–102. [Online].\nAvailable: https://doi.org/10.1145/3673791.3698409\n[5] A. Asai and E. Choi, “Challenges in information-seeking QA:\nunanswerable questions and paragraph retrieval,” in Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021 , C. Zong, F. Xia, W. Li, and R. Navigli,Eds. Association for Computational Linguistics, 2021, pp. 1492–1504.\n[Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.118\n[6] X. Zhang, H. Yang, and E. F. Y . Young, “Attentional transfer is all you\nneed: Technology-aware layout pattern generation,” in 58th ACM/IEEE\nDesign Automation Conference, DAC 2021, San Francisco, CA, USA,\nDecember 5-9, 2021 . IEEE, 2021, pp. 169–174. [Online]. Available:\nhttps://doi.org/10.1109/DAC18074.2021.9586227\n[7] Y . He, J. Wang, K. Li, Y . Wang, L. Sun, J. Yin, M. Zhang, and X. Wang,\n“Enhancing intent understanding for ambiguous prompts through\nhuman-machine co-adaptation,” arXiv preprint arXiv:2501.15167 , 2025.\n[8] Y . He, S. Li, J. Wang, K. Li, X. Song, X. Yuan, K. Li, K. Lu, M. Huo,\nJ. Chen et al. , “Enhancing low-cost video editing with lightweight adap-\ntors and temporal-aware inversion,” arXiv preprint arXiv:2501.04606 ,\n2025.\n[9] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers) , J. Burstein, C. Doran, and\nT. Solorio, Eds. Association for Computational Linguistics, 2019, pp.\n4171–4186. [Online]. Available: https://doi.org/10.18653/v1/n19-1423\n[10] Y . Zhou, T. Shen, X. Geng, G. Long, and D. Jiang, “Claret: Pre-training\na correlation-aware context-to-event transformer for event-centric gener-\nation and classification,” in Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) ,\n2022, pp. 2559–2575.\n[11] Z. Wang, M. Li, R. Xu, L. Zhou, J. Lei, X. Lin, S. Wang,\nZ. Yang, C. Zhu, D. Hoiem, S. Chang, M. Bansal, and\nH. Ji, “Language models with image descriptors are strong few-\nshot video-language learners,” in Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural Information\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022 , S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022.\n[Online]. Available: http://papers.nips.cc/paper files/paper/2022/hash/\n381ceeae4a1feb1abc59c773f7e61839-Abstract-Conference.html\n[12] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” J. Mach. Learn.\nRes., vol. 21, pp. 140:1–140:67, 2020. [Online]. Available: https:\n//jmlr.org/papers/v21/20-074.html\n[13] Y . Zhou, J. Zhang, G. Chen, J. Shen, and Y . Cheng, “Less is more:\n9\n--- Page 10 ---\nVision representation compression for efficient video generation with\nlarge language models,” 2024.\n[14] Y . Zhou and G. Long, “Multimodal event transformer for image-guided\nstory ending generation,” in Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics ,\n2023, pp. 3434–3444.\n[15] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling\nlaws for neural language models,” CoRR , vol. abs/2001.08361, 2020.\n[Online]. Available: https://arxiv.org/abs/2001.08361\n[16] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha,\nH. Cheng, A. Jin, T. Bos, L. Baker, Y . Du, Y . Li, H. Lee,\nH. S. Zheng, A. Ghafouri, M. Menegali, Y . Huang, M. Krikun,\nD. Lepikhin, J. Qin, D. Chen, Y . Xu, Z. Chen, A. Roberts,\nM. Bosma, Y . Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pickett,\nK. S. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos,\nT. Duke, J. Soraker, B. Zevenbergen, V . Prabhakaran, M. Diaz,\nB. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee,\nL. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V . Kuzmina,\nJ. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. A. y Arcas, C. Cui,\nM. Croak, E. H. Chi, and Q. Le, “Lamda: Language models for dialog\napplications,” CoRR , vol. abs/2201.08239, 2022. [Online]. Available:\nhttps://arxiv.org/abs/2201.08239\n[17] Y . Zhou, L. Song, and J. Shen, “Training medical large vision-\nlanguage models with abnormal-aware feedback,” arXiv preprint\narXiv:2501.01377 , 2025.\n[18] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,\nC. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,\nJ. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn,\nS. Hosseini, R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa,\nI. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,\nD. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,\nA. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,\nB. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,\nY . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\nS. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned\nchat models,” CoRR , vol. abs/2307.09288, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2307.09288\n[19] W. Lajewska and K. Balog, “Towards reliable and factual response\ngeneration: Detecting unanswerable questions in information-seeking\nconversations,” in Advances in Information Retrieval - 46th European\nConference on Information Retrieval, ECIR 2024, Glasgow, UK, March\n24-28, 2024, Proceedings, Part III , ser. Lecture Notes in Computer\nScience, N. Goharian, N. Tonellotto, Y . He, A. Lipani, G. McDonald,\nC. Macdonald, and I. Ounis, Eds., vol. 14610. Springer, 2024, pp. 336–\n344. [Online]. Available: https://doi.org/10.1007/978-3-031-56063-7 25\n[20] E. Durmus, H. He, and M. Diab, “Feqa: A question answering evaluation\nframework for faithfulness assessment in abstractive summarization,”\narXiv preprint arXiv:2005.03754 , 2020.\n[21] W. Kry ´sci´nski, B. McCann, C. Xiong, and R. Socher, “Evaluating the\nfactual consistency of abstractive text summarization,” arXiv preprint\narXiv:1910.12840 , 2019.\n[22] O. Honovich, R. Aharoni, J. Herzig, H. Taitelbaum, D. Kukliansy,\nV . Cohen, T. Scialom, I. Szpektor, A. Hassidim, and Y . Matias,\n“True: Re-evaluating factual consistency evaluation,” arXiv preprint\narXiv:2204.04991 , 2022.\n[23] B. Xue, W. Wang, H. Wang, F. Mi, R. Wang, Y . Wang, L. Shang,\nX. Jiang, Q. Liu, and K.-F. Wong, “Improving factual consistency for\nknowledge-grounded dialogue systems via knowledge enhancement and\nalignment,” arXiv preprint arXiv:2310.08372 , 2023.\n[24] Y . Zhou, X. Geng, T. Shen, W. Zhang, and D. Jiang, “Improving\nzero-shot cross-lingual transfer for multilingual question answering over\nknowledge graph,” in Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies , 2021, pp. 5822–5834.\n[25] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY . Zhang, Y . Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and\nS. Shi, “Siren’s song in the AI ocean: A survey on hallucination in\nlarge language models,” CoRR , vol. abs/2309.01219, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2309.01219\n[26] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958 , 2021.[27] Y . Laili, L. Zhang, and G. Yang, “A comprehensive method for model\ncredibility measurement,” in Model Engineering for Simulation . Else-\nvier, 2019, pp. 189–207.\n[28] J. Lee, “Instructpatentgpt: Training patent language models to follow\ninstructions with human feedback,” CoRR , vol. abs/2406.16897, 2024.\n[Online]. Available: https://doi.org/10.48550/arXiv.2406.16897\n10",
  "text_length": 55429
}