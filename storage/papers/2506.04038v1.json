{
  "id": "http://arxiv.org/abs/2506.04038v1",
  "title": "Generating Automotive Code: Large Language Models for Software\n  Development and Verification in Safety-Critical Systems",
  "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.",
  "authors": [
    "Sven Kirchner",
    "Alois C. Knoll"
  ],
  "published": "2025-06-04T15:01:59Z",
  "updated": "2025-06-04T15:01:59Z",
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04038v1",
  "full_text": "--- Page 1 ---\nGenerating Automotive Code: Large Language Models for Software\nDevelopment and Verification in Safety-Critical Systems\nSven Kirchner1, Alois C. Knoll1\n© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.Abstract — Developing safety-critical automotive software\npresents significant challenges due to increasing system com-\nplexity and strict regulatory demands. This paper proposes a\nnovel framework integrating Generative Artificial Intelligence\n(GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate\ncode generation in languages such as C++, incorporating safety-\nfocused practices such as static verification, test-driven devel-\nopment and iterative refinement. A feedback-driven pipeline\nensures the integration of test, simulation and verification for\ncompliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC)\nsystem. Comparative benchmarking of LLMs ensures optimal\nmodel selection for accuracy and reliability. Results demon-\nstrate that the framework enables automatic code generation\nwhile ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software en-\ngineering. This work advances the use of AI in safety-critical\ndomains, bridging the gap between state-of-the-art generative\nmodels and real-world safety requirements.\nI. INTRODUCTION\nRecent advancements in the automotive domain are driving\na paradigm shift from hardware-defined to software-defined\nintelligent vehicles, where software complexity and safety-\ncriticality have increased significantly. Traditional linear pro-\ncesses, such as the V-model or the waterfall model [1],\noffer limited flexibility to adapt to dynamically evolving\nrequirements. As the volume of automotive software grows,\neach change in requirements requires extensive changes to\nthe code base and repeated validation cycles, increasing both\ndevelopment time and cost. As a result, software architects\nand developers face increasing challenges in ensuring safety\ncompliance, especially given the continuous expansion of\nregulatory frameworks and standards, which are now reach-\ning levels of complexity that are difficult to track and imple-\nment manually [2]. Large Language Models (LLMs), such as\nChat GPT-3 [3], have recently shown promise in addressing\nthis complexity by transforming the role of developers from\ncode authors to orchestrators of generative pipelines. Instead\nof writing all application-level software manually, engineers\ncan leverage LLMs for automated code generation, using\nthe same standards that once hindered rapid development as\nstructured data sources for compliance.\nIn this work, we propose a novel framework that integrates\nGenerative Artificial Intelligence (GenAI) into the software\n1Technical University of Munich, Garching, Bayern, Germany\nThis research was funded by the Federal Ministry of Education and Research\nof Germany (BMBF) as part of the CeCaS project, FKZ: 16ME0800K.development lifecycle (SDLC). By using LLMs in conjunc-\ntion with test-driven development (TDD) and static analysis,\nour approach enables modular system architectures that can\nbe rapidly adapted to evolving requirements, while ensuring\ncompliance with critical safety standards. A core element of\nthe proposed framework is its focus on software generation\nduring the test and integration phases, making the LLM\nan active participant in the iterative refinement loop. Under\nthis paradigm, test suites and integration scripts assist the\nLLM by guiding automated code generation to meet specified\nrequirements. The automated process reduces the need for\nmanual recoding and retesting when system requirements\nchange. Our methodology therefore shifts engineering effort\nto the creation of specification artefacts and robust tools,\nrather than traditional manual coding. We detail how this\nframework improves development speed by minimizing hu-\nman intervention in code production and compliance checks\nand we illustrate its capabilities with an automotive case\nstudy that demonstrates its ability to save time and reduce\nerror rates. We therefore present the following contributions :\n•GenAI-Integrated SDLC: A novel LLM-driven de-\nvelopment cycle that combines TDD, static analysis\nand iterative refinement for safety-critical automotive\nsoftware.\n•Safety Monitoring Pipeline: A unified framework\nfor static analysis, formal verification, and automated\nintegration validation to ensure safety compliance in\nautomotive software systems.\n•LLM Handling: Evaluating the benchmark perfor-\nmance of LLMs and optimizing their implementation\nfor automated code generation and refinement in auto-\nmotive software development.\n•ISO-based ACC Case Study: Validation through auto-\nmated generation of an ISO-based ACC system in C++,\ntested on the CARLA simulator [4].\nII. R ELATED WORK\nThe fundamental principles of software engineering are\nbased on critical design decisions in software development.\nTo integrate safety as a priority, a focus on robust design\ncapabilities can be complemented by effective test and val-\nidation methodologies and the use of software verification\ntools. This enables GenAI to create scalable and safety-\ncritical applications while ensuring compliance with auto-\nmotive standards.arXiv:2506.04038v1  [cs.SE]  4 Jun 2025\n--- Page 2 ---\nFeedforward\nMultilayer PerceptronUser\nPrompt\nInput\nEmbedding\nMatrix\nSelf Attention MechanismLinear\nProjection\n&\nSoftmaxPost\nProcessingTokenization\nSystem\nPromptSoftware\nRequirements\n&\nSystem \nSpecificationPositional\nEncodingLayer Normalization\noutput\ncode\n.txt...\n...Fig. 1: Introduction of Attention Mechanism and Transformer Architecture in Large Language Models for Code Generation.\nA. Reducing software complexity through design choices\nManaging complexity in software engineering requires a\nsystematic approach to design choices, aimed at constraining\ndegrees of freedom and thereby reducing the likelihood\nof errors. Consequently, maintaining logical consistency in\nalgorithms and flow control becomes essential for verifying\ncorrectness [5]. Structured interactions between the system\nand its environment reduce ambiguity and improve inte-\ngration [6]. Efficient organization of data underpins perfor-\nmance and scalability [7], while modular architectures ensure\nextensibility and adaptability [8]. Test-driven development\nintegrates correctness into the development process [9], while\ncomputational precision mitigates numerical instability [10].\nDependency management and platform compatibility ensure\nconsistent behaviour across environments [11]. Addressing\nsecurity vulnerabilities is essential for demonstrating system\nsafety [12], while effectively managing concurrency is criti-\ncal for handling dynamic and parallel systems [13].\nDesign principles and standards provide a formal frame-\nwork for managing the inherent degrees of freedom in\nsoftware development, ultimately enabling the creation of\nrobust, scalable and extensible systems [8]. Strategically\ncontrolling each degree of freedom minimizes the potential\nfor error, thereby increasing the overall safety and reliability\nof the software.\nB. Safety-Critical Software\nSafety-critical software requires a systematic approach,\ntreating programming as an exact science with predictable\nand provable behaviour under all conditions [14]. Achieving\nthis requires careful selection of programming languages,\nrobust compiler validation and comprehensive verification\nand testing methodologies.\nC++ is widely used in safety-critical design because of\nto its balance of high performance, precise memory control,\nand deterministic resource management, which enables strict\nreal-time and reliability constraints to be met. Compiler\nvalidation further strengthens these guarantees by translat-\ning the code correctly. C++ compilers such as GCC and\nClang are highly optimized for performance and reliability,\noffering advanced static analysis, code optimization and\ndiagnostics [15]. Beyond language and compiler choice,\nstatic code analysis is essential to ensure logical consistency[5]. Tools such as cppcheck for C++ [16] identify memory\nleaks, race conditions and guideline violations, significantly\nreducing the likelihood of unexpected behaviors. Before\nfull integration, system behaviour is validated through unit\ntesting. Frameworks like Google Test [17] verify functional\ncorrectness by adapting testing preconditions and edge cases.\nBy integrating language safety, certified compilers, static\nanalysis and rigorous testing, this approach improves cor-\nrectness, compliance with safety standards and robustness in\nsafety-critical applications.\nC. Foundations of Requirements Engineering and Software\nDevelopment for Automotive Systems\nAutomotive software development bridges complex safety\nand functional requirements with robust code design. A\nSoftware Requirements Specification (SRS) defines both\nfunctional (e.g., system behaviour) and non-functional (e.g.,\nperformance, security) requirements, guiding the develop-\nment process. Stakeholder alignment is achieved through\nuse cases, user stories and prototypes, ensuring clarity\nand addressing safety from the outset. Safety and quality\nstandards are central. ISO 26262 [18] defines functional\nsafety requirements for road vehicles, while Automotive\nSoftware Process Improvement and Capability Determination\n(ASPICE) [19] provides a framework for assessing software\nquality and processes. The MISRA guidelines [20] stan-\ndardize programming practices, often implemented in C or\nC++ for their reliability and performance in safety-critical\nsystems. Detailed testing, including static analysis and unit\ntesting, ensures compliance with safety standards. General\nrequirements align with frameworks like ISO 26262, while\nfunction-specific requirements address unique system needs.\nBy integrating these principles, automotive software de-\nvelopment transforms complex requirements into reliable,\nmaintainable and safety-compliant systems.\nD. Integrating Generative AI into Software Engineering\nThe introduction of the Attention Mechanism revolution-\nized natural language processing, enabling the development\nof Large Language Models (LLMs) through the innovative\nTransformer Architecture. This architecture facilitates non-\nsequential data processing, overcoming the limitations of\nearlier recurrent models and introducing greater efficiency\nand scalability [21].\n--- Page 3 ---\nStructure\nCheckCompiler\nCheck\nLLMio-Check\nUnittest\nPrompt\nEngineeringgenerated\nCodePreprocessingPrompt\nLLM\nIntegration Testing Module\nSimulation\nEnvironmentIntegration\nMonitoringSystem Design\nSpecification\nSystem\nSoftwareRequirements\nEngineeringApplicationStatic Validation Module\nLLM HandlerStatic Code\nVerification\nSystem Behaviour\nSpecificationInitialize LLM, Prompt\n& Error HandlerFig. 2: The code generation architecture consists of three components: the LLM handler (light blue), the static validation\nmodule (grey) and the integration testing module (dark blue). The user (light yellow) provides specifications and the LLM\nHandler generates executable code that is iteratively refined by static checks and integration tests to ensure safety and\nfunctionality.\nIn the framework illustrated in Figure 1, a given require-\nment provided as textual input is tokenized and processed\nthrough input embeddings, where it is enriched with po-\nsitional encodings to preserve the sequential context. The\ntokens are then iteratively passed through a stack of self-\nattention layers and feedforward multilayer perceptrons.\nThese mechanisms ensure that the model captures both\nlocal and global dependencies within the input requirements.\nSubsequently, the processed tokens undergo linear projection\nand post-processing steps to generate the next token in the\nsequence. This iterative process enables the generation of\nsoftware in the form of coherent and contextually relevant\ntext. The ability to train or fine-tune foundation models like\nllama3 [22] on specific tasks has been further enhanced by\nleveraging large-scale datasets. For instance, LLMs such as\nthose designed for code generation like Qwen2.5-Coder [23]\nbenefit significantly from task-specific training datasets. The\nperformance of LLMs is heavily influenced by the quality of\nthe training data, the design of the prompt and the system\nspecifications.\nAn essential aspect of the effective use of LLMs is the\nmanagement of the context size, which is inherently limited\nby the architecture. To maximize the utility of the available\ncontext, effective prompting techniques have been developed,\nensuring that critical information is succinctly presented\nwithin the limited input space. Strategies such as Zero-shot or\nFew-shot Prompting [24], Chain-of-Thought [25] and Role-\nbased Prompting [26] enable LLMS to generate accurate\nand high quality output for a variety of tasks. Combination\nwith further validation and formal verification methods can\nimprove the overall quality of the generated code [27].\nIII. M ETHOD\nTo integrate generative AI into the software development\ncycle, we propose an approach that combines test-driven\ndevelopment with previously introduced verification meth-ods and static code analysis, ensuring safety through rapid\nfeedback and consistency.\nA. Architectural Design and Software Version Control\nThe framework illustrated in Figure 2) shows the GenAI\nintegrated SDLC. User input, including detailed specifica-\ntions, serves as the foundation. The LLM Handler transforms\nthis input into executable code and iteratively refines it based\non feedback from subsequent phases. The Static Validation\nModule analyzes the generated code for compliance with\nsafety standards and design principles. Finally, the Integra-\ntion Testing Module evaluates the system in a dynamic test\nenvironment, ensuring robust performance and functional\ncorrectness.\nSafe\nStatesVerified StatesUnsafe States\nIISI\nIs ssi\ni\nFig. 3: Software versions are categorized by safety clas-\nsification, progressing through three primary states. The\n”Verified State” is achieved upon successful completion of\nstatic testing. ”Safe States” are achieved after meeting static\nand integration test criteria. The LLM Handler dynamically\nmanages the generation of LLMs based on the current state\nof the software.\nThe use of automated code refinement and regeneration\nis highly dependent on the software’s current development\nstage and version. In safety-critical systems, version control\nadheres to a methodology combining integration monitoring,\n--- Page 4 ---\nstatic code analysis and iterative refinement (Figure 3). The\nultimate objective is to achieve a ”Safe State,” wherein all\npredefined functional and safety requirements are satisfied.\nEach iteration ISinvolves static analysis managed by the\nStatic Validation module to detect and resolve logical incon-\nsistencies and design violations. Upon successful completion,\nthe code moves from a static state ( sS\ni) to an integration\nstate ( sI\ni) within a simulation environment where iterative\nvalidation and refinement occurs ( II). Transition to a safe\nstate is only realized when both static and integration criteria\nare conclusively met.\nThis cyclical process of static and integration iterations\nensures continuous improvement of the software. Each sub-\nsequent state builds incrementally upon its predecessor,\nanchored in validated safety and integration protocols. Im-\nportantly, a new verified state only generated when the\nintegration phase is successful and all static checks are\nresolved, ensuring an uncompromising commitment to safety\nand correctness.\nB. LLM based generation: Specification and User Input\nUser input is given in two distinct classes: System Design\nSpecification and System Behaviour Specification. The Sys-\ntem Design Specification focuses on the inputs and outputs\nof the system, using precise mathematical language to define\nalgorithmic preconditions and postconditions. The System\nBehaviour Specification describes the overall structure and\nbehaviour of the system. For prompting LLMs, structured\ntext is provided as input. JSON is a widely used in software\nengineering due to its standardization and compatibility.\nYAML’s human-readable features, such as whitespace struc-\nturing, optional quotes and support for inline comments,\nenhance usability and interpretability during the specification\nphase [28]. To maximize prompt efficiency, JSON is used\nfor the system design specification, while YAML is used for\nthe system behaviour specification. This dual-format strategy\nensures effective context management and optimal use of\nprompt space to generate accurate and interpretable output.\nThe framework integrates zero-shot and few-shot prompt-\ning to optimize LLM performance. Zero-shot prompting\nestablishes baseline output, followed by iterative few-shot\nrefinement to improve accuracy and resolve errors. This pro-\ncess transitions from exploratory prompts to precise adjust-\nments based on output quality. Chain-of-Thought reasoning\nimproves version control by providing the LLM with the\nbest prior solution, enabling iterative improvement towards\na safe state, as shown in Figure 3. Role-based prompting\ndefines the LLM’s role as a ”specialized AI assistant for\nsafety-critical automotive code generation”, ensuring outputs\nare tailored to the framework’s requirements. Each iteration\nbuilds on previous results, driving continuous improvement\nand alignment with specifications.\nSelecting an appropriate LLM is critical to achieving op-\ntimal results. We use McEval: Massively Multilingual Code\nEvaluation [29], a benchmark designed to evaluate LLM\nperformance across 40 programming languages, including\nRust and C++, using 16,000 test cases. This provides acomprehensive framework for evaluating multilingual code\ngeneration. For reasoning and iterative code refinement, we\nadopt Aider’s code editing benchmark [30]. It evaluates\nprecision and consistency in modifying functions, imple-\nmenting missing functionality and refactoring code from\nnatural language instructions. By prioritizing editing over\ngeneration , the Aider benchmark evaluates the accuracy and\nconsistency of code review and refinement in a variety of\nprogramming challenges, making it critical for frameworks\nthat require robust iterative development capabilities. We\nalso include the widely recognized HumanEval benchmark\n[31]. Table I summarizes the performance of various large\nlanguage models across these benchmarks, highlighting their\nmultilingual code generation and iterative refinement ca-\npabilities. We evaluate state-of-the-art open-source LLMs,\nincluding Qwen2.5-Coder-7B-Instruct, Llama-3-8B-Instruct,\nDeepSeek-Coder-V2 Lite Instruct [32], DeepSeek-Coder\n33B Instruct [33], and CodeStral-22B [34], using GPT-4o\nas a benchmark for comparison.\nLLM Code Generation and Refactoring Benchmark\nModelAiderMcEvalHumanEval\nCodeEditing (0-shot)\nQwen2.5-Coder-7B-I 57.9 60.3 88.4\nLlama-3-8B-Instruct 37.6 32.0 62.2\nDeepSeek-Coder-V2 LI 48.9 54.7 81.1\nDeepSeek-Coder 33B I 49.6 54.3 79.3\nCodeStral-22B 48.1 50.5 78.1\nGPT-4o (240513) 54.0 72.9 90.2\nTABLE I: Evaluation of state-of-the-art large language mod-\nels, with all values reported in percentages. McEval results\nrepresent Pass@1 performance, while HumanEval scores\nreflect 0-shot capabilities. DeepSeek-Coder-V2 LI denotes\nthe Lite Instruct variant, DeepSeek-Coder 33B I refers to\nthe Instruct version, and Qwen2.5-Coder-7B-I indicates the\nInstruct variant.\nGiven the sensitivity of the data and the stringent data\nprotection requirements in the automotive domain, our use\ncase necessitates a locally deployable LLM capable of han-\ndling complex programming tasks without compromising\ndata security. Among models with a token size of less\nthan 10B, Qwen2.5-Coder-7B-Instruct proves as the most\neffective option, offering excellent performance while being\nthe smallest in this category.\nOnce the LLM has generated the output, the next step is to\nextract the code and install the necessary libraries. This phase\nincludes dependency management, ensuring all necessary\nlibraries and tools are properly defined and integrated into\nthe software environment. By automating these tasks, the\nframework accelerates the development cycle while main-\ntaining precision and reliability. This structured approach\nto LLM-based generation bridges the gap between user-\nprovided input and actionable software artefacts.\nC. Static Validation Module\nOnce the code has been extracted from the LLM analysis,\nthe first step in the evaluation process is static code analysis.\n--- Page 5 ---\nThis phase involves a series of checks to ensure the safety,\nfunctionality and adherence to design specifications of the\ngenerated code. Static code analysis plays a critical role in\nidentifying potential problems early on, providing a strong\nfoundation for seamless integration into the main framework.\nTo advance to the next state ( S) and proceed to integration\ntesting, the generated software ( SW) must successfully pass\na series of static checks ( ci). The process shown in 1 involves\nan iterative cycle of software generation and refinement,\nwhere each iteration systematically resolves errors ( E) iden-\ntified during the previous analysis. Through this approach,\nthe software progressively achieves compliance with the\nrequired safety and functional standards, ensuring readiness\nfor the integration phase.\nAlgorithm 1 Static Analysis and Error Handling\n1:while∃ss\ni∈Ssuch that ss\ni̸=success do\n2: SW←generate code (ss\ni, E)\n3: forci∈Cdo\n4: ss\ni←analyze (SW, c i)\n5: ifss\ni==success then\n6: continue\n7: else\n8: E←geterror analysis (SW, c i)\n9: break\n10: end if\n11: end for\n12:end while\n13:S←runintegration monitoring (SW )\nThe static analysis process consists of several key checks:\nIt starts with a structure check, which verifies that the\nfunction name of the primary functionality is in the appropri-\nate place in the generated code. This ensures compatibility\nwith the broader framework and establishes a baseline for\nintegration.\nThe compilation check in C++ ensures that the code\ncan be successfully compiled by validating it. The com-\npiler performs multiple checks, including syntactic verifi-\ncation to enforce correct grammar and semantic analysis\nto ensure meaningful execution. Modern compilers employ\nsophisticated multi-layered validation mechanisms to detect\ntype inconsistencies, scope violations and improper memory\nusage. This process guarantees both syntactic and semantic\ncorrectness, reinforcing code reliability and robustness in\nhigh-performance computing environments.\nNext, the static code style and design check enforces\nadherence to established formatting and design principles,\nensuring maintainability and consistency. Tools such as cp-\npcheck for C++ validate compliance with standards such as\nMISRA to address stylistic and semantic issues.\nFinally, unit testing validates the functional behaviour of\nthe code. Frameworks such as Google Test in C++ are used to\nrun comprehensive test suites, covering diverse scenarios to\nconfirm that inputs and outputs align with specified require-\nments. These tests serve as a safety net against regressionsand ensure correctness in future iterations. When using unit\ntesting as feedback, it is crucial to withhold specific failure\ndetails from the LLM, preventing over-fitting to individual\ntests and preserving generalizable behaviour.\nTaken together, these steps constitute a rigorous and sys-\ntematic approach to static analysis, ensuring that the code\nmeets the highest standards of safety, compatibility and\nreliability, while laying a robust foundation for subsequent\ndevelopment and integration.\nD. Integration Monitoring Module\nMonitoring Agent\n27,1m\n16,2m5,1mya ,v ,xδ\nxx\nψAD System\nSoftwareAPI sysLLM out\nSimulationLLM in\nFig. 4: The integration monitoring framework consists of\nthree key components: the monitoring agent, which acts as\na client to the simulation server and has access to ground\ntruth data; the simulation server, which provides synthetic\nsensor and vehicle state information; and the AD system,\nwhich provides an autonomous driving system that allows to\nintegrate the generated software through clear APIs.\nAfter successfully passing the static analysis phase and\ndemonstrating full compliance with static safety require-\nments, the generated software is integrated into the Au-\ntonomous Driving (AD) system. This integration is achieved\nthrough the use of well-defined and strictly verified Ap-\nplication Programming Interfaces (APIs), which have been\nvalidated during the static analysis phase for consistency\nwith the previously verified software components. These\nvalidated System APIs ( API sys) ensure a seamless and\nrobust connection between the generated software and the\nbroader AD system architecture. The AD system, which\nreceives both vehicle state information and sensor data from\na simulation environment, consists of the three core com-\nponents of autonomous driving systems: detection, planning\nand control. For the detection module, YOLOP [35] is used,\nenabling simultaneous semantic segmentation and object\ndetection, providing essential contextual information. The\nCARLA simulator serves as the simulation environment,\ngenerating synthetic sensor data, such as camera feeds, as\nshown in Figure 4. To enrich the object detection with depth\ninformation, a depth camera is integrated to provide distance\nmeasurements relative to the ego vehicle.\n--- Page 6 ---\nThis data, along with other vehicle states obtained from\nthe inertial measurement unit (IMU), is transmitted to the\nAD system. The integration monitoring framework includes\na monitoring agent (see figure 4) that uses a simple API to\ninspect the environment and system behaviour through its\naccess to the ground truth data from the simulation server.\nUser-provided system behaviour specifications serve as a\nfundamental element in this framework. These specifications\narticulate simple mathematical functions designed to validate\nvehicle behaviour against pre-defined performance criteria.\nThe integration process follows a structured timeline that\nincludes three distinct phases: initialization, data processing\nand evaluation. During the initialization phase, multi-\nprocessing is used to simultaneously start the simulation\nenvironment, the monitoring agent, and the automated\ndriving (AD) system, ensuring that the performance\nof the generated function remains unaffected. Once\noperational, the integration monitoring system performs\nthree critical functions: getdata from carla server ,\nprocess carla data() and\nadddata tostatistics() . At the end of a predefined\nintegration test duration, the evaluate data() function\nis triggered to perform the mathematical validations defined\nin the system behaviour specifications. These validations\nspan a spectrum of criteria, ranging from behavioural\nand comfort-related metrics to strict timing requirements.\nUpon successful completion of this stage, the generated\nsoftware, refined through iterative interactions with the\nLLM, achieves the robustness and reliability required\nfor seamless integration, meeting all safety-critical and\nperformance criteria.\nIV. E VALUATION\nTo evaluate the proposed framework, we generate a test\nfunction designed to demonstrate its effectiveness in a re-\nalistic scenario. Specifically, we select the adaptive cruise\ncontrol (ACC) system, a widely studied application in the\nautomotive domain. The ACC system offers robust evaluation\ncapabilities in simulation and aligns with the stringent ISO\nstandards, providing a benchmark for the performance and\nreliability of our framework.\nA. Simulation Environment and Hardware Setup\nThe primary objective of the ACC system is to control the\nlongitudinal motion of a vehicle. As a critical component of\nthe control subsystem in autonomous driving systems, the\nACC takes as input the bounding box of the lead vehicle\nincluding distance information and the inertial measurement\nunit (IMU) data of the ego vehicle. Using these inputs,\nthe system calculates the necessary longitudinal motion and\ngenerates throttle and brake commands as outputs. These\ninterfaces are essential for maintaining the correct vehicle\nspeed and spacing in dynamic driving conditions. To ensure\nseamless integration within the integration system, the AD\nsystem includes additional modules required for vehicle\noperation. These include object detection and segmentation\nto identify surrounding objects, as well as motion planningand a lateral control module to ensure coordinated vehicle\nmaneuvers. The simulation environment replicates real-world\ndriving scenarios, providing a controlled yet dynamic setting\nto evaluate the ACC function. The system is tested under\nvarious conditions to validate its robustness, efficiency and\ncompliance with ISO requirements. The hardware setup for\nthis evaluation shown in Table II is critical to achieve high-\nperformance execution and accurate real-time simulation.\nHardware Setup for the Generation and Testing Modules\nCPU AMD Ryzen 9 9950X (16x 4.3 GHz, 170W)\nGPU 2 x NVIDIA GeForce RTX 4090 24GB\nRAM 64GB DDR5-5600 Vengeance (2x 32GB)\nTABLE II: To accelerate the development process, we lever-\nage a high-performance hardware configuration to run the\nLLM locally and execute the algorithm within the simulation\nenvironment faster than real-time, ensuring efficient experi-\nmentation and iterative refinement.\nTo enhance computational efficiency, we use accelerated\ninference techniques to reduce latency and enable faster\nprocessing. Memory management optimizations are also im-\nplemented to minimize gaps in GPU memory allocation. By\ndynamically growing memory segments, the framework en-\nsures efficient utilization of VRAM, reducing fragmentation\nand improving stability during prolonged simulations.\nB. Requirements Engineering and System Specification\nThe requirements for the evaluation of the Adaptive Cruise\nControl (ACC) function are derived from a combination\nof multiple sources. The primary standard used is ISO\n15622[36], which specifies the performance, safety and\nfunctional behaviour requirements for ACC systems. This\nstandard serves as the foundation for defining the control\nstrategy and minimum functional requirements of ACC sys-\ntems, including parameters such as time gap ( τ), clearance\n(c) and ego vehicle speed ( v).\nτ, c\nv, a\nFig. 5: ISO 15622 (Intelligent Transport Systems: Adaptive\ncruise control systems – Performance requirements and test\nprocedures) defines the basic control strategy and minimum\nfunctional requirements for ACC systems. The time gap is\nintroduced as τ, the clearance as cand the ego vehicle speed\nasv.\nFrom this, we derive the requirement that the minimum\nclearance should satisfy:\nMAX (cmin, τmin·v) (1)\nIn addition, we define a nominal following distance of 10\nmeters as an optimal balance between safety and driving\n--- Page 7 ---\ncstructure−checkccompilation−checkccpp−check cio−check\nFailed check ci of the Static Iteration Is0246810Number of Iterationsgeneration 1\ngeneration 2\ngeneration 3Fig. 6: Across three independent code generation runs using\nQwen2.5-Coder-7B-I, an average of 16.3 iterations were\nrequired to pass all static tests. Structure and compilation\nchecks failed the least (2 iterations on average). While the\nCPP check, including MISRA compliance, took an average\nof 4.3 iterations, the most demanding phase was unit testing,\nwhich required 8 iterations on average to ensure correctness.\ncomfort. In addition, to ensure physically possible driving\nmanoeuvres, we set the requirement to reduce the maximum\npossible acceleration (positive and negative). The maximum\nacceleration is therefore defined as:\n|a|<5m/s2(2)\nif no emergency brake is applied. The architecture, inter-\nfaces and operating logic are carefully defined, following\nestablished best practice in control system development.\nGenerative AI is given controlled access to throttle and\nbrake APIs, complemented by real-time vehicle state data\nand object detection results, enabling seamless integration\nwith the wider autonomous driving (AD) system. Software\nquality and reliability is ensured by MISRA-compliant static\nanalysis, which facilitates early detection of potential prob-\nlems. In addition, preconditions for integration monitoring,\nas outlined in ISO 15622, form the foundation for subsequent\ntest phases, ensuring thorough evaluation and compliance\nwith critical standards.\nC. Evaluating the Performance of the generated ACC System\nTo analyze the behavior of the LLM across both software\nstates (Figure 3), we evaluate its ability to generate code that\nmeets the requirements of the verified state (static checks)\nand the safe state (combined static and integration checks).\nWe apply the defined requirement checks and initiate code\ngeneration, running three independent iterations to analyse\nand compare the number of attempts it takes the LLM frame-\nwork to produce code that satisfies the static requirements of\na given check (Figure 6).\nTo ensure effective testing, the behaviour of the leading\nvehicle is randomised during integration monitoring. This ap-\nproach prevents the LLM from tailoring the generated func-\ntion to a specific scenario, thereby improving generalization.\nThe results, shown in figures 7 and 8, demonstrate consistent\ncompliance with the defined requirements. The generated\nfunctions maintain the expected vehicle behaviour, and even\nunder emergency conditions the deceleration remains within\nthe prescribed safety threshold.\n0 2 4 6 8 10 12\nTime [s]024681012Distance [m]\ngeneration 1\ngeneration 2\ngeneration 3Fig. 7: The distance between the ego vehicle and the leading\nvehicle over time shows that all three generated functions\nmaintain a consistent following distance of approximately 10\nmetres. The behaviour shows only minor differences across\nall implementations, with adjustments occurring mainly in\nthe 8m to 12m range.\n0 2 4 6 8 10 12\nTime [s]−6−4−20246Acceleration [m/s²]\ngeneration 1\ngeneration 2\ngeneration 3\nFig. 8: The acceleration profiles over time show the typical\noscillatory behaviour of all the functions evaluated. Notably,\neach function adheres to the predefined safety requirement,\nensuring that the braking does not exceed the threshold of 5\nm/s².\nV. CONCLUSION AND FUTURE WORK\nThis work addresses the problem of integrating generative\nAI into safety-critical automotive software development.\nSpecifically, this work aims to (1) develop an LLM-driven\nsoftware development framework that ensures compliance\nwith functional safety requirements and (2) establish a struc-\ntured pipeline for validation through static code analysis and\nintegration monitoring. By using structured specifications,\nautomated refinement, and iterative validation, the proposed\nframework enables efficient and reliable software genera-\ntion for safety-critical applications. Using a case study on\nadaptive cruise control (ACC) case study, our experiments\ndemonstrate that the generated functions meet predefined\nsafety and performance constraints, maintaining safe follow-\ning distances and adhering to acceleration limits even in\nemergency scenarios.\nFuture work will extend the framework by adapting mathe-\nmatical guarantees to prove software correctness. Additional\nextensions could involve increasing the number and com-\nplexity of requirements to better assess the robustness of\nvarious LLMs, as well as evaluating their performance across\ndifferent automotive functions. Furthermore, the exploration\nof system-level AI-driven software design methodologies\ncould facilitate the development of a structured model that\nimproves the integration of generative AI into the automotive\n--- Page 8 ---\nsoftware development lifecycle, building upon and refining\nthe ASPICE standard. Further evaluation in real-world test-\ning will also be critical to validate the effectiveness of the\nframework in practical deployment scenarios.\nREFERENCES\n[1] Roger Pressman. Software Engineering: A Practitioner’s\nApproach . 7th ed. USA: McGraw-Hill, Inc., 2009. ISBN :\n0073375977.\n[2] Alexandru Constantin Serban et al. “A Standard Driven\nSoftware Architecture for Fully Autonomous Vehicles”. In:\n2018 IEEE International Conference on Software Architec-\nture Companion (ICSA-C) . 2018, pp. 120–127. DOI:10.\n1109/ICSA-C.2018.00040 .\n[3] Tom B. Brown et al. Language Models are Few-Shot Learn-\ners. 2020. arXiv: 2005.14165 [cs.CL] .URL:https:\n//arxiv.org/abs/2005.14165 .\n[4] Alexey Dosovitskiy et al. CARLA: An Open Urban Driving\nSimulator . 2017. arXiv: 1711 . 03938 [cs.LG] .URL:\nhttps://arxiv.org/abs/1711.03938 .\n[5] Ian Sommerville. Software Engineering . 9th. Boston, MA,\nUSA: Addison-Wesley, 2011.\n[6] Roger S. Pressman. Software Engineering: A Practitioner’s\nApproach . 8th. New York, NY , USA: McGraw-Hill, 2014.\n[7] Abraham Silberschatz et al. Database System Concepts . 7th.\nNew York, NY , USA: McGraw-Hill, 2020.\n[8] Len Bass et al. Software Architecture in Practice . 3rd.\nBoston, MA, USA: Addison-Wesley, 2012.\n[9] Kent Beck. Test-Driven Development by Example . Boston,\nMA, USA: Addison-Wesley, 2003.\n[10] Nicholas J. Higham. Accuracy and Stability of Numerical\nAlgorithms . 2nd. Philadelphia, PA, USA: Society for Indus-\ntrial and Applied Mathematics (SIAM), 2002.\n[11] Martin Fowler. Patterns of Enterprise Application Architec-\nture. Boston, MA, USA: Addison-Wesley, 2004.\n[12] John Viega and Gary McGraw. Building Secure Software:\nHow to Avoid Security Problems the Right Way . Boston,\nMA, USA: Addison-Wesley, 2001.\n[13] Nir Shavit and Maurice Herlihy. The Art of Multiprocessor\nProgramming . San Francisco, CA, USA: Morgan Kauf-\nmann, 2012.\n[14] Edsger Wybe Dijkstra. A Discipline of Programming . 1st.\nUSA: Prentice Hall PTR, 1997. ISBN : 013215871X.\n[15] Bjarne Stroustrup. The C++ Programming Language . 4th.\nAddison-Wesley Professional, 2013. ISBN : 0321563840.\n[16] Daniel Marjam ¨aki. Cppcheck: A static analysis tool\nfor C++ . 2024. URL:https : / / cppcheck .\nsourceforge.io/ .\n[17] Google. Google Test: C++ Testing Framework . Accessed:\n2023-12-10. 2023. URL:https : / / github . com /\ngoogle/googletest .\n[18] International Organization for Standardization. ISO 26262:\nRoad vehicles – Functional safety . Geneva, Switzerland:\nISO, 2018.\n[19] VDA QMC Working Group 13. Automotive SPICE Process\nAssessment Model . Berlin, Germany: German Association\nof the Automotive Industry (VDA), 2022.[20] Motor Industry Software Reliability Association. MISRA C:\nGuidelines for the use of the C language in critical systems .\n3rd. UK: MISRA, 2012.\n[21] Ashish Vaswani et al. Attention Is All You Need . 2023. arXiv:\n1706.03762 [cs.CL] .URL:https://arxiv.org/\nabs/1706.03762 .\n[22] Aaron Grattafiori et al. The Llama 3 Herd of Models . 2024.\narXiv: 2407.21783 [cs.AI] .URL:https://arxiv.\norg/abs/2407.21783 .\n[23] Binyuan Hui et al. “Qwen2. 5-Coder Technical Report”. In:\narXiv preprint arXiv:2409.12186 (2024).\n[24] Anna Scius-Bertrand et al. “Zero-Shot Prompting and Few-\nShot Fine-Tuning: Revisiting Document Image Classifica-\ntion Using Large Language Models”. In: Pattern Recogni-\ntion. Springer Nature Switzerland, Dec. 2024, pp. 152–166.\nISBN : 9783031784958. DOI:10.1007/978- 3- 031-\n78495-8_10 .\n[25] Zhuosheng Zhang et al. Multimodal Chain-of-Thought Rea-\nsoning in Language Models . 2024. arXiv: 2302.00923\n[cs.CL] .\n[26] Aobo Kong et al. Better Zero-Shot Reasoning with Role-\nPlay Prompting . 2024. arXiv: 2308 . 07702 [cs.CL] .\nURL:https://arxiv.org/abs/2308.07702 .\n[27] Merlijn Sevenhuijsen et al. VeCoGen: Automating Genera-\ntion of Formally Verified C Code with Large Language Mod-\nels. 2025. arXiv: 2411.19275 [cs.SE] .URL:https:\n//arxiv.org/abs/2411.19275 .\n[28] Malin Eriksson and Victor Hallberg. “Comparison between\nJSON and YAML for Data Serialization.” PhD thesis. 2011.\nURL:https://urn.kb.se/resolve?urn=urn:\nnbn:se:kth:diva-130815 .\n[29] Linzheng Chai et al. McEval: Massively Multilingual Code\nEvaluation . 2024. arXiv: 2406.07436 [cs.PL] .\n[30] Aider-AI. Aider Code Editing Benchmark .https : / /\ngithub . com / Aider - AI / aider / blob / main /\nbenchmark/README.md . Accessed: 2025-01-22. 2024.\n[31] Mark Chen et al. Evaluating Large Language Models\nTrained on Code . 2021. arXiv: 2107.03374 [cs.LG] .\n[32] DeepSeek-AI et al. DeepSeek-Coder-V2: Breaking the Bar-\nrier of Closed-Source Models in Code Intelligence . 2024.\narXiv: 2406.11931 [cs.SE] .\n[33] Daya Guo et al. DeepSeek-Coder: When the Large Language\nModel Meets Programming – The Rise of Code Intelligence .\n2024. arXiv: 2401.14196 [cs.SE] .\n[34] Mistral AI. Codestral: Hello, World! Accessed: January 27,\n2025. 2024. URL:https : / / mistral . ai / news /\ncodestral/ .\n[35] Dong Wu et al. “YOLOP: You Only Look Once for Panoptic\nDriving Perception”. In: Machine Intelligence Research 19.6\n(Nov. 2022), pp. 550–562. ISSN : 2731-5398. DOI:10 .\n1007/s11633- 022- 1339- y .URL:http://dx.\ndoi.org/10.1007/s11633-022-1339-y .\n[36] International Organization for Standardization. Intelligent\ntransport systems — Adaptive Cruise Control systems —\nPerformance requirements and test procedures . 2018. URL:\nhttps://www.iso.org/standard/71515.html .",
  "text_length": 41479
}