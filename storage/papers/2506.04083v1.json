{
  "id": "http://arxiv.org/abs/2506.04083v1",
  "title": "A Generative Adaptive Replay Continual Learning Model for Temporal\n  Knowledge Graph Reasoning",
  "summary": "Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning\n(TKGR) methods focus on significantly reducing computational cost and\nmitigating catastrophic forgetting caused by fine-tuning models with new data.\nHowever, existing CL-based TKGR methods still face two key limitations: (1)\nThey usually one-sidedly reorganize individual historical facts, while\noverlooking the historical context essential for accurately understanding the\nhistorical semantics of these facts; (2) They preserve historical knowledge by\nsimply replaying historical facts, while ignoring the potential conflicts\nbetween historical and emerging facts. In this paper, we propose a Deep\nGenerative Adaptive Replay (DGAR) method, which can generate and adaptively\nreplay historical entity distribution representations from the whole historical\ncontext. To address the first challenge, historical context prompts as sampling\nunits are built to preserve the whole historical context information. To\novercome the second challenge, a pre-trained diffusion model is adopted to\ngenerate the historical distribution. During the generation process, the common\nfeatures between the historical and current distributions are enhanced under\nthe guidance of the TKGR model. In addition, a layer-by-layer adaptive replay\nmechanism is designed to effectively integrate historical and current\ndistributions. Experimental results demonstrate that DGAR significantly\noutperforms baselines in reasoning and mitigating forgetting.",
  "authors": [
    "Zhiyu Zhang",
    "Wei Chen",
    "Youfang Lin",
    "Huaiyu Wan"
  ],
  "published": "2025-06-04T15:44:50Z",
  "updated": "2025-06-04T15:44:50Z",
  "categories": [
    "cs.IR"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.04083v1",
  "full_text": "--- Page 1 ---\narXiv:2506.04083v1  [cs.IR]  4 Jun 2025A Generative Adaptive Replay Continual Learning Model for\nTemporal Knowledge Graph Reasoning\nZhiyu Zhang1,3, Wei Chen2*, Youfang Lin1,3, Huaiyu Wan1,3\n1School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\n2Guilin University of Electronic Technology,\nSchool of Computer Science and Information Security, Guangxi, China\n3Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China\n{zyuzhang, yflin, hywan}@bjtu.edu.cn ,w_chen@guet.edu.cn\nAbstract\nRecent Continual Learning (CL)-based Tem-\nporal Knowledge Graph Reasoning (TKGR)\nmethods focus on significantly reducing com-\nputational cost and mitigating catastrophic for-\ngetting caused by fine-tuning models with new\ndata. However, existing CL-based TKGR meth-\nods still face two key limitations: (1) They\nusually one-sidedly reorganize individual his-\ntorical facts, while overlooking the historical\ncontext essential for accurately understanding\nthe historical semantics of these facts; (2) They\npreserve historical knowledge by simply replay-\ning historical facts, while ignoring the potential\nconflicts between historical and emerging facts.\nIn this paper, we propose a DeepGenerative\nAdaptive Replay (DGAR) method, which can\ngenerate and adaptively replay historical entity\ndistribution representations from the whole his-\ntorical context. To address the first challenge,\nhistorical context prompts as sampling units are\nbuilt to preserve the whole historical context in-\nformation. To overcome the second challenge,\na pre-trained diffusion model is adopted to gen-\nerate the historical distribution. During the gen-\neration process, the common features between\nthe historical and current distributions are en-\nhanced under the guidance of the TKGR model.\nIn addition, a layer-by-layer adaptive replay\nmechanism is designed to effectively integrate\nhistorical and current distributions. Experimen-\ntal results demonstrate that DGAR significantly\noutperforms baselines in reasoning and mitigat-\ning forgetting.\n1 Introduction\nTemporal Knowledge Graphs (TKGs) extend tra-\nditional Knowledge Graphs (KGs) by associating\ntriples with timestamps (Leblay and Chekol, 2018;\nDasgupta et al., 2018; Lacroix et al., 2020), provid-\ning dynamic and structured time-sensitive knowl-\nedge for various downstream applications (Chen\n*Corresponding authoret al., 2023; Gutiérrez et al., 2024; Wang et al.,\n2024; Zhao et al., 2025), such as Large Language\nModels reasoning, event prediction, and financial\nforecasting (Guan et al., 2022). Unfortunately,\nTKGs often suffer from incompleteness, hinder-\ning the capability of dynamic knowledge repre-\nsentation in downstream applications. Temporal\nknowledge Graph Reasoning (TKGR) is proposed\nto address this issue by inferring missing temporal\nfacts based on historical knowledge.\nIn real-world scenarios, TKGs are continuously\nupdated with unseen entities, relations, and new\nfacts. Existing TKGR studies (Leblay and Chekol,\n2018; Li et al., 2021, 2022b; Xu et al., 2023a) up-\ndate model parameters by retraining on the entire\nTKG when new data arrives. This process is com-\nputationally expensive and impractical for dynamic\nsettings, especially in the transportation and finance\ndomains where frequent knowledge updates are re-\nquired (Liu et al., 2024a). Continual Learning(CL)\nfine-tuning models with new data may seem intu-\nitive, but often results in catastrophic forgetting,\nwhere prior knowledge is lost (Mirtaheri et al.,\n2023).\nTo mitigate catastrophic forgetting, recent CL-\nbased TKGR studies (Wu et al., 2021; Mirtaheri\net al., 2023) rely on the mechanisms of replaying\nprior knowledge, further employing regularization\ntechniques to preserve old knowledge. These stud-\nies integrate new knowledge while preserving pre-\nviously acquired information, thus enabling reason-\ning over both historical and emerging data.\nDespite notable progress, the currently CL-based\nTKGR methods still face two primary challenges:\n(1) These methods often reorganize and replay\nthe historical data (e.g., based on frequency or clus-\ntering) to mitigate catastrophic forgetting. How-\never, such methods solely focus on the statistical\nproperties of individual historical events, which\nfails to correctly understand the historical seman-\ntics of these facts combining the necessary histor-\n--- Page 2 ---\n(a)\n (b)Stage 1 Stage 2 Stage 3 Stage 4Figure 1: The distributions of the same set of entity\nfeatures at different timestamps are visualized using U-\nMAP visualization. Entities involved in all facts at a\nrandomly selected time iare extracted. Stage 1 repre-\nsents the feature distribution of these entities at time\ni, while stage 2, 3, and 4 respectively correspond to\ntheir feature distributions at times j,mandn, where\ni < j < m < n . (a) depicts the distributions learned by\nthe base model across these timestamps, and (b) shows\nthe distributions learned by DGAR. The results demon-\nstrate that our approach effectively resolves distribution\nconflicts while preserving historical knowledge.\nical context. Besides, this fragmented approach\nmakes it difficult to capture the overarching trends\nof entity behavior, thus limiting the model’s capac-\nity in complex reasoning tasks.\n(2) Current approaches typically replay histor-\nical data directly overlooking potential conflicts\nbetween the distributions of historical and current\ndata (e.g., Figure 1). As entities associate with dif-\nferent neighbors over time, semantic differences\narise, which in turn cause conflicts in the distribu-\ntions of entities at different times. This oversight\nhinders the effectiveness of mitigating catastrophic\nforgetting.\nTo address these challenges, we propose a Deep\nGenerative Adaptive Replay (DGAR) method for\nTKGR, which can continually and adaptively re-\nplay historical information by generating the histor-\nical distribution representation of entities from the\nwhole historical context. For the first challenge, in-\nstead of using individual facts as sampling units, we\nbuild Historical Context Prompts (HCPs) as sam-\npling units to retain the context information of his-\ntorical data. For the second challenge, we enhance\nthe common features across different distributions\nand introduce a deep adaptive replay mechanism\nto mitigate distribution conflicts. Specifically, we\ndesign a Diffusion-Enhanced Historical Distribu-\ntion Generation (Diff-HDG) strategy that gener-\nates entity historical distribution representations.\nDuring the generation process, the features of the\nentity’s historical distribution that are common tothe entity’s current distribution are enhanced. In\naddition, a layer-by-layer Deep Adaptive Replay\n(DAR) mechanism is introduced to inject the en-\ntity’s historical distribution representation into its\ncurrent distribution representation.\nIn summary, the main contributions of this work\nare as follows:\n•We propose a novel Generative Adaptive Re-\nplay Continual Learning method for TKGR,\nwhich effectively addresses the issue of knowl-\nedge forgetting by incorporating the entire\nhistorical context and mitigating distribution\nconflicts.\n•A sophisticated historical context prompt is\ndesigned for replay data sampling, ensuring\nthe semantic integrity of the historical context\ninformation in the sampled facts.\n•A Diff-HDG strategy is proposed to generate\nhistorical distribution representations by en-\nhancing the common features. In addition,\na DAR mechanism is designed to efficiently\nintegrate historical and current distributions.\n•Extensive experiments conducted on widely\nused TKGR datasets demonstrate the supe-\nriority of our approach, consistently outper-\nforming all baseline methods across various\nmetrics.\n2 Related Work\n2.1 Reasoning on TKGs\nTKGR aims to infer missing facts by utilizing\nknown facts. Recent advancements in this field fall\ninto four main approaches. The distribution-based\napproaches (Leblay and Chekol, 2018; Lacroix\net al., 2020) perform reasoning by training a scor-\ning function that can evaluate the distance or se-\nmantic similarity between entities. The Graph\nNeural Network (GNN)-based methods (Li et al.,\n2021, 2022c; Xu et al., 2023b; Chen et al., 2024b;\nWu et al., 2023; Chen et al., 2024c) capture struc-\ntural and temporal patterns in graph sequence to\nenhance reasoning accuracy. Rule-based tempo-\nral knowledge graph reasoning methods (Huang\net al., 2024; Chen et al., 2024a) follow a symbolic\nparadigm that emphasizes interpretability, logical\nconsistency, and low resource requirements. These\nmethods typically mine temporal logical rules from\nhistorical fact sequences, then use these rules to\n1The source code of DGAR is available at:\nhttps://github.com/zyzhang11/DGAR.\n--- Page 3 ---\ninfer future events or fill in missing historical facts.\nWhen new data arrives, these methods often require\nretraining. Given the strong performance of GNN-\nbased methods in TKGR, our approach builds upon\nthis category of methods.\n2.2 CL for Knowledge Graphs\nCompared to existing approaches that necessitate\nrepeated retraining, CL adaptively incorporates se-\nquentially evolving knowledge. Recently, several\nmethods have applied CL to knowledge graph em-\nbedding (KGE) and TKGR. For instance, some\napproaches (Wu et al., 2021; Mirtaheri et al., 2023)\nintegrate experience replay with regularization tech-\nniques to address catastrophic forgetting in TKGR.\nTIE’s (Wu et al., 2021) overly restrictive regulariza-\ntion leads to a decline in overall performance. The\nregularization method restricts the applicability of\nDEWC (Mirtaheri et al., 2023) to a limited number\nof tasks. (Cui et al., 2023; Liu et al., 2024a,b)\napply CL to KGE by employing regularization con-\nstraints to retain historical knowledge, effectively\nmitigating catastrophic forgetting.\n2.3 Diffusion Models\nDiffusion models are generative frameworks that\nreconstruct structured data from Gaussian noise\nthrough a stepwise reverse denoising process (Sohl-\nDickstein et al., 2015; Ho et al., 2020). In contin-\nuous domains like image synthesis, DDPM and\nits variants effectively model complex distributions\nand generate high-quality outputs (Ho et al., 2020;\nRombach et al., 2022). Applying diffusion models\ntodiscrete domains is challenging due to Gaus-\nsian noise’s incompatibility with discrete structures.\nText generation employs polynomial diffusion or\ncontinuous-to-discrete mapping to link continuous\nprocesses with discrete data (Austin et al., 2021;\nGong et al., 2022; Li et al., 2022a). In KGs , (Long\net al., 2024; Cai et al., 2024) restore knowledge\nfrom noise by mapping discrete KG data to a con-\ntinuous space and applying conditional constraints.\n3 Preliminaries\n3.1 The Task of TKGR\nTKG can be represented as a sequence of snap-\nshots partitioned by time, denoted as G=\n{G1, G2, G3, ..., G T}. Each snapshot Gt=\n(V,R,Ft)is a directed multi-relational graph at\ntimestamp t.(s, r, o, t )∈ Ftis denoted as a fact,\nwhere s∈ V ando∈ V are subject entity and\nobject entity, respectively, r∈ R is denoted asa relation that connects the subject entity and the\nobject entity.\nThe task of TKGR aims to predict the miss-\ning object entity (or subject entity) given a query\n(sq, rq,?, tq)or(?, rq, oq, tq). To be consistent\nwith common representation, the inverse quadruple\nof a fact (s, r, o, t )is(o, r−1, s, t)which is added\nto the dataset. The TKG reasoning goal can be\nexpressed as the prediction of object entities.\n3.2 Continual Learning for TKGR\nUnder the CL setting, TKGs can be viewed\nas a sequence of KG snapshots arriving as a\nstream over time. A set of tasks can be denoted\nas{T1,T2, ...,TT}, where each task is denoted\nasTt= (Dt\ntrain, Dt\nvalid, Dt\ntest), where Gt=\n[Dt\ntrain :Dt\nvalid:Dt\ntest]. The model parame-\nters are updated sequentially for each task as the\ntask stream {T1,T2, ...,TT}arrives. The trained\nmodel parameters at each step can be represented\nas{θ1, θ2, ..., θ T}. At time t, the parameters θtare\ninitialized by the parameters θt−1at the previous\ntime. Then the model is trained on Dt\ntrain to update\nthe parameters.\nDuring CL for TKGR, we mitigate catastrophic\nforgetting based on KG snapshot sequence reason-\ning models, such as RE-GCN (Li et al., 2021). We\nfocus primarily on entity representations, as the se-\nmantics of entities tend to evolve more frequently\nover time, in contrast to the relatively negligible\nchanges in the semantics of relations (Goel et al.,\n2020).\n3.3 Denoising Diffusion Probabilistic Model\nThe Diffusion Model (DM) consists of a forward\ndiffusion process and a reverse diffusion process.\nIn the forward process, a continuous DM is adapted\nto handle discrete facts G. Given discrete data x,\nwe first project xinto a continuous embedding,\ndenoted as X0= Embedding( x),where X0∈\nRd.Embedding( ·)is a function that can map a\nword to a vector in Rd. Then a Markov chain of\nlatent variables X1, X2, ..., X nis generated in the\nforward process by gradually adding small amounts\nof standard Gaussian noise to the sample. This\nprocess can be obtained by:\nq(Xn|Xn−1) =N\u0010\nXn;p\n1−βnXn−1, βnI\u0011\n,(1)\nwhere βn, n∈[1, ..., N ]is a noise schedule used\nto control the step size of the added noise and Iis\nan identity matrix. Nis the Gaussian distribution.\nIn the reverse process, the standard Gaussian\nrepresentation Xtprogressively approximates the\n--- Page 4 ---\nHistorical Context ExtractionHistory\nCondition Encoder\nGuiderDiffusion-Enhanced Historical Distribution Generation\n...\nEmbedding Entity Noise\nPoolingGRU MeaasgeAggregation\nEmbedding Adaptive Fusion\n...Query...\nTraining for  Task \nHistorical Context Pr ompt Building...Task 1 Task 2\nDeep Adaptive Replay1\n2Decoder\nEntity Score\nfrozen\ntraining\nentity\nmissingentityTask TKGsFigure 2: The overall architecture diagram of DGAR. Following the CL paradigm, each snapshot of the TKGs is\ntreated as a separate task.\ntrue representation X0by iterative denoising. It\ncan be learned by a parameterized model:\npϕ(Xn−1|Xn, n) =N(Xn−1;µϕ(Xn, n),Σϕ(Xn, n)),\n(2)\nwhere µϕandΣϕare generally implemented by a\ndeep neural fϕ(·), such as Transformer or U-Net.\nInspired by the success of Transformer encoders\nin the field of graph data(Hu et al., 2020), we opt\nfor the Transformer architecture in this work. The\npretraining objectives are defined as follows:\nL=EqhPN\nn=2∥X0−fϕ(Xn, n)∥2i\n−logpϕ(x|X0),\n(3)\nwhere Eqis the expectation over joint distribution.\nIt is important to emphasize that the data employed\nin the testing phase remains entirely unseen during\nthe pretraining process.\n4 The DGAR Method\nThe overall architecture of DGAR is shown in Fig-\nure 2, primarily consisting of three parts: Historical\nContext Prompt Building, Diffusion-Enhanced His-\ntorical Distribution Generation, and Deep Adaptive\nReplay.\nInitially, when a new query arrives for the t-th\ntask, DGAR builds HCPs based on the queried en-\ntity (Section 4.1). Based on the obtained HCPs,\nwe adopt the latest TKGR model parameters θt−1\nto guide the historical distribution representation\ngeneration of entities (Section 4.2). To support\nthe following reasoning, DAR injects generated\nhistorical entity distribution into the current entity\ndistribution representation (Section 4.3). Finally,\nwe present the final loss function of DGAR (Sec-\ntion 4.4).4.1 Historical Context Prompt Building\nHistorical context prompts, serving as sampling\nunits of replay data, aim to accurately preserve\nentities’ complete historical semantics. Before con-\nstructing an HCP, it is critical to determine which\nentities at time tare most relevant to achieving\nthe ultimate goal of mitigating catastrophic forget-\nting. As a new query (eq, rq,?, t)arrives, eqis an\ninvolved entity and its semantics will be directly in-\nfluenced after fine-tuning at the current timestamp\nt(Zhang et al., 2023a).\nTo correctly reflect the historical context seman-\ntics of eq, we construct an HCP for entity eq. Ifeq\nappears at time t−1or earlier, it might be asso-\nciated with one or more entities in the past. The\nhistorical distribution of eqis determined by the\nentities historically associated with eq(Xing et al.,\n2024). The HCP building for eqcan be formalized\nas follows:\nPrompti\nreplay ={(s, r, e q)|(s, r, e q)∈Gi\nor\u0000\neq, r−1, s\u0001\n∈Gi, Gi∈ G\t\n,(4)\nwhere Prompti\nreplay is the HCP of entity eqat time\ni, which denotes the set of triples associated with eq\nat historical moment i. The triples in Prompti\nreplay\nconsist of the entity eq, the neighbor sassociated\nwitheqat time i, and the relation rbetween eqand\nsat time i. When no triple containing eqappears\nat time i,Prompti\nreplay is empty.\nTo reduce the computational and storage burden,\nwe do not select the HCP of eqacross its entire\nhistory. Instead, we treat a HCP as the sampling\nunit, and randomly select HCPs of kdistinct time\nto enhance the generalizability of the replay data.\nThe discussion about kis provided in Appendix\nB.4. The set of HCP after sampling is denoted\n--- Page 5 ---\nasPromptreplay , which serves as the prompt for\ngenerating entity historical distribution in Section\n4.2. The entities involved in Promptreplay are rep-\nresented as a set Vreplay , these entities are directly\nor indirectly influenced by newly arrived data:\nVreplay =\b\ne| ∀(e, r, o )∈Promptreplay\t\n∪\b\ne| ∀(s, r, e )∈Promptreplay\t\n,(5)\n4.2 Diffusion-enhanced Historical\nDistribution Generation\nThe target of Diff-HDG strategy is to generate the\nhistorical distribution of entity with minimal con-\nflicts against the current distribution of entity. For\nthis purpose, during the generation process, com-\nmon features between the historical and current\ndistributions need to be enhanced. In addition, fea-\ntures in the historical distribution of entity that dif-\nfer from the current distribution of entity need to be\nweakened. Motivated by previous work (Yang et al.,\n2023; V oynov et al., 2023; Zhang et al., 2023b),\npre-trained DMs have demonstrated exceptional ca-\npabilities in reproducing knowledge from prompt\ntexts. DMs possess a robust ability to generate gen-\neralized expressions. This capability is crucial for\nresolving conflicts that arise between different dis-\ntributions. Thus, we generate historical distribution\nrepresentations of entities through a pre-trained\nDM based on HCP. The generation of entity his-\ntorical distribution primarily relies on the inverse\ndiffusion process, which can be outlined as follows:\nHreplay\ne =pϕ\u0000\nXn, fθt, Promptreplay\u0001\n, (6)\nwhere Xndenotes the object to denoising, which is\nprocessed iteratively to yield the historical distribu-\ntion of entity Hreplay\ne . The function fθtrepresents\nthe parameters of the TKGR model at the current\ntimet.\nIn detail, for a fact (s, r, e q)∈Promptreplay , we\ntreat the entity sand the relation ras gener-\nation conditions. This condition-based genera-\ntion method integrates information from historical\nneighbors and relations, enabling a more precise\nmodeling of the historical distribution of entity:\nXn= Condition ( S0, R0, Z), Z∼ N(0, I), (7)\nwhere S0= Embedding ( s),R0= Embedding ( r),\nandCondition ( ·)represents concatenation. The\ncondition in Xncan guide the DM to generate dis-\ntributions that reflect the historical semantics of\nentities.To enhance the common features between his-\ntorical and current distribution representations, we\npropose a novel method to guide the generation\nprocess of historical entity representations:\nXn−1=pϕ(Xn), (8)\nXn−1=Xn−1+γ∂σ\n∂Xn−1, (9)\n∂σ\n∂Xn−1=∇Xn−1σ(fθt(Xn−1,(s, r, e q))),(10)\nwhere σdenotes the softmax function, γis a hyper-\nparameter, and Xn−1is the result of the first denois-\ning step performed on Xn. This process produces\na cleaner representation of Xn. After acquiring\nXn−1, we evaluate the scores of historical facts in\nPromptreplay with the current TKGR model fθt.\nThe gradient of scores is applied to optimize the\ngenerated historical distribution, ensuring that the\nscores of these historical facts are maximized at the\ncurrent time. Based on our empirical observations,\nadjacent timestamps in TKGs show only minor dis-\ntribution differences. Since the model parameters\nθtfor the current time can only be obtained after\nbeing updated at the current time, we approximate\nθtusing θt−1.\nAfter niterations of denoising with pϕ, we ob-\ntain the generated representation Xeq\n0for the query\nentity. Similarly, the historical neighboring entity s\nreceives an updated representation Xs\n0, influenced\nby the query entity Xeq\n0. Mean pooling is used\nto aggregate information from multiple neighbors\nacross different timestamps, as shown below:\nHreplay\ne =Pk\ni=1P\nϵ∈MieHi\nϵPk\ni=1|Mie|, (11)\nwhere Hreplay\ne represents the final historical distri-\nbution representation of entity e∈Vreplay , captur-\ning its historical characteristics in the TKGs. Hi\nϵ\nrepresents the entity representation Xe\n0generated\nfrom the facts ϵ.Mi\nerefers to the set of facts that\ncontain entity eat the i-th time slice. After itera-\ntive denoising, the features in Hreplay\ne that are the\nsame as the current distribution are enhanced, and\nthe features in Hreplay\ne that are different from the\ncurrent distribution are weakened. These historical\nentity representations are generated in parallel to\nimprove computational efficiency.\n4.3 Deep Adaptive Replay\nIn this section, we introduce a DAR mechanism\nthat effectively integrates the historical and current\n--- Page 6 ---\ndistributions of entities. Building upon the histori-\ncal distribution representation of entities obtained\nin section 4.2, these representations are incorpo-\nrated into the current distribution representation of\nentities.\nWe identify that overly complex historical\nknowledge injection mechanisms impose a consid-\nerable learning burden, whereas excessively sim-\nplistic approaches result in significant knowledge\nloss. To overcome these issues, we propose DAR\nfor historical knowledge replay, which performs\nthe following operations at each layer of the KG\nsnapshot sequence reasoning model:\nHl\ne=(\nHcurrent ,l\ne , e /∈Vreplay\nαHreplay\ne + (1−α)Hcurrent ,l\ne , e∈Vreplay,(12)\nwhere α∈[0,1], which adaptively balances new\nand old knowledge. Hcurrent ,l\ne denotes the entity dis-\ntribution representation of the current task at layer\nl. To preserve the evolutionary characteristics in\nthe temporal sequence, deep replay is conducted\nwithin the Levolution units; the final entity repre-\nsentation, denoted as Hfinal, is obtained.\n4.4 Model Training\nAfter obtaining the final representation, the decoder\ncomputes scores for candidate entities. We treat\nentity prediction as multi-class classification, and\nmodel parameters θtfor task tare optimized as\nfollows:\nLt,c=−X\n(s,r,o,t )∈Dt\ntrainye\ntfθt(s, r, o, t ),(13)\nwhere ye\ntrepresents the label vector. During ex-\nperiments, we observe that although we attempt to\npreserve historical knowledge by enhancing com-\nmon features between the representations of current\nand historical distributions, the model still suffers\nfrom historical information loss. This is primar-\nily due to the constraints of the guidance function\nand subsequent optimization for current data. To\naddress this issue, we incorporate facts from the\nhistorical context prompt as a regularization term\ninto the loss function. The final loss calculation is\nformulated as follows:\nLt=Lt,c+µLt,r, (14)\nwhere Lt,crepresents the training loss for the cur-\nrent task, Lt,rdenotes the loss associated with re-\nplaying historical facts, and µis a hyperparame-\nter, typically set to 1. The computation of Lt,rissimilar to that of Lt,c. The difference is that Lt,c\ncalculates the loss based on current facts, while\nLt,ruses historical fact in Prompt replay .\n5 Experiments\n5.1 Experimental Setup\nDatasets. We adopt four widely used benchmark\ndatasets for TKGR tasks: ICE14, ICE18, ICE05-15,\nand GDELT. The first three datasets originate from\nthe Integrated Crisis Early Warning System (Jin\net al., 2020), which records geopolitical events.\nThe statistical details of these datasets are summa-\nrized in Table B.1.\nMetrics. We utilize two evaluation metrics:\nMean Reciprocal Rank (MRR) and Hits@k\n(k=1,10), both of which are widely adopted to\nassess the performance of TKGR methods. Fol-\nlowing the approach (Mirtaheri et al., 2023), we\nevaluate the model’s ability to mitigate catastrophic\nforgetting. We evaluate the model trained on the\nfinal task tby testing its performance on the cur-\nrent test set (Current) and calculating its average\nperformance across all previous test sets (Average).\nBaselines. We adopt the following baselines: FT,\nER (Rolnick et al., 2019), TIE (Wu et al., 2021),\nLKGE (Cui et al., 2023) and IncDE (Liu et al.,\n2024a). Details about these baselines are provided\nin Appendix B.2. In the experiments, we use RE-\nGCN as the base model.\n5.2 Main Results\nThe results of main experiments are shown in Table\n1 and Table 2. Each dataset is tested five times and\nthe average results are reported. The same proce-\ndure is also followed in subsequent experiments.\nDGAR achieves consistent performance im-\nprovements compared to Fine-tuning. For histori-\ncal tasks, it achieves an average increase of 11.34%\nin MRR. This demonstrates that, compared to di-\nrect fine-tuning, our approach more effectively re-\ntains historical knowledge.\nMoreover, DGAR consistently outperforms all\nbaselines. Compared to the strongest baseline, it\nachieves an average MRR improvement of 4.01%\nin the current task across all evaluated datasets.\nFor historical tasks, the average improvement is\n8.23% in MRR, and 9.79% in Hits@10. DGAR\ndemonstrates improvements across various datasets\nby preserving historical knowledge.\nIn contrast, while the TIE model performs well\non current tasks, it exhibits poor average perfor-\nmance across all historical tasks. This result is\n--- Page 7 ---\nICE14 ICE18 ICE05-15\nAlgo.Current Average Current Average Current Average\nMRR MRR Hits@1 Hits@10 MRR MRR Hits@1 Hits@10 MRR MRR Hits@1 Hits@10\nFT 42.66 37.46 26.95 58.20 30.76 25.35 15.97 44.36 43.80 41.88 30.55 63.82\nER 48.75 42.14 31.03 63.80 30.39 27.20 16.88 48.19 52.50 45.55 33.34 69.07\nTIE 53.74 41.07 30.28 62.39 34.45 28.73 18.40 49.60 60.77 42.56 30.90 64.67\nLKGE 43.56 37.51 27.13 58.51 31.12 25.56 16.12 44.70 43.28 42.46 30.99 64.51\nIncDE 45.03 36.57 26.20 56.95 31.83 25.52 16.07 44.74 46.33 40.56 29.34 62.17\nDGAR 58.59 50.12 39.36 70.48 36.53 33.00 21.74 55.63 66.01 54.33 43.11 75.13\nTable 1: The main experimental results on the ICE14, ICE18, and ICE05-15 datasets are presented. Bolded scores\nindicate the best results.\nGDELT\nAlgo.Current Average\nMRR MRR Hits@1 Hits@10\nFT 14.74 15.60 8.73 29.05\nER 15.42 16.21 8.97 30.42\nTIE 15.56 16.40 8.94 30.98\nLKGE 14.43 15.52 8.69 28.90\nIncDE 15.14 15.49 8.64 28.86\nDGAR 23.25 28.30 17.38 51.39\nTable 2: The main experimental results on the GDELT.\n/uni0000002c/uni00000026/uni00000028/uni00000014/uni00000017 /uni0000002c/uni00000026/uni00000028/uni00000013/uni00000018/uni00000010/uni00000014/uni00000018 /uni0000002c/uni00000026/uni00000028/uni00000014/uni0000001b /uni0000002a/uni00000027/uni00000028/uni0000002f/uni00000037/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013\n/uni00000029/uni00000037/uni00000003/uni0000000b/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031/uni0000000c\n/uni00000027/uni0000002a/uni00000024/uni00000035/uni00000003/uni0000000b/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031/uni0000000c\n/uni00000029/uni00000037/uni00000003/uni0000000b/uni00000037/uni0000004c/uni00000035/uni0000002a/uni00000031/uni0000000c\n/uni00000027/uni0000002a/uni00000024/uni00000035/uni00000003/uni0000000b/uni00000037/uni0000004c/uni00000035/uni0000002a/uni00000031/uni0000000c\n(a) Current\n/uni0000002c/uni00000026/uni00000028/uni00000014/uni00000017 /uni0000002c/uni00000026/uni00000028/uni00000013/uni00000018/uni00000010/uni00000014/uni00000018 /uni0000002c/uni00000026/uni00000028/uni00000014/uni0000001b /uni0000002a/uni00000027/uni00000028/uni0000002f/uni00000037/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013\n/uni00000029/uni00000037/uni00000003/uni0000000b/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031/uni0000000c\n/uni00000027/uni0000002a/uni00000024/uni00000035/uni00000003/uni0000000b/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031/uni0000000c\n/uni00000029/uni00000037/uni00000003/uni0000000b/uni00000037/uni0000004c/uni00000035/uni0000002a/uni00000031/uni0000000c\n/uni00000027/uni0000002a/uni00000024/uni00000035/uni00000003/uni0000000b/uni00000037/uni0000004c/uni00000035/uni0000002a/uni00000031/uni0000000c (b) Average\nFigure 3: Performance of different base TKGR models.\nattributed to the strict regularization of TIE, which\nlimits its ability to retain historical knowledge. In-\ncDE and LKGE only use the embedding constraint\nmodel of entities and relations at the previous mo-\nment to retain old knowledge, which leads to the\ndecline of IncDE’s performance on historical tasks\ncompared to FT. LKGE additionally considers the\nconstraints of cumulative weights, so it has a slight\nimprovement on certain datasets compared to FT.\n5.3 Ablation Study\nIn this section, we examine the impact of various\ncomponents of the model on the final result, as\nshown in Table 3. To thoroughly evaluate their\nroles, we implement the following model variants:\n(1) w/o HP, where the HCP is replaced with ER; (2)\nw/o GR, where the variant discards DAR and Diff-\nHDG, relying only on the facts within the HCP for\nregularization; (3) w/o AR, where the historical andcurrent entity distributions are merged through di-\nrect addition instead of DAR as specified in Eq. 15;\nand (4) w/o Guider, where the operation of enhanc-\ning common features across different distributions\nin Diff-HDG is discarded; (5) w/o Lr, where the\nlossLt,rin Section 4.4 is removed during training.\nThe analysis of w/o Lrin Appendix B.6.\nEffect Analysis of Historical Context\nPrompt. In the w/o HP variant, the model’s per-\nformance noticeably declined, demonstrating that\nHCP effectively ensures the semantic integrity of\nthe historical information. It prevents catastrophic\nforgetting during CL and enhances predictions for\nthe current task. In contrast, ER merely replays\npartial historical information.\nEffect Analysis of Diffusion-enhanced Histori-\ncal Distribution Generation. In the w/o Guider\nvariant, different datasets show varying degrees of\nperformance drop. This demonstrates that incor-\nporating the guider aids in capturing common fea-\ntures between historical and current distributions,\nthereby mitigating performance losses caused by\ndistribution conflicts. The smaller drop observed\non the GDELT dataset likely results from its shorter\ntemporal gaps and less pronounced distribution\nshifts compared to other datasets.\nEffect Analysis of Deep Adaptive Re-\nplay. Removing the adaptive parameter α\nin the w/o AR variant causes varying levels of per-\nformance decrease across datasets, demonstrating\nthe effectiveness of our adaptive fusion method\nin balancing historical and current distribution\nrepresentations. The limited decrease observed on\nthe GDELT dataset can be attributed to the minimal\ndifference between the current distribution and that\nof GDELT, which restricts the adaptive parameter\nαin its capacity to adjust effectively.\nCombined Effect Analysis of DAR and Dif-\nHDG. DAR and Diff-HDG are proposed to re-\nsolve the conflict between historical and current\n--- Page 8 ---\nICE14 ICE18 ICE05-15 GDELT\nCurrent Average Current Average Current Average Current Average\nMRR MRR Hits@10 MRR MRR Hits@10 MRR MRR Hits@10 MRR MRR Hits@10\nw/o HP 53.43 46.74 67.58 31.67 25.89 45.31 53.53 45.71 68.18 15.49 17.18 32.73\nw/o GR 48.18 39.15 59.71 30.32 27.62 47.74 52.97 38.71 59.19 16.24 16.67 31.65\nw/o AR 49.27 45.55 65.27 32.28 29.79 50.90 58.48 51.93 72.57 22.04 26.98 49.76\nw/o Guider 55.23 49.32 70.29 35.16 32.25 54.67 58.70 52.53 72.85 22.18 27.99 50.93\nw/oLr 52.17 44.43 64.13 36.20 30.62 52.98 58.64 51.98 72.01 22.84 25.95 48.75\nOurs 58.59 50.12 70.48 36.53 33.00 55.63 66.01 54.33 75.13 23.25 28.30 51.39\nTable 3: Ablation experimental results on all datasets.\ndistribution. The w/o GR variants show a clear per-\nformance drop, likely due to memory contention\ncaused by distribution conflicts arising from the\nsimple replay of historical data. This suggests that\nDif-HDG and DAR alleviate distribution conflicts,\nthereby preserving historical knowledge more effi-\nciently.\nEffect Analysis of Different Base Mod-\nels. Although we choose the most typical model,\nRE-GCN, as our base model, our method can\nstill be extended to other GNN-based TKGR\nmodels. To verify the scalability of our method,\nwe extend DGAR to TiRGN (Li et al., 2022b) and\nconduct experiments on four benchmark datasets.\nThe experimental results of MRR in Figure 3\nindicate that DGAR consistently outperforms\ndirect fine-tuning on both current tasks and\nhistorical tasks, demonstrating that DGAR has\nrobust scalability and effectiveness for GNN-based\nTKGR models.\n5.4 Effect of Memorizing in CL\nTo further validate DGAR’s ability to retain his-\ntorical knowledge during forward learning, we\nevaluate the mean difference between pn,iandpi,i\n(1< i≤n) in Figure 4. Here, pi,jrepresents the\nMRR score of the j-th task after training the model\non the i-th task. A higher mean value indicates bet-\nter retention of prior knowledge during CL. When\nthe value exceeds zero, it indicates reverse trans-\nfer of newly learned knowledge, whereas a value\nbelow zero reflects the loss of prior knowledge dur-\ning CL (Lin et al., 2022). Experiments show that\nDGAR outperforms the best baseline, confirming\nits effectiveness in mitigating catastrophic forget-\nting. Since the data in TKGs are highly correlated,\nwe find that when the number of tasks is small, a\nreasonable strategy can help prevent catastrophic\nforgetting and facilitate the reverse transfer of new\nknowledge. This is evident in the performance of\nDGAR and ER on ICE14.\n/uni0000002c/uni00000026/uni00000028/uni00000014/uni00000017 /uni0000002c/uni00000026/uni00000028/uni00000014/uni0000001b/uni00000017\n/uni00000015\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000029/uni00000037\n/uni00000028/uni00000035\n/uni00000037/uni0000002c/uni00000028\n/uni0000002f/uni0000002e/uni0000002a/uni00000028\n/uni0000002c/uni00000051/uni00000046/uni00000027/uni00000028\n/uni00000027/uni0000002a/uni00000024/uni00000035(a) ICE14 and ICE18\n/uni0000002c/uni00000026/uni00000028/uni00000013/uni00000018/uni00000010/uni00000014/uni00000018 /uni0000002a/uni00000027/uni00000028/uni0000002f/uni00000037/uni00000014/uni00000017\n/uni00000014/uni00000015\n/uni00000014/uni00000013\n/uni0000001b\n/uni00000019\n/uni00000017\n/uni00000015\n/uni00000013\n/uni00000029/uni00000037\n/uni00000028/uni00000035\n/uni00000037/uni0000002c/uni00000028\n/uni0000002f/uni0000002e/uni0000002a/uni00000028\n/uni0000002c/uni00000051/uni00000046/uni00000027/uni00000028\n/uni00000027/uni0000002a/uni00000024/uni00000035 (b) ICE05-15 and GDELT\nFigure 4: Effect of memorizing old knowledge in CL.\n5.5 Case Study\nWe conduct a case study on ICE14 and ICE18\ndatasets to assess whether DGAR can handle con-\nflicts in entity distributions and retain old knowl-\nedge effectively in Figure 5. At a randomly chosen\ntimei, we extract all entities from the facts, save\ntheir feature distributions at time i(Stage 1), time\nj(Stage 2), time m(Stage 3) and at a later time n\n(n > m > j > i ) (Stage 4), and analyzed them\nusing U-MAP.\nFigures 5(a) and 5(b) compare the entity distribu-\ntion representations of the FT model and DGAR on\nICE14 across four stages. The entity distribution\nlearned by the FT model at the same time is more\nclustered, while the entity distributions at different\ntimes are more distinct, showing a clearer differ-\nence. In contrast, DGAR learns a more general and\nconsistent distribution, allowing it to share the fea-\nture space between tasks more effectively, thereby\nenhancing knowledge retention and reducing for-\ngetting. A similar pattern is observed in Figures\n5(c) and 5(d) on ICE18, further supporting these\nfindings.\n6 Conclusion\nThis paper introduces a deep generative adaptive\nreplay method to mitigate catastrophic forgetting\nin TKGR models during CL. A historical context\nprompt integrating contextual information is de-\nsigned to generate historical distribution represen-\ntations of entities via a pre-trained DM. The genera-\ntion process is guided by current model parameters\n--- Page 9 ---\n(a) FT on ICE14\n (b) DGAR on ICE14\n(c) FT on ICE18\n (d) DGAR on ICE18Stage 1 Stage 2 Stage 3 Stage 4Figure 5: Visualization case study of entity distribution.\nto reinforce common features, minimizing conflicts\nbetween historical and current entity distributions.\nIn addition, a deep adaptive replay strategy derives\nentity distribution representations with historical\nknowledge. These combined techniques enable\nthe proposed method to achieve outstanding perfor-\nmance across various datasets.\n7 Limitations\nIn this section, we examine the limitations of\nour approach. DGAR is designed to retain pre-\nviously acquired knowledge through CL, facilitat-\ning TKGR. Although DGAR is more time-efficient\nthan retraining and surpasses other models in miti-\ngating catastrophic forgetting, it still faces several\nchallenges.\nFirstly, the model addresses newly emerging en-\ntities and relations using Xavier initialization with-\nout further analysis or dedicated modeling. Such\na simplistic approach may constrain the model’s\nability to learn new knowledge effectively, partic-\nularly when complex interrelations exist between\nnew and previously learned knowledge. This high-\nlights the need for more sophisticated strategies to\nhandle new entities and relations in CL scenarios.\nSecondly, while DGAR demonstrates strong per-\nformance in reducing catastrophic forgetting, it in-\ntroduces additional learnable parameters. These\nparameters enhance adaptability to new knowledge\nbut also pose a potential risk of forgetting previ-\nously learned information. This risk arises since\nthe increased number of parameters may lead the\nmodel to prioritize new knowledge, thereby com-\npromising the retention of older knowledge. Fur-\nthermore, the inclusion of additional parameters\ninherently increases model complexity, making thetraining and reasoning process more cumbersome.\nSuch complexity necessitates careful consideration\nduring design to strike a balance between knowl-\nedge retention and model complexity.\n8 Ethics Statement\nFirstly, this study fully complies with the ethical\nguidelines in the ACL Code of Ethics. Secondly,\nall datasets involved in this study are from previ-\nous studies. The datasets we used do not contain\nindividual privacy data. Finally, DGAR focuses on\nthe research and experiments of TKGR tasks. Like\nother TKGR methods, the results of our method\nreasoning may be toxic or erroneous, so manual\ninspection of the results may be required in the\napplications.\nReferences\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel\nTarlow, and Rianne Van Den Berg. 2021. Structured\ndenoising diffusion models in discrete state-spaces.\nAdvances in Neural Information Processing Systems ,\n34:17981–17993.\nYuxiang Cai, Qiao Liu, Yanglei Gan, Changlin Li,\nXueyi Liu, Run Lin, Da Luo, and JiayeYang\nJiayeYang. 2024. Predicting the unpredictable:\nUncertainty-aware reasoning over temporal knowl-\nedge graphs via diffusion process. In Findings of the\nAssociation for Computational Linguistics ACL 2024 ,\npages 5766–5778.\nKai Chen, Ye Wang, Yitong Li, Aiping Li, Han Yu,\nand Xin Song. 2024a. A unified temporal knowl-\nedge graph reasoning model towards interpolation\nand extrapolation. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 117–132,\nBangkok, Thailand. Association for Computational\nLinguistics.\nWei Chen, Huaiyu Wan, Yuting Wu, Shuyuan Zhao,\nJiayaqi Cheng, Yuxin Li, and Youfang Lin. 2024b.\nLocal-global history-aware contrastive learning for\ntemporal knowledge graph reasoning. In 2024 IEEE\n40th International Conference on Data Engineering\n(ICDE) , pages 733–746.\nWei Chen, Yuting Wu, Shuhan Wu, Zhiyu Zhang,\nMengqi Liao, Youfang Lin, and Huaiyu Wan. 2024c.\nCogntke: A cognitive temporal knowledge extrapola-\ntion framework. ArXiv , abs/2412.16557.\nYubo Chen, Shaoru Guo, Kang Liu, and Jun Zhao. 2023.\nLarge language models and knowledge graphs. In\nProceedings of the 22nd Chinese National Confer-\nence on Computational Linguistics (Volume 2: Fron-\ntier Forum) , pages 67–76.\n--- Page 10 ---\nYuanning Cui, Yuxin Wang, Zequn Sun, Wenqiang Liu,\nYiqiao Jiang, Kexin Han, and Wei Hu. 2023. Life-\nlong embedding learning and transfer for growing\nknowledge graphs. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 37, pages\n4217–4224.\nShib Sankar Dasgupta, Swayambhu Nath Ray, and\nPartha Talukdar. 2018. Hyte: Hyperplane-based\ntemporally aware knowledge graph embedding. In\nProceedings of the 2018 conference on empirical\nmethods in natural language processing , pages 2001–\n2011.\nRishab Goel, Seyed Mehran Kazemi, Marcus Brubaker,\nand Pascal Poupart. 2020. Diachronic embedding for\ntemporal knowledge graph completion. In Proceed-\nings of the AAAI conference on artificial intelligence ,\nvolume 34, pages 3988–3995.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand LingPeng Kong. 2022. Diffuseq: Sequence to se-\nquence text generation with diffusion models. arXiv\npreprint arXiv:2210.08933 .\nSaiping Guan, Xueqi Cheng, Long Bai, Fujun Zhang,\nZixuan Li, Yutao Zeng, Xiaolong Jin, and Jiafeng\nGuo. 2022. What is event knowledge graph: A sur-\nvey. IEEE Transactions on Knowledge and Data\nEngineering , 35(7):7569–7589.\nBernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michi-\nhiro Yasunaga, and Yu Su. 2024. Hipporag: Neu-\nrobiologically inspired long-term memory for large\nlanguage models. arXiv preprint arXiv:2405.14831 .\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. Advances\nin neural information processing systems , 33:6840–\n6851.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020. Heterogeneous graph transformer. In\nProceedings of the web conference 2020 , pages 2704–\n2710.\nRikui Huang, Wei Wei, Xiaoye Qu, Shengzhe Zhang,\nDangyang Chen, and Yu Cheng. 2024. Confidence\nis not timeless: Modeling temporal validity for rule-\nbased temporal knowledge graph forecasting. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers) , pages 10783–10794, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nWoojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren.\n2020. Recurrent event network: Autoregressive struc-\nture inferenceover temporal knowledge graphs. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 6669–6683, Online. Association for Computa-\ntional Linguistics.\nTimothée Lacroix, Guillaume Obozinski, and Nicolas\nUsunier. 2020. Tensor decompositions for temporal\nknowledge base completion. In 8th InternationalConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net.\nJulien Leblay and Melisachew Wudage Chekol. 2018.\nDeriving validity time in knowledge graph. In Com-\npanion proceedings of the the web conference 2018 ,\npages 1771–1776.\nEunhae Lee. 2024. The impact of model size on\ncatastrophic forgetting in online continual learning.\nPreprint , arXiv:2407.00176.\nXiang Li, John Thickstun, Ishaan Gulrajani, Percy S\nLiang, and Tatsunori B Hashimoto. 2022a. Diffusion-\nlm improves controllable text generation. Advances\nin Neural Information Processing Systems , 35:4328–\n4343.\nYujia Li, Shiliang Sun, and Jing Zhao. 2022b. Tirgn:\nTime-guided recurrent graph network with local-\nglobal historical patterns for temporal knowledge\ngraph reasoning. In IJCAI , pages 2152–2158.\nYujia Li, Shiliang Sun, and Jing Zhao. 2022c. Tirgn:\nTime-guided recurrent graph network with local-\nglobal historical patterns for temporal knowledge\ngraph reasoning. In Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2022, Vienna, Austria, 23-29 July 2022 ,\npages 2152–2158.\nZixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng\nGuo, Huawei Shen, Yuanzhuo Wang, and Xueqi\nCheng. 2021. Temporal knowledge graph reason-\ning based on evolutional representation learning.\nSen Lin, Li Yang, Deliang Fan, and Junshan Zhang.\n2022. Beyond not-forgetting: Continual learning\nwith backward knowledge transfer. Advances in\nNeural Information Processing Systems , 35:16165–\n16177.\nJiajun Liu, Wenjun Ke, Peng Wang, Ziyu Shang, Jin-\nhua Gao, Guozheng Li, Ke Ji, and Yanhe Liu. 2024a.\nTowards continual knowledge graph embedding via\nincremental distillation. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 38,\npages 8759–8768.\nJiajun Liu, Wenjun Ke, Peng Wang, Jiahao Wang, Jin-\nhua Gao, Ziyu Shang, Guozheng Li, Zijie Xu, Ke Ji,\nand Yining Li. 2024b. Fast and continual knowl-\nedge graph embedding via incremental lora. arXiv\npreprint arXiv:2407.05705 .\nXiao Long, Liansheng Zhuang, Aodi Li, Houqiang Li,\nand Shafei Wang. 2024. Fact embedding through\ndiffusion model for knowledge graph completion. In\nProceedings of the ACM on Web Conference 2024 ,\npages 2020–2029.\nMehrnoosh Mirtaheri, Mohammad Rostami, and Aram\nGalstyan. 2023. History repeats: Overcoming\ncatastrophic forgetting for event-centric temporal\nknowledge graph completion. arXiv preprint\narXiv:2305.18675 .\n--- Page 11 ---\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-\nthy Lillicrap, and Gregory Wayne. 2019. Experience\nreplay for continual learning. Advances in neural\ninformation processing systems , 32.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages\n10684–10695.\nJascha Sohl-Dickstein, Eric Weiss, Niru Mah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermo-\ndynamics. In International conference on machine\nlearning , pages 2256–2265. PMLR.\nAndrey V oynov, Kfir Aberman, and Daniel Cohen-Or.\n2023. Sketch-guided text-to-image diffusion models.\nInACM SIGGRAPH 2023 Conference Proceedings ,\npages 1–11.\nJiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli\nHu, Alan Wee-Chung Liew, Shirui Pan, and Baocai\nYin. 2024. Large language models-guided dynamic\nadaptation for temporal knowledge graph reasoning.\narXiv preprint arXiv:2405.14170 .\nJiapeng Wu, Yishi Xu, Yingxue Zhang, Chen Ma, Mark\nCoates, and Jackie Chi Kit Cheung. 2021. Tie: A\nframework for embedding-based incremental tem-\nporal knowledge graph completion. In Proceedings\nof the 44th international ACM SIGIR conference on\nresearch and development in information retrieval ,\npages 428–437.\nShuhan Wu, Huaiyu Wan, Wei Chen, Yuting Wu, Jun-\nfeng Shen, and Youfang Lin. 2023. Towards en-\nhancing relational rules for knowledge graph link\nprediction. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023 , pages 10082–\n10097, Singapore. Association for Computational\nLinguistics.\nYujie Xing, Xiao Wang, Yibo Li, Hai Huang, and\nChuan Shi. 2024. Less is more: on the over-\nglobalizing problem in graph transformers. arXiv\npreprint arXiv:2405.01102 .\nYi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. 2023a. Tem-\nporal knowledge graph reasoning with historical con-\ntrastive learning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 37, pages\n4765–4773.\nYi Xu, Junjie Ou, Hui Xu, and Luoyi Fu. 2023b. Tem-\nporal knowledge graph reasoning with historical con-\ntrastive learning. In AAAI .\nZhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li,\nKevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu,\nCe Liu, Michael Zeng, et al. 2023. Reco: Region-\ncontrolled text-to-image generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 14246–14255.Jiasheng Zhang, Jie Shao, and Bin Cui. 2023a. Streame:\nLearning to update representations for temporal\nknowledge graphs in streaming scenarios. In Pro-\nceedings of the 46th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval , pages 622–631.\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\n2023b. Adding conditional control to text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages\n3836–3847.\nShuyuan Zhao, Wei Chen, Boyan Shi, Liyong Zhou,\nShuohao Lin, and Huaiyu Wan. 2025. Spatial-\ntemporal knowledge distillation for takeaway rec-\nommendation. In AAAI-25, Sponsored by the Associ-\nation for the Advancement of Artificial Intelligence,\nFebruary 25 - March 4, 2025, Philadelphia, PA, USA ,\npages 13365–13373. AAAI Press.\nA Details about DGAR\nA.1 Details about Deep Adaptive Replay\nWhile initially exploring more complex mecha-\nnisms for this integration, it is observed that the ad-\nditional parameters introduced hinders the model’s\nconvergence due to increased fitting complexity.\nAs a result, a direct injection approach is adopted\nto integrate the historical distributions into the cur-\nrent representations, as detailed below:\nHfinal=\u001aHcurrent ,\ne , e /∈Vreplay\nHreplay\ne +Hcurrent\ne , e∈Vreplay.(15)\nHowever, it is observed that such a simple and di-\nrect fusion approach results in performance degra-\ndation. To address this issue, a straightforward\nparameter is introduced to balance the historical\ndistribution representation Hreplay\ne and the current\ndistribution representation Hcurrent\ne , thereby gen-\nerating the final entity representation Hfinal. This\nparameter effectively adjusts the weighting of the\ntwo distributions, mitigating the performance loss\ncaused by direct fusion while preserving the ex-\npressive power of historical knowledge and the\ndynamic characteristics of the current distribution:\nHfinal=αHreplay\ne + (1−α)Hcurrent\ne , e∈Vreplay,(16)\nwhere α∈[0,1].Hfinaldenotes the final entity\nrepresentation, combining the most recent and his-\ntorical information of the entity.\nTo prevent the loss of entity evolution patterns\nover time caused by direct injection, we inte-\ngrate distribution representation from the KG snap-\nshot sequence reasoning model’s evolution unit to\n--- Page 12 ---\nachieve a deeper incorporation of historical dis-\ntribution representations without introducing ad-\nditional parameters. After applying the relation-\naware GCN in the l-th evolution unit, we obtain\nthe current distribution representation of entity\nHcurrent ,l\ne . The historical distribution representation\nof entities is then injected into the current distribu-\ntion representation of entities as follows:\nHcurrent ,l\ne =αHreplay\ne + (1−α)Hcurrent ,l\ne , e∈Vreplay.(17)\nAfter passing through multiple evolution units,\nthe final entity representation Hfinal, which incor-\nporates historical distributions, is obtained.\nA.2 Pre-train for DM\nIn this section, we will discuss how we obtain the\npre-trained DM. Considering that using a large\namount of knowledge to train diffusion will not\nonly increase the risk of data leakage, but also fail\nto adapt to new data arriving over time, we adopt\nCL to train DM. For example, at the t-th moment,\nwe can obtain the DM ϕt−1pre-trained at the previ-\nous moment, and ϕt−1is used as a pre-trained DM\nto assist in generating the entity history distribution\nin Diff-HDG. After completing all the operations\nin Section 4, ϕtis initialized with ϕt−1, and the\nmodel parameters are updated on Dt\ntrain. Eq. 16 is\nemployed to preserve the historical knowledge of\nthe entity for DM. Upon completion of the training,\na new pre-trained DM, ϕt, is obtained and will be\nused in the subsequent learning process.\nB Further Analysis\nB.1 Datasets Details\nICE14 ICE18 ICE05-15 GDELT\nEntities 6869 23033 10094 7,691\nRelations 230 256 251 240\nTasks 365 304 4017 2,751\nTask granularity 24 hours 24 hours 24 hours 15mins\nTotal number of train 74,845 373,018 368,868 1,734,399\nTotal number of valid 8,514 45,995 46,302 238,765\nTotal number of test 7,371 49,545 46,159 305,241\nTable 4: Details of the TKG datasets.\nWe follow the common division ratio of TKGR\ntasks: The facts in each task are partitioned into\ntrain, valid, and test sets in a ratio of 8:1:1 (Li et al.,\n2021; Xu et al., 2023a). The statistical details of\nthe datasets are shown in Table 4.\n2AI such as GPT only assists us in translation and grammar\nchecking.B.2 Baselines Details\nFT (fine-tuning) is a naive baseline where the\nmodel is fine-tuned using newly added facts with-\nout any mechanism to alleviate catastrophic forget-\nting. FT is set up following the previous works\nsuch as TIE (Wu et al., 2021), LKGE (Wu et al.,\n2021), and IncDE (Wu et al., 2021). FT enables\nthe base model (e.g., RE-GCN and TiRGN) to per-\nform CL without applying any additional strategies.\nSpecifically, the base model inherits the parameters\nfrom the previous time step i−1and continual\ntraining on the training data Dt\ntrain at time step i.\nER (Rolnick et al., 2019) mitigates forgetting by re-\nplaying a subset of previously stored events along-\nside newly added facts during training. TIE (Wu\net al., 2021) incorporates temporal regularization,\nexperience replay with positive facts, and the use\nof deleted facts as negative examples to effectively\naddress both catastrophic forgetting and intransi-\ngence. LKGE (Cui et al., 2023) preserves historical\nknowledge by leveraging historical weights and em-\nbeddings through the L2 paradigm. We incorporate\nthe reconstruction loss and embedding regulariza-\ntion from LKGE into our objective function. When\ninitializing new entities, their embedding transfer\nstrategies are adopted. Based on the method pro-\nposed by IncDE (Liu et al., 2024a), we leverage\nthe hierarchical ordering measure of IncDE and\nincorporate the distillation loss proposed by IncDE\ninto our objective function.\nB.3 Implementation Details\nFor all datasets, the embedding size dis set to\n200, the learning rate lris set to 0.001, and the\nbatch size is determined by the number of facts\nat each time step. The number of layers of the\nTransformer encoder for all datasets is set to 2.\nThe temperature coefficient τfor all datasets is set\nto 0.5. The parameters of DAGR are optimized\nby using Adam during the training process. The\noptimal coefficient γin the Diff-HDG is set to 1.\nThe optimal number of layers Lin the DAR is set to\n3. The optimal loss coefficient µin model training\nis set to 1. The number of samples of the best HCP\nkfor ICE14, ICE18, ICE05-15, and GDELT is set\nto 35, 25, 40, and 32, respectively. We conduct\nhyperparameter search experiments on the primary\nparameters of DGAR using control variables. The\nnumber of parameters in ICE14, ICE18, ICE05-15,\nand GDELT is 17.55 MB, 33.71 MB, 21.44 MB,\nand 21.38 MB, respectively. All experiments are\n--- Page 13 ---\n/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018\n/uni0000004e/uni00000017/uni00000017/uni00000017/uni00000019/uni00000017/uni0000001b/uni00000018/uni00000013/uni00000018/uni00000015/uni00000018/uni00000017/uni00000018/uni00000019/uni00000018/uni0000001b\n/uni00000030/uni00000035/uni00000035\n/uni0000002b/uni0000004c/uni00000057/uni00000056/uni00000023/uni00000016(a) ICE14\n/uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018\n/uni0000004e/uni00000016/uni00000013/uni00000016/uni00000015/uni00000016/uni00000017/uni00000016/uni00000019/uni00000016/uni0000001b/uni00000017/uni00000013/uni00000017/uni00000015\n/uni00000030/uni00000035/uni00000035\n/uni0000002b/uni0000004c/uni00000057/uni00000056/uni00000023/uni00000016 (b) ICE18\nFigure 6: Sensitivity Analysis.\nconducted on NVIDIA A40.\nB.4 Sensitivity Analysis\nIn this section, we conduct experiments on ICE14\nand ICE18 to further analyze the impact of hy-\nperparameter kin DGAR. The hyperparameter k\nmeans the replay data consists of HCPs at kdiffer-\nent times.\nTo explore how the number of kaffects the\nmodel’s ability to retain historical knowledge, we\nset different values for k. The results are shown in\nFigure 6, including MRR and Hits@3 results on\nthe historical tasks. A larger k means the replay\ndata consists of HCPs at more different times. The\nresults reveal that on the ICE14 dataset, DGAR\ndemonstrates an initial improvement followed by\na plateau as kincreases. These results indicate\nthat selecting HCPs at more different times for\nhistorical distribution representations replay does\nnot significantly enhance the final performance but\ninstead increases computational costs. Notably,\neven with only 5 recall time slices, DGAR outper-\nforms all baseline models. This demonstrates that\nour approach can effectively help the model retain\nhistorical knowledge, even under limited memory\nconstraints.\nB.5 Inference Efficiency\nICE14 ICE18\nkTime(s) / Task MRR kTime(s) / Task MRR\nRetrain — 553.48 49.80 — 530.48 31.35\nDGAR5 4.00 44.93 5 13.12 31.11\n25 5.21 49.90 15 16.32 32.43\n35 5.42 50.12 25 18.83 33.00\n45 5.68 50.05 35 19.41 33.11\nFT — 2.53 37.46 — 5.48 25.35\nER — 2.86 42.14 — 5.16 27.20\nTIE — 4.38 41.07 — 10.71 29.31\nLKGE — 2.70 37.51 — 5.03 25.56\nIncDE — 4.07 36.57 — 9.71 25.52\nTable 5: Inference efficiency analysis.\nWe report the average time cost of each task andthe MRR on historical tasks for DGAR at different\nkvalues in Table 5. The hyperparameter kmeans\nthe replay data consists of HCPs at kdifferent times.\nWe further report the average time consumption of\neach task and the MRR on historical tasks under\ndifferent baselines and retaining settings in Table\n5. Unlike DGAR and the baselines, retraining in-\nvolves reprocessing the entire dataset whenever\nnew data arrives. By comparing the results, our\nmethod outperforms retraining and requires less\ntime. This shows the powerful ability of our model\nin dealing with catastrophic forgetting. Because\nour model uses more complex operations than the\nbaseline in order to retain more historical knowl-\nedge, it is more time-consuming. Future research\nwill focus on enhancing reasoning efficiency while\npreserving the accuracy of historical knowledge\nretention.\nB.6 Effect Analysis of Lr\nIn the w/o Lrvariant, the performance of the his-\ntorical tasks drops significantly. This shows that\ntheLr,tloss in Section 4.4 effectively alleviates\nthe loss of historical information caused by current\ndata optimization and Diff-HDG.\nB.7 Random Selection of HCPs\nIn order to verify whether the generalization of\nreplay data is enhanced, we added an additional\nexperiment to prove. Compared to replaying histor-\nical data from the specified ktime slices, randomly\nselecting across the entire history provides more\nglobal and generalizable information. Therefore,\nwe specifically added an experiment where the his-\ntorical context prompts from the knearest time\nslices was selected as the replay data. The experi-\nmental results are in the Table 6 :\nICE14 ICE18\nk Average (MRR) k Average (MRR)\nnearest random nearest random\n25 46.64 49.90 15 31.23 32.43\n35 48.11 50.12 25 31.89 33.00\n45 47.86 50.05 35 32.03 33.11\nTable 6: Effect of Random Selection\nThe nearest refers to replaying with historical\ncontext prompts sampled from the nearest ktime\nslices. As shown in the Table 6, regardless of differ-\nentkvalues, the random outperforms the nearest\non the historical tasks. The experimental results\n--- Page 14 ---\nare consistent with our expectations. Mainly be-\ncause the random selection of historical context\nprompts provides the model with more generalized\ndata, thus improving the model’s performance on\nthe test set.\nB.8 Compare Base on LogCL\nTo further verify whether DGAR can enhance the\nperformance of recent GNN-based models under\nthe CL setting, we conducted the following experi-\nment.\nWe selected LogCL (Chen et al., 2024b) , a rep-\nresentative GNN-based TKGR model from recent\nworks, as the base model. Below, we report its\nperformance under the CL setting (FT) and its per-\nformance when combined with DGAR in the same\nsetting on two datasets.\nICE14 ICE18\nAlgo.Current Average Current Average\nMRR MRR Hits@10 MRR MRR Hits@10\nFT 31.63 28.93 48.61 38.46 30.47 55.52\nDGAR 37.12 33.08 53.83 43.20 32.88 58.88\nTable 7: Performance base on LogCL\nThe above experiments show that DGAR signif-\nicantly enhances LogCL’s reasoning performance\nunder the CL setting. Interestingly, LogCL per-\nforms worse on the ICE14 dataset than on ICE18,\nwhich contrasts with its performance in the full-\ntraining setting. This discrepancy occurs because\nICE14 has a simpler data distribution compared to\nICE18, while LogCL’s complex model structure\nmakes it prone to overfitting on ICE14. Under\nthe CL setting, highly complex models struggle\nto maintain stable learned features as new data is\nintroduced (Lee, 2024). DGAR mitigates this issue\nby replaying historical information, helping LogCL\nretain its learned features more effectively. Conse-\nquently, TKGR methods that excel in full-training\nscenarios may not necessarily achieve better rea-\nsoning performance under CL setting.",
  "text_length": 61403
}