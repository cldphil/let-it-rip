{
  "id": "http://arxiv.org/abs/2506.03142v1",
  "title": "Not All Tokens Are Meant to Be Forgotten",
  "summary": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results.",
  "authors": [
    "Xiangyu Zhou",
    "Yao Qiang",
    "Saleh Zare Zade",
    "Douglas Zytko",
    "Prashant Khanduri",
    "Dongxiao Zhu"
  ],
  "published": "2025-06-03T17:59:05Z",
  "updated": "2025-06-03T17:59:05Z",
  "categories": [
    "cs.LG"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.03142v1"
}