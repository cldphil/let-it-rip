{
  "id": "http://arxiv.org/abs/2506.01056v2",
  "title": "MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch",
  "summary": "Function-calling has enabled large language models (LLMs) to act as\ntool-using agents, but injecting thousands of tool schemas into the prompt is\ncostly and error-prone. We introduce MCP-Zero, a proactive agent framework that\nlets the LLM itself decide when and which external tools to retrieve, thereby\nassembling a task-specific toolchain from scratch. The framework is built upon\nthree components: (1) Proactive Tool Request, where the model emits a\nstructured $\\left<\\operatorname{tool\\_assistant}\\right>$ block that explicitly\nspecifies the desired server and task; (2) Hierarchical Vector Routing, a\ncoarse-to-fine retrieval algorithm that first selects candidate servers and\nthen ranks tools within each server based on the semantic similarity; (3)\nIterative Proactive Invocation, enabling multi-round, cross-domain toolchain\nconstruction with minimal context overhead, and allowing the model to\niteratively revise its request when the returned tools are insufficient. To\nevaluate our approach we also compile MCP-tools, a retrieval dataset comprising\n308 MCP servers and 2,797 tools extracted from the official\nModel-Context-Protocol repository and normalized into a unified JSON schema.\nExperiments show that MCP-Zero (i) effectively addresses the context overhead\nproblem of existing methods and accurately selects the correct tool from a pool\nof nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by\n98\\% on the APIbank while maintaining high accuracy; and (iii) supports\nmulti-turn tool invocation with consistent accuracy across rounds.",
  "authors": [
    "Xiang Fei",
    "Xiawu Zheng",
    "Hao Feng"
  ],
  "published": "2025-06-01T15:48:53Z",
  "updated": "2025-06-04T06:37:09Z",
  "categories": [
    "cs.AI",
    "cs.SE"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.01056v2",
  "full_text": "--- Page 1 ---\narXiv:2506.01056v2  [cs.AI]  4 Jun 2025MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch\nXiang Fei\nxiangf@stu.xmu.edu.cnXiawu Zheng*\nzhengxiawu@xmu.edu.cnHao Feng\nhaof@mail.ustc.edu.cn\nLLM\nQuery\nLLM\n<Need>Fork\nRequestResponse\n<Need>Knife\n<Need>Spoon\nMCPsLLM\nLLM\nFinish!MCPs\nRetriever\nQueryToolNot Enough\nMCPs\nRetriever\nLong Context(b) Retrieval-augmented tool selection(a) MCP info in system prompt\n(c) Our method\nFigure 1. Comparison of tool selection paradigms for LLM agents. (a) System-prompt-based methods inject all MCP tool schemas into\nthe context, resulting in excessive prompt length and inefficiency. (b) Retrieval-augmented approaches select tools by matching the user\nquery once, which may lead to inaccurate or insufficient tool selection, especially in multi-turn scenarios. (c) MCP-Zero enables the LLM\nto proactively analyze the task, iteratively request the most relevant tools as needed, and dynamically construct a multi-step toolchain with\nminimal context overhead and high accuracy.\nAbstract\nFunction-calling has enabled large language models\n(LLMs) to act as tool-using agents, but injecting thousands\nof tool schemas into the prompt is costly and error-prone.\nWe introduce MCP-Zero, a proactive agent framework that\nlets the LLM itself decide when and which external tools\nto retrieve, thereby assembling a task-specific toolchain\nfrom scratch. The framework is built upon three compo-\nnents: (1) Proactive Tool Request, where the model emits\na structured ⟨toolassistant ⟩block that explicitly specifies\nthe desired server and task; (2) Hierarchical Vector Rout-\ning, a coarse-to-fine retrieval algorithm that first selects\ncandidate servers and then ranks tools within each server\nbased on the semantic similarity; (3) Iterative Proactive\nInvocation, enabling multi-round, cross-domain toolchain\n*Corresponding authorconstruction with minimal context overhead, and allow-\ning the model to iteratively revise its request when the re-\nturned tools are insufficient. To evaluate our approach\nwe also compile MCP-tools, a retrieval dataset compris-\ning 308 MCP servers and 2,797 tools extracted from the\nofficial Model-Context-Protocol repository and normalized\ninto a unified JSON schema. Experiments show that MCP-\nZero (i) effectively addresses the context overhead prob-\nlem of existing methods and accurately selects the cor-\nrect tool from a pool of nearly 3,000 candidates (248.1k\ntokens); (ii) reduces token consumption by 98% on the\nAPIbank while maintaining high accuracy; and (iii) sup-\nports multi-turn tool invocation with consistent accuracy\nacross rounds. The code and dataset will be released soon\nathttps://github.com/xfey/MCP-Zero .\n1\n--- Page 2 ---\n1. Introduction\nThe rapid advancement of large language models (LLMs)\nhas catalyzed a paradigm shift from pure text understand-\ning and generation to sophisticated tool-using agents capa-\nble of interacting with external systems [1, 8, 30]. With the\nintroduction of function-calling mechanisms, LLMs have\ntranscended the boundaries of their parametric knowledge,\nenabling them to leverage external tools, third-party APIs,\nand code execution environments to accomplish complex\nreasoning chains and real-world tasks [14, 18, 21, 26].\nHowever, as the ecosystem of available tools continues\nto expand exponentially, the conventional approach of in-\njecting comprehensive tool schemas into the system prompt\nhas emerged as a critical bottleneck (Figure 1a). Main-\nstream practices typically embed complete JSON-Schema\nof all available tools within the system prompt, leading to\nsubstantial context overhead, as shown in Figure 2. For in-\nstance, the GitHub MCP server encompasses 26 tools re-\nquiring over 4,600 tokens, substantially compressing the\navailable context window for actual task content.\nTo mitigate context window constraints, recent ap-\nproaches have adopted retrieval-based strategies that match\nand inject only the most relevant tool based on semantic\nsimilarity with user queries [7, 15]. However, these meth-\nods face significant limitations when dealing with complex,\nmulti-step tasks that require coordination across multiple\ndomains (Figure 1b). For example, a query like “Debug the\nfile” requires coordinating tools across different domains:\nfilesystem for file access, code generation for updates, and\ncommand execution for debugging. Single-tool retrieval\nfails to capture such complex workflows, as the initial query\ncannot determine all required tools across domains.\nIn summary, while retrieval methods demonstrate ef-\nficacy in single-step, single-domain scenarios, they ex-\nhibit fundamental limitations in realistic multi-turn, multi-\ndomain agent environments: (1) passive retrieval - exter-\nnal systems select tools based on initial query rather than\nallowing models to actively express their evolving needs;\n(2)semantic misalignment between colloquial user inputs\nand formal API documentation creates distributional mis-\nmatches that reduce retrieval precision; and (3) single-round\ninvocation - tool retrieval is performed only once per query,\nfailing to accommodate the progressive refinement of sub-\ntask requirements or iterative correction when initially re-\ntrieved tools prove inadequate.\nModern LLMs possess powerful capabilities in chain-\nof-thought reasoning, self-reflection, and planning. Rather\nthan having external systems passively select tools based on\ninitial queries, we propose letting the model proactively an-\nalyze the context, identify capability gaps, and request tools\nwhen external assistance is needed. Based on this insight,\nwe introduce MCP-Zero (Figure 1c), a proactive retrieval\nframework with the following key components:<function>\n{\"description\": \"Search for GitHub repositories\",\n\"name\": \"mcp github search repositories\",\n\"parameters\": {\"$schema\":\n\"http://json-schema.org/draft-07/schema#\",\n\"additionalProperties\": false, \"properties\":\n{\"page\": {\"description\": \"Page number for\npagination (default: 1)\", \"type\": \"number\" },\n\"perPage\": {\"description\": \"Number of results\nper page (default: 30, max: 100)\", \"type\":\n\"number\" }, \"query\": {\"description\": \"Search query\n(see GitHub search syntax)\", \"type\": \"string\" }},\n\"required\": [\"query\"], \"type\": \"object\" }}\n</function>\nFigure 2. Example of a single MCP tool definition from the\nGitHub MCP server. This tool requires 143 tokens, while the com-\nplete server requires over 4,600 tokens.\nProactive Tool Request. Unlike traditional approaches\nthat passively wait for external retrieval, we return the au-\nthority of tool requirement specification to the LLM itself.\nWhen external tool assistance is needed, the model proac-\ntively generates structured tool request declarations in the\nfollowing format:\n<tool assistant>\nserver: ... # Platform/permission domain\ntool: ... # Operation type + target\n</tool assistant>\nThis mechanism enables spontaneous need expression\nwhile ensuring semantic consistency between requests and\ntool documentation, avoiding colloquial query issues and\nimproving retrieval quality.\nHierarchical Vector Routing. The system employs\ntwo-stage retrieval: first filtering candidate servers by plat-\nform requirements, then matching specific tools within se-\nlected servers. Only top-k tool descriptions are returned,\nsignificantly reducing context overhead.\nIterative Proactive Invocation. The model can initiate\nmultiple tool requests throughout the conversation for dif-\nferent subtasks, enabling cross-server toolchain construc-\ntion from scratch, while introducing only necessary tools\nat each step. If returned tools are insufficient, the model can\nrefine its requests and reinitiate retrieval, providing fault tol-\nerance and self-correction capabilities.\nTo support our approach, we also compile MCP-tools ,\nthe first retrieval dataset in the MCP domain, compris-\ning 308 servers and 2,797 tools extracted from the official\nModel-Context-Protocol repository1and normalized into a\nunified JSON schema. Our comprehensive experiments\ndemonstrate that MCP-Zero effectively addresses the con-\ntext overhead problem while maintaining high accuracy in\ntool selection from nearly 3,000 candidates, reduces token\nconsumption by 98% compared to existing methods, and\nsupports consistent multi-turn tool invocation across con-\nversation rounds.\n1github.com/modelcontextprotocol/servers\n2\n--- Page 3 ---\nOur main contributions are summarized as follows:\n• We propose MCP-Zero, enabling LLMs to proactively an-\nalyze tasks, iteratively request tools on demand, and dy-\nnamically construct multi-step toolchains with minimal\ncontext overhead and high accuracy.\n• We design a hierarchical vector routing mechanism that\neffectively improves tool retrieval accuracy and context\nutilization efficiency.\n• We construct and release MCP-tools, a comprehensive re-\ntrieval dataset covering 308 servers and 2,797 tools from\nthe official Model-Context-Protocol repository.\n• Extensive experiments demonstrate that our method sig-\nnificantly reduces context overhead while maintaining\nconsistent performance, effectively addressing the con-\ntext length problem in tool-calling scenarios.\n2. Related Work\n2.1. Tool-Augmented LLMs\nThe evolution of tool-augmented large language models has\nprogressed through distinct paradigms, each addressing dif-\nferent aspects of the fundamental challenge: how to effec-\ntively integrate external capabilities with language model\nreasoning.\nEarly Task-Specific Integration. Initial approaches fo-\ncused on hard-coding specific tools into language models.\nMRKL [10] introduced modular reasoning by combining\nneural networks with symbolic modules like calculators,\nwhile WebGPT [16] demonstrated web browsing capabili-\nties for information retrieval tasks. Though effective within\ntheir domains, these systems suffered from limited scalabil-\nity and poor generalization to new tool types.\nUniversal Agent Protocols. The introduction of Re-\nAct [31] marked a paradigm shift by establishing the\n“observation-action-thought” pattern, creating a universal\nprotocol for tool-augmented reasoning. This framework be-\ncame the foundation for modern agent systems including\nLangChain Agents [4] and AutoGen [29], enabling flexible\nand extensible tool integration architectures.\nTraining-Based Tool Learning. Parallel developments\nexplored learning tool usage through model training. Tool-\nformer [26] pioneered self-supervised learning for API call\ngeneration, teaching models to insert tool invocation mark-\ners within natural text generation. Gorilla [18] extended this\napproach by constructing large-scale instruction datasets\nwith tool calls, enabling supervised learning of query-to-\ntool mappings across diverse API collections.\nContext-Based Tool Injection. The emergence of Chat-\nGPT Function Calling and systems like HuggingGPT [27]\nintroduced context-based approaches, which inject JSON-\nSchema tool descriptions into system prompts, eliminat-\ning the need for specialized training. ART [17] further re-\nfined this paradigm by constructing demonstration librariesin chain-of-thought format, leveraging in-context learning\nfor tool selection and usage.\nFundamental Limitations. Despite these advances,\nexisting approaches face critical scalability challenges.\nTraining-based methods require expensive retraining for\neach toolset update, limiting their adaptability to evolving\ntool ecosystems. Context-based methods, while more flexi-\nble, suffer from prohibitive context overhead when injecting\ncomprehensive tool descriptions—a problem that becomes\nacute as tool collections scale to thousands of APIs. Most\ncritically, when relevant tools are absent from the prede-\nfined context, task completion becomes impossible, high-\nlighting the need for dynamic, on-demand tool discovery\nmechanisms that can adapt to diverse and evolving task re-\nquirements.\n2.2. Tool Retrieval for LLMs\nThe success of Retrieval-Augmented Generation (RAG)\nin addressing knowledge limitations through the “retrieve-\ninsert-generate” paradigm has inspired its adaptation to\ntool selection for LLMs [11]. Classical RAG frameworks\nlike REALM [9], RETRO [3] and In-Context RALM [25]\ndemonstrated the effectiveness of dynamically incorporat-\ning external knowledge into generation processes. Recent\nwork has extended this paradigm to tool selection, aiming\nto reduce context overhead by retrieving only the most rel-\nevant tools for a given query.\nSemantic Similarity-Based Retrieval. Early ap-\nproaches focused on direct semantic matching between user\nqueries and tool descriptions. Gorilla [18] constructed vec-\ntor databases from API documentation and usage examples,\nemploying semantic similarity to retrieve relevant tools.\nTool2Vec [15] addressed the semantic gap between user\nrequests and formal API descriptions by pre-collecting di-\nverse user invocation patterns and computing averaged em-\nbeddings, though this approach requires extensive user in-\nteraction datasets for training. RAG-MCP [7] performs\nserver-level matching between user queries and documen-\ntation from MCPBench [13], returning all tools from the\nmost similar server as the context of LLMs.\nHierarchical Tool Retrieval. Recognizing the limita-\ntions of flat retrieval, several works have explored coarse-\nto-fine approaches. AnyTool [6] implemented a multi-level\nretrieval system based on the RapidAPI dataset, construct-\ning separate retrievers for “category-tool-API” hierarchies.\nToolRerank [32] leveraged pre-trained BERT models for se-\nmantic matching, while COLT [23] employed specialized\nlanguage models for tool selection. Re-Invoke [5] intro-\nduced key information extraction from user queries before\ntool matching.\nLimitations of Current Approaches. Despite reduc-\ning context overhead, existing retrieval-based methods face\nfundamental limitations. First, they usually rely on single-\n3\n--- Page 4 ---\nDebug my code: `src/train.py`\nSemanticMatching\n<tool_response>Server: FilesystemTool: `read_file`Params:{\"path\": \"path of thetarget file\"}</tool_response>\n<function_call_response>import numpy as np...if __name__ == \"__main__\":main()</function_call_response><function_call><invoke name=\"read_file\"><parameter name=\"path\">src/train.py</parameter></invoke></function_call>\n<function_call> `edit_file` </function_call>\n<function_call_response>... </function_call_response>\nFinally, I’ll run this code and check if the issue is solved now.<tool_assistant>Server: Terminal environment to execute command.Tool: Run command and get return value.</tool_assistant>Okay, I’ll fix that. First, let me read the file.<tool_assistant>Server: File system allowing file operationsand management.Tool:Readfilebyfilename.</tool_assistant>\nGreat, I have found the issue. Let me fix the code.<tool_assistant>Server: File system allowing file operationsand management.Tool:Edit target file with new string.</tool_assistant>\n<tool_response> `edit_file` </tool_response>\n<tool_response> `run_cmd` </tool_response>\n<function_call> `run_cmd` </function_call>\n<function_call_response>... </function_call_response>\nGreat! The bug is now fixed:) Figure 3. Illustrative example of MCP-Zero’s iterative proactive\ninvocation. The model progressively identifies capability gaps and\nrequests tools across three domains on-demand.\nround matching based on initial user queries: if the retrieval\nfails, task completion becomes impossible. Second, and\nmore critically, user requests are often broad and composite\nin nature (e.g., “write and run code to crawl AI reposito-ries from GitHub”), requiring decomposition into multiple\nsubtasks that cannot be solved by any single tool. Such sce-\nnarios demand progressive task breakdown where the model\niteratively identifies capability gaps and requests appropri-\nate tools on-demand, a challenge that current “query-once,\nretrieve-once” paradigms cannot adequately address.\n2.3. Model Context Protocol\nThe Model Context Protocol (MCP) is an open standard in-\ntroduced in 2024 to enable secure and uniform access to ex-\nternal tools and services for large language models [2]. By\ndefining a standardized interface through JSON-RPC mes-\nsage exchange, MCP addresses the fragmentation problem\nin AI tool integration, where different platforms previously\nrequired custom connectors and proprietary protocols. This\nstandardization has led to rapid adoption across major AI\nplatforms and development environments, facilitating the\ncreation of hundreds of MCP servers spanning diverse do-\nmains including file systems, databases, web services, and\nspecialized APIs.\nHowever, this expansion creates significant context over-\nhead, as traditional approaches inject all server JSON-\nSchemas into system prompts simultaneously. Current so-\nlutions for MCP context optimization remain limited and\nfollow conventional retrieval paradigms [7]. Given the en-\nhanced capabilities of modern LLMs and MCP’s potential\nfor agentic AI, we argue that existing methods fail to fully\nleverage contemporary language models’ tool-calling capa-\nbilities, motivating our MCP-Zero framework for dynamic,\non-demand tool discovery.\n3. Method: MCP-Zero\nWe introduce MCP-Zero through two key aspects: the com-\nprehensive framework design (Section 3.1) and theoretical\nanalysis of proactive retrieval (Section 3.2). Our approach\nfundamentally shifts from passive tool injection to active\ntool discovery, enabling LLMs to dynamically construct\ntask-specific toolchains across diverse domains with min-\nimal context overhead.\n3.1. Proactive Tool Discovery Framework\nMCP-Zero is a proactive agent framework that enables\nLLMs to dynamically construct task-specific toolchains\nthrough on-demand tool retrieval. The framework operates\nthrough three core components that work synergistically to\naddress the context overhead and multi-domain coordina-\ntion challenges of existing approaches.\nOverall Workflow. Given a user query such as “De-\nbug my code: src/train.py ”, the LLM analyzes the\ntask and autonomously determines when external tool assis-\ntance is needed. As shown in Figure 3, the model progres-\nsively breaks down the complex debugging task into sub-\ntasks: reading the file, analyzing and fixing the code, and\n4\n--- Page 5 ---\nvalidating the fix through execution. At each step, instead\nof relying on pre-injected tool schemas, the model proac-\ntively generates structured tool requests. These requests are\nprocessed through hierarchical vector routing to retrieve the\nmost relevant tools, which are then injected into the con-\ntext for immediate use. The model iteratively repeats this\nprocess throughout the conversation, constructing a cross-\ndomain toolchain spanning filesystem operations, code edit-\ning, and command execution.\nProactive Tool Request. The foundation of our frame-\nwork lies in returning tool requirement specification author-\nity to the LLM itself. When the model identifies a capability\ngap that requires external assistance, it generates a struc-\ntured request block:\n<tool assistant>\nserver: ... # Platform/permission domain\ntool: ... # Operation type + target\n</tool assistant>\nThis mechanism enables the model to express tool needs\nspontaneously as they arise during task execution. Com-\npared to raw user queries, the model-generated requests en-\nsures better semantic alignment with tool documentation.\nTheserver field specifies the platform or permission do-\nmain requirements, while the tool field describes the de-\nsired operation type and target.\nWe design our framework around these two fields be-\ncause they align naturally with the MCP specification,\nwhich mandates that all servers and tools provide descrip-\ntive documentation. This inherent requirement ensures con-\nsistent semantic information availability across the entire\nMCP ecosystem, making our retrieval approach universally\napplicable without additional metadata engineering.\nImportantly, the model can generate multiple such re-\nquests throughout a single conversation, with each request\ntriggering an independent retrieval process.\nHierarchical Vector Routing. To efficiently locate rel-\nevant tools from thousands of candidates, we employ a\ntwo-stage coarse-to-fine retrieval algorithm, using OpenAI\ntext-embedding-3-large embeddings for semantic\nsimilarity matching.\nThe system first filters candidate MCP servers by match-\ning the server field against server descriptions. Since\nserver descriptions are typically brief single sentences, we\nconstruct extended summaries that include comprehensive\nusage examples essential for accurate matching (detailed in\nSection 4). We then perform matching against both the orig-\ninal descriptions and these enhanced summaries, taking the\nhigher similarity score between the two approaches. This\ndual-matching strategy leverages both original MCP doc-\numentation and enhanced summaries to improve retrieval\nprecision.\nSubsequently, tools within selected servers are ranked\nbased on semantic similarity between the tool field andtool descriptions. The final ranking score combines both\nserver-level and tool-level similarities:\nscore = (sserver×stool)×max( sserver, stool) (1)\nwhere sserver andstoolrepresent cosine similarities at server\nand tool levels respectively. This scoring mechanism en-\nsures that high similarity in either dimension contributes\nsignificantly to the final ranking, enabling effective recall\nof highly relevant tools. The system returns the top- k\ntools, with dynamic adjustment possible when similarity\nscores are closely clustered. In our experiments, we achieve\nhigh accuracy with top- 1retrieval, though we can configure\nlarger kvalues to enhance fault tolerance when needed.\nIterative Proactive Invocation. Unlike traditional\nsingle-round retrieval approaches, MCP-Zero supports it-\nerative tool discovery throughout task execution. After re-\nceiving retrieved tools, the model autonomously evaluates\ntheir adequacy for the current subtask. If the returned tools\nare insufficient or inappropriate, the model can refine its re-\nquest specification and reinitiate retrieval, providing natural\nfault tolerance and self-correction capabilities. This itera-\ntive process continues until the model determines that either\n(1) suitable tools have been found and the task can proceed,\nor (2) no appropriate tools exist and the task should rely on\nthe model’s parametric knowledge.\nThe framework’s key advantage lies in its ability to con-\nstruct cross-domain toolchains dynamically. For complex\ntasks requiring coordination across multiple domains (e.g.,\nfilesystem access, code generation, and command execu-\ntion in Figure 3), the model can progressively identify and\nrequest tools from different servers as subtask requirements\nbecome clear, avoiding the context overhead of pre-loading\ncomprehensive tool collections while maintaining high task\ncompletion accuracy. MCP-Zero represents a fundamental\nshift from “predefined toolset” to “dynamic on-demand tool\ndiscovery” for the community.\n3.2. Theoretical Analysis\nWe provide a theoretical analysis of MCP-Zero’s advan-\ntages over traditional approaches through formal probabil-\nity modeling and complexity analysis.\nProblem Formulation. LetT={t1, t2, ..., t n}denote\nthe complete tool collection, qrepresent the user query, c\nthe conversation context, and t∗the optimal tool selection.\nIn traditional approaches, models must select tools from the\nentire collection simultaneously:\nPtraditional (t∗|q, c, T ) =P(q, c|t∗, T)P(t∗|T)P\nti∈TP(q, c|ti, T)P(ti|T).(2)\nIn contrast, MCP-Zero employs a two-stage process\nwhere models first generate structured requests r, then re-\n5\n--- Page 6 ---\ntrieve tools based on these requests:\nPproposed (t∗|q, c) =X\nrP(t∗|r)P(r|q, c), (3)\nwhere P(r|q, c)represents the probability of generating re-\nquest rgiven the query and context, and P(t∗|r)denotes the\nprobability of retrieving the optimal tool given the request.\nComplexity Bottlenecks of Traditional Approaches.\nTraditional methods face three fundamental limitations that\ncompound as tool collections scale:\nComputational Complexity Growth: Models must evalu-\nate all ntools simultaneously, resulting in O(n)computa-\ntional complexity. This linear growth becomes prohibitive\nas tool ecosystems expand.\nAttention Dilution Effects: Assuming finite attention re-\nsources, each tool receives approximately1\nnof the model’s\nattention. Introducing a noise factor η(n)∝log(n)that\nincreases with collection size, the effective selection proba-\nbility degrades to:\nPnoise(t∗|q, c, T ) =Ptraditional (t∗|q, c, T )·(1−η(n)).(4)\nSemantic Distribution Mismatch: User queries and for-\nmal tool descriptions exhibit semantic distribution gaps\nquantifiable through KL divergence DKL(Pq∥Pt), which re-\nduces matching accuracy in vector-based retrieval systems.\nAdvantages of Proactive Request Mechanism. MCP-\nZero addresses these limitations through two key mecha-\nnisms:\nSearch Space Compression: The hierarchical routing op-\nerates in two stages: first filtering among mservers where\nm≪n, then matching within the filtered tool subset Tfiltered\nfrom top-m selected servers. This reduces total complexity\nfrom O(n)toO(m+|Tfiltered|)where |Tfiltered| ≪n.\nSemantic Consistency Enhancement: Model-generated\nrequests rexhibit higher semantic alignment with tool de-\nscriptions compared to raw user queries:\nDKL(Pr∥Pt)< D KL(Pq∥Pt), (5)\nwhich enhances vector matching accuracy.\nInformation-Theoretic Analysis and Probability\nGains. From an information-theoretic perspective, tradi-\ntional methods require decisions over high-entropy spaces.\nThe decision entropy for traditional approaches is:\nHtraditional (T) =−X\nti∈TP(ti) logP(ti)≈log(n).(6)\nMCP-Zero’s hierarchical filtering reduces the effective\ndecision entropy to:\nHproposed (Tfiltered) =−X\nti∈TfilteredP(ti) logP(ti)\n≈log(|Tfiltered|)≪log(n).(7)This entropy reduction translates to measurable prob-\nability gains. Defining the probability gain as ∆P=\nPproposed (t∗|q, c)−Ptraditional (t∗|q, c, T ). In the traditional\nuniform, the baseline probability for selecting any specific\ntool approaches1\nn. However, the attention dilution effect in-\ntroduced by processing large tool collections degrades this\nbaseline by the noise factor η(n), yielding an effective suc-\ncess probability bounded by1\nn(1−η(n)).\nIn contrast, MCP-Zero operates over a compressed\nsearch space of size m+|Tfiltered|where m≪nand\n|Tfiltered| ≪n. More critically, the semantic alignment im-\nprovement quantified by the KL divergence reduction intro-\nduces a multiplicative enhancement factor α >1:\nPproposed (t∗|q, c)≈α\nm+|Tfiltered|(8)\nPtraditional (t∗|q, c, T )≈1−η(n)\nn(9)\nSince m+|Tfiltered| ≪nandα > 1due to improved\nsemantic consistency, we have ∆P > 0for practical tool\ncollection sizes. Furthermore, as nincreases while mand\n|Tfiltered|remain relatively stable (reflecting the hierarchi-\ncal structure of tool ecosystems), the probability gain ∆P\ngrows approximately linearly with n, demonstrating MCP-\nZero’s superior scalability as tool ecosystems expand.\n4. Dataset: MCP-Tools\nTo support our proactive tool discovery framework and en-\nable comprehensive evaluation, we construct MCP-tools ,\nthe first retrieval-oriented dataset in the MCP domain. Un-\nlike existing MCP evaluation frameworks such as MCP-\nBench [13] that focus on server availability and latency test-\ning, our dataset is specifically designed to facilitate seman-\ntic tool discovery and retrieval for large language models.\n4.1. Data Collection and Scope\nWe systematically collected MCP server information from\nthe official Model Context Protocol servers repository2\n(Tag: 2025.4.28 , Commit: ad2d4e6 ). The repository\ncontains three categories of servers: (1) 20 reference servers\nmaintained by the MCP team, (2) 114 third-party official\nservers, and (3) 262 community-contributed servers, total-\ning 396 servers.\nOur collection process involved extracting initial meta-\ndata including server names, brief descriptions, and reposi-\ntory URLs. Most servers link to GitHub repositories con-\ntaining comprehensive documentation in README files,\nwhich include detailed server descriptions, tool specifica-\ntions, usage examples, and implementation guidelines.\n2github.com/modelcontextprotocol/servers\n6\n--- Page 7 ---\nClaude-3.5-SonnetGemini-2.5-FlashGPT-4.1Standard Tool CallMCP-ZeroMCP-Zero(w/ Context Example)\nSucceedFailedTarget Tool Position0%        50%        100%\n1  5   11   24   53   117   258   572    1265     2797Number of ToolsFigure 4. Needle-in-a-haystack test results demonstrating MCP-Zero’s performance under extreme scale conditions with varying tool\ncollection sizes. The left column shows baseline methods with standard tool call schemas; the middle column shows our MCP-Zero\napproach; the right column shows MCP-Zero enhanced with one ICL example to guide the model’s description generation. Our method\nshows significant gains on Claude-3.5-Sonnet and Gemini-2.5-Flash, while GPT-4.1 shows no improvement due to its already strong\nbaseline performance.\n4.2. Data Processing and Quality Control\nWe implemented a rigorous filtering process to ensure data\nquality and completeness. Servers were excluded if they:\n(1) lacked MCP-compliant tool definitions, (2) missing doc-\numentation, or (3) failed to provide sufficient semantic in-\nformation for retrieval purposes. This filtering process re-\nduced our dataset from 396 to 308 high-quality servers.\nFor structured information extraction, we employed\nQwen2.5-72B-Instruct [24] with manually crafted few-shot\nin-context learning examples to guide the extraction pro-\ncess. The extraction follows a standardized JSON schema:\n{\n\"server_name\": string,\n\"server_description\": string,\n\"server_summary\": string,\n\"tools\": [\n{\n\"name\": string,\n\"description\": string,\n\"parameter\": {\n\"param1\": \"(type) description1\",\n\"param2\": \"(Optional, type) description2\"\n}\n}\n]\n}\nA critical component of the MCP-tools dataset is theserver summary field, which we generate by leverag-\ning LLMs to synthesize comprehensive summaries from\nREADME content. These summaries capture server capa-\nbilities, features, and usage patterns while excluding opera-\ntional details like installation instructions. This design en-\nables effective matching regardless of whether the model\nexpresses tool requirements in formal descriptions or more\nspecific, invocation-oriented language, thereby improving\nretrieval precision across diverse query formulations.\n4.3. Dataset Statistics and Characteristics\nThe final MCP-tools dataset comprises 308 servers and\n2,797 tools, representing a comprehensive coverage of the\nMCP ecosystem. The tool distribution exhibits significant\nvariance: the mean number of tools per server is 9.08 (me-\ndian: 5.0, standard deviation: 11.40). Over half of the\nservers (162 servers) contain 5 or fewer tools, while some\nspecialized servers include over 60 tools, reflecting the di-\nverse nature of MCP applications ranging from simple util-\nities to comprehensive API suites.\nTo support efficient retrieval, we pre-compute embed-\ndings for all textual descriptions and summaries using Ope-\nnAItext-embedding-3-large , creating a searchable\n7\n--- Page 8 ---\n10!10\"10#10$10%10&10'10$Claude-3.5-SonnetGemini-2.5-FlashGPT-4.1StandardTool CallMCP-ZeroClaude-3.5-SonnetGemini-2.5-FlashGPT-4.1Number of MCP ToolsAverage Token CostFigure 5. Token efficiency comparison in needle-in-a-haystack\nexperiments. The graph shows the average token cost per success-\nful retrieval across different collection sizes.\nvector index that enables rapid semantic matching during\nthe hierarchical routing process. These pre-computed em-\nbeddings are included as part of the dataset release.\nTo our knowledge, this represents the first dataset specif-\nically designed for semantic tool retrieval in the MCP do-\nmain, complementing existing evaluation frameworks like\nMCPBench [13] by focusing on discovery rather than per-\nformance testing. The dataset’s design directly supports our\nMCP-Zero framework while remaining general enough to\nbenefit other retrieval-based tool selection approaches. By\nproviding both original MCP documentation and enhanced\nsemantic summaries, MCP-tools enables researchers to ex-\nplore various retrieval strategies and evaluate their effective-\nness across diverse tool domains.\n5. Experiments\n5.1. Existing Datasets and Their Limitations\nBefore presenting our experimental evaluation, we analyze\nexisting tool-calling datasets and explain why they are in-\nsufficient for evaluating our MCP-Zero framework.\nAPI-Bank [12] provides tool information with multi-\nturn conversations and human-annotated test sets, mak-\ning it the most suitable existing dataset for our evaluation.\nToolBench [20] collected 16,464 REST APIs from Rapi-\ndAPI Hub but many APIs lack essential descriptions. Tool-\nBank [15] improved upon ToolBench but relies on model-\ngenerated data. ToolAlpaca [28] synthesized 3,938 in-\nstances from 400 APIs but targets scenarios mismatched\nwith contemporary use cases. The APIBench1 from Go-\nrilla [18] contains 1,645 APIs but uses entirely GPT-\nsynthesized conversations. Another APIBench2 [19] is\nproposed, but its application scenarios have low relevance\nto MCP tool calls.\nExisting datasets have limitations for evaluating MCP-\nZero: API scenarios differ from contemporary MCP useCollection Method Claude-3.5 GPT-4.1 Gemini-2.5 Avg. Tokens ↓\nSingle-turn Conversation\nDomainQ.Retrieval 71.63 –\nStandard 97.60 98.08 92.79 312.4\nMCP-Zero 96.15 96.62 97.12 111.0 (-64.47%)\nFullQ.Retrieval 71.63 –\nStandard 69.23 94.71 94.23 6308.2\nMCP-Zero 95.19 95.19 96.63 111.0 (-98.24%)\nMulti-turn Conversation\nDomainQ.Retrieval 65.05 –\nStandard 100.00 99.46 91.40 406.4\nMCP-Zero 91.40 93.01 93.01 159.0 (-60.84%)\nFullQ.Retrieval 65.05 –\nStandard 60.22 93.01 92.47 6402.2\nMCP-Zero 90.32 92.47 94.62 159.0 (-97.52%)\nTable 1. APIBank evaluation results comparing MCP-Zero with\nstandard tool calling methods across different scenarios. The ac-\ncuracy is based on the top- 1result. Results show accuracy (%) for\nthree LLMs and average token consumption. “Q.Retriaval” indi-\ncates retrieval based on user query.\ncases, lack hierarchical server-tool organization, or miss\ncritical evaluation fields. Therefore, we use API-Bank as\na reference and create the MCP-tools dataset for compre-\nhensive evaluation. We are continuing to investigate other\navailable datasets and conducting further validation.\n5.2. Needle-in-a-Haystack Evaluation\nTo evaluate MCP-Zero’s ability to accurately retrieve tools\nfrom large-scale collections under extreme context condi-\ntions, we conduct needle-in-a-haystack experiments on our\nMCP-tools dataset.\nExperimental Setup. We construct test scenarios by\ninjecting 1 to 2,797 tools into the environment, selecting\ntask descriptions from various positions as queries, and re-\nquiring models to retrieve the target tool. This setup sim-\nulates the challenging scenario where relevant tools are\nburied within massive tool collections. We compare three\napproaches:\n•Baseline : Standard tool call schemas with all tools in-\njected into context\n•MCP-Zero : Our proactive retrieval approach\n•MCP-Zero + ICL : MCP-Zero enhanced with one in-\ncontext learning example to guide description generation\nResults Analysis. As shown in Figure 4, our method\ndemonstrates significant performance gains on Claude-3.5-\nSonnet and Gemini-2.5-Flash, while GPT-4.1 shows no im-\nprovement due to its already strong baseline performance\nacross all tool collection sizes. Figure 5 illustrates the dra-\nmatic difference in token consumption between traditional\napproaches and MCP-Zero. While standard tool call meth-\nods exhibit exponential growth in token costs as the number\nof MCP tools increases, MCP-Zero maintains consistently\nlow token usage.\n8\n--- Page 9 ---\n5.3. APIBank Evaluation\nTo validate MCP-Zero’s effectiveness in realistic conversa-\ntional tool retrieval scenarios, we conduct comprehensive\nexperiments on the APIBank dataset.\nExperimental Setup. We extract description informa-\ntion from the APIBank level-1 dataset for retrieval tasks,\nget 48 unique tools in total, and process it for our evalua-\ntion framework. Since APIBank organizes data by individ-\nual APIs without server-level hierarchy, we directly retrieve\ntools without the server filtering stage.\nWe evaluate across two key dimensions: conversation\ncontext and tool collection scope. For conversation context,\nwe test both single-turn scenarios (one user query for one\nresponse) and multi-turn scenarios (extended conversations\nwith multiple exchanges). For tool collection scope, we ex-\namine both domain collections (using a curated subset of\ntools relevant to the specific domain) and full collections\n(retrieving from the complete set of all available tools).\nResults Analysis. As shown in Table 1, we can conlude\nthe following findings:\n•Extreme Context Efficiency. MCP-Zero cuts prompt\nlength by 60–98% across all settings (e.g. 111 vs. 6.3k\ntokens in the full single-turn case), validating its ability\nto ”pay for tools only when they are needed”.\n•Robust Scalability. When moving from a hand-curated\nDomain subset to the Full tool pool (40x more APIs),\nstandard schema-injection accuracy on Claude-3.5 plum-\nmets from 97.60 to 69.23 (single-turn) and 100.00 to\n60.22 (multi-turn); MCP-Zero instead keeps accuracy\nat 95.19 / 90.32 respectively, demonstrating strong re-\nsilience to attention dilution.\n•Multi-turn Consistency. MCP-Zero maintains high ac-\ncuracy over conversation rounds ( ≤3% drop from single-\nto multi-turn), whereas standard methods degrade sharply\nonce the context accumulates previous calls and larger\ntool sets.\n•Necessity of Proactive Requests. Pure query-retrieval\nbaselines stall at 65–72 % accuracy, confirming that let-\nting the model author semantically aligned requests is\ncrucial.\nExperiments on APIBank corroborate our claims: MCP-\nZero delivers near-optimal or superior tool-selection accu-\nracy while slashing context usage by up to two orders of\nmagnitude, remaining robust in both single- and multi-turn\nconversations and under massive tool-pool scaling. These\nresults highlight proactive, iterative tool discovery as a prac-\ntical path toward scalable, cost-efficient agent systems.\n6. Discussion\nIn this section we reflect on how the MCP-Zero paradigm\ncan be adopted by other researchers (§6.1), analyse the\nsurprisingly gain from a single in–context example (§6.2),and position MCP-Zero with respect to the contemporane-\nousAlita system, outlining a promising path toward self-\nimproving agentic AI (§6.3).\n6.1. Cookbook: Integrate MCP-Zero Into Agent\nMCP-Zero is fundamentally a simple yet effective approach\nthat we hope will benefit the broader MCP community. The\ncore methodology distills into three straightforward steps:\nprompting models to proactively request tools, maintain-\ning a lightweight tool index with semantic descriptions,\nand leveraging the improved semantic alignment for high-\nprecision retrieval. Below we provide a practical guide for\nintegrating these ideas into existing agent frameworks.\nStep 1 – Prompting the LLM to askfor tools. Give the\nmodel an explicit “permission” to declare missing capabili-\nties. In practice this is a system instruction such as:\nIf the current task cannot be solved with your\nown knowledge, emit a <tool assistant> block\nspecifying the server domain and the tool\noperation you require.\nIn addition, the output structure needs to be specified as\nwe mentioned in Section 3.1. This step aims to stimulate\nthe model’s ability to ”proactively” propose requirements.\nStep 2 – Curate a lightweight MCP-style tool index.\nFirstly, choose a scope based on your needs: the entire\nMCP-tools collection, a vertical slice (e.g. databases only),\nor your in-house APIs. Then, for every server/tool:\n• extract the name and description from metadata;\n• optionally let a strong LLM generate an enhanced sum-\nmary that emphasises capabilities and usage patterns;\n• store all texts in a vector store with pre-computed embed-\ndings such as text-embedding-3-large .\nStep 3 – Marry model output and retrieval.\nWhen the agent emits a <tool assistant> block:\n• Match the server field against server descriptions and\nsummaries; take top- mcandidates.\n• Within each candidate server, rank tools by the tool\nfield with the tool description embeddings.\n• Feed the best (or top- k) JSON-schemas back to the LLM.\nBecause the request text is already semantically aligned\nwith the documents, retrieval precision is higher than “user\nquery→API doc” matching, maintaining performances\nwhile significantly conserving context.\n6.2. Why Does a Single ICL Example Help?\nIn §5.2 we observed that adding one in-context exam-\nple (“ICL-1”) helps lifting needle-in-haystack accuracy\nmarginally. We hypothesise two simple but potent effects:\n1.Stylistic anchor. Our base prompt merely says “output\nthe server and tool you need”, but gives no example of\nhow the sentence should look like. The single in-context\n9\n--- Page 10 ---\nsample provides the writing style as the reference, help-\ning the generated requests land much closer to the cu-\nrated descriptions, thus semantic matching becomes eas-\nier.\n2.Semantic grounding. The example also clarifies the\nmeaning of each field, helping the model understand the\nspecific definitions of MCP server and tool, thereby lim-\niting its expression scope. After seeing this, the model\nreliably emits phrases such as filesystem read in-\nstead of a vague “read the file”, sharply reducing seman-\ntic mismatch.\nIn short, a tiny demonstration patch acts as a schema an-\nchor; future work could replace ICL with a short grammar-\nbased decoder rule, but the one-shot approach is free and\nhighly effective.\n6.3. Synergy with Alita: Using andMaking Tools\nConcurrently, Alita [22] proposes a united manager agent\nthat creates its own toolchain: it web-searches for code,\nclones GitHub repos, builds environments, and executes the\nresulting programs to accomplish tasks. We were pleasantly\nsurprised by the contribution of this article, and found that\nthe two lines of work are complementary:\n• MCP-Zero: efficiently finds and invokes existing tools\n• Alita : automatically builds missing tools on-the-fly\nMCP-Zero and Alita address complementary halves of\nthe same problem: the former maximises tool discovery\nwhile the latter maximises tool creation . When combined,\nthey form a virtuous loop: an agent first proactively discover\ntools from allavailable resources; if none fits, it switches\nto Alita’s workflow to synthesize a new one, then registers\nthe freshly built tool for the community. We believe such\na pipeline is a compelling direction toward self-evolving,\ncost-aware agentic AI systems.\n6.4. Future Work\nWhile MCP-Zero demonstrates significant improvements in\ntool retrieval efficiency and accuracy, several promising di-\nrections warrant further investigation:\nEnhanced Experimental Validation. Future work\nshould expand evaluation across diverse domains. We\nplan to conduct comprehensive experiments on additional\ndatasets to validate generalizability.\nAdvanced Matching Algorithms. The current semantic\nsimilarity approach could be enhanced. We envision incor-\nporating multi-modal descriptions (e.g., code examples, us-\nage patterns, parameter schemas) into the retrieval process,\nand exploring usage co-occurrence patterns for improved\ncontextual understanding.\nMCP Server Implementation. A natural extension in-\nvolves packaging MCP-Zero as a dedicated MCP server\nproviding tool discovery services. This ”meta-server”\nwould expose standardized APIs for proactive tool retrieval,enabling seamless integration into existing MCP ecosys-\ntems and serving as a centralized discovery hub for dis-\ntributed tool collections.\nMulti-Agent Orchestration. MCP-Zero’s proactive\ndiscovery approach could enable better multi-agent collab-\noration. Future work could investigate how different agents\ncan automatically discover and share tools with each other,\nallowing them to work together more effectively on com-\nplex tasks that require diverse capabilities.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report. arXiv preprint arXiv:2303.08774 ,\n2023. 2\n[2] Anthropic. Model context protocol. https://docs.\nanthropic.com/en/docs/agents-and-tools/\nmcp, 2024. Accessed: June 5, 2025. 4\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,\nAidan Clark, et al. Improving language models by retriev-\ning from trillions of tokens. In International conference on\nmachine learning , pages 2206–2240. PMLR, 2022. 3\n[4] Harrison Chase. Langchain. https://github.com/\nlangchain-ai/langchain , 2022. Python framework\nfor developing applications powered by language models. 3\n[5] Yanfei Chen, Jinsung Yoon, Devendra Singh Sachan, Qingze\nWang, Vincent Cohen-Addad, Mohammadhossein Bateni,\nChen-Yu Lee, and Tomas Pfister. Re-invoke: Tool invo-\ncation rewriting for zero-shot tool retrieval. arXiv preprint\narXiv:2408.01875 , 2024. 3\n[6] Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-\nreflective, hierarchical agents for large-scale api calls. arXiv\npreprint arXiv:2402.04253 , 2024. 3\n[7] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt\nbloat in llm tool selection via retrieval-augmented genera-\ntion. arXiv preprint arXiv:2505.03275 , 2025. 2, 3, 4\n[8] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Ab-\nhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Alex Vaughan,\net al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024. 2\n[9] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and\nMingwei Chang. Retrieval augmented language model pre-\ntraining. In International conference on machine learning ,\npages 3929–3938. PMLR, 2020. 3\n[10] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,\nOpher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav\nLevine, Kevin Leyton-Brown, et al. Mrkl systems: A mod-\nular, neuro-symbolic architecture that combines large lan-\nguage models, external knowledge sources and discrete rea-\nsoning. arXiv preprint arXiv:2205.00445 , 2022. 3\n[11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich\n10\n--- Page 11 ---\nK¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, et al.\nRetrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in neural information processing systems ,\n33:9459–9474, 2020. 3\n[12] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu\nLi, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li.\nApi-bank: A comprehensive benchmark for tool-augmented\nllms. arXiv preprint arXiv:2304.08244 , 2023. 8\n[13] Zhiling Luo, Xiaorong Shi, Xuanrui Lin, and Jinyang\nGao. Evaluation report on mcp servers. arXiv preprint\narXiv:2504.11094 , 2025. 3, 6, 8\n[14] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex\nChao. The landscape of emerging ai agent architectures\nfor reasoning, planning, and tool calling: A survey. arXiv\npreprint arXiv:2404.11584 , 2024. 2\n[15] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon\nKim, Woosang Lim, Kurt Keutzer, and Amir Gholami. Effi-\ncient and scalable estimation of tool representations in vector\nspace. arXiv preprint arXiv:2409.02141 , 2024. 2, 3, 8\n[16] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al. Webgpt:\nBrowser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332 , 2021. 3\n[17] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Han-\nnaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio\nRibeiro. Art: Automatic multi-step reasoning and tool-use\nfor large language models. arXiv preprint arXiv:2303.09014 ,\n2023. 3\n[18] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E\nGonzalez. Gorilla: Large language model connected with\nmassive apis. Advances in Neural Information Processing\nSystems , 37:126544–126565, 2024. 2, 3, 8\n[19] Yun Peng, Shuqing Li, Wenwei Gu, Yichen Li, Wenxuan\nWang, Cuiyun Gao, and Michael R Lyu. Revisiting, bench-\nmarking and exploring api recommendation: How far are\nwe? IEEE Transactions on Software Engineering , 49(4):\n1876–1897, 2022. 8\n[20] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,\nYaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,\net al. Toolllm: Facilitating large language models to master\n16000+ real-world apis. arXiv preprint arXiv:2307.16789 ,\n2023. 8\n[21] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning\nDing, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang,\nChaojun Xiao, et al. Tool learning with foundation models.\nACM Computing Surveys , 57(4):1–40, 2024. 2\n[22] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Ji-\nacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren,\nXun Jiang, et al. Alita: Generalist agent enabling scalable\nagentic reasoning with minimal predefinition and maximal\nself-evolution. arXiv preprint arXiv:2505.20286 , 2025. 10\n[23] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen.\nColt: Towards completeness-oriented tool retrieval for large\nlanguage models. arXiv e-prints , pages arXiv–2405, 2024. 3\n[24] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Jun-\nyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le\nYu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,\nRunji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang\nRen, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan\nQiu. Qwen2.5 technical report, 2025. 7\n[25] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Am-\nnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-\ncontext retrieval-augmented language models. Transactions\nof the Association for Computational Linguistics , 11:1316–\n1331, 2023. 3\n[26] Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Lan-\nguage models can teach themselves to use tools. Advances\nin Neural Information Processing Systems , 36:68539–68551,\n2023. 2, 3\n[27] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai\ntasks with chatgpt and its friends in hugging face. Advances\nin Neural Information Processing Systems , 36:38154–38180,\n2023. 3\n[28] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao\nLiang, Boxi Cao, and Le Sun. Toolalpaca: Generalized\ntool learning for language models with 3000 simulated cases.\narXiv preprint arXiv:2306.05301 , 2023. 8\n[29] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin\nLi, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang,\nJiale Liu, et al. Autogen: Enabling next-gen llm ap-\nplications via multi-agent conversation. arXiv preprint\narXiv:2308.08155 , 2023. 3\n[30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, et al. Qwen3 technical report. arXiv\npreprint arXiv:2505.09388 , 2025. 2\n[31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao. React: Synergizing rea-\nsoning and acting in language models. In International Con-\nference on Learning Representations (ICLR) , 2023. 3\n[32] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian\nLuan, and Bin Wang. Toolrerank: Adaptive and\nhierarchy-aware reranking for tool retrieval. arXiv preprint\narXiv:2403.06551 , 2024. 3\n11",
  "text_length": 51635
}