{
  "id": "http://arxiv.org/abs/2506.06279v1",
  "title": "CoMemo: LVLMs Need Image Context with Image Memory",
  "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
  "authors": [
    "Shi Liu",
    "Weijie Su",
    "Xizhou Zhu",
    "Wenhai Wang",
    "Jifeng Dai"
  ],
  "published": "2025-06-06T17:59:06Z",
  "updated": "2025-06-06T17:59:06Z",
  "categories": [
    "cs.CV"
  ],
  "pdf_url": "http://arxiv.org/pdf/2506.06279v1",
  "comments": "ICML 2025",
  "full_text": "--- Page 1 ---\narXiv:2506.06279v1  [cs.CV]  6 Jun 2025CoMemo: LVLMs Need Image Context with Image Memory\nShi Liu* 1Weijie Su*† \u00001Xizhou Zhu2 1Wenhai Wang3 1Jifeng Dai2 1\nAbstract\nRecent advancements in Large Vision-Language\nModels built upon Large Language Models have\nestablished aligning visual features with LLM\nrepresentations as the dominant paradigm. How-\never, inherited LLM architectural designs intro-\nduce suboptimal characteristics for multimodal\nprocessing. First, LVLMs exhibit a bimodal dis-\ntribution in attention allocation, leading to the\nprogressive neglect of middle visual content as\ncontext expands. Second, conventional posi-\ntional encoding schemes fail to preserve vital 2D\nstructural relationships when processing dynamic\nhigh-resolution images. To address these limi-\ntations, we propose CoMemo - a dual-path ar-\nchitecture that combines a Context image path\nwith an image Memo ry path for visual process-\ning, effectively alleviating visual information ne-\nglect. Additionally, we introduce RoPE-DHR,\na novel positional encoding mechanism that em-\nploys thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating\nremote decay in extended sequences. Evaluations\nacross seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and vi-\nsual question answering, demonstrate CoMemo’s\nsuperior performance compared to conventional\nLVLM architectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.\n1. Introduction\nRecent advances in large language models (LLMs)\nhave demonstrated unprecedented generative capabili-\nties (Ouyang et al., 2022; Touvron et al., 2023), primarily\ndriven by the exponential scaling of training data and model\n*Equal contribution†Project lead1Shanghai Artificial Intel-\nligence Laboratory2Tsinghua University3The Chinese Univer-\nsity of Hong Kong. Correspondence to: Weijie Su <suwei-\njie@pjlab.org.cn >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).parameters. Building upon this foundation, large vision-\nlanguage models (LVLMs) have emerged as powerful mul-\ntimodal systems that align visual representations with LLM\nembedding spaces to enable cross-modal reasoning (Liu\net al., 2024b; Alayrac et al., 2022). Current methodologies\nfor visual information injection predominantly follow two\narchitectural paradigms.\nThe first paradigm (referred to as LVLM-X, e.g.,\nFlamingo (Alayrac et al., 2022)) employs cross-attention\nmechanisms to integrate visual features into textual repre-\nsentations. While this approach offers flexible modality\ninteraction, recent studies (Lauren c ¸on et al., 2024) have re-\nvealed its suboptimal performance compared to alternative\napproaches when using identical LLM backbones and train-\ning data - a finding corroborated by our studies in Figure 1.\nThe second paradigm (referred to as LVLM-S, e.g.,\nLLaV A (Liu et al., 2024b)) aligns visual tokens into text\ntoken embeddings space and then performs autoregressive\nprocessing. This paradigm is more compatible with LLM\narchitectures; however, the preservation intrinsic mecha-\nnisms such as attention bimodality (Xiao et al., 2023; Liu\net al., 2025a) and the linearly increasing position encoding\nleads to critical limitations: (1) the “lost in the middle” (Liu\net al., 2024c; Song et al., 2024) phenomenon degrades per-\nformance with increasing context length, and (2) positional\nencoding sparsity induces remote decay and 2d-dimensional\nlost in high-resolution image processing.\nIn this paper, we propose a novel framework for LVLM,\nnamed CoMemo. Our key idea is to introduce an addi-\ntional image-processing mechanism that is unaffected by\ncontext, without modifying the internal mechanisms of the\nLLM. Specifically, we concatenate image tokens with text\ntokens as the input sequence for fully autoregressive pro-\ncessing, while simultaneously feeding the image tokens\ninto a mixin layer for cross-attention computation. Cross-\nattention retrieves image information based on text, avoiding\nthe issue of image neglect that can occur in causal self-\nattention. However, simply combining these two structures,\nas in NVLM-H (Dai et al., 2024), does not work well. To\naddress this, we first investigated a balanced scheme for\nvisual representation inputs. Then, we introduced a three-\nstage training technique to prevent overreliance on the cross-\nattention path, effectively transforming it into a “memory\n1\n--- Page 2 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nFigure 1: Evaluation results of\nthree architectures with same train-\ning data and model size (2B).\nPlease refer to Tables 2 to 4 for\ndetails.\nTokenizerInput SequenceLLMLayer 0Layer 1Layer n\nImg. Encoder…(a) LVLM-S.\nTokenizerInput SequenceMixin\nLLMLayer 0Layer n\nImg. Encoder……MixinLayer (b) LVLM-X.\nTokenizerInput SequenceMixin\nLLMLayer 0Layer n\nImg. Encoder.\nMixinLayer\n…\nMemory\nPath…\nContext Path (c) Ours.\nFigure 2: Comparison three types of architectures for LVLMs. Method (a) use image\nencoder to align visual features with the LLM’s continuous token representation space.\nMethod (b) employs mixin layer with cross-attention to update LLM’s hidden states\nbased on visual features. And Method (c) contrust a dual-path structure to enable the\nmodel to focus more on visual content during generation.\npath.” Meanwhile, the fully autoregressive process serves\nas the primary path for introducing image context, referred\nto as the “context path.”\nThe positional encoding scheme in LVLMs typically adopts\nRoPE from LLMs, treating each image patch token as an in-\ndividual token for encoding. However, this approach results\nin highly sparse positional encodings for dynamic high-\nresolution image patch tokens. Such sparse encoding can\nlead to remote decay issues in positional encoding, and the\none-dimensional incremental encoding scheme also loses\nthe two-dimensional information of the image. To address\nthis, we propose a novel positional encoding scheme based\non the dynamic high-resolution method. Specifically, in\nthe dynamic high-resolution approach, the image is divided\ninto multiple image tiles and a single image thumbnail. We\ntreat the image thumbnail as part of the input sequence for\nstandard sequential positional encoding, while mapping the\nimage tiles to the image thumbnail index based on their\ntwo-dimensional positional relationships.\nTo fully evaluate the impact of the CoMemo architecture, we\ncollected a set of multimodal benchmarks and categorized\nthem into seven evaluation tasks: Caption, Long-generation,\nMulti-image, Long-context, Math, General VQA, and OCR-\nrelated tasks. The results demonstrates CoMemo’s superior-\nity over LVLM-X/S baselines under same data and model\nsettings. Our framework achieves 17.2%, 7.0% and 5.6% rel-\native improvement on Caption, Long-Generation and Long-\nContext tasks respectively, with consistent gains across vari-\nous benchmarks.\n2. Design Thinking for CoMemo\n2.1. Why LVLMs Tend to “lose in the middle”?\nPrevious studies have shown that both LLMs and LVLMs\nexhibit the “Lost in the middle” phenomenon (Liu et al.,\nFigure 3: Average gradients and attention weights assigned\nto tokens at corresponding positions. We computed the\naverage over 1,000 samples.\n2024c; Song et al., 2024), as illustrated in Figure 8a. This\nrefers to their struggle to capture key information placed in\nthe middle of the input. However, this phenomenon arises\nfrom the causal self-attention and in-context mechanisms in-\nherent in these models. Both LLMs and LVLMs are trained\nusing the next-token prediction paradigm, which heavily\nrelies on contextual tokens during prediction.\nAs shown in Figure 3, we plot the gradients of each input\ntoken during training and the attention weights assigned to\neach token during inference for the InternVL2 model. To\nhandle sequences of varying lengths, we map each sequence\ninto 100 bins based on the position of each token. A signifi-\ncant portion of the gradient for the current predicted token\nis backpropagated to nearby tokens. As a result, models\ntrained in this way tend to allocate most of their attention\nto adjacent tokens during inference, while the initial token,\nwhich has a larger gradient and attention, acts as an attention\nsink to release redundant attention (Xiao et al., 2023). This\nresults in the effective capture of key information at the be-\n2\n--- Page 3 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nFigure 4: Remote decay estimation for InternVL2-2B. The\nrelative distance refers to the difference between absolute\nposition IDs. In RoPE, the position ID of each input token\nincrements by 1 with the input sequence.\nginning of the sequence, as well as nearby tokens benefiting\nfrom contextual attention.\nHowever, key information placed in the middle of the se-\nquence is more likely to be lost. As the context length\nincreases, the middle segment, which is at a higher risk of\nbeing lost, also extends.\nFinding 1: The attention distribution mechanism\nin causal self-attention is the cause of the “Lost in\nthe middle” phenomenon. This mechanism also\nlimits the performance of LVLMs in long-context\nscenarios.\n2.2. Remote Decay in LVLMs with DHR\nDynamic high resolution significantly enhances the per-\nformance of visual tasks by introducing more visual con-\ntext, especially in tasks that require high resolution, such as\nOCR-related evaluations. However, this benefit introduces\na critical trade-off: excessive visual context exacerbates the\nremote decay issue in Rotary Position Embedding (RoPE),\nultimately limiting model effectiveness in long-context sce-\nnarios.\nRoPE implements relative position encoding through abso-\nlute encoding mechanisms, formally expressed as:\n(Rmq)⊤(Rnk) =Re\nd/2−1X\ni=0q[2i:2i+1]k∗\n[2i:2i+1]ei(m−n)θi\n,\n(1)\nwhere m, n denote token positions, dthe embedding di-\nmension, and θifollows the sinusoidal position encoding\nscheme. This formulation inherently inherits the remote\ndecay characteristics of sinusoidal encodings.\nRoPE exhibits the remote decay property where the relative\nsize between tokens decreases as the relative distance in-\nFigure 5: Balancing experiments. Experiment settings are\ndescribed in Section 2.3. “1k”, “2k” and “4k” means pre-\ntrain steps. All scores are evaluated after fine-tuning the\npretrained checkpoint corresponding to the x-axis.\ncreases (Su, 2021), as shown in Figure 4. While standard\nInternVL2 processes 256 image tokens per image, activating\nDHR with a dynamic number of 6 expands this to 1,792\ntokens—a 7 ×increase that further reduces the influence of\nimage tokens during generation.\nFinding 2: The context expansion from DHR fun-\ndamentally aggravates image neglect.\n2.3. The Balance Between tow Pathways\nCoMemo faces a critical challenge in balancing two visual\nprocessing pathways: the cross-attention path and the fully\nautoregressive path. Through systematic experiments with\ndifferent high-resolution allocation strategies and training\nstrategies, we identify two key balancing principles. In\nthe mixin layers, we incorporate a gating mechanism that\nutilizes gate values to reflect the influence of the cross-\nattention path during the decoding process. Therefore, we\nchoose to evaluate the dependency of LVLM on these two\npaths from this perspective. Our analysis averages the gates\nvalues of mixin layers to quantify pathway preference, with\nperformance evaluated across captioning, general VQA, and\nOCR tasks.\nBalance in DHR Allocation. We compare three distinct\nDHR allocation strategies: (1) allocating DHR information\nexclusively to the fully autoregressive path, termed DHR-S,\n(2) assigning it solely to the cross-attention path, termed\nDHR-X, which corresponds to the allocation mechanism\nof NVLM-H (Dai et al., 2024), and (3) distributing it to\nboth pathways, termed DHR-B. In unilateral allocation sce-\nnarios, the counterpart pathway receives only thumbnail\nresolution information. As shown in Figure 5, DHR alloca-\ntion significantly influences pathway specialization. When\nDHR is exclusively allocated to one pathway, the model\nshows a pronounced bias toward that pathway. In contrast,\ndual-pathway allocation results in more stable and balanced\n3\n--- Page 4 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nmodel behavior.\nBalance in training steps. Our findings reveal that pretrain-\ning steps profoundly affects the equilibrium between the\ntwo mechanisms. Although fine-tuning involves extended\ntraining (9,000 steps), the visual processing paradigm is\nlargely established during pretraining. In Figure 5, the\nDHR-B configuration demonstrates that insufficient pre-\ntraining steps result in suboptimal alignment, leading to\ncompromised fine-tuning performance. Conversely, exces-\nsive pretraining induces over-reliance on specific pathways.\nWhile fine-tuning stage attempts to mitigate this imbalance,\nit ultimately fails to fully rectify the entrenched dependency\nin “DHR-B-4k” in Figure 5.\nThis phenomenon arises because during pretraining, only\nthe memory branch and projector parameters are trainable.\nThe projector’s limited function of mapping image repre-\nsentations into the text space provides minimal gains in\nvisual comprehension. Consequently, prolonged pretraining\nnaturally reinforces reliance on the cross-attention branch.\nFinding 3: CoMemo exhibits a fundamental bal-\nancing challenge between dual visual processing\npathways.\nFinding 4: CoMemo’s dual-path visual process-\ning paradigm establishes during pretraining.\n3. Method\n3.1. RoPE-DHR\nStandard RoPE implementations in LLMs employ a con-\ntinuous incremental positional encoding scheme, which is\ninherited by LVLMs. While effective for sequential text\ndata, this approach faces significant limitations when pro-\ncessing high-resolution visual inputs: (1) remote decay due\nto extended context lengths, and (2) dimensional collapse\nof 2D spatial relationships. To address these limitations, we\npropose RoPE-DHR, a novel position encoding method that\nemploys a hierarchical strategy.\nWe first processes the thumbnail patch tokens using con-\nventional RoPE to generate base position IDs. For high-\nresolution tiles, we establish geometric correspondence by\nmapping each tile patch’s coordinates (xtile, ytile)to its cor-\nresponding thumbnail patch index ithumb . This mapping is\ndefined as:\nithumb = (⌊xtile×Wtile\nWorig⌋+wbtile,⌊ytile×Horig\nHthumb⌋+hbtile)\n(2)\nwhere (Worig, Horig)and(Wtile, Htile)denote the original\nand tile dimensions respectively. The terms wbtileandhbtilerepresents the start position biases for the tile in the width\nand height dimensions.This mapping preserves 2D spatial\nrelationships while maintaining compatibility with existing\nRoPE implementations. In Figure 6, we visualize the raw\nimage,the thumnail and their corresponding patch token\nposition IDs using a color bar.\nCrucially, our method decouples positional encoding from\nabsolute sequence positions through: (1) length reduction:\nprevent the sparse position encoding for DHR by compres-\nsion the position length in global perspective. (2) geometry\nreserve: tile patches inherit positional context from their\nthumbnail anchors.\n3.2. Architecture of CoMemo\nWhile LLaV A-like architectures demonstrate effective\nvisual-language alignment, they exhibit a tendency to disre-\ngard visual information when processing lengthy contextual\ninputs or generating extended responses. To address this\nlimitation, we propose the CoMemo architecture, which is\nbased on three key structures:\nDual-stream Structure. CoMemo maintains tow visual\nprocessing streams: (1) context path serves as the primary\nprocessing stream where image representations are treated\nas special tokens, concatenated with text tokens to form the\ninput sequence for autoregressive modeling. (2) memory\npath establishes an auxiliary processing stream where im-\nage representations interact with the input sequence through\ncross-attention mechanisms. The two paths maintain iden-\ntical image representations, ensuring feature consistency\nwhile enabling complementary processing and the balance\nbetween in tow path as we disccussed in Section 2.3.\nPosition-aware Cross-attention. Existing LVLM-X typi-\ncally employ absolute positional encoding for image patch\ntokens during encoding (Dubey et al., 2024), with some\nvariants introducing additional positional semantics through\nspecialized tokens (Dai et al., 2024). However, these ap-\nproaches provide only unidirectional positional awareness.\nTo address this limitation, we implement RoPE in cross-\nmodal attention, establishing bidirectional positional aware-\nness: query positions ( poss) correspond to input sequence\ntoken ordering and key positions ( posi) align with visual\ntoken indices in the input sequence. The attention mask em-\nploys bidirectional visibility constraints, similar to mPlug-\nowl3 (Ye et al., 2024), where image tokens are visible to\ntheir corresponding sequence positions while maintaining\nbidirectional attention.\nMemory Mixin Strategy As shown in Figure 7, CoMemo\nmixin layers are interleaved with standard transformer\nblocks at a 1:4 ratio. Each memory layer performs: (1)\nGated Cross-attention: Modulates visual influence through\nlearnable attention gates ( attn gate). (2) Adaptive Feed-\n4\n--- Page 5 ---\nCoMemo: LVLMs Need Image Context with Image Memory\n(1, 1)(2, 1)(1, 2)(2, 2)thumnail\n(1, 1)(2, 1)(1, 2)(2, 2)thumnail\nFigure 6: The computation\nprocess of Rope-DHR. The\ncolors are assigned based on\na mapping of position IDs in\nRoPE.\nInput SequenceMixinLLMLayer 0EncoderDynamic High ResolutionPixel ShuffleProjectorImage Tokens\nUser MessageTokenizerMixin LayerLayer n....\nFigure 7: Framework of CoMemo. Both paths share\nthe same encoder and projector. The pixel shuffle\ntechnique is adopted from InternVL series (Team,\n2024; Chen et al., 2024c).Algorithm 1 Mixin Layers\nRequire: hs(sequence hidden\nstates),\nhi(image hidden states),\nattn gate , ffw gate ,\nposs(sequence position IDs),\nposi(image position IDs)\nEnsure: Updated hs\n1:hs←hs+ tanh( attn gate)⊙\ncross attn(q=hs, kv=\nhi, pos s=poss, pos i=posi)\n2:hs←\nhs+ tanh( ffwgate)⊙ffw(hs)\n3:Return: hs\nforward: Enhances feature transformation via gated nonlin-\nearity (ffw gate).\nDuring autoregressive decoding, CoMemo requires only\na single-step computation between the current decoding\ntoken and the cached visual memory states, eliminating\nthe need for key-value caches. This approach circumvents\nthe issue of increasing key-value cache size as the sequence\nlengthens. The orthogonal design of the architecture ensures\ncompatibility with existing LLaV A variants.\n3.3. Training Stages\nTraditional LVLMs training consists of two phases: pre-\ntraining and fine-tuning. During the pretraining phase, we\nselectively update the projector module and memory archi-\ntecture parameters while keeping other components frozen.\nThis phase prioritizes cross-modal representation alignment\nand dynamic equilibrium between dual visual processing\npathways.\nA critical challenge emerges during pretraining stemming\nfrom asymmetric parameter updates: (1) Insufficient train-\ning iterations lead to suboptimal projector learning; (2)\nProlonged training induces excessive dependency on the\nmemory pathway for LVLM decoding. This imbalance orig-\ninates from the disparate update strategies - full fine-tuning\nof memory parameters versus partial tuning of the context\nprojector. Consequently, the LVLM exhibits inherent bias\ntowards memory-based information retrieval to achieve loss\nminimization.\nTo mitigate this optimization bias, we propose a three-stage\ntraining strategy. In the first stage, we tune the parametersof projector and mixin layers. In the next stage, we freeze\nthe gate parameters. This design aims to enable the model\nto learn representation alignment and balance the dual-path\nstructure in the first stage. After a certain number of training\nsteps to prevent over-reliance on the cross-attention path,\nwe freeze the corresponding gate control parameters and\ncontinue training until the alignment structure is sufficiently\nlearned.\nThe subsequent fine-tuning stage adopts full-parameter train-\ning paradigm. During this stage, all model parameters be-\ncome trainable with the training objective shifted towards\ninstruction-following.\n4. Experiments\n4.1. Setup\nTo ensure a fair comparison of the capabilities across various\narchitectures, all three of our models utilize the same pre-\ntraining and fine-tuning datasets. Each architecture adopts\nInternLM-1.8B as the LLM backbone and InternViT-300M\nas the image encoder. The hidden dimensions of each ar-\nchitecture’s network, along with hyperparameters such as\nlearning rate and weight decay during training, are also kept\nconsistent. We use the same training data as InternVL-2.\nTherefore, LVLM-S in our experiments essentially repre-\nsents InternVL-2. Training for all architectures is divided\ninto two phases: pretraining and fine-tuning. For specific\nsettings, please refer to the appendix.\n5\n--- Page 6 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nTable 1: Comparison with other LVLMs of different architectures.*indicates results obtained from our own experiments.\nOther results are sourced from official reports or third-party evaluation leaderboards.\nModel Params.Caption Long-Generation Multi-Image Long-Context Math General VQA OCR-RelatedCOCO\nFlickr\nNo-Caps\nLLaV ABen.\nMMDU\nBLINK\nMantis\nMMT\nMM-NIAH\nMileBench\nMathVista\nMathVision\nMMBench\nMME\nMMVP\nAI2D\nChartQA\nTextVQA\nClosed-Source\nGPT-4o - - - - - 70.2 68.0 - 65.4 - 60.3 63.8 - - - - 84.6 85.7 77.4\nGPT-4V - - - - - - 54.6 62.7 64.3 - 53.0 58.1 - - 1926.6 38.7 78.2 78.5 78.0\nLVLM-X\nmPlug-owl3 8B 90*72*94.2*64.2*- 50.3 63.1 - - 50.0*- - - - - 73.4 - -\nLLama3.2-V 11B - - - 80.9 - 39.8 - 53.1 - - 51.5 - 77.3 1820.5 - 91.1 83.4 73.1\nLVLM-S\nMiniCPM-V-2 2.8B - - - 66.1 - 41.2 - 54.5 - - 39 15.4 62.9 1808.2 - 64.8 59.6 -\nInternVL-2 2B 79.1*65.4*60.0*62.9*28.7*38.2*48.3*50.2*27.0*52.1*48.0*16.4*73.1*1869*31.3*74.3*75.6*74.2*\nInternVL-2.5 2B 114*80.2*108.6*- 33.6*44.0 54.8 54.5 22.2*50.1*51.3 13.5 74.7 2138.2 40 74.9 79.2 74.3\nQwen2-VL 2B 108.2 86.3 - 52.5 - 44.4 - 55.1 - - 43.0 19.7 - 2326.8 - 74.7 73.5 79.7\nCoMemo 2B 98.6 78.5 78.8 66.9 38.7 43.5 50.6 51.3 34.2 54.6 50 17.0 74.2 1904 36.0 74.2 73.6 72.6\nTable 2: The results on Generation and Math benchmarks.\nThe test settings are described in Section 4.3. The highest\nscores are highlighted in bold .\nModelCaption Long Generation Math\nCOCO Flickr No-Caps LLaV ABench MMDU MathVista MathVision\nLVLM-X 84.9 68.9 62.9 50.8 31.5 44.2 15.8\nLVLM-S 79.1 65.4 60.0 62.9 28.7 48.0 16.4\nOurs 98.6 78.5 78.8 66.9 38.7 50.0 17.0\n4.2. Comparison with Other LVLMs\nAs shown in Table 1, we benchmark CoMemo against lead-\ning open-source and proprietary LVLMs. It is important\nto note that our primary objective is architectural ablation\nrather than absolute performance maximization. The com-\nparison with other models serves to demonstrate that our\nmodel was trained on a large-scale dataset and achieves top-\ntier performance, rather than being tested on toy datasets.\n4.3. Generation and Math Benchmark\nContemporary multimodal benchmarks primarily rely on\nconstrained response formats (e.g., multiple-choice ques-\ntions and Yes/No queries) to facilitate evaluation. However,\nsuch approaches inadequately assess open-ended reasoning\ncapabilities, as free-form responses introduce higher diver-\nsity and complexity. To comprehensively evaluate model\ncapabilities across different granularities, we collected sev-\neral benchmarks for open-ended evaluation:\n•Caption Generation: Evaluates the model’s abil-\nity to generate concise image descriptions (10-15\nwords) using CIDEr scores on COCO (Lin et al.,\n2014a), Flickr30k (Plummer et al., 2015), and No-\nCaps (Agrawal et al., 2019) datasets.\n• Long Generation: Evaluates extended inference using\nLLaV ABench (Liu et al., 2024b) and MMDU (Liu\net al., 2024e). LLaV ABench focuses on single-image\ncontext reasoning, involving a few hundred context\ntokens, while MMDU is a multi-image analysis withTable 3: The results on Multi-image and Long-context\nbenchmarks. The test settings are described in Section 4.4.\nThe highest scores are highlighted in bold .\nModelMulti-Image Long-Context\nBLINK Mantis MMT MM-NIAH-M MM-NIAH-T MileBench\nLVLM-X 41.5 46.5 47.8 39.5129.5153.21\nLVLM-S 38.2 48.3 50.2 26.7 27.3 52.1\nOurs 43.5 50.6 51.3 33.7 34.6 54.6\nlengthy textual context (avg. 6.4k tokens) and multiple\nimages (ranging from 2 to 20 images).\n•Math: Measures model reasoning ability on math\ndiagrams and visual math problems using MathVi-\nsion (Wang et al., 2024a) and MathVista (Lu et al.,\n2023). During evaluation, we asked the model to pro-\nvide step-by-step reasoning and extracted the final an-\nswer to calculate accuracy.\nAs shown in 2, our analysis reveals three key findings. First,\nLVLM-X’s continuous attention mechanism demonstrates\nsuperiority in pure visual captioning tasks, achieving a +4%\naverage improvement. Second, LVLM-S’s causal atten-\ntion architecture achieves better performance in knowledge-\nintensive scenarios, demonstrating enhanced contextual rea-\nsoning capabilities from its LLM backbone. Our proposed\nCoMemo combines the advantages of both approaches, out-\nperforming the original architectures across various tasks.\nThis supports our hypothesis that dual-path attention alloca-\ntion effectively integrates the benefits of both architectures:\nmaintaining visual grounding while enabling complex rea-\nsoning.\n4.4. Multi-image and Long-context Benchmark\nTo systematically evaluate multimodal long-context under-\nstanding, we establish two complementary evaluation di-\nmensions:\n1LVLM-X’s single image token compression reduces average\ncontext length by 50% (e.g., 32k →16k).\n6\n--- Page 7 ---\nCoMemo: LVLMs Need Image Context with Image Memory\n•Multi-image: When DHR is enabled, each image con-\ntributes approximately 2k tokens to the context length,\nthereby necessitating extended context capacity for\nmulti-image analysis. We evaluate performance on the\nBLINK (Fu et al., 2025), Mantis (Jiang et al., 2024),\nand MMT (Ying et al., 2024) datasets.\n•Long-context: Tests information extraction in long\ncontext scenarios. We select MM-NIAH (Wang\net al., 2024c) evaluation that detects image/text needles\nwithin hybrid long contexts. And MileBench (Song\net al., 2024) progressively challenging tasks with 2-\n109 images. These benchmarks systematically quantify\nlong-context capabilities from both textual token and\nvisual token perspectives.\nAs shown in Table 3, our proposed architecture achieves\nthe best performance in scenarios involving multiple images\nand long contexts. Specifically, MM-Niah-T represents the\nneedle that is the key information placed in the text, while\nMM-NIAH-M represents the needle placed in the image.\nIn the evaluation of MM-NIAH-T, the memory structure\nstores image data that is unrelated to the needle, redundant\ninformation. Nevertheless, our model still achieves the best\nperformance. This not only demonstrates that compress-\ning the image token position space through RoPE-DHR\nenhances the model’s ability to understand long sequence\ntexts but also indicates that the Memory structure does not\ncause the model to overfocus on image information, thereby\npreserving its ability to retrieve and reason with language\neffectively.\nIn the MileBench evaluation, due to the potential for ex-\ncessive image tokens leading to long context sequences\nand out-of-memory issues, we did not enable DHR set-\ntings. Therefore, in this evaluation, each image in the input\nsequence has only a single image thumbnail. In this sce-\nnario, our architecture’s positional encoding is the same\nas LVLM-S, primarily reflecting the role of the memory\nstructure. Despite this, our architecture still achieved a 2.5%\nimprovement over LVLM-S.\nThe MileBench benchmark also includes needle in haystack\ntasks for both text and image scenarios. In Figure 8, we\nvisualize the average results for these two types of NIAH\ntasks in MileBench. As mentioned earlier, we observed that\nthe attention mechanism of LLMs and LVLMs exhibits a\nbimodal distribution, where LVLMs tend to focus more on\nthe beginning and the most recent tokens, leading to the\n“Lost in the middle” phenomenon. This means that when the\nneedle is placed in the middle of the sequence, the model’s\nperformance on NIAH tasks deteriorates. Our architecture,\nhowever, addresses this issue by continuously focusing on\nthe “middle image information” during the token generation\nprocess, effectively mitigating this problem.\n(a) LVLM-S.\n (b) CoMemo\nFigure 8: Heatmap of results for the NIAH evaluation on\nMileBench benchmark. The depth percentage indicates the\nposition of the target information (needle) relative to the\nentire sequence.\nTable 4: The results on General VQA and OCR-related\nbenchmarks. The test settings are described in Section 4.5.\nThe highest scores are highlighted in bold .\nModelGeneral VQA OCR-Related\nMMBench MME MMVP AI2D ChartQA TextVQA\nLVLM-X 69.6 1812 27.3 69.9 69.4 68.8\nLVLM-S 73.1 1869 31.3 74.3 75.6 74.2\nOurs 74.2 1904 36.0 74.2 73.6 72.6\n4.5. General VQA and OCR Benchmark\nTo comprehensively evaluate CoMemo’s multimodal un-\nderstanding capabilities, we conduct extensive experiments\non conventional vision-language benchmarks covering two\nmain categories:\n•General VQA: Assessing visual perception and reason-\ning abilities through single-image question answering,\nincluding MMBench (Liu et al., 2025b), MME (Fu\net al., 2023), and MMVP (Zhong et al., 2023).\n•OCR-Related: Requiring fine-grained information\nextraction from diagrams and charts, evaluated on\nAI2D (Hiippala et al., 2021), TextVQA (Singh et al.,\n2019b), and ChartQA (Masry et al., 2022b).\nThe input sequences in these two types of benchmarks are\nrelatively short, and responses typically consisting of single\nwords or short phrases. In contrast, CoMemo incorporates\na memory structure that alleviates image neglect by intro-\nducing an additional image focus mechanism. Additionally,\nRoPE-DHR addresses image neglect by compressing im-\nage information to reduce the long-range decay caused by\npositional encoding. While these techniques are not specif-\nically tailored for the benchmarks mentioned above, our\narchitecture still performs competitively when compared to\n7\n--- Page 8 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nLVLM-S.\nAs shown in Table 4, our architecture performs slightly\nbetter than LVLM-S on some general multimodal bench-\nmarks. However, in tasks such as text and OCR, which\nrequire high-resolution image information, traditional ap-\nproaches typically rely on more granular image representa-\ntions. This approach contrasts with the philosophy underly-\ning our approach, which focuses on improving the model’s\nlong-context and long-generation capabilities.\n4.6. Training and Inference Efficiency Comparison\nTable 5: Training efficiency comparison across different 2B\nparameter models.\nModel Batch size # of A100 GPUs Train steps/s Train samples/s\nLVLM-X 1024 64 0.123 15.71\nLVLM-S 1024 64 0.105 13.4\nCoMemo 1024 64 0.096 12.26\nTable 6: Inference speed comparison across different 2B\nparameter models (lower is better).\nModel Batch size # of A100 GPUsCaption COCO\n(sec)MMBench\n(sec)MMNIAH\n(min)TextVQA\n(sec)\nLVLM-X 1 8 260 76 22 88\nLVLM-S 1 8 270 90 25 100\nCoMemo 1 8 280 110 30 120\nTo comprehensively compare the training and inference ef-\nficiency of three architectures, we measured their sample\nthroughput during training and inference times across mul-\ntiple benchmarks. As shown in Table 5 and Table 6, we\nreport the training and inference efficiency for each archi-\ntecture. The results demonstrate that CoMemo achieves\nlatency comparable to LVLM-S. Although LVLM-X ex-\nhibits higher efficiency due to the use of fewer image tokens,\nits performance is significantly inferior to both CoMemo\nand LVLM-S.\n4.7. Ablation Study\nAblation on Components of CoMemo. We conducted a\ncomplete ablation study on components using a 2B-scale\nmodel, as shown in Variants 1 to 5 in Table 7. When\nonly RoPE-DHR is introduced, the compressed position\nencoding significantly improves performance on both long-\ngeneration and long-context tasks (Variant 2). When only\nthe memory path was introduced, it addressed issues related\nto neglecting image information, leading to some improve-\nment in the model’s performance (Variant 3). However,\nsince the image tokens in the memory path lack positional\ninformation, during cross-attention computation, there is no\npositional correspondence between image tokens and text\ntokens. Moreover, the lack of distinguishing features be-\ntween image tiles, thumnails, and multiple images hindered\nthe model’s capabilities in different dimensions. Therefore,after incorporating RoPE-DHR, the model’s capabilities\nin different dimensions were further enhanced (Variant 5).\nHowever, since RoPE-DHR is essentially a compression-\noriented encoding, it may affect scenarios like OCR that\nrequire fine-grained information.\nAblation on compression ratio of RoPE-DHR. In the\nmain experiments, RoPE-DHR uses shared position IDs\nfor image tokens in the thumbnail and their corresponding\nsubimage tokens. This approach effectively compresses the\nposition encoding. Therefore, we propose a variant, RoPE-\nDHR without compression, where the position encodings\nfor subimage tokens corresponding to a two-dimensional\nposition increment by 1 relative to the thumbnail image\ntokens, while the position IDs between thumbnail image\ntokens increment based on the number of tokens at their\ncorresponding positions, rather than by 1. The experimental\nresults are shown in Table 7 in Variants 4. It can be ob-\nserved that, without compression, the model outperforms\nthe LVLM-S architecture across all dimensions.\nAblation on different scale models. To verify that\nCoMemo adheres to the scaling law, we select InternLM-\n7B as the language model for experiments at the 7B scale.\nAs shown in Table 7 in Variant 6 and 7, at the 8B scale,\nCoMemo’s average performance across all dimensions re-\nmains superior to the LVLM-S architecture. The CoMemo\narchitecture continues to deliver outstanding performance\nin both Cation and Long-context tasks. As language mod-\nels scale up, the impact of compressed position encoding\nbecomes more pronounced in OCR tasks.\nConsistency on Different Datasets. We also con-\nducted dataset-switching experiments using the open-source\nInternVL-1.2 1.2M fine-tuning dataset (OpenGVLab, 2024)\nduring the SFT stage, while keeping the pretraining data\nconsistent. As shown in Variants 8 to 9 in Table 7, even with\nthe changes in the dataset, our CoMemo consistently outper-\nforms the LVLM-S architecture across various dimensions.\n5. Related Works\n5.1. Mainstream LVLMs and Their Architectures\nContemporary LVLMs typically employ pre-trained lan-\nguage models as decoders, utilizing two dominant strategies\nfor visual-text alignment: (1) cross-attention mechanisms\nand (2) joint projector-autoregressive architectures.\nFully Autoregressive LVLMs: LLaV A (Liu et al., 2024b)\npioneered this approach by projecting image representa-\ntions into the LLM’s space and jointly decoding them with\ntext tokens. Subsequent models like VILA-v1.5 (Lin et al.,\n2024) and LLaV A-Next (Liu et al., 2024a) built on this\narchitecture, with LLaV A-Next introducing dynamic high-\nresolution techniques for improved performance. Other ad-\n8\n--- Page 9 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nTable 7: Ablation Study. The test settings are described in Section 4.7. “NC” means to use RoPE-DHR without compression.\nName Params. Memory RoPE-DHR Datasets Overall Caption Long-Generation Multi-Image Long-Context Math General VQA OCR-Related\nVariant 1 2B × × Ours 55.4 68.1 45.8 45.5 39.5 32.2 65.9 74.7\nVariant 2 2B × ✓ Ours 56.7 67.2 51.4 47.5 42.0 31.7 69.2 72.9\nVariant 3 2B ✓ × Ours 59.0 82.8 49.6 46.0 43.0 32.2 66.7 75.6\nVariant 4 2B ✓ NC Ours 59.5 79.7 51.9 48.5 43.1 34.7 67.3 74.8\nVariant 5 2B ✓ ✓ Ours 60.5 85.3 52.8 48.5 44.4 33.5 68.4 73.4\nVariant 6 8B × × Ours 65.4 77.7 61.0 57.4 45.3 38.7 78.3 82.3\nVariant 7 8B ✓ ✓ Ours 68.0 92.1 61.4 57.7 50.8 38.9 78.1 79.1\nVariant 8 2B × × 1.2M 51.1 79.7 36.1 44.1 28.7 28.8 58.5 61.8\nVariant 9 2B ✓ ✓ 1.2M 54.6 89.5 38.4 46.2 31.5 28.7 62.3 63.5\nvancements include Qwen2-VL (Wang et al., 2024b), which\nintroduced M-RoPE, and InternVL-2.5 (Chen et al., 2024c),\nwhich used pixel shuffle to reduce token count after DHR.\nDeepSeek-VL2 (Wu et al., 2024) employed a pretrained\nMoE-based LLM. However, this alignment approach inher-\nits the LLM’s generation mechanism, which can lead to\nissues such as “image neglect” or the “lost in the middle”\nproblem.\nCross-Attention-Based LVLMs: Flamingo (Alayrac et al.,\n2022) is an early example of LVLMs using cross-attention\nmechanisms. Later models, like Idefics (Lauren c ¸on et al.,\n2024) and LLaMa-3.2-Vision (Dubey et al., 2024), adopted\nits mixin layer design, which introduces cross-attention and\ngating mechanisms. EVLM (Chen et al., 2024b) experi-\nmented with using intermediate Vision Transformer repre-\nsentations as inputs to the mixin layer. mPlug-owl3 (Ye\net al., 2024) added adaptive gating and hyper-layer fusion to\ncombine cross-attention and self-attention. This approach\nenables visual understanding while maintaining the LLM’s\nfrozen language ability, as seen in LLaMa-3.2-Vision. How-\never, in these models, image representations are aligned\ndirectly to the LLM’s hidden state, whereas LLaV A-like\nmethods align them with the text token space, better lever-\naging autoregressive decoding capabilities and improving\nperformance.\n5.2. Position Encoding Schemes in LVLMs\nMost LVLMs use position encoding methods inherited from\nLLMs, primarily RoPE. In these models, each image patch\ntoken is treated like a text token and assigned position IDs\nfor RoPE computation. However, several advancements\naddress specific challenges. MiniCPM-V (Yao et al., 2024)\nintroduced an absolute position encoding for each image\ntile in the context of DHR, while LLaMA-3.2-V (Yao et al.,\n2024) designed encodings for both image tiles and patch\ntokens. NVLM (Yao et al., 2024), also leveraging DHR,\nadded special tokens before each tile to convey positional\ninformation. While effective for predefined DHR ratios,\nthese methods lack scalability.\nIn contrast, Qwen-VL2 (Wang et al., 2024b) introduced\nM-RoPE, a multi-dimensional position encoding extending\nRoPE to three channels (temporal, height, width) for im-\nages and videos. However, this position encoding requiresa customized ViT and thus cannot be applied to LVLMs\nemploying DHR.\nOur proposed RoPE-DHR, based on 1D principles, offers a\n2D-aware encoding scheme that addresses these challenges\nwithout the extra computational burden.\n6. Conclusion\nWe present CoMemo, a novel architecture for Large Vision-\nLanguage Models specifically designed for long-form gen-\neration and extended context understanding. Our approach\nfeatures a dual-path image processing mechanism and in-\ntroduces RoPE-DHR to alleviate remote decay in DHR sce-\nnarios while restoring critical two-dimensional spatial infor-\nmation. These innovations significantly enhance model per-\nformance across multiple tasks including image captioning,\nlong-form generation, long-context understanding, multi-\nimage analysis, and general visual question answering. We\nhope this work will contribute to the advancement of the\nvision-language modeling community.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Large Vision-Language Model. There are many potential\nsocietal consequences of our work, none which we feel must\nbe specifically highlighted here.\nAcknowledgments\nThe work is supported by the National Key R&D Program\nof China (NO. 2022ZD0161301), by the National Natu-\nral Science Foundation of China (U24A20325, 62321005,\n62376134).\nReferences\nAgrawal, H., Desai, K., Wang, Y ., Chen, X., Jain, R., John-\nson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P.\nNocaps: Novel object captioning at scale. In Proceedings\nof the IEEE/CVF international conference on computer\nvision , pp. 8948–8957, 2019.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\n9\n--- Page 10 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nM., et al. Flamingo: a visual language model for few-\nshot learning. Advances in neural information processing\nsystems , 35:23716–23736, 2022.\nAmini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi,\nY ., and Hajishirzi, H. Mathqa: Towards interpretable math\nword problem solving with operation-based formalisms.\narXiv preprint arXiv:1905.13319 , 2019.\nBen Abacha, A., Hasan, S. A., Datla, V . V ., Demner-\nFushman, D., and M ¨uller, H. Vqa-med: Overview of\nthe medical visual question answering task at imageclef\n2019. In Proceedings of CLEF (Conference and Labs of\nthe Evaluation Forum) 2019 Working Notes . 9-12 Septem-\nber 2019, 2019.\nBiten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M.,\nValveny, E., Jawahar, C., and Karatzas, D. Scene text\nvisual question answering. pp. 4291–4301, 2019.\nByeon, M., Park, B., Kim, H., Lee, S., Baek,\nW., and Kim, S. Coyo-700m: Image-text pair\ndataset. https://github.com/kakaobrain/\ncoyo-dataset , 2022.\nCao, J. and Xiao, J. An augmented benchmark dataset for\ngeometric question answering through dual parallel text\nencoding. In COLING , pp. 1511–1520, 2022.\nCarter, J. Textocr-gpt4v. https://huggingface.\nco/datasets/jimmycarter/textocr-gpt4v ,\n2024.\nChang, S., Palzer, D., Li, J., Fosler-Lussier, E., and Xiao, N.\nMapqa: A dataset for question answering on choropleth\nmaps. arXiv preprint arXiv:2211.08545 , 2022.\nChen, G. H., Chen, S., Zhang, R., Chen, J., Wu, X., Zhang,\nZ., Chen, Z., Li, J., Wan, X., and Wang, B. Allava: Har-\nnessing gpt4v-synthesized data for a lite vision-language\nmodel. arXiv preprint arXiv:2402.11684 , 2024a.\nChen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., and Liang,\nX. Unigeo: Unifying geometry logical reasoning via\nreformulating mathematical expression. arXiv preprint\narXiv:2212.02746 , 2022.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm’s referential\ndialogue magic. arXiv preprint arXiv:2306.15195 , 2023.\nChen, K., Shen, D., Zhong, H., Zhong, H., Xia, K., Xu, D.,\nYuan, W., Hu, Y ., Wen, B., Zhang, T., et al. Evlm: An\nefficient vision-language model for visual understanding.\narXiv preprint arXiv:2407.14177 , 2024b.\nChen, Z., Wang, W., Cao, Y ., Liu, Y ., Gao, Z., Cui, E.,\nZhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding per-\nformance boundaries of open-source multimodal modelswith model, data, and test-time scaling. arXiv preprint\narXiv:2412.05271 , 2024c.\nChi, Z., Huang, H., Xu, H.-D., Yu, H., Yin, W., and Mao,\nX.-L. Complicated table structure recognition. arXiv\npreprint arXiv:1908.04729 , 2019.\nChng, C. K., Liu, Y ., Sun, Y ., Ng, C. C., Luo, C., Ni, Z.,\nFang, C., Zhang, S., Han, J., Ding, E., et al. Icdar2019\nrobust reading challenge on arbitrary-shaped text-rrc-art.\nInICDAR , pp. 1571–1576. IEEE, 2019.\nClark, C. and Gardner, M. Simple and effective multi-\nparagraph reading comprehension. arXiv preprint\narXiv:1710.10723 , 2017.\nDai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J.,\nRintamaki, T., Shoeybi, M., Catanzaro, B., and Ping,\nW. Nvlm: Open frontier-class multimodal llms. arXiv\npreprint arXiv:2409.11402 , 2024.\nDavis, B., Morse, B., Cohen, S., Price, B., and Tensmeyer,\nC. Deep visual template-free form parsing. In ICDAR ,\npp. 134–141. IEEE, 2019.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 , 2024.\nFu, C., Chen, P., Shen, Y ., Qin, Y ., Zhang, M., Lin, X., Yang,\nJ., Zheng, X., Li, K., Sun, X., et al. Mme: A comprehen-\nsive evaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394 , 2023.\nFu, X., Hu, Y ., Li, B., Feng, Y ., Wang, H., Lin, X., Roth, D.,\nSmith, N. A., Ma, W.-C., and Krishna, R. Blink: Multi-\nmodal large language models can see but not perceive. In\nEuropean Conference on Computer Vision , pp. 148–166.\nSpringer, 2025.\nGoyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answering.\npp. 6904–6913, 2017.\nGu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang,\nX., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.\nWukong: A 100 million large-scale chinese cross-modal\npre-training benchmark. 35:26418–26431, 2022.\nGuo, H., Qin, X., Liu, J., Han, J., Liu, J., and Ding, E.\nEaten: Entity-aware attention for single shot visual text\nextraction. In ICDAR , pp. 254–259. IEEE, 2019.\nHe, M., Liu, Y ., Yang, Z., Zhang, S., Luo, C., Gao, F.,\nZheng, Q., Wang, Y ., Zhang, X., and Jin, L. Icpr2018\ncontest on robust reading for multi-type web images. pp.\n7–12. IEEE, 2018.\n10\n--- Page 11 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nHe, X., Zhang, Y ., Mou, L., Xing, E., and Xie, P. Pathvqa:\n30000+ questions for medical visual question answering.\narXiv preprint arXiv:2003.10286 , 2020.\nHiippala, T., Alikhani, M., Haverinen, J., Kalliokoski, T.,\nLogacheva, E., Orekhova, S., Tuomainen, A., Stone, M.,\nand Bateman, J. A. Ai2d-rst: A multimodal corpus of\n1000 primary school science diagrams. Language Re-\nsources and Evaluation , 55:661–688, 2021.\nHosu, V ., Lin, H., Sziranyi, T., and Saupe, D. Koniq-10k:\nAn ecologically valid database for deep learning of blind\nimage quality assessment. 29:4041–4056, 2020.\nHu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li,\nC., Zhang, J., Jin, Q., Huang, F., et al. mplug-docowl 1.5:\nUnified structure learning for ocr-free document under-\nstanding. arXiv preprint arXiv:2403.12895 , 2024.\nHu, X., Gu, L., An, Q., Zhang, M., Liu, L., Kobayashi, K.,\nHarada, T., Summers, R., and Zhu, Y . Medical-diff-vqa: a\nlarge-scale medical dataset for difference visual question\nanswering on chest x-ray images, 2023.\nHuang, Q., Xiong, Y ., Rao, A., Wang, J., and Lin, D.\nMovienet: A holistic dataset for movie understanding.\npp. 709–727. Springer, 2020.\nHuang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu,\nS., and Jawahar, C. Icdar2019 competition on scanned\nreceipt ocr and information extraction. In ICDAR , pp.\n1516–1520. IEEE, 2019.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. pp. 6700–6709, 2019.\nJiang, D., He, X., Zeng, H., Wei, C., Ku, M., Liu, Q., and\nChen, W. Mantis: Interleaved multi-image instruction\ntuning. arXiv preprint arXiv:2405.01483 , 2024.\nJohnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L.,\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\nnostic dataset for compositional language and elementary\nvisual reasoning. pp. 2901–2910, 2017.\nKafle, K., Price, B., Cohen, S., and Kanan, C. Dvqa: Under-\nstanding data visualizations via question answering. pp.\n5648–5656, 2018.\nKahou, S. E., Michalski, V ., Atkinson, A., K ´ad´ar,´A.,\nTrischler, A., and Bengio, Y . Figureqa: An anno-\ntated figure dataset for visual reasoning. arXiv preprint\narXiv:1710.07300 , 2017.\nKantharaj, S., Leong, R. T. K., Lin, X., Masry, A., Thakkar,\nM., Hoque, E., and Joty, S. Chart-to-text: A large-\nscale benchmark for chart summarization. arXiv preprint\narXiv:2203.06486 , 2022.Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T.\nReferitgame: Referring to objects in photographs of natu-\nral scenes. pp. 787–798, 2014.\nKembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi,\nH., and Farhadi, A. A diagram is worth a dozen images.\npp. 235–251, 2016.\nKembhavi, A., Seo, M., Schwenk, D., Choi, J., Farhadi, A.,\nand Hajishirzi, H. Are you smarter than a sixth grader?\ntextbook question answering for multimodal machine\ncomprehension. pp. 4999–5007, 2017.\nKuang, J., Hua, W., Liang, D., Yang, M., Jiang, D., Ren,\nB., and Bai, X. Visual information extraction in the wild:\npractical dataset and end-to-end solution. In ICDAR , pp.\n36–53. Springer, 2023.\nLAION. Laion-gpt4v dataset. https://huggingface.\nco/datasets/laion/gpt4v-dataset , 2023.\nLau, J. J., Gayen, S., Ben Abacha, A., and Demner-\nFushman, D. A dataset of clinically generated visual\nquestions and answers about radiology images. Scientific\ndata, 5(1):1–10, 2018.\nLauren c ¸on, H., Tronchon, L., Cord, M., and Sanh, V . What\nmatters when building vision-language models? arXiv\npreprint arXiv:2405.02246 , 2024.\nLerner, P., Ferret, O., Guinaudeau, C., Le Borgne, H.,\nBesan c ¸on, R., Moreno, J. G., and Lov ´on Melgarejo, J.\nViquae, a dataset for knowledge-based visual question an-\nswering about named entities. In SIGIR , pp. 3108–3120,\n2022.\nLi, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou,\nC., Liu, W., Yang, Y ., Xiong, X., et al. Chemvlm: Explor-\ning the power of multimodal large language models in\nchemistry area. arXiv preprint arXiv:2408.07246 , 2024.\nLi, Z., Wang, X., Stengel-Eskin, E., Kortylewski, A., Ma,\nW., Van Durme, B., and Yuille, A. L. Super-clevr: A vir-\ntual benchmark to diagnose domain robustness in visual\nreasoning. pp. 14963–14973, 2023.\nLin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M.,\nand Han, S. Vila: On pre-training for visual language\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pp. 26689–\n26699, 2024.\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13 , pp. 740–\n755. Springer, 2014a.\n11\n--- Page 12 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft\ncoco: Common objects in context. pp. 740–755. Springer,\n2014b.\nLiu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y ., and Wu, X.-\nM. Slake: A semantically-labeled knowledge-enhanced\ndataset for medical visual question answering. In ISBI,\npp. 1650–1654. IEEE, 2021.\nLiu, C.-L., Yin, F., Wang, D.-H., and Wang, Q.-F. Casia\nonline and offline chinese handwriting databases. In\nICDAR , pp. 37–41. IEEE, 2011.\nLiu, F., Emerson, G., and Collier, N. Visual spatial reason-\ning. Transactions of the Association for Computational\nLinguistics , 11:635–651, 2023a.\nLiu, F., Lin, K., Li, L., Wang, J., Yacoob, Y ., and Wang, L.\nMitigating hallucination in large multi-modal models via\nrobust instruction tuning. 2023b.\nLiu, F., Wang, X., Yao, W., Chen, J., Song, K., Cho, S.,\nYacoob, Y ., and Yu, D. Mmc: Advancing multimodal\nchart understanding with large-scale instruction tuning.\narXiv preprint arXiv:2311.10774 , 2023c.\nLiu, H., Li, C., Li, Y ., Li, B., Zhang, Y ., Shen, S.,\nand Lee, Y . J. Llava-next: Improved reason-\ning, ocr, and world knowledge, January 2024a.\nURL https://llava-vl.github.io/blog/\n2024-01-30-llava-next/ .\nLiu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun-\ning. Advances in neural information processing systems ,\n36, 2024b.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\nM., Petroni, F., and Liang, P. Lost in the middle: How\nlanguage models use long contexts. Transactions of the\nAssociation for Computational Linguistics , 12:157–173,\n2024c.\nLiu, S., Zheng, K., and Chen, W. Paying more attention\nto image: A training-free method for alleviating halluci-\nnation in lvlms. In European Conference on Computer\nVision , pp. 125–140. Springer, 2025a.\nLiu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W.,\nYuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench:\nIs your multi-modal model an all-around player? In\nEuropean conference on computer vision , pp. 216–233.\nSpringer, 2025b.\nLiu, Z., Chu, T., Zang, Y ., Wei, X., Dong, X., Zhang, P.,\nLiang, Z., Xiong, Y ., Qiao, Y ., Lin, D., et al. Mmdu: A\nmulti-turn multi-image dialog understanding benchmark\nand instruction-tuning dataset for lvlms. arXiv preprint\narXiv:2406.11833 , 2024d.Liu, Z., Chu, T., Zang, Y ., Wei, X., Dong, X., Zhang, P.,\nLiang, Z., Xiong, Y ., Qiao, Y ., Lin, D., et al. Mmdu: A\nmulti-turn multi-image dialog understanding benchmark\nand instruction-tuning dataset for lvlms. arXiv preprint\narXiv:2406.11833 , 2024e.\nLu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X.,\nand Zhu, S.-C. Inter-gps: Interpretable geometry problem\nsolving with formal language and symbolic reasoning.\narXiv preprint arXiv:2105.04165 , 2021.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. 35:2507–2521, 2022a.\nLu, P., Qiu, L., Chang, K.-W., Wu, Y . N., Zhu, S.-C., Ra-\njpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt\nlearning via policy gradient for semi-structured math-\nematical reasoning. arXiv preprint arXiv:2209.14610 ,\n2022b.\nLu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi,\nH., Cheng, H., Chang, K.-W., Galley, M., and Gao,\nJ. Mathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. arXiv preprint\narXiv:2310.02255 , 2023.\nMao, H., Cheung, M., and She, J. Deepart: Learning joint\nrepresentations of visual arts. pp. 1183–1191, 2017.\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-\nvqa: A visual question answering benchmark requiring\nexternal knowledge. pp. 3195–3204, 2019.\nMarti, U.-V . and Bunke, H. The iam-database: an english\nsentence database for offline handwriting recognition. In-\nternational journal on document analysis and recognition ,\n5:39–46, 2002.\nMasry, A., Do, X. L., Tan, J. Q., Joty, S., and Hoque, E.\nChartqa: A benchmark for question answering about\ncharts with visual and logical reasoning. In ACL, pp.\n2263–2279, 2022a.\nMasry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque,\nE. Chartqa: A benchmark for question answering about\ncharts with visual and logical reasoning. arXiv preprint\narXiv:2203.10244 , 2022b.\nMasry, A., Kavehzadeh, P., Do, X. L., Hoque, E., and Joty, S.\nUnichart: A universal vision-language pretrained model\nfor chart comprehension and reasoning. arXiv preprint\narXiv:2305.14761 , 2023.\nMathew, M., Bagal, V ., Tito, R., Karatzas, D., Valveny, E.,\nand Jawahar, C. Infographicvqa. pp. 1697–1706, 2022.\n12\n--- Page 13 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nMethani, N., Ganguly, P., Khapra, M. M., and Kumar, P.\nPlotqa: Reasoning over scientific plots. pp. 1527–1536,\n2020.\nOpenGVLab. Opengvlab/internvl-chat-v1-2-sft-data,\nJan 2024. URL https://huggingface.co/\nOpenGVLab/InternVL-Chat-V1-2 .\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems , 35:27730–27744, 2022.\nPeng, Z., Wang, W., Dong, L., Hao, Y ., Huang, S., Ma,\nS., and Wei, F. Kosmos-2: Grounding multimodal\nlarge language models to the world. arXiv preprint\narXiv:2306.14824 , 2023.\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision , pp. 2641–\n2649, 2015.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\n35:25278–25294, 2022a.\nSchuhmann, C., K ¨opf, A., Vencu, R., Coombes, T.,\nand Beaumont, R. Laion coco: 600m synthetic cap-\ntions from laion2b-en. https://laion.ai/blog/\nlaion-coco/ , 2022b.\nSeo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., and Mal-\ncolm, C. Solving geometry problems: Combining text\nand diagram interpretation. pp. 1466–1476, 2015.\nShah, S., Mishra, A., Yadati, N., and Talukdar, P. P. Kvqa:\nKnowledge-aware visual question answering. volume 33,\npp. 8876–8884, 2019.\nShao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li,\nJ., and Sun, J. Objects365: A large-scale, high-quality\ndataset for object detection. pp. 8430–8439, 2019.\nSingh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. pp. 8317–8326, 2019a.\nSingh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\npp. 8317–8326, 2019b.Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., and\nHassner, T. Textocr: Towards large-scale end-to-end\nreasoning for arbitrary-shaped scene text. pp. 8802–8812,\n2021.\nSong, D., Chen, S., Chen, G. H., Yu, F., Wan, X., and Wang,\nB. Milebench: Benchmarking mllms in long context.\narXiv preprint arXiv:2404.18532 , 2024.\nSu, J. Transformer roadmap: 2. rotary position embed-\nding, Mar 2021. URL https://spaces.ac.cn/\narchives/8265 .\nSun, Y ., Ni, Z., Chng, C.-K., Liu, Y ., Luo, C., Ng, C. C.,\nHan, J., Ding, E., Liu, J., Karatzas, D., et al. Icdar 2019\ncompetition on large-scale street view text with partial\nlabeling-rrc-lsvt. In ICDAR , pp. 1557–1562. IEEE, 2019.\nTeam, O. Internvl2: Better than the best—expanding\nperformance boundaries of open-source multimodal\nmodels with the progressive scaling strategy, 2024.\nURL https://internvl.github.io/blog/\n2024-07-02-InternVL-2.0/ .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288 ,\n2023.\nVeit, A., Matera, T., Neumann, L., Matas, J., and Belongie,\nS. Coco-text: Dataset and benchmark for text detec-\ntion and recognition in natural images. arXiv preprint\narXiv:1601.07140 , 2016.\nWang, J., Zhang, P., Chu, T., Cao, Y ., Zhou, Y ., Wu, T.,\nWang, B., He, C., and Lin, D. V3det: Vast vocabulary\nvisual detection dataset. pp. 19844–19854, 2023a.\nWang, K., Pan, J., Shi, W., Lu, Z., Zhan, M., and Li, H. Mea-\nsuring multimodal mathematical reasoning with math-\nvision dataset. arXiv preprint arXiv:2402.14804 , 2024a.\nWang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,\nK., Liu, X., Wang, J., Ge, W., Fan, Y ., Dang, K., Du,\nM., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and\nLin, J. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution. arXiv preprint\narXiv:2409.12191 , 2024b.\nWang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing,\nL., Chen, Z., Li, H., Zhu, X., Cao, Z., et al. The\nall-seeing project: Towards panoptic visual recognition\nand understanding of the open world. arXiv preprint\narXiv:2308.01907 , 2023b.\nWang, W., Zhang, S., Ren, Y ., Duan, Y ., Li, T., Liu, S.,\nHu, M., Chen, Z., Zhang, K., Lu, L., et al. Needle in a\n13\n--- Page 14 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nmultimodal haystack. arXiv preprint arXiv:2406.07230 ,\n2024c.\nWang, X., Liu, Y ., Shen, C., Ng, C. C., Luo, C., Jin, L.,\nChan, C. S., Hengel, A. v. d., and Wang, L. On the\ngeneral value of evidence, and bilingual scene-text visual\nquestion answering. pp. 10126–10135, 2020.\nWu, C. Pmc-casereport. https://huggingface.\nco/datasets/chaoyi-wu/PMC-CaseReport ,\n2023.\nWu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H.,\nMa, Y ., Wu, C., Wang, B., et al. Deepseek-vl2: Mixture-\nof-experts vision-language models for advanced multi-\nmodal understanding. arXiv preprint arXiv:2412.10302 ,\n2024.\nXiao, G., Tian, Y ., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453 , 2023.\nYao, Y ., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T.,\nLi, H., Zhao, W., He, Z., et al. Minicpm-v: A gpt-4v level\nmllm on your phone. arXiv preprint arXiv:2408.01800 ,\n2024.\nYe, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., Zhang,\nJ., Huang, F., and Zhou, J. mplug-owl3: Towards long\nimage-sequence understanding in multi-modal large lan-\nguage models. arXiv preprint arXiv:2408.04840 , 2024.\nYing, K., Meng, F., Wang, J., Li, Z., Lin, H., Yang, Y .,\nZhang, H., Zhang, W., Lin, Y ., Liu, S., et al. Mmt-\nbench: A comprehensive multimodal benchmark for eval-\nuating large vision-language models towards multitask\nagi.arXiv preprint arXiv:2404.16006 , 2024.\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y ., Kwok,\nJ. T., Li, Z., Weller, A., and Liu, W. Metamath: Boot-\nstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284 , 2023.\nYuan, T., Zhu, Z., Xu, K., Li, C., Mu, T., and Hu, S. A large\nchinese text dataset in the wild. 34(3):509–521, 2019.\nYuan, Y ., Liu, X., Dikubab, W., Liu, H., Ji, Z., Wu, Z.,\nand Bai, X. Syntax-aware network for handwritten\nmathematical expression recognition. arXiv preprint\narXiv:2203.01601 , 2022.\nZhang, R., Zhou, Y ., Jiang, Q., Song, Q., Li, N., Zhou, K.,\nWang, L., Wang, D., Liao, M., Yang, M., et al. Icdar\n2019 robust reading challenge on reading chinese text on\nsignboard. In ICDAR , pp. 1577–1581. IEEE, 2019.\nZhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y ., Wang,\nY ., and Xie, W. Pmc-vqa: Visual instruction tuningfor medical visual question answering. arXiv preprint\narXiv:2305.10415 , 2023a.\nZhang, Y ., Zhang, R., Gu, J., Zhou, Y ., Lipka, N., Yang,\nD., and Sun, T. Llavar: Enhanced visual instruction\ntuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107 , 2023b.\nZheng, X., Burdick, D., Popa, L., Zhong, X., and Wang, N.\nX. R. Global table extractor (gte): A framework for joint\ntable identification and cell structure recognition using\nvisual context. pp. 697–706, 2021.\nZhong, Y ., Liang, L., Zharkov, I., and Neumann, U. Mmvp:\nMotion-matrix-based video prediction. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision , pp. 4273–4283, 2023.\n14\n--- Page 15 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nA. Theoretical Derivation of Remote Decay\nDefinition A.1 (RoPE-induced Inner Product) .Given a query vector q∈Cdand a key vector k∈Cd, their inner product\nunder Rotary Position Embedding (RoPE) is defined as:\n(Rmq)⊤(Rnk)≜Re\nd/2−1X\ni=0q[2i:2i+1]k∗\n[2i:2i+1]ei(m−n)θi\n\nwhere q[2i:2i+1]denotes the i-th 2D subvector of q,θiis the preset rotational frequency, and∗represents complex conjugation.\nBuilding on Definition A.1, we establish the following core lemma:\nLemma A.2 (Abel Transform Representation) .Lethi≜q[2i:2i+1]k∗\n[2i:2i+1]andSj≜Pj−1\ni=0ei(m−n)θi, with boundary\nconditions hd/2= 0andS0= 0. Then:\nd/2−1X\ni=0hiei(m−n)θi=d/2−1X\ni=0hi(Si+1−Si) =−d/2−1X\ni=0Si+1(hi+1−hi)\nProof. By Abel summation by parts:\nd/2−1X\ni=0hi(Si+1−Si) =d/2−1X\ni=0hiSi+1−d/2−1X\ni=0hiSi\n=d/2X\ni=1hi−1Si−d/2−1X\ni=0hiSi(index shift )\n=−d/2−1X\ni=0Si(hi−hi−1) +hd/2−1Sd/2−h−1S0\n=−d/2−1X\ni=0Si+1(hi+1−hi) (using boundary conditions )\nTheorem A.3 (Decay Rate Bound) .Under the assumptions of Lemma A.2, the absolute value of the inner product satisfies:\n\f\f\f\f\f\fd/2−1X\ni=0hiei(m−n)θi\f\f\f\f\f\f≤\u0012\nmax\n0≤i≤d/2−1|hi+1−hi|\u0013d/2−1X\ni=0|Si+1|\nProof. From Lemma A.2 and the triangle inequality:\n\f\f\f\f\f\fd/2−1X\ni=0hiei(m−n)θi\f\f\f\f\f\f=\f\f\f\f\f\fd/2−1X\ni=0Si+1(hi+1−hi)\f\f\f\f\f\f\n≤d/2−1X\ni=0|Si+1| · |hi+1−hi|\n≤\u0010\nmax\ni|hi+1−hi|\u0011d/2−1X\ni=0|Si+1|\n15\n--- Page 16 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nB. Training Details\nThe hyperparameters used for pretraining and finetuning across the three architectures are listed in Table 5. We observed\nthat the 2B model, with its smaller hidden dimension, requires less extensive training, making Phase 2 optional. Therefore,\nin the ablation experiments in the main text, we only used Pretraining Phase 1 for the 2B model, while the 8B model utilized\nboth pretraining phases.\nTable 5: Hyperparameters for Training and Inference\nParameter Pretraining Phase 1 Pretraining Phase 2 Finetuning\nMax sequence length 8192 8192 8192\nMax tile/image 12 12 12\nOptimizer AdamW AdamW AdamW\nLearning rate 1×10−41×10−44×10−5\nWeight decay 0.01 0.01 0.01\nOptimizer momentum β1, β2= 0.9,0.999 β1, β2= 0.9,0.999 β1, β2= 0.9,0.999\nLearning rate schedule Constant with warmup Constant with warmup Cosine decay\nWarmup ratio 0.03 0.03 0.03\nTraining steps 2000 2000 9000\nBatch size 1024 1024 1024\nNumber of mixin layers 4 4 4\nTrainable weights LVLM-S: MLP All\nLVLM-X: Mixin layers + MLP\nCoMemo: Mixin layers + MLP\n(Freeze gate in Phase 2)\nC. Detailed Expereiment Results\nWe provide the detailed ablation study results in Table 9.\nTable 9: Detailed results on albation study.\nModelCaption Long-Generation Multi-Image Long-Context Math General VQA OCR-Related COCO\nFlickr\nNo-Caps\nLLaV ABen.\nMMDU\nBLINK\nMantis\nMMT\nMM-NIAH\nMileBench\nMathVista\nMathVision\nMMBench\nMME\nMMVP\nAI2D\nChartQA\nTextVQA\nVariant 1 79.1 65.4 60.0 62.9 28.7 38.2 48.3 50.2 27.0 52.1 48 16.5 73.1 1869 31.3 74.3 75.6 74.2\nVariant 2 74.1 63.5 64.0 64.0 38.8 42.7 48.8 51.1 30.2 53.8 48.1 15.2 72.8 1899 40 73.4 72.5 72.9\nVariant 3 98.0 74.4 75.7 65.2 34.1 37 51.6 49.4 31.9 54.2 50.1 14.3 73.1 1834 35.3 75.9 76.2 74.6\nVariant 4 90.2 67.2 81.9 64.2 39.6 42.7 52.1 50.6 30.6 55.6 49.9 19.4 74.0 1826 36.7 74.2 76.1 74.2\nVariant 5 98.6 78.5 78.8 66.9 38.7 43.5 50.6 51.3 34.2 54.6 50.0 17.0 74.2 1904 36 74.2 73.6 72.6\nVariant 6 90.1 73.0 69.9 73.7 48.2 49.2 65.0 57.9 29.8 60.8 58.3 19.1 79.1 2210 45.3 83.5 84.6 78.7\nVariant 7 102.0 82.2 92.2 76.2 46.5 49.7 65.4 58.0 38.2 63.4 59.7 18.1 79.8 2182 45.3 83.5 77.2 76.6\nVariant 8 92.0 61.7 85.6 42 30.2 40.5 44.2 47.8 15.6 41.9 39.9 17.7 63.8 1674 28 63.2 63.8 58.6\nVariant 9 100.5 71.5 96.6 44.8 32.0 40.6 48.8 49.2 17.5 45.6 44.2 13.2 67.6 1759 31.3 63.3 64.7 62.6\nD. Dataset Details\nThe data used in the pre-training stage are listed in Table 6. And datasets used for instruction tuning are listed in Table 7.\n16\n--- Page 17 ---\nCoMemo: LVLMs Need Image Context with Image Memory\nTable 6: Summary of datasets used in the pretraining stage.\ntask dataset\nShort Caption Laion (en&zh) (Schuhmann et al., 2022a), COYO (Byeon et al., 2022), COCO (Lin et al., 2014b)\nOCR Wukong-OCR (Gu et al., 2022), LaionCOCO-OCR (Schuhmann et al., 2022b)\nDetection GRIT (Peng et al., 2023), Objects365 (Shao et al., 2019)\nConversation All-Seeing (en&zh) (Wang et al., 2023b)\nImage-text instruction data (see Table 7)\nTable 7: Summary of datasets used in the instruction tuning stage.\ntask dataset\nGeneral QA VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), OKVQA (Marino et al., 2019), VSR (Liu et al., 2023a)\nAI2D (Kembhavi et al., 2016), ScienceQA (Lu et al., 2022a), Chemistry Data (Li et al., 2024)ScienceTQA (Kembhavi et al., 2017)\nPMC-VQA (Zhang et al., 2023a), VQA-RAD (Lau et al., 2018), VQA-Med (Ben Abacha et al., 2019)\nMedical-Diff-VQA (Hu et al., 2023), PathVQA (He et al., 2020), Medical\nSLAKE (Liu et al., 2021), PMC-CaseReport (Wu, 2023)\nChartQA (Masry et al., 2022a), LRV-Instruction (Liu et al., 2023b), PlotQA (Methani et al., 2020)\nUnichart (Masry et al., 2023), MMC-Inst (Liu et al., 2023c), DVQA (Kafle et al., 2018)\nTableMWP (Lu et al., 2022b), FigureQA (Kahou et al., 2017), MapQA (Chang et al., 2022)Chart\nSciTSR (Chi et al., 2019), Fintabnet (Zheng et al., 2021)\nCLEVR (Johnson et al., 2017), MetaMath (Yu et al., 2023), GeoQA+ (Cao & Xiao, 2022)\nGeometry3k (Lu et al., 2021), GeoS (Seo et al., 2015), Unigeo (Chen et al., 2022) Mathematics\nSuper-CLEVR (Li et al., 2023), MathQA (Amini et al., 2019)\nArt500k (Mao et al., 2017), MovieNet (Huang et al., 2020), KonIQ-10k (Hosu et al., 2020)KnowledgeKVQA (Shah et al., 2019), ViQuAE (Lerner et al., 2022)\nInfoVQA (Mathew et al., 2022), TextVQA (Singh et al., 2019a), ArT (Chng et al., 2019)\nCASIA (Liu et al., 2011), Chart-to-text (Kantharaj et al., 2022), COCO-text (Veit et al., 2016)\nCTW (Yuan et al., 2019), EATEN (Guo et al., 2019), ICDAR2019-LSVT (Sun et al., 2019)\nICPR MTWI (He et al., 2018), NAF (Davis et al., 2019), ReCTS (Zhang et al., 2019)\nTextOCR (Singh et al., 2021), LLaV AR (Zhang et al., 2023b), HME-100k (Yuan et al., 2022)\nPOIE (Kuang et al., 2023), SROIE (Huang et al., 2019), ST-VQA (Biten et al., 2019)OCR\nEST-VQA (Wang et al., 2020), IAM (Marti & Bunke, 2002)\nDocument DocVQA (Clark & Gardner, 2017), DocReason25k (Hu et al., 2024)\nRefCOCO (Kazemzadeh et al., 2014), RefCOCO+ (Kazemzadeh et al., 2014), RefCOCOg (Kazemzadeh et al., 2014)GroundingRD-BoxCoT (Chen et al., 2023)\nALLaV A (Chen et al., 2024a), LAION-GPT4V (LAION, 2023)ConversationMMDU (Liu et al., 2024d), TextOCR-GPT4V (Carter, 2024)\nDetection Objects365 (Shao et al., 2019), V3Det (Wang et al., 2023a)\n17",
  "text_length": 68555
}